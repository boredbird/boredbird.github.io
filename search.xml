<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F01%2Fwin10%E7%9A%84%E5%BC%80%E5%A7%8B%E8%8F%9C%E5%8D%95%E7%82%B9%E5%87%BB%E6%B2%A1%E6%9C%89%E5%8F%8D%E5%BA%94%2F</url>
    <content type="text"><![CDATA[1、在键盘上按下win+R键，或在开始菜单图标上点击右键选择运行;2、输入powershell，按下“确定”运行;3、在窗口里输入或复制粘贴以下命令，注意只有一行：Get-AppxPackage | % { Add-AppxPackage -DisableDevelopmentMode -Register “$($_.InstallLocation)\AppxManifest.xml” -verbose }4、点击enter键，等待修复命令运行完成，重启电脑，完成之后BUG就被修复了 via]]></content>
  </entry>
  <entry>
    <title><![CDATA[py2转py3遇到的问题]]></title>
    <url>%2F2018%2F03%2F01%2Fpy2%E8%BD%ACpy3%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[python2与python3在处理异常的区别： 1.所以异常都从 BaseException继承，并删除了StardardError 2.去除了异常类的序列行为和.message属性 3.用 raise Exception(args)代替 raise Exception, args语法 4.捕获异常的语法改变，引入了as关键字来标识异常实例，在Py2中：1234567&gt;&gt;&gt; try:... raise NotImplementedError('Error')... except NotImplementedError, error:... print error.message...Error 在Py3中：12345&gt;&gt;&gt; try: raise NotImplementedError('Error') except NotImplementedError as error: #注意这个 as print(str(error))Error 5.异常链，因为__context__在3.0a1版本中没有实现 tuple parameter unpacking is not supported in Pyhton 3As tuple parameters are used by lambdas because of the single expression limitation, they must also be supported. This is done by having the expected sequence argument bound to a single parameter and then indexing on that parameter:1lambda (x, y): x + y will be translated into:1lambda x_y: x_y[0] + x_y[1] py2与py3在map返回类型上的区别py2：123456789Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32In[2]: list_a = [2,3,4,5]In[3]: list_b = [1,2,3,4]In[4]: map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b))Out[4]:[1.9999999979999998, 1.49999999925, 1.3333333328888888, 1.2499999996875]In[5]: type(map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b)))Out[5]:list py3:123456789Python 3.5.2 |Anaconda custom (64-bit)| (default, Jul 5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]In[2]: list_a = [2,3,4,5]In[3]: list_b = [1,2,3,4]In[4]: map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b))Out[4]:&lt;map at 0x24dc066f160&gt;In[5]: type(map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b)))Out[5]:map 在py3中需要用list()转换一下。 windows下py2与py3共存切换问题配置环境变量 修改文件名称 效果如下]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git切换分支报错Permission denied]]></title>
    <url>%2F2018%2F03%2F01%2Fgit%E5%88%87%E6%8D%A2%E5%88%86%E6%94%AF%E6%8A%A5%E9%94%99Permission%20denied%2F</url>
    <content type="text"><![CDATA[git切换分支报错，如下：123456789101112131415161718192021222324maomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (master)$ git checkout sourceerror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniedmaomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (master)$ git checkout sourcewarning: unable to rmdir .deploy_git: Directory not emptyChecking out files: 100% (10345/10345), done.M themes/nextSwitched to branch 'source'Your branch is up-to-date with 'origin/source'.maomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (source)$ 最后发现是文件占用问题，关掉后台的编辑器，顺利切换分支。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ERROR Local hexo not found]]></title>
    <url>%2F2018%2F03%2F01%2FERROR%20Local%20hexo%20not%20found%2F</url>
    <content type="text"><![CDATA[via]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用atom插件打造一套python的IDE]]></title>
    <url>%2F2018%2F02%2F12%2F%E5%88%A9%E7%94%A8atom%E6%8F%92%E4%BB%B6%E6%89%93%E9%80%A0%E4%B8%80%E5%A5%97python%E7%9A%84IDE%2F</url>
    <content type="text"><![CDATA[利用atom插件打造一套python的IDE使用快捷键：Shift + Ctrl + M快速预览markdown 插件 说明 Atom Runner 运行python脚本 autocomplete-python python代码自动补全 hydrogen 在写脚本的同时运行ipython notebook linter+linter-pylama 代码错误提示 platformio-ide-terminal Atom内置终端 project-manager 快速切换你的多个项目文件夹 python-autopep8 自动规范化python代码(使用pep8准则) isort+python-isort 自动规范化代码中的import python-tools+hyperclick ctrl+单击转到源代码 todo-show 管理你代码中的TODO(在你没有完成的代码最后加上TODO即可) python-debugger 调试python代码 atom-python-test 测试python代码 atom-python-virtualenv python的虚拟环境 以上是常用的python插件, 足够满足你的日常使用了]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Tips]]></title>
    <url>%2F2018%2F02%2F12%2FVisual%20Studio%20Tips%2F</url>
    <content type="text"><![CDATA[在 Visual Studio 中开发代码而无需创建项目或解决方案在 Visual Studio 2017 中，你可以在 Visual Studio 中打开几乎任何类型的基于目录的项目的代码，而无需创建解决方案或者项目文件。 这意味着（例如，在 Git 上找到一个代码项目时）可以克隆该项目，然后在 Visual Studio 中直接打开并开始开发，而无需创建解决方案或项目。 在任意位置打开代码可以通过以下方式，在 Visual Studio 中打开代码： 在 Visual Studio 菜单栏上，依次选择“文件”、“打开”、“文件夹”，然后浏览到代码位置。 在包含该代码的文件夹的上下文（右键单击）菜单上，选择“在 Visual Studio 中打开”命令。 选择 Visual Studio“开始”页上的“打开文件夹”链接。 打开从 GitHub 存储库中克隆的代码。 调试代码不通过项目或解决方案即可在 Visual Studio 中调试代码。 对某些语言进行调试时，可能需要在代码项目中指定一个有效的启动文件，例如脚本、可执行文件或项目。 调试代码时，Visual Studio 会首先运行此指定代码。 工具栏上“开始”按钮旁的下拉列表框中列出了 Visual Studio 检测到的所有启动项，以及你在文件夹中专门选择的项。 Visual Studio 会自动识别项目，但是需要你将脚本（例如 Python 和 JavaScript）显式选择为启动项之后，项目才会出现在列表中。 此外，某些启动项（例如 MSBuild 和 CMake）可能有多个生成配置，这些生成配置会显示在运行按钮的下拉列表中。 调试可执行文件 在 Visual Studio 菜单上，选择“调试”。 在下拉菜单上，选择该项目，或者选择想要在解决方案资源管理器中显示为启动项的项目或文件。 选择 F5 键开始调试。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cannot start process the working directory '' does not exist]]></title>
    <url>%2F2017%2F12%2F26%2Fcannot%20start%20process%20the%20working%20directory%20''%20does%20not%20exist%2F</url>
    <content type="text"><![CDATA[问题描述突然有一天启动pycharm时，Python Console报出错误：cannot start process the working directory ‘’ does not exist 如图所示： 解决方法：删除项目文件夹下面的两个文件: 再次重启pycharm打开项目文件，则恢复正常。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Statistical Inference and Estimation]]></title>
    <url>%2F2017%2F12%2F04%2FStatistical%20Inference%20and%20Estimation%2F</url>
    <content type="text"><![CDATA[Statistical InferenceRecall, a statistical inference aims at learning characteristics of the population from a sample; the population characteristics are parameters and sample characteristics are statistics. A statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. Estimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Confidence Intervals = gives a range of values for the parameter Interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. Hypothesis tests = tests for a specific value(s) of the parameter. In order to perform these inferential tasks, i.e., make inference about the unknown population parameter from the sample statistic, we need to know the likely values of the sample statistic. What would happen if we do sampling many times? CLT,Central Limit TheoremSampling distribution of the sample mean: For categorical data, the CLT holds for the sampling distribution of the sample proportion. So, what is the 95% confidence interval? Based on the CLT, the 95% CI is Point EstimationEstimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Model &amp; EstimationA statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. In models, the focus is on estimating the model parameters. The basic inference tools (e.g., point estimation, hypothesis testing, and confidence intervals) will be applied to the these parameters. When discussing models, we will keep in mind the following parts: ObjectiveState what the objective is for this model. For instance, “Estimate the probability that a characteristic is present given the value of the explanatory values are … “ Model Structure Confidence IntervalsGeneral form of a confidence interval (CI)A confidence interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. The general form: estimate ± critical value × std.dev of the estimate estimate ± margin of errorFor example: sample mean ± critical value × estimated standard error The CIs differ based on: The parameter of interest, e.g., population mean, population proportion, difference in population’s means, etc… Design of the sample: SRS, stratified, experiments Confidence level or a confidence coefficient, (1 - α)100%, e.g., 95%, 99%, 90%, 80%, corresponding, respectively, to α values of 0.05, 0.01, 0.1, 0.2, etc… Interpretation of a Confidence IntervalIn most general terms, for a 95% CI, we say “we are 95% confident that the true population parameter is between the lower and upper calculated values”. A 95% CI for a population parameter DOES NOT mean that the interval has a probability of 0.95 that the true value of the parameter falls in the interval. The CI either contains the parameter or it does not contain it. The probability is associated with the process that generated the interval. And if we repeat this process many times, 95% of all intervals should in fact contain the true value of the parameter. What does a 99% CI say? Would you choose a 99% or 95% CI, and why? TradeoffsWe want confidence coefficient to be closer to 1. We want the sample size to be as small as possible (but not too small). This is a practical issue. We want the CI to be as narrow as possible As we increase the sample estimate, the CI …? As we decrease st. dev, the CI …? As we decrease the confidence level, (1-α), the CI …? As we increase sample size….? z-Tests &amp; Intervals More About Confidence IntervalsSimplified Expression for a 95% Confidence Interval Generalizing the 95% Confidence Interval Height Example Assume that the s is known and is equal to 3. We want to estimate the unknown true height of our population. Point sample estimate, can be the sample mean, 66.463. What is the distribution of the sample mean? What is the 95% confidence interval? What does it mean? Confidence Intervals for Proportions in NewspapersAs found in CNN in June, 2006: The stated Margin of error: +/- 3% Therefore, this would be the Confidence interval: 62%+/- 3%. We can be really confident that between 59% and 65% of all U.S. adults disapprove of how President Bush is handling the situation in Iraq. Hypothesis TestingBasic approach to hypothesis testing State a model describing the relationship between the explanatory variables and the outcome variable(s) in the population and the nature of the variability. State all of your assumptions. Specify the null and alternative hypotheses in terms of the parameters of the model. Invent a test statistic that will tend to be different under the null and alternative hypotheses. Using the assumptions of step 1, find the theoretical sampling distribution of the statistic under the null hypothesis of step 2. Ideally the form of the sampling distribution should be one of the “standard distributions”(e.g. normal, t, binomial..) Calculate a p-value, as the area under the sampling distribution more extreme than your statistic. Depends on the form of the alternative hypothesis. Choose your acceptable type 1 error rate (alpha) and apply the decision rule: reject the null hypothesis if the p-value is less than alpha, otherwise do not reject. Making the DecisionIt is either likely or unlikely that we would collect the evidence we did given the initial assumption. (Note: “likely” or “unlikely” is measured by calculating a probability!) If it is likely, then we “do not reject” our initial assumption. There is not enough evidence to do otherwise. If it is unlikely, then: either our initial assumption is correct and we experienced an unusual event or,our initial assumption is incorrectIn statistics, if it is unlikely, we decide to “reject” our initial assumption. Example: Criminal Trial AnalogyFirst, state 2 hypotheses, the null hypothesis (“H0”) and the alternative hypothesis (“HA”) H0: Defendant is not guilty. HA: Defendant is guilty. Usually the H0 is a statement of “no effect”, or “no change”, or “chance only” about a population parameter. While the HA , depending on the situation, is that there is a difference, trend, effect, or a relationship with respect to a population parameter. It can one-sided and two-sided. In two-sided we only care there is a difference, but not the direction of it. In one-sided we care about a particular direction of the relationship. We want to know if the value is strictly larger or smaller. Then, collect evidence, such as finger prints, blood spots, hair samples, carpet fibers, shoe prints, ransom notes, handwriting samples, etc. (In statistics, the data are the evidence.) Next, you make your initial assumption. Defendant is innocent until proven guilty. In statistics, we always assume the null hypothesis is true. Then, make a decision based on the available evidence. If there is sufficient evidence (“beyond a reasonable doubt”), reject the null hypothesis. (Behave as if defendant is guilty.) If there is not enough evidence, do not reject the null hypothesis. (Behave as if defendant is not guilty.) If the observed outcome, e.g., a sample statistic, is surprising under the assumption that the null hypothesis is true, but more probable if the alternative is true, then this outcome is evidence against H0 and in favor of HA. An observed effect so large that it would rarely occur by chance is called statistically significant (i.e., not likely to happen by chance). Using the p-value to make the decisionThe p-value represents how likely we would be to observe such an extreme sample if the null hypothesis were true. The p-value is a probability computed assuming the null hypothesis is true, that the test statistic would take a value as extreme or more extreme than that actually observed. Since it’s a probability, it is a number between 0 and 1. The closer the number is to 0 means the event is “unlikely.” So if p-value is “small,” (typically, less than 0.05), we can then reject the null hypothesis. Significance level and p-valueSignificance level, α, is a decisive value for p-value. In this context, significant does not mean “important”, but it means “not likely to happened just by chance”. α is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If α = 1 we always reject the null, if α = 0 we never reject the null hypothesis. In articles, journals, etc… you may read: “The results were significant (p&lt;0.05).” So if p=0.03, it’s significant at the level of α = 0.05 but not at the level of α = 0.01. If we reject the H0 at the level of α = 0.05 (which corresponds to 95% CI), we are saying that if H0 is true, the observed phenomenon would happen no more than 5% of the time (that is 1 in 20). If we choose to compare the p-value to α = 0.01, we are insisting on a stronger evidence! Errors in Hypothesis Testing PowerThe power of a statistical test is its probability of rejecting the null hypothesis if the null hypothesis is false. That is, power is the ability to correctly reject H0 and detect a significant effect. In other words, power is one minus the type II error risk. Which error is worse? Type I = you are innocent, yet accused of cheating on the test. Type II = you cheated on the test, but you are found innocent. This depends on the context of the problem too. But in most cases scientists are trying to be “conservative”; it’s worse to make a spurious discovery than to fail to make a good one. Our goal it to increase the power of the test that is to minimize the length of the CI. We need to keep in mind: the effect of the sample size, the correctness of the underlying assumptions about the population, statistical vs. practical significance, etc… sometimes the p-value is too low because of the large sample size, and we may have statistical significance but not really practical significance! That’s why most statisticians are much more comfortable with using CI than tests. There is a need for a further generalization. What if we can’t assume that σ is known? In this case we would use s (the sample standard deviation) to estimate σ. If the sample is very large, we can treat σ as known by assuming that σ = s. According to the law of large numbers, this is not too bad a thing to do. But if the sample is small, the fact that we have to estimate both the standard deviation and the mean adds extra uncertainty to our inference. In practice this means that we need a larger multiplier for the standard error. We need one-sample t-test. One sample t-test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Sampling distribution</tag>
        <tag>Central Limit Theorem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One-Way Tables and Goodness-of-Fit Test]]></title>
    <url>%2F2017%2F12%2F04%2FOne-Way%20Tables%20and%20Goodness-of-Fit%20Test%2F</url>
    <content type="text"><![CDATA[Statistical InferenceRecall, a statistical inference aims at learning characteristics of the population from a sample; the population characteristics are parameters and sample characteristics are statistics. A statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. Estimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Confidence Intervals = gives a range of values for the parameter Interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. Hypothesis tests = tests for a specific value(s) of the parameter. In order to perform these inferential tasks, i.e., make inference about the unknown population parameter from the sample statistic, we need to know the likely values of the sample statistic. What would happen if we do sampling many times? CLT,Central Limit TheoremSampling distribution of the sample mean: For categorical data, the CLT holds for the sampling distribution of the sample proportion. So, what is the 95% confidence interval? Based on the CLT, the 95% CI is Point EstimationEstimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Model &amp; EstimationA statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. In models, the focus is on estimating the model parameters. The basic inference tools (e.g., point estimation, hypothesis testing, and confidence intervals) will be applied to the these parameters. When discussing models, we will keep in mind the following parts: ObjectiveState what the objective is for this model. For instance, “Estimate the probability that a characteristic is present given the value of the explanatory values are … “ Model Structure Confidence IntervalsGeneral form of a confidence interval (CI)A confidence interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. The general form: estimate ± critical value × std.dev of the estimate estimate ± margin of errorFor example: sample mean ± critical value × estimated standard error The CIs differ based on: The parameter of interest, e.g., population mean, population proportion, difference in population’s means, etc… Design of the sample: SRS, stratified, experiments Confidence level or a confidence coefficient, (1 - α)100%, e.g., 95%, 99%, 90%, 80%, corresponding, respectively, to α values of 0.05, 0.01, 0.1, 0.2, etc… Interpretation of a Confidence IntervalIn most general terms, for a 95% CI, we say “we are 95% confident that the true population parameter is between the lower and upper calculated values”. A 95% CI for a population parameter DOES NOT mean that the interval has a probability of 0.95 that the true value of the parameter falls in the interval. The CI either contains the parameter or it does not contain it. The probability is associated with the process that generated the interval. And if we repeat this process many times, 95% of all intervals should in fact contain the true value of the parameter. What does a 99% CI say? Would you choose a 99% or 95% CI, and why? TradeoffsWe want confidence coefficient to be closer to 1. We want the sample size to be as small as possible (but not too small). This is a practical issue. We want the CI to be as narrow as possible As we increase the sample estimate, the CI …? As we decrease st. dev, the CI …? As we decrease the confidence level, (1-α), the CI …? As we increase sample size….? z-Tests &amp; Intervals More About Confidence IntervalsSimplified Expression for a 95% Confidence Interval Generalizing the 95% Confidence Interval Height Example Assume that the s is known and is equal to 3. We want to estimate the unknown true height of our population. Point sample estimate, can be the sample mean, 66.463. What is the distribution of the sample mean? What is the 95% confidence interval? What does it mean? Confidence Intervals for Proportions in NewspapersAs found in CNN in June, 2006: The stated Margin of error: +/- 3% Therefore, this would be the Confidence interval: 62%+/- 3%. We can be really confident that between 59% and 65% of all U.S. adults disapprove of how President Bush is handling the situation in Iraq. Hypothesis TestingBasic approach to hypothesis testing State a model describing the relationship between the explanatory variables and the outcome variable(s) in the population and the nature of the variability. State all of your assumptions. Specify the null and alternative hypotheses in terms of the parameters of the model. Invent a test statistic that will tend to be different under the null and alternative hypotheses. Using the assumptions of step 1, find the theoretical sampling distribution of the statistic under the null hypothesis of step 2. Ideally the form of the sampling distribution should be one of the “standard distributions”(e.g. normal, t, binomial..) Calculate a p-value, as the area under the sampling distribution more extreme than your statistic. Depends on the form of the alternative hypothesis. Choose your acceptable type 1 error rate (alpha) and apply the decision rule: reject the null hypothesis if the p-value is less than alpha, otherwise do not reject. Making the DecisionIt is either likely or unlikely that we would collect the evidence we did given the initial assumption. (Note: “likely” or “unlikely” is measured by calculating a probability!) If it is likely, then we “do not reject” our initial assumption. There is not enough evidence to do otherwise. If it is unlikely, then: either our initial assumption is correct and we experienced an unusual event or,our initial assumption is incorrectIn statistics, if it is unlikely, we decide to “reject” our initial assumption. Example: Criminal Trial AnalogyFirst, state 2 hypotheses, the null hypothesis (“H0”) and the alternative hypothesis (“HA”) H0: Defendant is not guilty. HA: Defendant is guilty. Usually the H0 is a statement of “no effect”, or “no change”, or “chance only” about a population parameter. While the HA , depending on the situation, is that there is a difference, trend, effect, or a relationship with respect to a population parameter. It can one-sided and two-sided. In two-sided we only care there is a difference, but not the direction of it. In one-sided we care about a particular direction of the relationship. We want to know if the value is strictly larger or smaller. Then, collect evidence, such as finger prints, blood spots, hair samples, carpet fibers, shoe prints, ransom notes, handwriting samples, etc. (In statistics, the data are the evidence.) Next, you make your initial assumption. Defendant is innocent until proven guilty. In statistics, we always assume the null hypothesis is true. Then, make a decision based on the available evidence. If there is sufficient evidence (“beyond a reasonable doubt”), reject the null hypothesis. (Behave as if defendant is guilty.) If there is not enough evidence, do not reject the null hypothesis. (Behave as if defendant is not guilty.) If the observed outcome, e.g., a sample statistic, is surprising under the assumption that the null hypothesis is true, but more probable if the alternative is true, then this outcome is evidence against H0 and in favor of HA. An observed effect so large that it would rarely occur by chance is called statistically significant (i.e., not likely to happen by chance). Using the p-value to make the decisionThe p-value represents how likely we would be to observe such an extreme sample if the null hypothesis were true. The p-value is a probability computed assuming the null hypothesis is true, that the test statistic would take a value as extreme or more extreme than that actually observed. Since it’s a probability, it is a number between 0 and 1. The closer the number is to 0 means the event is “unlikely.” So if p-value is “small,” (typically, less than 0.05), we can then reject the null hypothesis. Significance level and p-valueSignificance level, α, is a decisive value for p-value. In this context, significant does not mean “important”, but it means “not likely to happened just by chance”. α is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If α = 1 we always reject the null, if α = 0 we never reject the null hypothesis. In articles, journals, etc… you may read: “The results were significant (p&lt;0.05).” So if p=0.03, it’s significant at the level of α = 0.05 but not at the level of α = 0.01. If we reject the H0 at the level of α = 0.05 (which corresponds to 95% CI), we are saying that if H0 is true, the observed phenomenon would happen no more than 5% of the time (that is 1 in 20). If we choose to compare the p-value to α = 0.01, we are insisting on a stronger evidence! Errors in Hypothesis Testing PowerThe power of a statistical test is its probability of rejecting the null hypothesis if the null hypothesis is false. That is, power is the ability to correctly reject H0 and detect a significant effect. In other words, power is one minus the type II error risk. Which error is worse? Type I = you are innocent, yet accused of cheating on the test. Type II = you cheated on the test, but you are found innocent. This depends on the context of the problem too. But in most cases scientists are trying to be “conservative”; it’s worse to make a spurious discovery than to fail to make a good one. Our goal it to increase the power of the test that is to minimize the length of the CI. We need to keep in mind: the effect of the sample size, the correctness of the underlying assumptions about the population, statistical vs. practical significance, etc… sometimes the p-value is too low because of the large sample size, and we may have statistical significance but not really practical significance! That’s why most statisticians are much more comfortable with using CI than tests. There is a need for a further generalization. What if we can’t assume that σ is known? In this case we would use s (the sample standard deviation) to estimate σ. If the sample is very large, we can treat σ as known by assuming that σ = s. According to the law of large numbers, this is not too bad a thing to do. But if the sample is small, the fact that we have to estimate both the standard deviation and the mean adds extra uncertainty to our inference. In practice this means that we need a larger multiplier for the standard error. We need one-sample t-test. One sample t-test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Sampling distribution</tag>
        <tag>Central Limit Theorem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸PCA]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8PCA%2F</url>
    <content type="text"><![CDATA[降维技术PCA(Principal Component Analysis,主成分分析)在PCA中，数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。然后会发现，大部分方差都包含在最前面的几个新坐标轴中。因此，我们可以忽略余下的坐标轴，即对数据进行了降维处理。 FA(Factor Analysis,因子分析)在因子分析中，假设在观察数据的生成中有一些观察不到的隐变量(latent variable)。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。 ICA(Independent Component Analysis,独立成分分析)ICA假设数据是从N个数据源生成的，这额鹅鹅鹅一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。 PCA用一组正交基对坐标轴进行旋转，坐标轴的旋转并没有减少数据的维度。特征值分析是线性代数中的一个领域，它能够通过数据的一般格式来揭示数据的“真实”结构，即我们常说的特征向量和特征值。NumPy中有寻找特征向量和特征值得模块linalg,它有eig()方法，该方法用于求解特征向量和特征值。 在NumPy中实现PCA的伪代码1234567去除平均值计算协方差矩阵计算协方差矩阵的特征值和特征向量将特征值从大到小排序保留最上面的N个特征向量将数据转换到上述N个特征向量构建的新空间中 PCA的代码实现12345678910111213141516171819202122232425262728from numpy import *def loadDataSet(fileName, delim='\t'): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [map(float,line) for line in stringArr] return mat(datArr)def pca(dataMat, topNfeat=9999999): meanVals = mean(dataMat, axis=0) meanRemoved = dataMat - meanVals #remove mean covMat = cov(meanRemoved, rowvar=0) eigVals,eigVects = linalg.eig(mat(covMat)) eigValInd = argsort(eigVals) #sort, sort goes smallest to largest eigValInd = eigValInd[:-(topNfeat+1):-1] #cut off unwanted dimensions redEigVects = eigVects[:,eigValInd] #reorganize eig vects largest to smallest lowDDataMat = meanRemoved * redEigVects#transform data into new dimensions reconMat = (lowDDataMat * redEigVects.T) + meanVals return lowDDataMat, reconMatdef replaceNanWithMean(): datMat = loadDataSet('secom.data', ' ') numFeat = shape(datMat)[1] for i in range(numFeat): meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[0],i]) #values that are not NaN (a number) datMat[nonzero(isnan(datMat[:,i].A))[0],i] = meanVal #set NaN values to mean return datMat 在上面的代码中，首先计算并减去原始数据集的平均值。 然后，计算协方差矩阵及其特征值，接着利用argsort()函数对特征值进行从小到大的排序。 根据特征值排序结果的逆序就可以得到topNfeat个最大的特征向量。这些特征向量构成后面对数据进行转换的矩阵，该矩阵则利用N个特征将原始数据转换到新空间中。 最后，原始数据乘上特征向量矩阵，数据被重构返回。 PCA小结降维技术使得数据变得更易使用，并且它们往往能够去除数据中的噪声，使得其他机器学习任务更加精确。降维往往作为预处理步骤，在数据应用到其他算法之前清洗数据。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>因子分析</tag>
        <tag>独立成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸k-Means]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8k-Means%2F</url>
    <content type="text"><![CDATA[K-Means聚类算法K-Means聚类算法伪代码12345678创建k个点作为起始质心当任意一个点的簇分配结果发生改变时 对数据集中的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到距其最近的簇 对每一个簇，计算簇中所有点的均值并将均值作为质心 K-Means聚类辅助函数1234567891011121314151617181920212223from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\t') fltLine = map(float,curLine) #map all elements to float() dataMat.append(fltLine) return dataMatdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)def randCent(dataSet, k): n = shape(dataSet)[1] centroids = mat(zeros((k,n)))#create centroid mat for j in range(n):#create random cluster centers, within bounds of each dimension minJ = min(dataSet[:,j]) rangeJ = float(max(dataSet[:,j]) - minJ) centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) return centroids K-Means聚类主函数12345678910111213141516171819202122def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2)))#create mat to assign data points #to a centroid, also holds SE of each point centroids = createCent(dataSet, k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m):#for each data point assign it to the closest centroid minDist = inf; minIndex = -1 for j in range(k): distJI = distMeas(centroids[j,:],dataSet[i,:]) if distJI &lt; minDist: minDist = distJI; minIndex = j if clusterAssment[i,0] != minIndex: clusterChanged = True clusterAssment[i,:] = minIndex,minDist**2 print centroids for cent in range(k):#recalculate centroids ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean return centroids, clusterAssment K-Means算法只能收敛到局部最小值，而非全局最小值。 对聚类得到的簇进行后处理上面在包含簇分配结果额矩阵中保存着每个点的误差，即该点到簇质心的距离平方值。下面会利用该误差来评价聚类质量的方法。 一种用于度量聚类效果的指标是SSE(Sum of Squared Error,误差平方和)。SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值得方法是增加簇的个数，但是这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。 那么如何进行改进？ 一种方法是将具有最大SSE值得簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-Means算法，其中的k设为2. 为了保持簇总数不变，可以将某两个簇进行合并。如何合并？ 有两种可以量化的办法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。第一种思路通过计算所有质心之间的距离，然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。 二分K-Means聚类算法为克服K-Means算法收敛于局部最小值得问题，有人提出了另一个称为二分K-Means（bisecting K-Means）的算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。 二分K-Means算法的伪代码：12345678将所有点看成一个簇当簇数目小于k时对于每一个簇 计算总误差 在给定的簇上面进行K-Means聚类（k=2） 计算将该簇一分为二之后的总误差选择使得误差最小的那个簇进行划分操作 二分K-Means的代码实现：12345678910111213141516171819202122232425262728293031def biKmeans(dataSet, k, distMeas=distEclud): m = shape(dataSet)[0] clusterAssment = mat(zeros((m, 2))) centroid0 = mean(dataSet, axis=0).tolist()[0] centList = [centroid0] # create a list with one centroid for j in range(m): # calc initial Error clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :]) ** 2 while (len(centList) &lt; k): lowestSSE = inf for i in range(len(centList)): ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :] # get the data points currently in cluster i centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) sseSplit = sum(splitClustAss[:, 1]) # compare the SSE to the currrent minimum sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1]) print "sseSplit, and notSplit: ", sseSplit, sseNotSplit if (sseSplit + sseNotSplit) &lt; lowestSSE: bestCentToSplit = i bestNewCents = centroidMat bestClustAss = splitClustAss.copy() lowestSSE = sseSplit + sseNotSplit bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # change 1 to 3,4, or whatever bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit print 'the bestCentToSplit is: ', bestCentToSplit print 'the len of bestClustAss is: ', len(bestClustAss) centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0] # replace a centroid with two best centroids centList.append(bestNewCents[1, :].tolist()[0]) clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # reassign new clusters, and SSE return mat(centList), clusterAssment]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>KMeans</tag>
        <tag>二分K-Means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸LinearRegression]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8LinearRegression%2F</url>
    <content type="text"><![CDATA[线性回归解析式求解： 线性回归解析式解Python代码实现123456789101112131415161718192021222324from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats numFeat = len(open(fileName).readline().split('\t')) - 1 #get number of fields dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr =[] curLine = line.strip().split('\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMatdef standRegres(xArr,yArr): xMat = mat(xArr); yMat = mat(yArr).T xTx = xMat.T*xMat if linalg.det(xTx) == 0.0: print "This matrix is singular, cannot do inverse" return ws = xTx.I * (xMat.T*yMat) return ws 局部加权线性回归局部加权线性回归(Locally Weighted Linear Regression,LWLR)。在该算法中，我们给待预测点附近的每个点赋予一定的权重；然后在这个子集上基于最小均方差来进行普通的回归。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。 该算法解出回归系数w的形式如下： 其中w是一个矩阵，用来给每个数据点赋予权重。 LWLR使用“核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下： 这样就构建了一个只含对角元素的权重矩阵w，并且点x与x(i)越近，w(i,i)将会越大。参数k决定了对附近的点赋予多大的权重，则也是使用LWLR时唯一需要考虑的参数。 下面是参数k与权重的关系： 局部加权线性回归解析式解Python代码实现123456789101112131415161718192021def lwlr(testPoint,xArr,yArr,k=1.0): xMat = mat(xArr); yMat = mat(yArr).T m = shape(xMat)[0] weights = mat(eye((m))) for j in range(m): #next 2 lines create weights matrix diffMat = testPoint - xMat[j,:] # weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2)) xTx = xMat.T * (weights * xMat) if linalg.det(xTx) == 0.0: print "This matrix is singular, cannot do inverse" return ws = xTx.I * (xMat.T * (weights * yMat)) return testPoint * wsdef lwlrTest(testArr,xArr,yArr,k=1.0): #loops over all the data points and applies lwlr to each one m = shape(testArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHat 局部加权线性回归的问题在于，每次必须在整个数据集上运行，也就是说为了做出预测，必须保存所有的训练数据。 岭回归和逐步线性回归岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入lamda来限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫做缩减（shrinkage）。 岭回归中的岭是什么？岭回归使用了单位矩阵乘以常量lamda，最先是用来处理特征数多于样本数的情况，我们观察其中的单位矩阵I，可以看到值1贯穿整个对角线，其余元素全是0。形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中的“岭”的由来。 岭回归解析式解Python代码实现123456789101112131415161718192021222324def ridgeRegres(xMat,yMat,lam=0.2): xTx = xMat.T*xMat denom = xTx + eye(shape(xMat)[1])*lam if linalg.det(denom) == 0.0: print "This matrix is singular, cannot do inverse" return ws = denom.I * (xMat.T*yMat) return ws def ridgeTest(xArr,yArr): xMat = mat(xArr); yMat=mat(yArr).T yMean = mean(yMat,0) yMat = yMat - yMean #to eliminate X0 take mean off of Y #regularize X's xMeans = mean(xMat,0) #calc mean then subtract it off xVar = var(xMat,0) #calc variance of Xi then divide by it xMat = (xMat - xMeans)/xVar numTestPts = 30 wMat = zeros((numTestPts,shape(xMat)[1])) for i in range(numTestPts): ws = ridgeRegres(xMat,yMat,exp(i-10)) wMat[i,:]=ws.T return wMat 岭回归的回归系数变化图这里的lambda应以指数级变化，这样可以看出lambda在取非常小的值时和取非常大的值时分别对结果造成的影响。 还有一些其他缩减方法，如lasso、LAR、PCA回归以及子集选择等。与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。 在增加如下约束时，普通的最小二乘法回归会得到与岭回归的一样的公式： 上式限定了所有回归系数的平方和不能大于lambda。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得出一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。 与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下： 唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭：在lambda足够小的时候，一些系数会因此被迫缩减到0，这个特性可以帮助我们更好地理解数据。这两个约束条件在公式上看起来相差无几，但细微的变化却极大地增加了计算复杂度（为了在这个新的约束条件下解出回归系数，需要使用二次规划算法）。 前向逐步回归前向逐步回归属于一种贪心算法，即每一步都尽可能减少误差。一开始所有的权重都设为0，然后每一步所做的决策是对某个权重增加或减少一个很小的值。 该算法的伪代码如下所示：12345678910数据标准化，使其分布满足0均值和单位方差在每轮迭代过程中： 设置当前最小误差lowestError为正无穷 对每个特征： 增大或缩小： 改变一个系数得到一个新的W 计算新W下的误差 如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W 将W设置为新的Wbest 前向逐步回归python代码实现123456789101112131415161718192021222324252627def rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays return ((yArr-yHatArr)**2).sum()def stageWise(xArr,yArr,eps=0.01,numIt=100): xMat = mat(xArr); yMat=mat(yArr).T yMean = mean(yMat,0) yMat = yMat - yMean #can also regularize ys but will get smaller coef xMat = regularize(xMat) m,n=shape(xMat) returnMat = zeros((numIt,n)) #testing code remove ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy() for i in range(numIt):#could change this to while loop #print ws.T lowestError = inf; for j in range(n): for sign in [-1,1]: wsTest = ws.copy() wsTest[j] += eps*sign yTest = xMat*wsTest rssE = rssError(yMat.A,yTest.A) if rssE &lt; lowestError: lowestError = rssE wsMax = wsTest ws = wsMax.copy() returnMat[i,:]=ws.T return returnMat 逐步线性回归，主要的优点在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。 当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias),与此同时却减少了模型的方差。 权衡偏差与方差 误差由三个部分组成：偏差、误差和噪声。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>岭回归</tag>
        <tag>局部加权线性回归</tag>
        <tag>逐步回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸CART]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8CART%2F</url>
    <content type="text"><![CDATA[CART(Classification And Regression Trees)既可以用于分类还可以用于回归。 CART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一棵子树或者单个值。字典还包含特征和特征值这两个键，它们给出切分算法所有的特征和特征值。 建立树节点：1234567classtreeNode()： def __init__(self,feat,val,right,left)： featureToSplitOn = feat valueOfSplit = val rightBranch = right leftBranch = left 函数createTree()的伪代码大致如下：123456找到最佳的待切分特征： 如果该节点不能再分，将该节点存为叶节点 执行二元切分 在右子树调用createTree()方法 在左子树调用createTree()方法 CART算法的实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\t') fltLine = map(float,curLine) #map all elements to float() dataMat.append(fltLine) return dataMatdef binSplitDataSet(dataSet, feature, value): mat0 = dataSet[nonzero(dataSet[:,feature] &gt; value)[0],:][0] mat1 = dataSet[nonzero(dataSet[:,feature] &lt;= value)[0],:][0] return mat0,mat1def regLeaf(dataSet):#returns the value used for each leaf return mean(dataSet[:,-1])def regErr(dataSet): return var(dataSet[:,-1]) * shape(dataSet)[0]def linearSolve(dataSet): #helper function used in two places m,n = shape(dataSet) X = mat(ones((m,n))); Y = mat(ones((m,1)))#create a copy of data with 1 in 0th postion X[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]#and strip out Y xTx = X.T*X if linalg.det(xTx) == 0.0: raise NameError('This matrix is singular, cannot do inverse,\n\ try increasing the second value of ops') ws = xTx.I * (X.T * Y) return ws,X,Ydef modelLeaf(dataSet):#create linear model and return coeficients ws,X,Y = linearSolve(dataSet) return wsdef modelErr(dataSet): ws,X,Y = linearSolve(dataSet) yHat = X * ws return sum(power(Y - yHat,2))def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)): tolS = ops[0]; tolN = ops[1] #if all the target variables are the same value: quit and return value if len(set(dataSet[:,-1].T.tolist()[0])) == 1: #exit cond 1 return None, leafType(dataSet) m,n = shape(dataSet) #the choice of the best feature is driven by Reduction in RSS error from mean S = errType(dataSet) bestS = inf; bestIndex = 0; bestValue = 0 for featIndex in range(n-1): for splitVal in set(dataSet[:,featIndex]): mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): continue newS = errType(mat0) + errType(mat1) if newS &lt; bestS: bestIndex = featIndex bestValue = splitVal bestS = newS #if the decrease (S-bestS) is less than a threshold don't do the split if (S - bestS) &lt; tolS: return None, leafType(dataSet) #exit cond 2 mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): #exit cond 3 return None, leafType(dataSet) return bestIndex,bestValue#returns the best feature to split on #and the value used for that splitdef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering feat, val = chooseBestSplit(dataSet, leafType, errType, ops)#choose the best split if feat == None: return val #if the splitting hit a stop condition return val retTree = &#123;&#125; retTree['spInd'] = feat retTree['spVal'] = val lSet, rSet = binSplitDataSet(dataSet, feat, val) retTree['left'] = createTree(lSet, leafType, errType, ops) retTree['right'] = createTree(rSet, leafType, errType, ops) return retTree 树剪枝通过降低决策树的复杂度来避免过拟合的过程称为剪枝。在chooseBestSplit()中的提前终止条件，实际上是在进行一种所谓的预剪枝操作。另一种形式的剪枝需要使用测试集和训练集，称为后剪枝。 停止条件对误差的数量级十分敏感，通过不断修改停止条件来得到合理结果并不是很好的办法。后剪枝，利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。 后剪枝函数prune()的伪代码如下： 123456基于已有的树切分测试数据： 如果存在任一子集是一棵树，则在该子集递归剪枝过程 计算将当前两个叶节点合并后的误差 计算不合并的误差 如果合并会降低误差的话，就将叶节点合并 回归树剪枝函数Python代码： 123456789101112131415161718192021222324252627def isTree(obj): return (type(obj).__name__=='dict')def getMean(tree): if isTree(tree['right']): tree['right'] = getMean(tree['right']) if isTree(tree['left']): tree['left'] = getMean(tree['left']) return (tree['left']+tree['right'])/2.0 def prune(tree, testData): if shape(testData)[0] == 0: return getMean(tree) #if we have no test data collapse the tree if (isTree(tree['right']) or isTree(tree['left'])):#if the branches are not trees try to prune them lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) if isTree(tree['left']): tree['left'] = prune(tree['left'], lSet) if isTree(tree['right']): tree['right'] = prune(tree['right'], rSet) #if they are now both leafs, see if we can merge them if not isTree(tree['left']) and not isTree(tree['right']): lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +\ sum(power(rSet[:,-1] - tree['right'],2)) treeMean = (tree['left']+tree['right'])/2.0 errorMerge = sum(power(testData[:,-1] - treeMean,2)) if errorMerge &lt; errorNoMerge: print "merging" return treeMean else: return tree else: return tree]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>CART</tag>
        <tag>regression tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸SVD]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8SVD%2F</url>
    <content type="text"><![CDATA[利用SVD，我们能够用小得多的数据集来表示原始数据集。这样做，实际上是去除了噪声和冗余信息。 在PCA中得到的是矩阵的特征值，它们告诉我们数据集中的重要特征。这里的奇异值也是如此。奇异值和特征值是有关系的。这里的奇异值就是矩阵Data*DataT特征值的平方根。 相似度计算：123456789101112131415from numpy import *from numpy import linalg as ladef ecludSim(inA,inB): return 1.0/(1.0 + la.norm(inA - inB))def pearsSim(inA,inB): if len(inA) &lt; 3 : return 1.0 return 0.5+0.5*corrcoef(inA, inB, rowvar = 0)[0][1]def cosSim(inA,inB): num = float(inA.T*inB) denom = la.norm(inA)*la.norm(inB) return 0.5+0.5*(num/denom) 基于物品相似度的推荐引擎：1234567891011121314151617181920212223242526def standEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 for j in range(n): userRating = dataMat[user,j] if userRating == 0: continue overLap = nonzero(logical_and(dataMat[:,item].A&gt;0, \ dataMat[:,j].A&gt;0))[0] if len(overLap) == 0: similarity = 0 else: similarity = simMeas(dataMat[overLap,item], \ dataMat[overLap,j]) print 'the %d and %d similarity is: %f' % (item, j, similarity) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotaldef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst): unratedItems = nonzero(dataMat[user,:].A==0)[1]#find unrated items if len(unratedItems) == 0: return 'you rated everything' itemScores = [] for item in unratedItems: estimatedScore = estMethod(dataMat, user, simMeas, item) itemScores.append((item, estimatedScore)) return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N] 利用SVD提高推荐的效果基于SVD的评分估计 1234567891011121314151617def svdEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 U,Sigma,VT = la.svd(dataMat) Sig4 = mat(eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix xformedItems = dataMat.T * U[:,:4] * Sig4.I #create transformed items for j in range(n): userRating = dataMat[user,j] if userRating == 0 or j==item: continue similarity = simMeas(xformedItems[item,:].T,\ xformedItems[j,:].T) print 'the %d and %d similarity is: %f' % (item, j, similarity) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotal 小结SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留均值80%~90%的能量，就可以得到重要的特征并去掉噪声。协同过滤的核心是相似度计算方法；有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐引擎的效果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸ModelTree]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8ModelTree%2F</url>
    <content type="text"><![CDATA[模型树用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把也节点设定为分段线性函数，这里所谓的分段线性(piecewis linear)是指模型由多个线性片段组成。即分段线性模型。 相比CART，叶节点误差的计算需要稍加变化，对于给定的数据集，应该先用线性的模型来对它进行拟合，然后计算真实值与预测值的差值。最后将这些差值的平方求和就得到了所需的误差。 叶节点误差的计算：1234567891011121314151617181920def linearSolve(dataSet): #helper function used in two places m,n = shape(dataSet) X = mat(ones((m,n))); Y = mat(ones((m,1)))#create a copy of data with 1 in 0th postion X[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]#and strip out Y xTx = X.T*X if linalg.det(xTx) == 0.0: raise NameError('This matrix is singular, cannot do inverse,\n\ try increasing the second value of ops') ws = xTx.I * (X.T * Y) return ws,X,Ydef modelLeaf(dataSet):#create linear model and return coeficients ws,X,Y = linearSolve(dataSet) return wsdef modelErr(dataSet): ws,X,Y = linearSolve(dataSet) yHat = X * ws return sum(power(Y - yHat,2)) 用树回归进行预测的代码：12345678910111213141516171819202122232425def regTreeEval(model, inDat): return float(model)def modelTreeEval(model, inDat): n = shape(inDat)[1] X = mat(ones((1,n+1))) X[:,1:n+1]=inDat return float(X*model)def treeForeCast(tree, inData, modelEval=regTreeEval): if not isTree(tree): return modelEval(tree, inData) if inData[tree['spInd']] &gt; tree['spVal']: if isTree(tree['left']): return treeForeCast(tree['left'], inData, modelEval) else: return modelEval(tree['left'], inData) else: if isTree(tree['right']): return treeForeCast(tree['right'], inData, modelEval) else: return modelEval(tree['right'], inData) def createForeCast(tree, testData, modelEval=regTreeEval): m=len(testData) yHat = mat(zeros((m,1))) for i in range(m): yHat[i,0] = treeForeCast(tree, mat(testData[i]), modelEval) return yHat 调用函数treeForeCast()时需要指定树的类型，以便在叶节点上能够调用合适的模型。参数modelEval是对叶节点数据进行预测的函数的引用。treeForeCast()自顶向下遍历整棵树，直到命中叶节点为止。一旦到达叶节点，它就会在输入数据上调用modelEval()函数，而该函数的默认值是regTreeEval()。 若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则成为模型树。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>model tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sequential Minimal Optimization]]></title>
    <url>%2F2017%2F11%2F29%2FSMO%2F</url>
    <content type="text"><![CDATA[INTRODUCTIONTraining a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size,which allows SMO to handle very large training sets.SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets.Instead of previous SVM learning algorithms that use numerical quadratic programming (QP) as an inner loop, SMO uses an analytic QP step. Overview of Support Vector Machines Lagrange dual formSVM在求解过程中首先用了拉格朗日对偶，作用是将w的计算提前并消除w，使得优化函数变为拉格朗日乘子的单一参数优化问题。对偶函数最后的优化问题： There is a one-to-one relationship between each Lagrange multiplier and each training example.Once the Lagrange multipliers are determined, the normal vector wand the threshold b can be derived from the Lagrange multipliers: linearly unseparableOf course, not all data sets are linearly separable. There may be no hyperplane that splits the positive examples from the negative examples. In the formulation above, the non-separable case would correspond to an infinite solution. However, in 1995, Cortes &amp; Vapnik suggested a modification to the original optimization statement (3) which allows, but penalizes, the failure of an example to reach the correct margin. That modification is: generalized to non-linear classifiersSVMs can be even further generalized to non-linear classifiers. The output of a non-linear SVM is explicitly computed from the Lagrange multipliers: Karush-Kuhn-Tucker (KKT) conditionsThe QP problem in equation (11), above, is the QP problem that the SMO algorithm will solve. In order to make the QP problem above be positive definite, the kernel function K must obey Mercer’s conditions.The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient conditions for an optimal point of a positive definite QP problem. The KKT conditions for the QP problem (11) are particularly simple. The QP problem is solved when, for all i: where ui is the output of the SVM for the ith training example. Notice that the KKT conditionscan be evaluated on one example at a time, which will be useful in the construction of the SMOalgorithm.### SEQUENTIAL MINIMAL OPTIMIZATIONSequential Minimal Optimization (SMO) is a simple algorithm that can quickly solve the SVM QP problem without any extra matrix storage and without using numerical QP optimization steps at all. SMO decomposes the overall QP problem into QP sub-problems, using Osuna’s theorem to ensure convergence.Unlike the previous methods, SMO chooses to solve the smallest possible optimization problem at every step. For the standard SVM QP problem, the smallest possible optimization problem involves two Lagrange multipliers, because the Lagrange multipliers must obey a linear equality constraint. At every step, SMO chooses two Lagrange multipliers to jointly optimize, finds the optimal values for these multipliers, and updates the SVM to reflect the new optimal values.The advantage of SMO lies in the fact that solving for two Lagrange multipliers can be done analytically. Thus, numerical QP optimization is avoided entirely. The inner loop of the algorithm can be expressed in a short amount of C code, rather than invoking an entire QP library routine. Even though more optimization sub-problems are solved in the course of the algorithm,each sub-problem is so fast that the overall QP problem is solved quickly.In addition, SMO requires no extra matrix storage at all. Thus, very large SVM training problems can fit inside of the memory of an ordinary personal computer or workstation. Because no matrix algorithms are used in SMO, it is less susceptible to numerical precision problems.There are two components to SMO: an analytic method for solving for the two Lagrange multipliers, and a heuristic for choosing which multipliers to optimize.假设我们选取了初始值{a_1,a_2,a_3,…,a_n}满足了问题中的约束条件。接下来，{a_3,a_4,…,a_n}，这样W就是a_1和a_2的函数。并且a_1和a_2满足条件： 由于{a_3,a_4,…,a_n}都是已知固定值，因此右边是一个具体的常数。当y_1和y_2异号时，也就是一个为1，一个为-1时，他们可以表示成一条直线，斜率为1；同理当y_1和y_2同号时，斜率为-1. Solving for Two Lagrange MultipliersIn order to solve for the two Lagrange multipliers, SMO first computes the constraints on these multipliers and then solves for the constrained minimum. For convenience, all quantities that refer to the first multiplier will have a subscript 1, while all quantities that refer to the second multiplier will have a subscript 2. Because there are only two multipliers, the constraints can be easily be displayed in two dimensions (see figure 3). The bound constraints (9) cause the Lagrange multipliers to lie within a box, while the linear equality constraint (6) causes the Lagrange multipliers to lie on a diagonal line. Thus, the constrained minimum of the objective function must lie on a diagonal line segment (as shown in figure 3). This constraint explains why two is the minimum number of Lagrange multipliers that can be optimized: if SMO optimized only one multiplier, it could not fulfill the linear equality constraint at every step. The ends of the diagonal line segment can be expressed quite simply. Without loss of generality,the algorithm first computes the second Lagrange multiplier a_2 and computes the ends of thediagonal line segment in terms of a_2. If the target y_1 does not equal the target y_2, then thefollowing bounds apply to a_2: L = max(0,a_2 - a_1), H = min(C,C + a_2- a_1). If the target y_1 equals the target y_2, then the following bounds apply to a_2: L = max(0,a_2 + a_1 - C), H = min(C, a_2 + a_1). 然后将a_1用a_2表示： 然后反代入W中，得： 展开后W可以表示成a_2的二次函数。这样，直接求导可以得到a_2，然而要保证满足L&lt;=a_2&lt;=H，我们使用new,unclipped表示求导求出来的a_2，然后最后的a_2,要根据下面情况得到： 这样得到新的a_2，就可以得到新的a_1。 论文中的表达方式： Heuristics for Choosing Which Multipliers To OptimizeAs long as SMO always optimizes and alters two Lagrange multipliers at every step and at least one of the Lagrange multipliers violated the KKT conditions before the step, then each step will decrease the objective function according to Osuna’s theorem [16]. Convergence is thus guaranteed. In order to speed convergence, SMO uses heuristics to choose which two Lagrange multipliers to jointly optimize. There are two separate choice heuristics: one for the first Lagrange multiplier and one for the second. The choice of the first heuristic provides the outer loop of the SMO algorithm. The outer loop first iterates over the entire training set, determining whether each example violates the KKT conditions (12). If an example violates the KKT conditions, it is then eligible for optimization. After one pass through the entire training set, the outer loop iterates over all examples whose Lagrange multipliers are neither 0 nor C (the non-bound examples). Again, each example is checked against the KKT conditions and violating examples are eligible for optimization. The outer loop makes repeated passes over the non-bound examples until all of the non-bound examples obey the KKT conditions within e. The outer loop then goes back and iterates over the entire training set. The outer loop keeps alternating between single passes over the entire training set and multiple passes over the non-bound subset until the entire training set obeys the KKT conditions within e, whereupon the algorithm terminates. The first choice heuristic concentrates the CPU time on the examples that are most likely to violate the KKT conditions: the non-bound subset. As the SMO algorithm progresses, examples that are at the bounds are likely to stay at the bounds, while examples that are not at the bounds will move as other examples are optimized. The SMO algorithm will thus iterate over the nonbound subset until that subset is self-consistent, then SMO will scan the entire data set to search for any bound examples that have become KKT violated due to optimizing the non-bound subset. Notice that the KKT conditions are checked to be within e of fulfillment. Typically, e is set to be 10-3. Recognition systems typically do not need to have the KKT conditions fulfilled to high accuracy: it is acceptable for examples on the positive margin to have outputs between 0.999 and 1.001. The SMO algorithm (and other SVM algorithms) will not converge as quickly if it is required to produce very high accuracy output. Once a first Lagrange multiplier is chosen, SMO chooses the second Lagrange multiplier tomaximize the size of the step taken during joint optimization. Now, evaluating the kernelfunction K is time consuming, so SMO approximates the step size by the absolute value of the numerator in equation (16): |E_1 - E_2|. SMO keeps a cached error value E for every non-bound example in the training set and then chooses an error to approximately maximize the step size. If E_1 is positive, SMO chooses an example with minimum error E2. If E1 is negative, SMO choosesan example with maximum error E_2. Under unusual circumstances, SMO cannot make positive progress using the second choiceheuristic described above. For example, positive progress cannot be made if the first and second training examples share identical input vectors x, which causes the objective function to become semi-definite. In this case, SMO uses a hierarchy of second choice heuristics until it finds a pair of Lagrange multipliers that can be make positive progress. Positive progress can be determined by making a non-zero step size upon joint optimization of the two Lagrange multipliers . The hierarchy of second choice heuristics consists of the following. If the above heuristic does not make positive progress, then SMO starts iterating through the non-bound examples, searching for an second example that can make positive progress. If none of the non-bound examples make positive progress, then SMO starts iterating through the entire training set until an example is found that makes positive progress. Both the iteration through the non-bound examples and the iteration through the entire training set are started at random locations, in order not to bias SMO towards the examples at the beginning of the training set. In extremely degenerate circumstances, none of the examples will make an adequate second example. When this happens, the first example is skipped and SMO continues with another chosen first example. Computing the ThresholdThe threshold b is re-computed after each step, so that the KKT conditions are fulfilled for both optimized examples. The following threshold b_1 is valid when the new a_1 is not at the bounds,because it forces the output of the SVM to be y_1 when the input is x_1: The following threshold b_2 is valid when the new a_2 is not at bounds, because it forces the output of the SVM to be y_2 when the input is x_2: When both b_1 and b_2 are valid, they are equal. When both new Lagrange multipliers are at bound and if L is not equal to H, then the interval between b_1 and b_2 are all thresholds that are consistent with the KKT conditions. SMO chooses the threshold to be halfway in between b_1 and b_2.### An Optimization for Linear SVMsTo compute a linear SVM, only a single weight vector w needs to be stored, rather than all of the training examples that correspond to non-zero Lagrange multipliers. If the joint optimization succeeds, the stored weight vector needs to be updated to reflect the new Lagrange multiplier values. The weight vector update is easy, due to the linearity of the SVM: Pseudo Code DetailsThe pseudo-code below describes the entire SMO algorithm:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293target = desired output vectorpoint = training point matrixprocedure takeStep(i1,i2) if (i1 == i2) return 0 alph1 = Lagrange multiplier for i1 y1 = target[i1] E1 = SVM output on point[i1] – y1 (check in error cache) s = y1*y2 Compute L, H via equations (13) and (14) if (L == H) return 0 k11 = kernel(point[i1],point[i1]) k12 = kernel(point[i1],point[i2]) k22 = kernel(point[i2],point[i2]) eta = k11+k22-2*k12 if (eta &gt; 0) &#123; a2 = alph2 + y2*(E1-E2)/eta if (a2 &lt; L) a2 = L else if (a2 &gt; H) a2 = H &#125; else &#123; Lobj = objective function at a2=L Hobj = objective function at a2=H if (Lobj &lt; Hobj-eps) a2 = L else if (Lobj &gt; Hobj+eps) a2 = H else a2 = alph2 &#125; if (|a2-alph2| &lt; eps*(a2+alph2+eps)) return 0 a1 = alph1+s*(alph2-a2) Update threshold to reflect change in Lagrange multipliers Update weight vector to reflect change in a1 &amp; a2, if SVM is linear Update error cache using new Lagrange multipliers Store a1 in the alpha array Store a2 in the alpha array return 1endprocedureprocedure examineExample(i2) y2 = target[i2] alph2 = Lagrange multiplier for i2 E2 = SVM output on point[i2] – y2 (check in error cache) r2 = E2*y2 if ((r2 &lt; -tol &amp;&amp; alph2 &lt; C) || (r2 &gt; tol &amp;&amp; alph2 &gt; 0)) &#123; if (number of non-zero &amp; non-C alpha &gt; 1) &#123; i1 = result of second choice heuristic (section 2.2) if takeStep(i1,i2) return 1 &#125; loop over all non-zero and non-C alpha, starting at a random point &#123; i1 = identity of current alpha if takeStep(i1,i2) return 1 &#125; loop over all possible i1, starting at a random point &#123; i1 = loop variable if (takeStep(i1,i2) return 1 &#125; &#125; return 0endproceduremain routine: numChanged = 0; examineAll = 1; while (numChanged &gt; 0 | examineAll) &#123; numChanged = 0; if (examineAll) loop I over all training examples numChanged += examineExample(I) else loop I over examples where alpha is not 0 &amp; not C numChanged += examineExample(I) if (examineAll == 1) examineAll = 0 else if (numChanged == 0) examineAll = 1 &#125; via Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines;© 1998 John Platt 支持向量机（五）SMO算法 【机器学习详解】SMO算法剖析]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BFGS算法]]></title>
    <url>%2F2017%2F11%2F19%2FBFGS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>BFGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸AdaBoost]]></title>
    <url>%2F2017%2F11%2F17%2F%E6%89%8B%E6%92%B8AdaBoost%2F</url>
    <content type="text"><![CDATA[AdaBoost算法概述： 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。 缺点：对离群点敏感。 适用数据类型：数值型和标称型数据。 将不同的分类器组合起来，成为集成方法（ensemble method）或者元算法（meta-algorithm)。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。 bagging(bootstrap aggregating)，是在从原始数据集选择s次后得到s个新的数据集的一种技术，新数据集和原始数据集大小相等。有放回的抽样，所以允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。在s个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了s个分类器。当我们要对新数据进行分类时，就可以应用这s个分类器进行分类。选择分类器投票结果中最多的类别作为最后的分类结果。 boosting和bagging类似，所使用的多个分类器类型都是一致的，但是boosting是通过串行训练而获得，每个新分类器都是根据已训练出的分类器的性能来进行训练。 AdaBoost代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# 生成数据集from numpy import *def loadSimpData(): datMat = matrix([[ 1. , 2.1], [ 2. , 1.1], [ 1.3, 1. ], [ 1. , 1. ], [ 2. , 1. ]]) classLabels = [1.0, 1.0, -1.0, -1.0, 1.0] return datMat,classLabels# 单层决策树生成函数# 单层决策树预测函数def stumpClassify(dataMatrix, dimen, threshVal, threshIneq): # just classify the data """ 通过阈值比较对数据进行分类。只用于连续输入变量。 :param dataMatrix: :param dimen:特征 :param threshVal:阈值 :param threshIneq:小于还是大于等于 :return: """ retArray = ones((shape(dataMatrix)[0], 1)) if threshIneq == 'lt': retArray[dataMatrix[:, dimen] &lt;= threshVal] = -1.0 else: retArray[dataMatrix[:, dimen] &gt; threshVal] = -1.0 return retArray# 单层决策树的创建def buildStump(dataArr, classLabels, D): """ 遍历stumpClassify()函数所有的可能输入值，并找到数据集上最佳的单层决策树。只用于连续输入变量。 :param dataArr: :param classLabels: :param D:权重向量 :return: """ dataMatrix = mat(dataArr) labelMat = mat(classLabels).T m, n = shape(dataMatrix) numSteps = 10.0 # 循环次数，用于计算步长 bestStump = &#123;&#125; # 用于存储给定权重向量D时所得到的最佳单层决策树的相关信息 bestClasEst = mat(zeros((m, 1))) # 标签的估计值 minError = inf # init error sum, to +infinity for i in range(n): # loop over all dimensions rangeMin = dataMatrix[:, i].min() rangeMax = dataMatrix[:, i].max() stepSize = (rangeMax - rangeMin) / numSteps for j in range(-1, int(numSteps) + 1): # loop over all range in current dimension for inequal in ['lt', 'gt']: # go over less than and greater than threshVal = (rangeMin + float(j) * stepSize) predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal) # call stump classify with i, j, lessThan errArr = mat(ones((m, 1))) errArr[predictedVals == labelMat] = 0 weightedError = D.T * errArr # calc total error multiplied by D # print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError) if weightedError &lt; minError: minError = weightedError bestClasEst = predictedVals.copy() bestStump['dim'] = i bestStump['thresh'] = threshVal bestStump['ineq'] = inequal return bestStump, minError, bestClasEst# 基于单层决策树的AdaBoost训练过程：def adaBoostTrainDS(dataArr,classLabels,numIt=40): """ 基于单层决策树的AdaBoost训练过程 :param dataArr:数据集 :param classLabels:类别标签 :param numIt:迭代次数 :return: """ weakClassArr = [] m = shape(dataArr)[0] D = mat(ones((m,1))/m) #init D to all equal aggClassEst = mat(zeros((m,1))) for i in range(numIt): bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump #print "D:",D.T alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0 bestStump['alpha'] = alpha weakClassArr.append(bestStump) #store Stump Params in Array #print "classEst: ",classEst.T expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy D = multiply(D,exp(expon)) #Calc New D for next iteration D = D/D.sum() #calc training error of all classifiers, if this is 0 quit for loop early (use break) aggClassEst += alpha*classEst #print "aggClassEst: ",aggClassEst.T aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1))) errorRate = aggErrors.sum()/m print "total error: ",errorRate if errorRate == 0.0: break return weakClassArr,aggClassEstimport matplotlib.pyplot as pltdef plotROC(predStrengths, classLabels): """ 画ROC曲线 :param predStrengths:预测概率 :param classLabels:真实标签 :return: """ cur = (1.0,1.0) #cursor ySum = 0.0 #variable to calculate AUC numPosClas = sum(array(classLabels)==1.0) yStep = 1/float(numPosClas) xStep = 1/float(len(classLabels)-numPosClas) sortedIndicies = predStrengths.argsort()#get sorted index, it's reverse fig = plt.figure() fig.clf() ax = plt.subplot(111) #loop through all the values, drawing a line segment at each point for index in sortedIndicies.tolist()[0]: if classLabels[index] == 1.0: delX = 0; delY = yStep; else: delX = xStep; delY = 0; ySum += cur[1] #draw line from cur to (cur[0]-delX,cur[1]-delY) ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY], c='b') cur = (cur[0]-delX,cur[1]-delY) ax.plot([0,1],[0,1],'b--') plt.xlabel('False positive rate'); plt.ylabel('True positive rate') plt.title('ROC curve for AdaBoost horse colic detection system') ax.axis([0,1,0,1]) plt.show() print "the Area Under the Curve is: ",ySum*xStep via 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸SVM]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8SVM%2F</url>
    <content type="text"><![CDATA[基于最大间隔分隔数据 优点：泛化错误率低，计算开销不大，结果易解释。 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。 适用数据类型：数值型和标称型数据。 SMO伪代码123456789创建一个alpha向量并将其初始化为0向量当迭代次数小于最大迭代次数时（外循环） 对数据集中的每个数据向量（内循环）： 如果该数据向量可以被优化： 随机选择另外一个数据向量 同时优化这两个向量 如果两个向量都不能被优化，退出内循环 如果所有向量都没被优化，增加迭代数目，继续下一次循环 SMO代码实现首先定义一些SMO算法中的辅助函数12345678910111213141516171819202122232425def loadDataSet(fileName): dataMat = [] labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = line.strip().split('\t') dataMat.append([float(lineArr[0]), float(lineArr[1])]) labelMat.append(float(lineArr[2])) return dataMat, labelMatdef selectJrand(i, m): j = i # we want to select any J not equal to i while (j == i): j = int(random.uniform(0, m)) return jdef clipAlpha(aj, H, L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj 简化版SMO算法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def smoSimple(dataMatIn, classLabels, C, toler, maxIter): dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() b = 0 m, n = shape(dataMatrix) alphas = mat(zeros((m, 1))) iter = 0 while (iter &lt; maxIter): alphaPairsChanged = 0 for i in range(m): fXi = float(multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[i, :].T)) + b Ei = fXi - float(labelMat[i]) # if checks if an example violates KKT conditions if ((labelMat[i] * Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i] * Ei &gt; toler) and (alphas[i] &gt; 0)): j = selectJrand(i, m) fXj = float(multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[j, :].T)) + b Ej = fXj - float(labelMat[j]) alphaIold = alphas[i].copy() alphaJold = alphas[j].copy() if (labelMat[i] != labelMat[j]): L = max(0, alphas[j] - alphas[i]) H = min(C, C + alphas[j] - alphas[i]) else: L = max(0, alphas[j] + alphas[i] - C) H = min(C, alphas[j] + alphas[i]) if L == H: print "L==H" continue eta = 2.0 * dataMatrix[i, :] * dataMatrix[j, :].T - dataMatrix[i, :] * dataMatrix[i, :].T - dataMatrix[j,:] * dataMatrix[j, :].T if eta &gt;= 0: print "eta&gt;=0" continue alphas[j] -= labelMat[j] * (Ei - Ej) / eta alphas[j] = clipAlpha(alphas[j], H, L) if (abs(alphas[j] - alphaJold) &lt; 0.00001): print "j not moving enough" continue alphas[i] += labelMat[j] * labelMat[i] * (alphaJold - alphas[j]) # update i by the same amount as j # the update is in the oppostie direction b1 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[i, :].T - labelMat[j] * (alphas[j] - alphaJold) * dataMatrix[i,:] * dataMatrix[j,:].T b2 = b - Ej - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[j, :].T - labelMat[j] * (alphas[j] - alphaJold) * dataMatrix[j,:] * dataMatrix[j,:].T if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): b = b2 else: b = (b1 + b2) / 2.0 alphaPairsChanged += 1 print "iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) if (alphaPairsChanged == 0): iter += 1 else: iter = 0 print "iteration number: %d" % iter return b, alphas 上面函数有5个输入参数，分别是：数据集、类别标签、常数C、容错率和取消前最大的循环次数。函数将多个列表和输入参数转换成NumPy矩阵，这样就可以简化很多数学处理操作。由于转置了类别标签，因此我们得到的就是一个列向量而不是列表。于是类别标签向量的每行元素都和数据矩阵中的行一一对应。构建一个alpha列矩阵，矩阵中元素都初始化为0，并建立一个iter变量。该变量存储的则是没有任何alpha改变的情况下遍历数据集的次数。当该变量达到输入值maxIter时，函数结束运行并退出。 每次循环当中，将alphaParirsChanged先设为0，然后再对整个集合顺序遍历。变量alphaPairsChanged用于记录alpha是否已经进行优化。 首先，fxi能够计算出来，这就是我们预测的类别。然后，基于这个实例的预测结果和真是结果的比对，就可以计算误差Ei。如果误差很大，那么可以对该数据实例所对应的alpha值进行优化。在if语句中，不管是正间隔还是负间隔都会被测试。并且在该if语句中，也要同时检查alpha值，以保证其不能等于0或C。由于后面alpha小于0或大于C时将调整为0或C，所以一旦在该if语句中它们等于这两个值得话，那么它们就已经在“边界”上了，因而不再能够减少或增大，因此也就不值得再对它们进行优化了。 接下来，利用辅助函数来随机选择第二个alpha值，即alpha[j]。同样，可以采用第一个alpha值（alpha[i]）的误差计算方法，来计算这个alpha值得误差。这个过程可以通过copy（）的方法来实现，因此稍后可以将新的alpha值与老的alpha值进行比较。之后开始计算L和H，它们用于将alpha[j]调整到0到C之间。如果L和H相等，就不做任何改变，直接执行continue语句。 Eta是alpha[j]的最优修改量，如果eta为0，那就是说需要退出for循环的当前迭代过程。该过程对真实SMO算法进行了简化处理。于是，可以计算出一个新的alpha[j]，然后利用辅助函数以及L和H值对其进行调整。 然后，就是检查alpha[j]是否轻微改变。如果是的话，就退出for循环。然后alpha[i]和alpha[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反（即如果一个增加，那么另外一个减少）。在对alpha[i]和alpha[j]进行优化之后，给这两个alpha值设置一个常数项b。 最后，在优化过程结束的同时，必须确保在合适的时机结束循环。如果程序执行到for循环的最后一行都不执行continue语句，那么就已经成功地改变了一对alpha，同时可以增加alphaPairsChanged的值。在for循环之外，需要检查alpha值是否做了更新，如果有更新则将iter设为0后继续运行程序。只有在所有数据集上遍历maxIter次，且不再发生任何alpha修规之后，程序才会停止并退出while循环。 利用完整Platt SMO算法加速优化在这两个版本中，实现alpha的更改和代数运算的优化环节一模一样。在优化过程中，唯一的不同就是选择alpha的方式。完整版的Platt SMO算法应用了一些能够提速的启发方法。 Platt SMO 算法是通过一个外循环来选择第一个alpha值的，并且其选择过程会在两种方式之间进行交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界alpha中实现单遍扫描。而所谓非边界alpha指的就是那些不等于边界0或C的alpha值。对整个数据集的扫描相当容易，而实现非边界alpha值的扫描时，首先需要建立这些alpha值的列表，然后再对这个表进行遍历。同时，该步骤会跳过那些已知的不会改变的alpha值。 在选择第一个alpha值后，算法会通过一个内循环来选择第二个alpha值 。在优化过程中，会通过最大化步长的方式来获得第二个alpha值。在简化版SMO算法中，我们会在选择j之后计算错误率E_j。但在这里，我们会建立一个全局的缓存用于保存误差值，并从中选择使得步长或者说E_i-E_j最大的alpha值。 完整Platt SMO的辅助函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # Initialize the structure with the parameters self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m, 1))) self.b = 0 self.eCache = mat(zeros((self.m, 2))) # first column is valid flag self.K = mat(zeros((self.m, self.m))) for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)def calcEk(oS, k): fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ekdef selectJ(i, oS, Ei): # this is the second choice -heurstic, and calcs Ej maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] # set valid #choose the alpha that gives the maximum delta E validEcacheList = nonzero(oS.eCache[:, 0].A)[0] if (len(validEcacheList)) &gt; 1: for k in validEcacheList: # loop through valid Ecache values and find the one that maximizes delta E if k == i: continue # don't calc for i, waste of time Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE &gt; maxDeltaE): maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: # in this case (first time around) we don't have any valid eCache values j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ejdef updateEk(oS, k): # after any alpha has changed update the new value in the cache Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek] 首要的事情就是建立一个数据结构来保存所有的重要值，而这个过程可以通过一个对象来完成。这里使用对象的目的并不是为了面向对象的编程，而只是作为一个数据结构来使用对象。在将值传给函数时，我们可以通过将所有数据移到一个结构中来实现，这样就可以省掉手工输入的麻烦了。而此时，数据就可以通过一个对象来进行传递。实际上，当完成其实现时，可以很容易通过Python的字典来完成。但是在访问对象成员变量时，这样做会有更多的手工输入操作，对比一下myObject.X和myObject[‘X’]就可以知道这一点。为达到这个目的，需要构建一个仅包含init方法的optStruct类。该方法可以实现其成员变量的填充。除了增加一个m*2的矩阵成员变量eCache之外，这些做法和简化SMO一模一样。eCache的第一列给出的是eCache是否有效的标志位，而第二列给出的是实际的E值。 对于给定的alpha值，第一个辅助函数calcEk()能够计算E值并返回。以前，该过程是采用内嵌的方式来完成的，这里作为单独的函数。 下一个函数selectJ()用于选择第二个alpha或者说内循环的alpha值。目标是选择合适的第二个alpha值以保证在每次优化中采用最大步长。该函数的误差值与第一个alpha值E_i和下标i有关。首先将输入值E_i在缓存中设置成为有效的。这里的有效意味着它已经计算好了。在eCaohe中，代码nonzero(oS.eCache[:,0].A)[0]构建出了一个非零表。NumPy函数nonzero()返回了一个列表，而这个列表中包含以输入列表为目录的列表值。nonzero()语句返回的是非零E值所对应的alpha值，而不是E值本身。程序会在所有的值上进行循环并选择其中使得改变最大的那个值。如果这是第一次循环的话，那么就随机选择一个alpha值。 最后一个辅助函数updateEk(),它会计算误差值并存入缓存当中。在对alpha值进行优化之后会用到这个值。 完整Platt SMO算法中的优化部分123456789101112131415161718192021222324252627282930313233343536373839404142def innerL(i, oS): Ei = calcEk(oS, i) if ((oS.labelMat[i] * Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ( (oS.labelMat[i] * Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): j, Ej = selectJ(i, oS, Ei) # this has been changed from selectJrand alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print "L==H" return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # changed for kernel if eta &gt;= 0: print "eta&gt;=0" return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) updateEk(oS, j) # added this for the Ecache if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print "j not moving enough" return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # update i by the same amount as j updateEk(oS, i) # added this for the Ecache #the update is in the oppostie direction b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1 elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 使用SelectJ()而不是selectJrand()来选择第二个alpha值。最后，在alpha值改变时更新Ecache. 完整版Platt SMO的外循环代码12345678910111213141516171819202122232425def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # full Platt SMO oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) iter = 0 entireSet = True alphaPairsChanged = 0 while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: # go over all for i in range(oS.m): alphaPairsChanged += innerL(i, oS) print "fullSet, iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) iter += 1 else: # go over non-bound (railed) alphas nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0] for i in nonBoundIs: alphaPairsChanged += innerL(i, oS) print "non-bound, iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) iter += 1 if entireSet: entireSet = False # toggle entire set loop elif (alphaPairsChanged == 0): entireSet = True print "iteration number: %d" % iter return oS.b, oS.alphas 完整版的Platt SMO算法，其输入和函数smoSimple()完全一样。函数一开始构建一个数据结构来容纳所有的数据，然后需要对控制函数退出的一些变量进行初始化。整个代码的主体是while循环，这与smosimple（）有些类似，但是这里的循环退出条件更多一些。当迭代次数超过指定的最大值，或者遍历整个集合都未对任意alpha对进行修改时，就退出循环。这里的maxIter变量和函数smoSimple()中的作用有一点不同，后者当没有任何alpha发生改变时会将整个集合的一次遍历过程记成一次迭代，而这里的一次迭代定义为一次循环过程，而不管该循环具体做了什么事。此时，如果在优化过程中存在波动就会停止，因此这里的做法优于SMOSimple()函数中的计数方法。 while循环的内部与smoSimple()中有所不同，一开始的for循环在数据集上遍历任意可能的alpha。我们通过调用innerL()来选择第二个alpha，并在可能时对其进行优化处理。如果有任意一对alpha值发生改变，那么会返回1.第二个for循环遍历所有的非边界alpha值，也就是不在边界0或C上的值。 核函数SVM优化中一个特别好的地方就是，所有的运算都可以写成内积的形式。可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为核技巧(kernel trick)。 径向基核函数 核转换函数123456789101112131415161718192021222324252627282930def kernelTrans(X, A, kTup): # calc the kernel or transform data to a higher dimensional space m, n = shape(X) K = mat(zeros((m, 1))) if kTup[0] == 'lin': K = X * A.T # linear kernel elif kTup[0] == 'rbf': for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # divide in NumPy is element-wise not matrix like Matlab else: raise NameError('Houston We Have a Problem -- \ That Kernel is not recognized') return Kclass optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # Initialize the structure with the parameters self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m, 1))) self.b = 0 self.eCache = mat(zeros((self.m, 2))) # first column is valid flag self.K = mat(zeros((self.m, self.m))) for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup) 其中kTup是一个包含核函数信息的元组。元组的第一个参数是描述所用核函数类型的一个字符串，其他2个参数则都是核函数可能需要的可选参数。在初始化方法结束时，矩阵k先被构建，然后再通过调用函数kernelTrans()进行填充。全局的k值只需计算一次。然后，当想要使用核函数时，就可以对它进行调用。 via 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸LogisticRegression]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8LogisticRegression%2F</url>
    <content type="text"><![CDATA[LogisticRegression算法概述：利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。 优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 适用数据类型：数值型和标称型数据。 梯度下降法求最佳参数伪代码： 每个回归系数初始化为1 重复R次： a.计算整个数据集的梯度 b.使用alpha*gradient更新回归系数的向量 c.返回回归系数 梯度下降法求最佳参数代码实现：123456789101112131415161718192021222324# 定义sigmoid函数：def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 梯度下降法def gradAscent(dataMatIn, classLabels): """ 利用梯度下降法求解逻辑回归系数 :param dataMatIn:2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本 :param classLabels: :return: 返回训练好的回归系数 """ dataMatrix = mat(dataMatIn) # convert to NumPy matrix labelMat = mat(classLabels).transpose() # convert to NumPy matrix m, n = shape(dataMatrix) alpha = 0.001 # 步长 maxCycles = 500 # 迭代次数 weights = ones((n, 1)) for k in range(maxCycles): # heavy on matrix operations h = sigmoid(dataMatrix * weights) # matrix mult error = (labelMat - h) # vector subtraction weights = weights + alpha * dataMatrix.transpose() * error # matrix mult return weights 随机梯度下降法求最佳参数伪代码： 随机梯度下降法求最佳参数代码实现：12345678910def stocGradAscent(dataMatrix, classLabels): m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) # initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i] * weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights mini-batch梯度下降法求最佳参数代码实现：1234567891011121314def batchGradAscent(dataMatrix, classLabels, numIter=150): m, n = shape(dataMatrix) weights = ones(n) # initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4 / (1.0 + j + i) + 0.0001 # apha decreases with iteration, does not randIndex = int(random.uniform(0, len(dataIndex))) # go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex] * weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del (dataIndex[randIndex]) return weights LogisticRegression小结Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度下降算法。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸DecisionTree]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8DecisionTree%2F</url>
    <content type="text"><![CDATA[DecisionTree算法概述： 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配问题。 适用数据范围：数值型和标称型 DecisionTree伪代码： DecisionTree代码实现：创建数据集12345678910111213from math import logimport operatordef createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] # change to discrete values return dataSet, labels 计算给定数据集的香农熵123456789101112131415def calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = &#123;&#125; # 为所有可能分类创建字典 # 循环遍历数据集的每一行 for featVec in dataSet: # the the number of unique elements and their occurance currentLabel = featVec[-1] # 假设最后一列是label if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob, 2) # log base 2 return shannonEnt 按照给定特征划分数据集123456789def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: # 只能筛选离散取值的变量 reducedFeatVec = featVec[:axis] # chop out axis used for splitting reducedFeatVec.extend(featVec[axis + 1:]) retDataSet.append(reducedFeatVec) return retDataSet 选择最好的数据集划分方式123456789101112131415161718192021222324252627def chooseBestFeatureToSplit(dataSet): """ 选择最好的数据集划分方式 :param dataSet: :return: 最佳分割变量所在的下标 """ numFeatures = len(dataSet[0]) - 1 # the last column is used for the labels baseEntropy = calcShannonEnt(dataSet) #计算分割之前的熵 bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): # iterate over all the features # 笨拙的筛选数据集的某一列 # create a list of all the examples of this feature featList = [example[i] for example in dataSet] # get a set of unique values uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: # 遍历所有取值作为可能的分割点,每个离散值作为一个单独的分组 subDataSet = splitDataSet(dataSet, i, value) # 只能筛选离散取值的变量 prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # calculate the info gain; ie reduction in entropy if (infoGain &gt; bestInfoGain): # compare this to the best gain so far bestInfoGain = infoGain # if better than current best, set to best bestFeature = i return bestFeature # returns an integer 计算出现次数最多的分类：12345678910111213def majorityCnt(classList): """ 采用多数表决得方式确定该叶子节点的分类 :param classList: 叶子节点的所有样本标签 :return: 投票的最终类别 """ classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 创建树的函数代码12345678910111213141516171819def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 标签列 if classList.count(classList[0]) == len(classList): return classList[0] # stop splitting when all of the classes are equal if len(dataSet[0]) == 1: # stop splitting when there are no more features in dataSet return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 最佳分割变量的下标 bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel: &#123;&#125;&#125; del (labels[bestFeat]) # 从候选集中删除当前分割变量 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # copy all of labels, so trees don't mess up existing labels # 在当前节点的所有可能取值上递归继续分割 # 字典的key:特征名称、特征取值；字典的value: 字典本身 或者 叶节点的类别名称 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTree 使用决策树进行分类：12345678910111213141516171819202122def classify(inputTree, featLabels, testVec): """ 递归决策树字典，输出所在叶节点的类别 :param inputTree: 树结构以字典的形式存储 :param featLabels: 特征列表 :param testVec: 待预测的数据集 :return: 节点类别 """ firstStr = inputTree.keys()[0] secondDict = inputTree[firstStr] # 将标签字符串转换为索引 featIndex = featLabels.index(firstStr) key = testVec[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): # 如果当前节点的value仍是一个dict则继续递归 classLabel = classify(valueOfFeat, featLabels, testVec) else: # 叶节点 classLabel = valueOfFeat return classLabel 示例123dataSet, labels = createDataSet()clf_tree = createTree(dataSet, labels) 输出1234clf_treeOut[5]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; DecisionTree小结构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用Python语言内嵌的数据结构字典存储树节点信息。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸kNN]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8kNN%2F</url>
    <content type="text"><![CDATA[k近邻算法概述：k-近邻算法采用测量不同特征值之间的距离方法进行分类。 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型 kNN伪代码：对未知类别属性的数据集中的每个点依次执行以下操作： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点出现频率最高的类别作为当前点的预测分类。 kNN代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from numpy import *import operatorfrom os import listdirdef createDataSet(): """ 创建数据集 :return:特征数据 array，标注数据 list """ group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labelsdef autoNorm(dataSet): # 参数0使得函数可以从列中选取最小值，而不是选取当前行的最小值 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m, 1)) # element wise divide 矩阵数值相除 normDataSet = normDataSet / tile(ranges, (m, 1)) return normDataSet, ranges, minValsdef knn_classifier(inX, dataSet, labels, k): """ k-近邻算法 :param inX:待分类的输入向量 :param dataSet:训练样本集 :param labels:标签向量 :param k:用于选择最近邻居的数目 :return:k个近邻投票的最终类别 """ dataSetSize = dataSet.shape[0] # 计算两个向量点的距离 # tile: 复制输出，将变量内容复制成输入矩阵同样大小的矩阵 diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() # 选择距离最小的k个点 classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # 排序 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] kNN小结: k-近邻算法是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。 k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。 此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。 k-近邻算法的另一缺陷是他无法给出任何数据的基础结构信息，因此我们无法知晓平均实例样本和典型实例样本具有什么特征。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NumPy</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clustering with sklearn]]></title>
    <url>%2F2017%2F11%2F15%2FClustering%20with%20sklearn%2F</url>
    <content type="text"><![CDATA[生成聚类数据集的方法生成数据集方法：sklearn.datasets.make_blobs(n_samples,n_featurs,centers)可以生成数据集,n_samples表示个数，n_features表示特征个数，centers表示y的种类数: make_blobs函数是为聚类产生数据集产生一个数据集和相应的标签 n_samples:表示数据样本点个数,默认值100 n_features:表示数据的维度，默认值是2 centers:产生数据的中心点，默认值3 cluster_std：数据集的标准差，浮点数或者浮点数序列，默认值1.0 center_box：中心确定之后的数据边界，默认值(-10.0, 10.0) shuffle ：洗乱，默认值是True random_state:官网解释是随机生成器的种子 y3 = np.array([0]100 + [1]50 + [2]20 + [3]5)可以这样建立array数组 k-means对于方差不相等和数据与坐标轴不平行时效果不理想；对于数据大小量纲敏感。 The Influence of Data Distribution on KMeans Clustering123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import KMeansdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 400centers = 4data, y = ds.make_blobs(N, n_features=2, centers=centers, random_state=2)data2, y2 = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=(1,2.5,0.5,2), random_state=2)data3 = np.vstack((data[y == 0][:], data[y == 1][:50], data[y == 2][:20], data[y == 3][:5]))y3 = np.array([0] * 100 + [1] * 50 + [2] * 20 + [3] * 5)cls = KMeans(n_clusters=4, init='k-means++')y_hat = cls.fit_predict(data)y2_hat = cls.fit_predict(data2)y3_hat = cls.fit_predict(data3)m = np.array(((1, 1), (1, 3)))data_r = data.dot(m)y_r_hat = cls.fit_predict(data_r)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falsecm = matplotlib.colors.ListedColormap(list('rgbm'))plt.figure(figsize=(9, 10), facecolor='w')plt.subplot(421)plt.title(u'Raw data')plt.scatter(data[:, 0], data[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data, axis=0)x1_max, x2_max = np.max(data, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(422)plt.title(u'KMeans++ clustering')plt.scatter(data[:, 0], data[:, 1], c=y_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(423)plt.title(u'Data after rotation')plt.scatter(data_r[:, 0], data_r[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data_r, axis=0)x1_max, x2_max = np.max(data_r, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(424)plt.title(u'Data rotated KMeans++ clustering')plt.scatter(data_r[:, 0], data_r[:, 1], c=y_r_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(425)plt.title(u'Unequal variance data')plt.scatter(data2[:, 0], data2[:, 1], c=y2, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data2, axis=0)x1_max, x2_max = np.max(data2, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(426)plt.title(u'Data with unequal variance KMeans++ clustering')plt.scatter(data2[:, 0], data2[:, 1], c=y2_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(427)plt.title(u'Data with unequal numbers')plt.scatter(data3[:, 0], data3[:, 1], s=30, c=y3, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data3, axis=0)x1_max, x2_max = np.max(data3, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(428)plt.title(u'Data with unequal numbers KMeans++ clustering')plt.scatter(data3[:, 0], data3[:, 1], c=y3_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.tight_layout(2)plt.suptitle(u'The Influence of Data Distribution on KMeans Clustering', fontsize=18)# https://github.com/matplotlib/matplotlib/issues/829plt.subplots_adjust(top=0.92)plt.show() 输出 聚类性能的评价指标 均一性sklearn.metrics.homogeneity_score 完整性sklearn.metrics.completeness_score 均一性完整性二者的加权平均v_measure_score ARI（Adjusted Rand index(调整兰德指数)：sklearn.metrics.adjusted_rand_score AMI: sklearn.metrics.adjusted_mutual_info_score 关于指标的定义请看这里 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn import metricsy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cv2 = 2 * c * h / (c + h)v = metrics.v_measure_score(y, y_hat)print u'V-Measure：', v2, vy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 2, 3, 3]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', v# 允许不同值y = [0, 0, 0, 1, 1, 1]y_hat = [1, 1, 1, 0, 0, 0]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', vy = [0, 0, 1, 1]y_hat = [0, 1, 0, 1]ari = metrics.adjusted_rand_score(y, y_hat)print ariy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]ari = metrics.adjusted_rand_score(y, y_hat)print ari 输出123456789101112同一性(Homogeneity)： 0.666666666667完整性(Completeness)： 0.420619835714V-Measure： 0.515803742979 0.515803742979同一性(Homogeneity)： 1.0完整性(Completeness)： 0.521296028614V-Measure： 0.685331478962同一性(Homogeneity)： 1.0完整性(Completeness)： 1.0V-Measure： 1.0-0.50.242424242424 AffinityPropagation12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import AffinityPropagationfrom sklearn.metrics import euclidean_distancesN = 400centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)m = euclidean_distances(data, squared=True)preference = -np.median(m)print 'Preference：', preferencematplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 9), facecolor='w')for i, mul in enumerate(np.linspace(1, 4, 9)): print mul p = mul * preference model = AffinityPropagation(affinity='euclidean', preference=p) af = model.fit(data) center_indices = af.cluster_centers_indices_ n_clusters = len(center_indices) print ('p = %.1f' % mul), p, '聚类簇的个数为：', n_clusters y_hat = af.labels_ plt.subplot(3, 3, i+1) plt.title(u'Preference：%.2f，簇个数：%d' % (p, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') center = data[center_indices[k]] for x in data[cur]: plt.plot([x[0], center[0]], [x[1], center[1]], color=clr, zorder=1) plt.scatter(data[center_indices, 0], data[center_indices, 1], s=100, c=clrs, marker='*', edgecolors='k', zorder=2) plt.grid(True)plt.tight_layout()plt.suptitle(u'AP聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出1234567891011121314151617181920Preference： -5.299145530341.0p = 1.0 -5.29914553034 聚类簇的个数为： 161.375p = 1.4 -7.28632510422 聚类簇的个数为： 121.75p = 1.8 -9.27350467809 聚类簇的个数为： 112.125p = 2.1 -11.260684252 聚类簇的个数为： 102.5p = 2.5 -13.2478638258 聚类簇的个数为： 82.875p = 2.9 -15.2350433997 聚类簇的个数为： 83.25p = 3.2 -17.2222229736 聚类簇的个数为： 753.625p = 3.6 -19.2094025475 聚类簇的个数为： 74.0p = 4.0 -21.1965821214 聚类簇的个数为： 7 MeanShift Clustering12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import MeanShiftfrom sklearn.metrics import euclidean_distancesN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 9), facecolor='w')m = euclidean_distances(data, squared=True)bw = np.median(m)print bwfor i, mul in enumerate(np.linspace(0.1, 0.4, 4)): band_width = mul * bw model = MeanShift(bin_seeding=True, bandwidth=band_width) ms = model.fit(data) centers = ms.cluster_centers_ y_hat = ms.labels_ n_clusters = np.unique(y_hat).size print '带宽：', mul, band_width, '聚类簇的个数为：', n_clusters plt.subplot(2, 2, i+1) plt.title(u'带宽：%.2f，聚类簇的个数为：%d' % (band_width, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) print clrs for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') plt.scatter(centers[:, 0], centers[:, 1], s=150, c=clrs, marker='*', edgecolors='k') plt.grid(True)plt.tight_layout(2)plt.suptitle(u'MeanShift聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出123456789105.31661129692带宽： 0.1 0.531661129692 聚类簇的个数为： 7['#ff0000', '#d4802a', '#aa0055', '#7f807f', '#5500aa', '#2a80d4', '#0000ff']带宽： 0.2 1.06332225938 聚类簇的个数为： 4['#ff0000', '#aa0055', '#5500aa', '#0000ff']带宽： 0.3 1.59498338907 聚类簇的个数为： 3['#ff0000', '#7f807f', '#0000ff']带宽： 0.4 2.12664451877 聚类簇的个数为： 1['#ff0000'] DBSCAN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import DBSCANfrom sklearn.preprocessing import StandardScalerdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)data = StandardScaler().fit_transform(data)params = ((0.2, 5), (0.2, 10), (0.2, 15), (0.3, 5), (0.3, 10), (0.3, 15))matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'DBSCAN聚类', fontsize=20)for i in range(6): eps, min_samples = params[i] model = DBSCAN(eps=eps, min_samples=min_samples) model.fit(data) y_hat = model.labels_ core_indices = np.zeros_like(y_hat, dtype=bool) core_indices[model.core_sample_indices_] = True y_unique = np.unique(y_hat) n_clusters = y_unique.size - (1 if -1 in y_hat else 0) print y_unique, '聚类簇的个数为：', n_clusters # clrs = [] # for c in np.linspace(16711680, 255, y_unique.size): # clrs.append('#%06x' % c) plt.subplot(2, 3, i+1) clrs = plt.cm.Spectral(np.linspace(0, 0.8, y_unique.size)) for k, clr in zip(y_unique, clrs): cur = (y_hat == k) if k == -1: plt.scatter(data[cur, 0], data[cur, 1], s=20, c='k') continue plt.scatter(data[cur, 0], data[cur, 1], s=30, c=clr, edgecolors='k') plt.scatter(data[cur &amp; core_indices][:, 0], data[cur &amp; core_indices][:, 1], s=60, c=clr, marker='o', edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\epsilon$ = %.1f m = %d，聚类数目：%d' % (eps, min_samples, n_clusters), fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出1234567[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3 4] 聚类簇的个数为： 5[-1 0] 聚类簇的个数为： 1[-1 0 1] 聚类簇的个数为： 2[-1 0 1 2 3] 聚类簇的个数为： 4 谱聚类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import spectral_clusteringfrom sklearn.metrics import euclidean_distancesdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dmatplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falset = np.arange(0, 2*np.pi, 0.1)data1 = np.vstack((np.cos(t), np.sin(t))).Tdata2 = np.vstack((2*np.cos(t), 2*np.sin(t))).Tdata3 = np.vstack((3*np.cos(t), 3*np.sin(t))).Tdata = np.vstack((data1, data2, data3))n_clusters = 3m = euclidean_distances(data, squared=True)sigma = np.median(m)plt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'谱聚类', fontsize=20)clrs = plt.cm.Spectral(np.linspace(0, 0.8, n_clusters))for i, s in enumerate(np.logspace(-2, 0, 6)): print s af = np.exp(-m ** 2 / (s ** 2)) + 1e-6 y_hat = spectral_clustering(af, n_clusters=n_clusters, assign_labels='kmeans', random_state=1) plt.subplot(2, 3, i+1) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], s=40, c=clr, edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\sigma$ = %.2f' % s, fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>make_blobs</tag>
        <tag>euclidean_distances</tag>
        <tag>KMeans</tag>
        <tag>KMeans++</tag>
        <tag>homogeneity_score</tag>
        <tag>completeness_score</tag>
        <tag>v_measure_score</tag>
        <tag>adjusted_rand_score</tag>
        <tag>AffinityPropagation</tag>
        <tag>MeanShift</tag>
        <tag>spectral_clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.svm]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.svm%2F</url>
    <content type="text"><![CDATA[SVC:ovr123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import numpy as npfrom sklearn import svmfrom sklearn.model_selection import train_test_splitimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.datasets import load_irisiris = load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b, tip): acc = a.ravel() == b.ravel() print tip + '正确率：', np.mean(acc)# 分类器# clf = svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr')# ‘ovo’ 一对一, ‘ovr’ 多对多 or None 无, default=Noneclf = svm.SVC(C=0.8, kernel='rbf', gamma=20, decision_function_shape='ovr')clf.fit(x_train, y_train.ravel())# 准确率# score(X, y, sample_weight=None) ：Returns the mean accuracy on the given test data and labels.print clf.score(x_train, y_train) # 精度y_hat = clf.predict(x_train)show_accuracy(y_hat, y_train, '训练集')print clf.score(x_test, y_test)y_hat = clf.predict(x_test)show_accuracy(y_hat, y_test, '测试集')# decision_functionprint 'decision_function:\n', clf.decision_function(x_train)print '\npredict:\n', clf.predict(x_train)"""# decision_function(): Distance of the samples X to the separating hyperplane.# 分类归属于距离数值最小的类别，距离有正有负只是平面的两侧而已decision_function:[[-0.25631211 0.80529378 2.45101833] [ 2.35232967 0.82494155 -0.17727121] [ 2.45418203 0.77495649 -0.22913852] [ 2.45421283 0.77518383 -0.22939666] [-0.24854074 2.39614366 0.85239708] [ 2.46621083 0.76927647 -0.23548729] [ 2.45410944 0.77775367 -0.2318631 ] [-0.24677667 0.84905678 2.39771989] [-0.46050688 1.21154683 2.24896005] [-0.26893412 0.80449581 2.46443831]predict:[2 0 0 0 1 0 0 2 2 2"""# 画图x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点# numpy.ndarray.flat : A 1-D iterator over the array.# This is a numpy.flatiter instance, which acts similarly to,# but is not a subclass of, Python’s built-in iterator object.grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点# print 'grid_test = \n', grid_test# Z = clf.decision_function(grid_test) # 样本到决策面的距离# print Zgrid_hat = clf.predict(grid_test) # 预测分类值print grid_hatgrid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本plt.scatter(x_test[:, 0], x_test[:, 1], s=120, facecolors='none', zorder=10) # 圈中测试集样本plt.xlabel(iris_feature[0], fontsize=13)plt.ylabel(iris_feature[1], fontsize=13)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.title(u'鸢尾花SVM二特征分类', fontsize=15)plt.grid()plt.show() 输出： SVC:ovo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npfrom sklearn import svmfrom scipy import statsfrom sklearn.metrics import accuracy_scoreimport matplotlib as mplimport matplotlib.pyplot as pltdef extend(a, b, r): x = a - b m = (a + b) / 2 return m-r*x/2, m+r*x/2np.random.seed(0)N = 20x = np.empty((4*N, 2))means = [(-1, 1), (1, 1), (1, -1), (-1, -1)]sigmas = [np.eye(2), 2*np.eye(2), np.diag((1,2)), np.array(((2,1),(1,2)))]for i in range(4): # scipy.stats.multivariate_normal: # A multivariate normal random variable. # The mean keyword specifies the mean. The cov keyword specifies the covariance matrix. mn = stats.multivariate_normal(means[i], sigmas[i]*0.3) # rvs(mean=None, cov=1) Draw random samples from a multivariate normal distribution. x[i*N:(i+1)*N, :] = mn.rvs(N)a = np.array((0,1,2,3)).reshape((-1, 1))y = np.tile(a, N).flatten()clf = svm.SVC(C=1, kernel='rbf', gamma=1, decision_function_shape='ovo')# clf = svm.SVC(C=1, kernel='linear', decision_function_shape='ovr')clf.fit(x, y)y_hat = clf.predict(x)acc = accuracy_score(y, y_hat)np.set_printoptions(suppress=True)print u'预测正确的样本个数：%d，正确率：%.2f%%' % (round(acc*4*N), 100*acc)# decision_functionprint clf.decision_function(x)# print y_hatx1_min, x2_min = np.min(x, axis=0)x1_max, x2_max = np.max(x, axis=0)x1_min, x1_max = extend(x1_min, x1_max, 1.05)x2_min, x2_max = extend(x2_min, x2_max, 1.05)x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j]x_test = np.stack((x1.flat, x2.flat), axis=1)y_test = clf.predict(x_test)y_test = y_test.reshape(x1.shape)cm_light = mpl.colors.ListedColormap(['#FF8080', '#A0FFA0', '#6060FF', '#F080F0'])cm_dark = mpl.colors.ListedColormap(['r', 'g', 'b', 'm'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_test, cmap=cm_light)plt.scatter(x[:, 0], x[:, 1], s=40, c=y, cmap=cm_dark, alpha=0.7)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(b=True)plt.tight_layout(pad=2.5)plt.title(u'SVM多分类方法：One/One or One/Other', fontsize=18)plt.show() 输出： 参数选择:linear/rbf,c,gamma1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_irisimport numpy as npfrom sklearn import svmimport matplotlib as mplimport matplotlib.colorsimport matplotlib.pyplot as pltiris = load_iris()X = iris.datay = iris.targetX = X[y != 0, :2]y = y[y != 0]x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)# 分类器clf_param = (('linear', 0.1), ('linear', 0.5), ('linear', 1), ('linear', 2), ('rbf', 1, 0.1), ('rbf', 1, 1), ('rbf', 1, 10), ('rbf', 1, 100), ('rbf', 5, 0.1), ('rbf', 5, 1), ('rbf', 5, 10), ('rbf', 5, 100))x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = mpl.colors.ListedColormap(['#77E0A0', '#FFA0A0'])cm_dark = mpl.colors.ListedColormap(['g', 'r'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(14, 10), facecolor='w')for i, param in enumerate(clf_param): clf = svm.SVC(C=param[1], kernel=param[0]) if param[0] == 'rbf': clf.gamma = param[2] title = u'高斯核，C=%.1f，$\gamma$ =%.1f' % (param[1], param[2]) else: title = u'线性核，C=%.1f' % param[1] clf.fit(X, y) y_hat = clf.predict(X) show_accuracy(y_hat, y) # 准确率 # 画图 print title print '支撑向量的数目：', clf.n_support_ print '支撑向量的系数：', clf.dual_coef_ print '支撑向量：', clf.support_ plt.subplot(3, 4, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=40, cmap=cm_dark) # 样本的显示 plt.scatter(X[clf.support_, 0], X[clf.support_, 1], edgecolors='k', facecolors='none', s=100, marker='o') # 支撑向量 z = clf.decision_function(grid_test) # print 'z = \n', z z = z.reshape(x1.shape) plt.contour(x1, x2, z, colors=list('kmrmk'), linestyles=['--', '-.', '-', '-.', '--'], linewidths=[1, 0.5, 1.5, 0.5, 1], levels=[-1, -0.5, 0, 0.5, 1]) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(title, fontsize=14)plt.suptitle(u'SVM不同参数的分类', fontsize=20)plt.tight_layout(1.4)plt.subplots_adjust(top=0.92)plt.savefig('1.png')plt.show() 输出： sklearn.metrics模型评价指标：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import precision_score, recall_score, f1_score, fbeta_scorefrom sklearn.metrics import precision_recall_fscore_support, classification_reporty_true = np.array([1, 1, 1, 1, 0, 0])y_hat = np.array([1, 0, 1, 1, 1, 1])print 'Accuracy：\t', accuracy_score(y_true, y_hat)# The precision is the ratio 'tp / (tp + fp)' where 'tp' is the number of# true positives and 'fp' the number of false positives. The precision is# intuitively the ability of the classifier not to label as positive a sample# that is negative.# The best value is 1 and the worst value is 0.precision = precision_score(y_true, y_hat)print 'Precision:\t', precision# The recall is the ratio 'tp / (tp + fn)' where 'tp' is the number of# true positives and 'fn' the number of false negatives. The recall is# intuitively the ability of the classifier to find all the positive samples.# The best value is 1 and the worst value is 0.recall = recall_score(y_true, y_hat)print 'Recall: \t', recall# F1 score, also known as balanced F-score or F-measure# The F1 score can be interpreted as a weighted average of the precision and# recall, where an F1 score reaches its best value at 1 and worst score at 0.# The relative contribution of precision and recall to the F1 score are# equal. The formula for the F1 score is:# F1 = 2 * (precision * recall) / (precision + recall)print 'f1 score: \t', f1_score(y_true, y_hat)print 2 * (precision * recall) / (precision + recall)# The F-beta score is the weighted harmonic mean of precision and recall,# reaching its optimal value at 1 and its worst value at 0.# The 'beta' parameter determines the weight of precision in the combined# score. 'beta &lt; 1' lends more weight to precision, while 'beta &gt; 1'# favors recall ('beta -&gt; 0' considers only precision, 'beta -&gt; inf' only recall).print 'F-beta：'for beta in np.logspace(-3, 3, num=7, base=10): fbeta = fbeta_score(y_true, y_hat, beta=beta) print '\tbeta=%9.3f\tF-beta=%.5f' % (beta, fbeta) #print (1+beta**2)*precision*recall / (beta**2 * precision + recall)print precision_recall_fscore_support(y_true, y_hat, beta=1)print classification_report(y_true, y_hat) 输出：123456789101112131415161718192021Accuracy： 0.5Precision: 0.6Recall: 0.75f1 score: 0.6666666666670.666666666667F-beta： beta= 0.001 F-beta=0.60000 beta= 0.010 F-beta=0.60001 beta= 0.100 F-beta=0.60119 beta= 1.000 F-beta=0.66667 beta= 10.000 F-beta=0.74815 beta= 100.000 F-beta=0.74998 beta= 1000.000 F-beta=0.75000(array([ 0. , 0.6]), array([ 0. , 0.75]), array([ 0. , 0.66666667]), array([2, 4], dtype=int64)) precision recall f1-score support 0 0.00 0.00 0.00 2 1 0.60 0.75 0.67 4avg / total 0.40 0.50 0.44 6 样本不均衡问题：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import numpy as npfrom sklearn import svmimport matplotlib.colorsimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_scoreimport warningsdef show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)def show_recall(y, y_hat): # print y_hat[y == 1] print '召回率：%.2f%%' % (100 * float(np.sum(y_hat[y == 1] == 1)) / np.extract(y == 1, y).size)warnings.filterwarnings("ignore") # UndefinedMetricWarningnp.random.seed(0) # 保持每次生成的数据相同c1 = 990c2 = 10N = c1 + c2x_c1 = 3*np.random.randn(c1, 2)x_c2 = 0.5*np.random.randn(c2, 2) + (4, 4)x = np.vstack((x_c1, x_c2))y = np.ones(N)y[:c1] = -1# 显示大小s = np.ones(N) * 30s[:c1] = 10# 分类器clfs = [svm.SVC(C=1, kernel='linear'), svm.SVC(C=1, kernel='linear', class_weight=&#123;-1: 1, 1: 50&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 2&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 10&#125;)]titles = 'Linear', 'Linear, Weight=50', 'RBF, Weight=2', 'RBF, Weight=10'x1_min, x1_max = x[:, 0].min(), x[:, 0].max() # 第0列的范围x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = matplotlib.colors.ListedColormap(['#77E0A0', '#FF8080'])cm_dark = matplotlib.colors.ListedColormap(['g', 'r'])matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 8), facecolor='w')for i, clf in enumerate(clfs): clf.fit(x, y) y_hat = clf.predict(x) # show_accuracy(y_hat, y) # 正确率 # show_recall(y, y_hat) # 召回率 print i+1, '次：' print '正确率：\t', accuracy_score(y, y_hat) print ' 精度 ：\t', precision_score(y, y_hat, pos_label=1) print '召回率：\t', recall_score(y, y_hat, pos_label=1) print 'F1-score：\t', f1_score(y, y_hat, pos_label=1) print # 画图 plt.subplot(2, 2, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k', s=s, cmap=cm_dark) # 样本的显示 plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(titles[i]) plt.grid()plt.suptitle(u'不平衡数据的处理', fontsize=18)plt.tight_layout(1.5)plt.subplots_adjust(top=0.92)plt.show() 输出： 1234567891011121314151617181920211 次：正确率： 0.99 精度 ： 0.0召回率： 0.0F1-score： 0.02 次：正确率： 0.94 精度 ： 0.142857142857召回率： 1.0F1-score： 0.253 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.7692307692314 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.769230769231 SVR：123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npfrom sklearn import svmimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', yprint 'SVR - RBF'svr_rbf = svm.SVR(kernel='rbf', gamma=0.2, C=100)svr_rbf.fit(x, y)print 'SVR - Linear'svr_linear = svm.SVR(kernel='linear', C=100)svr_linear.fit(x, y)print 'SVR - Polynomial'svr_poly = svm.SVR(kernel='poly', degree=3, C=100)svr_poly.fit(x, y)print 'Fit OK.'x_test = np.linspace(x.min(), 1.1*x.max(), 100).reshape(-1, 1)y_rbf = svr_rbf.predict(x_test)y_linear = svr_linear.predict(x_test)y_poly = svr_poly.predict(x_test)plt.figure(figsize=(9, 8), facecolor='w')plt.plot(x_test, y_rbf, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x_test, y_linear, 'g-', linewidth=2, label='Linear Kernel')plt.plot(x_test, y_poly, 'b-', linewidth=2, label='Polynomial Kernel')plt.plot(x, y, 'mo', markersize=6)plt.scatter(x[svr_rbf.support_], y[svr_rbf.support_], s=130, c='r', marker='*', label='RBF Support Vectors')plt.legend(loc='lower left')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.tight_layout(2)plt.show() 输出： SVR GridSearchCV example:1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom sklearn import svmfrom sklearn.model_selection import GridSearchCV # 0.17 grid_searchimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', ymodel = svm.SVR(kernel='rbf')c_can = np.logspace(-2, 2, 10)gamma_can = np.logspace(-2, 2, 10)svr = GridSearchCV(model, param_grid=&#123;'C': c_can, 'gamma': gamma_can&#125;, cv=5)svr.fit(x, y)print '验证参数：\n', svr.best_params_x_test = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)y_hat = svr.predict(x_test)sp = svr.best_estimator_.support_plt.figure(facecolor='w')plt.scatter(x[sp], y[sp], s=120, c='r', marker='*', label='Support Vectors', zorder=3)plt.plot(x_test, y_hat, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x, y, 'go', markersize=5)plt.legend(loc='upper right')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.show() 输出： via SVM的两个参数 C 和 gamma]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>train_test_split</tag>
        <tag>GridSearchCV</tag>
        <tag>svm</tag>
        <tag>SVC</tag>
        <tag>SVR</tag>
        <tag>precision_score</tag>
        <tag>recall_score</tag>
        <tag>f1_score</tag>
        <tag>fbeta_score</tag>
        <tag>precision_recall_fscore_support</tag>
        <tag>classification_report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging]]></title>
    <url>%2F2017%2F11%2F14%2FBagging%2F</url>
    <content type="text"><![CDATA[引子假设一个分类器分类正确的概率为0.6，如果有n个独立的分类器，选择其中的k个采用投票的方式，作为预测结果，则预测正确的概率为： 123456789101112131415161718import operator# operator.__mul__(a, b)# Return a * b, for a and b numbers.def c(n, k): # 求组合数 return reduce(operator.mul, range(n-k+1, n+1)) / reduce(operator.mul, range(1, k+1))def bagging(n, p): s = 0 for i in range(n / 2 + 1, n + 1): s += c(n, i) * p ** i * (1 - p) ** (n - i) return sfor t in range(9, 100, 10): # 假设事件发生的概率为0.6 print t, '次采样正确率：', bagging(t, 0.6) 输出12345678910119 次采样正确率： 0.7334323219 次采样正确率： 0.81390797858529 次采样正确率： 0.86378705133639 次采样正确率： 0.89794136871149 次采样正确率： 0.92242443765259 次采样正确率： 0.94044799573269 次采样正确率： 0.95394975650579 次采样正确率： 0.96418969283989 次采样正确率： 0.97202751600799 次采样正确率： 0.97806955787 BaggingRegressor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn.linear_model import RidgeCVfrom sklearn.ensemble import BaggingRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesdef f(x): return 0.5*np.exp(-(x+3) **2) + np.exp(-x**2) + + 0.5*np.exp(-(x-3) ** 2)np.random.seed(0)N = 200x = np.random.rand(N) * 10 - 5 # [-5,5)x = np.sort(x)y = f(x) + 0.05*np.random.randn(N)x.shape = -1, 1ridge = RidgeCV(alphas=np.logspace(-3, 2, 10), fit_intercept=False)ridged = Pipeline([('poly', PolynomialFeatures(degree=10)), ('Ridge', ridge)])bagging_ridged = BaggingRegressor(ridged, n_estimators=100, max_samples=0.3)dtr = DecisionTreeRegressor(max_depth=5)regs = [ ('DecisionTree Regressor', dtr), ('Ridge Regressor(6 Degree)', ridged), ('Bagging Ridge(6 Degree)', bagging_ridged), ('Bagging DecisionTree Regressor', BaggingRegressor(dtr, n_estimators=100, max_samples=0.3))]x_test = np.linspace(1.1*x.min(), 1.1*x.max(), 1000)mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.plot(x, y, 'ro', label=u'训练数据')plt.plot(x_test, f(x_test), color='k', lw=3.5, label=u'真实值')clrs = 'bmyg'for i, (name, reg) in enumerate(regs): reg.fit(x, y) y_test = reg.predict(x_test.reshape(-1, 1)) plt.plot(x_test, y_test.ravel(), color=clrs[i], lw=i+1, label=name, zorder=6-i)plt.legend(loc='upper left')plt.xlabel('X', fontsize=15)plt.ylabel('Y', fontsize=15)plt.title(u'回归曲线拟合', fontsize=21)plt.ylim((-0.2, 1.2))plt.tight_layout(2)plt.grid(True)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>Bagging</tag>
        <tag>BaggingRegressor</tag>
        <tag>reduce</tag>
        <tag>DecisionTreeRegressor</tag>
        <tag>RidgeCV</tag>
        <tag>PolynomialFeatures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeRegressor]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.tree.DecisionTreeRegressor%2F</url>
    <content type="text"><![CDATA[导入包：1234import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressor 生成数据集：12345678N = 100x = np.random.rand(N) * 6 - 3 # [-3,3)x.sort()y = np.sin(x) + np.random.randn(N) * 0.05print yx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的print x DecisionTreeRegressor：12345678910reg = DecisionTreeRegressor(criterion='mse', max_depth=9)dt = reg.fit(x, y)x_test = np.linspace(-3, 3, 50).reshape(-1, 1)y_hat = dt.predict(x_test)plt.plot(x, y, 'ro', ms=6, label='Actual')plt.plot(x_test, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='upper left')plt.grid()plt.show() 比较决策树的深度影响：123456789101112131415161718depth = [2, 4, 6, 8, 10]clr = 'rgbmy'reg = [DecisionTreeRegressor(criterion='mse', max_depth=depth[0]), DecisionTreeRegressor(criterion='mse', max_depth=depth[1]), DecisionTreeRegressor(criterion='mse', max_depth=depth[2]), DecisionTreeRegressor(criterion='mse', max_depth=depth[3]), DecisionTreeRegressor(criterion='mse', max_depth=depth[4])]plt.plot(x, y, 'k^', linewidth=2, label='Actual')x_test = np.linspace(-3, 3, 50).reshape(-1, 1)for i, r in enumerate(reg): dt = r.fit(x, y) y_hat = dt.predict(x_test) plt.plot(x_test, y_hat, '-', color=clr[i], linewidth=2, label='Depth=%d' % depth[i])plt.legend(loc='upper left')plt.grid()plt.show() 多变量决策树回归：12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressorN = 400x = np.random.rand(N) * 8 - 4 # [-4,4)x.sort()y1 = np.sin(x) + 3 + np.random.randn(N) * 0.1y2 = np.cos(0.3*x) + np.random.randn(N) * 0.01y = np.vstack((y1, y2)).Tx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的deep = 5 # 树的深度reg = DecisionTreeRegressor(criterion='mse', max_depth=deep)dt = reg.fit(x, y)x_test = np.linspace(-4, 4, num=1000).reshape(-1, 1)print x_testy_hat = dt.predict(x_test)print y_hatplt.scatter(y[:, 0], y[:, 1], c='r', s=40, label='Actual')plt.scatter(y_hat[:, 0], y_hat[:, 1], c='g', marker='s', s=100, label='Depth=%d' % deep, alpha=1)plt.legend(loc='upper left')plt.xlabel('y1')plt.ylabel('y2')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>DecisionTreeRegressor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeClassifier]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.tree.DecisionTreeClassifier%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn import treefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pydotplus 导入数据集：1234567891011121314def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]from sklearn.datasets import load_irisiris=load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica' 决策树参数估计：123456# min_samples_split = 10：如果该结点包含的样本数目大于10，则(有可能)对其分支# min_samples_leaf = 10：若将某结点分支后，得到的每个子结点样本数目都大于10，则完成分支；否则，不进行分支model = DecisionTreeClassifier(criterion='entropy', max_depth=6)model = model.fit(x_train, y_train)y_test_hat = model.predict(x_test) # 测试数据 决策树保存1234567891011121314# 1、输出with open('iris.dot', 'w') as f: tree.export_graphviz(model, out_file=f)# 2、给定文件名# tree.export_graphviz(model, out_file='iris.dot')# 3、输出为pdf格式dot_data = tree.export_graphviz(model, out_file=None, feature_names=iris_feature_E, class_names=iris_class, filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf('iris.pdf')f = open('iris.png', 'wb')f.write(graph.create_png())f.close() 决策树画图1234567891011121314151617181920212223242526272829303132mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = FalseN, M = 50, 50 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_show = np.stack((x1.flat, x2.flat), axis=1) # 测试点print x_show.shapecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_show_hat = model.predict(x_show) # 预测值print y_show_hat.shapeprint y_show_haty_show_hat = y_show_hat.reshape(x1.shape) # 使之与输入的形状相同print y_show_hatplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light) # 预测值的显示print y_testprint y_test.ravel()plt.scatter(x_test[:, 0], x_test[:, 1], c=y_test.ravel(), edgecolors='k', s=120, cmap=cm_dark, marker='*') # 测试数据plt.scatter(x[:, 0], x[:, 1], c=y.ravel(), edgecolors='k', s=40, cmap=cm_dark) # 全部数据plt.xlabel(iris_feature[0], fontsize=15)plt.ylabel(iris_feature[1], fontsize=15)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid(True)plt.title(u'鸢尾花数据的决策树分类', fontsize=17)plt.show() 决策树树状图 决策分割面 预测1234567y_test = y_test.reshape(-1)print y_test_hatprint y_testresult = (y_test_hat == y_test) # True则预测正确，False则预测错误acc = np.mean(result)print '准确度: %.2f%%' % (100 * acc) 过拟合：错误率12345678910111213141516171819depth = np.arange(1, 15)err_list = []for d in depth: clf = DecisionTreeClassifier(criterion='entropy', max_depth=d) clf = clf.fit(x_train, y_train) y_test_hat = clf.predict(x_test) # 测试数据 result = (y_test_hat == y_test) # True则预测正确，False则预测错误 err = 1 - np.mean(result) err_list.append(err) # print d, ' 准确度: %.2f%%' % (100 * err) print d, ' 错误率: %.2f%%' % (100 * err)plt.figure(facecolor='w')plt.plot(depth, err_list, 'ro-', lw=2)plt.xlabel(u'决策树深度', fontsize=15)plt.ylabel(u'错误率', fontsize=15)plt.title(u'决策树深度与过拟合', fontsize=17)plt.grid(True)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>train_test_split</tag>
        <tag>DecisionTreeClassifier</tag>
        <tag>pydotplus</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插值]]></title>
    <url>%2F2017%2F11%2F13%2F%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[插值的定义插值法又称“内插法”，是利用函数f(x)在某区间中已知的若干点的函数值，作出适当的特定函数，在区间的其他点上用这特定函数的值作为函数f(x)的近似值，这种方法称为插值法。如果这特定函数是多项式，就称它为插值多项式。 插入法的拉丁文原意是“内部插入”，即在已知的函数表中，插入一些表中没有列出的、所需要的中间值。 若函数f(x)在自变数x一些离散值所对应的函数值为已知，则可以作一个适当的特定函数p(x)，使得p(x)在这些离散值所取的函数值，就是f(x)的已知值。从而可以用p(x)来估计f(x)在这些离散值之间的自变数所对应的函数值，这种方法称为插值法。 scipy的插值模块 Lagrange插值Lagrange插值是n次多项式插值，其成功地用构造插值基函数的 方法解决了求n次多项式插值函数问题。 ★基本思想 将待求的n次多项式插值函数pn(x）改写成另一种表示方式，再利用插值条件⑴确定其中的待定函数，从而求出插值多项式。 Newton插值Newton插值也是n次多项式插值，它提出另一种构造插值多项式的方法，与Lagrange插值相比，具有承袭性和易于变动节点的特点。 ★基本思想 将待求的n次插值多项式Pn（x）改写为具有承袭性的形式，然后利用插值条件⑴确定Pn（x）的待定系数，以求出所要的插值函数。 Hermite插值Hermite插值是利用未知函数f(x)在插值节点上的函数值及导数值来构造插值多项式的，其提法为：给定n+1个互异的节点x0,x1，……,xn上的函数值和导数值求一个2n+1次多项式H2n+1(x)满足插值条件 H2n+1(xk)=yk H’2n+1(xk)=y’k k=0,1,2，……，n ⒀ 如上求出的H2n+1(x）称为2n+1次Hermite插值函数，它与被插函数一般有更好的密合度. ★基本思想 利用Lagrange插值函数的构造方法，先设定函数形式，再利用插值条件⒀求出插值函数. 插值举例1234567891011121314151617181920212223242526272829303132#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poissonfrom scipy.interpolate import BarycentricInterpolatorfrom scipy.interpolate import CubicSpline x = np.random.poisson(lam=5, size=10000) print x pillar = 15 a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) plt.grid() # plt.show() print a print a[0].sum() rv = poisson(5) x1 = a[1] y1 = rv.pmf(x1) itp = BarycentricInterpolator(x1, y1) # 重心插值 x2 = np.linspace(x.min(), x.max(), 50) y2 = itp(x2) cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 plt.legend(loc='upper right') plt.grid() plt.show() 拟合与插值的区别插值曲线要过数据点，拟合曲线整体效果更好。 插值是指已知某函数的在若干离散点上的函数值或者导数信息，通过求解该函数中待定形式的插值函数以及待定系数，使得该函数在给定离散点上满足约束。 拟合是指已知某函数的若干离散函数值，通过调整该函数中若干待定系数，使得该函数与已知点集的差别（最小二乘意义）最小。如果待定函数是线性，就叫线性拟合或线性回归，否则叫非线性拟合或非线性回归。表达式也可以是分段函数，这种情况下叫样条拟合。 从几何意义上讲，拟合是给定了空间中的一些点，找到一个已知形式未知参数的连续曲面来最大限度地逼近这些点；而插值是找到一个（或几个分片光滑的）连续曲面来穿过这些点。 随着插值节点的增多，多项式次数也在增高，插值曲线在一些区域出现跳跃，并且越来越偏离原始曲线。 为了解决这个问题，人们发明了分段插值法。分段插值一般不会使用四次以上的多项式，而二次多项式会出现尖点，也是有问题的。所以就剩下线性和三次插值，最后使用最多的还是线性分段插值，这个好处是显而易见的。 via 百度百科-插值法 scipy.interpolate 知乎-拟合与插值的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>插值</tag>
        <tag>重心插值</tag>
        <tag>三次样条插值</tag>
        <tag>Hermite插值</tag>
        <tag>临近点插值</tag>
        <tag>线性插值</tag>
        <tag>Newton插值</tag>
        <tag>Lagrange插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticNetCV]]></title>
    <url>%2F2017%2F11%2F13%2FElasticNetCV%2F</url>
    <content type="text"><![CDATA[导入包：12345678910import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import ElasticNetCVimport sklearn.datasetsfrom sklearn.preprocessing import PolynomialFeatures, StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import mean_squared_error 导入数据集：1234567891011def not_empty(s): return s != ''data = sklearn.datasets.load_boston()x = np.array(data.data)y = np.array(data.target)print x.shapeprint y.shapex_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=0) 初始化模型Pipeline：1234567model = Pipeline([ ('ss', StandardScaler()), ('poly', PolynomialFeatures(degree=3, include_bias=True)), ('linear', ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.99, 1], alphas=np.logspace(-3, 2, 5), fit_intercept=False, max_iter=1e3, cv=3))]) 模型训练：123456model.fit(x_train, y_train.ravel())linear = model.get_params('linear')['linear']print u'超参数：', linear.alpha_print u'L1 ratio：', linear.l1_ratio_print u'系数：', linear.coef_.ravel() 模型预测：123456y_pred = model.predict(x_test)r2 = model.score(x_test, y_test)mse = mean_squared_error(y_test, y_pred)print 'R2:', r2print u'均方误差：', mse 模型效果图形化展示：12345678910111213t = np.arange(len(y_pred))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.plot(t, y_test.ravel(), 'r-', lw=2, label=u'真实值')plt.plot(t, y_pred, 'g-', lw=2, label=u'估计值')plt.legend(loc='best')plt.title(u'波士顿房价预测', fontsize=18)plt.xlabel(u'样本编号', fontsize=15)plt.ylabel(u'房屋价格', fontsize=15)plt.grid()plt.show() 输出1234567(506L, 13L)(506L,)超参数： 0.316227766017L1 ratio： 0.99R2: 0.772203419261均方误差： 18.9675965682]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>PolynomialFeatures</tag>
        <tag>linear_model</tag>
        <tag>ElasticNetCV</tag>
        <tag>ElasticNet</tag>
        <tag>StandardScaler</tag>
        <tag>train_test_split</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scipy.optimize使用]]></title>
    <url>%2F2017%2F11%2F13%2Fscipy.optimize%2F</url>
    <content type="text"><![CDATA[定义损失函数123456789101112131415161718192021222324252627#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npfrom scipy.optimize import leastsqimport matplotlib.pyplot as pltdef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return y 利用scipy.optimize求解线性回归1123456789101112131415x = np.linspace(-2, 2, 50)A, B, C = 2, 3, -1y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75t = leastsq(residual, [0, 0, 0], args=(x, y))theta = t[0]print '真实值：', A, B, Cprint '预测值：', thetay_hat = theta[0] * x ** 2 + theta[1] * x + theta[2]plt.plot(x, y, 'r-', linewidth=2, label=u'Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict')plt.legend(loc='upper left')plt.grid()plt.show() 利用scipy.optimize求解线性回归1输出图例 利用scipy.optimize求解非线性回归12345678910111213141516x = np.linspace(0, 5, 100)A = 5w = 1.5y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5t = leastsq(residual2, [3, 1], args=(x, y))theta = t[0]print '真实值：', A, wprint '预测值：', thetay_hat = theta[0] * np.sin(theta[1] * x)plt.plot(x, y, 'r-', linewidth=2, label='Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='lower left')plt.grid()plt.show() 利用scipy.optimize求解非线性回归输出图例 使用scipy计算函数极值1234567a = opt.fmin(f, 1)b = opt.fmin_cg(f, 1)c = opt.fmin_bfgs(f, 1)print a, 1/a, math.eprint bprint c 使用scipy计算函数极值输出123456789101112131415161718Optimization terminated successfully. Current function value: 0.692201 Iterations: 16 Function evaluations: 32Optimization terminated successfully. Current function value: 0.692201 Iterations: 4 Function evaluations: 30 Gradient evaluations: 10Optimization terminated successfully. Current function value: 0.692201 Iterations: 5 Function evaluations: 24 Gradient evaluations: 8[ 0.36787109] [ 2.71834351] 2.71828182846[ 0.36787948][ 0.36787942]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimize</tag>
        <tag>scipy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model.LogisticRegression]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model.LogisticRegression%2F</url>
    <content type="text"><![CDATA[导入数据集12345from sklearn import datasetsiris = datasets.load_iris()X = iris.data[:, :2]y = iris.target 数据处理与模型拟合Pipeline1234567891011121314import numpy as npfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegressionlr = Pipeline([('sc', StandardScaler()), ('clf', LogisticRegression()) ])lr.fit(X, y.ravel())y_hat = lr.predict(X)y_hat_prob = lr.predict_proba(X)np.set_printoptions(suppress=True)print 'y_hat = \n', y_hatprint 'y_hat_prob = \n', y_hat_probprint u'准确度：%.2f%%' % (100*np.mean(y_hat == y.ravel())) 绘图：分割面123456789101112131415161718192021222324252627import matplotlib.pyplot as pltimport matplotlib as mplN, M = 500, 500 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#77E0A0', '#FF8080', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_hat = lr.predict(x_test) # 预测值y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同plt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值的显示plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本的显示plt.xlabel(u'花萼长度', fontsize=14)plt.ylabel(u'花萼宽度', fontsize=14)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid()plt.title(u'鸢尾花Logistic回归分类效果 - 标准化', fontsize=17)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>Pipeline</tag>
        <tag>LogisticRegression</tag>
        <tag>分割面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model%2F</url>
    <content type="text"><![CDATA[导入数据集12345678910from sklearn import datasetsfrom sklearn.model_selection import train_test_splitboston = datasets.load_boston()X = boston.datay = boston.target"""划分数据集"""x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1) GridSearchCV寻参123456789lr = linear_model.LinearRegression()model = Lasso()# model = Ridge()alpha_can = np.logspace(-3, 2, 10)lasso_model = GridSearchCV(model, param_grid=&#123;'alpha': alpha_can&#125;, cv=5)lasso_model.fit(x_train, y_train)print '超参数：\n', lasso_model.best_params_ 预测12y_hat = lasso_model.predict(np.array(x_test)) 模型评价1234mse = np.average((y_hat - np.array(y_test)) ** 2) # Mean Squared Errorrmse = np.sqrt(mse) # Root Mean Squared Errorprint mse, rmse 或者用scikit-learn计算MSE/RMSE1234from sklearn import metricsprint "MSE:",metrics.mean_squared_error(y_test, y_hat)print "RMSE:",np.sqrt(metrics.mean_squared_error(y_test, y_hat)) cross_val_predict123456789boston = datasets.load_boston()X = boston.datay = boston.targetfrom sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(model, X, y, cv=10)print "MSE:",metrics.mean_squared_error(y, predicted)print "RMSE:",np.sqrt(metrics.mean_squared_error(y, predicted)) 输出图形展示这里画图真实值和预测值的变化关系，离中间的直线y=x直线越近的点代表预测损失越低。 12345678910t = np.arange(len(x_test))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.plot(t, y_test, 'r-', linewidth=2, label=u'真实数据')plt.plot(t, y_hat, 'g-', linewidth=2, label=u'预测数据')plt.title(u'线性回归预测销量', fontsize=18)plt.legend(loc='upper right')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>scipy</tag>
        <tag>GridSearchCV</tag>
        <tag>MSE</tag>
        <tag>RMSE</tag>
        <tag>cross_val_predict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model中的Ridge、Lasso、ElasticNet回归]]></title>
    <url>%2F2017%2F11%2F13%2FRidge_Lasso_ElasticNet%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCVfrom sklearn.preprocessing import PolynomialFeaturesimport matplotlib.pyplot as pltfrom sklearn.pipeline import Pipelineimport matplotlib as mplimport warnings 定义模型评价函数：123456789def xss(y, y_hat): y = y.ravel() y_hat = y_hat.ravel() tss = np.var(y) rss = np.average((y_hat - y) ** 2) r2 = 1 - rss / tss corr_coef = np.corrcoef(y, y_hat)[0, 1] return r2, corr_coef 生成训练数据12345678910warnings.filterwarnings("ignore") # ConvergenceWarningnp.random.seed(0)np.set_printoptions(linewidth=1000)N = 9x = np.linspace(0, 6, N) + np.random.randn(N)x = np.sort(x)y = x**2 - 4*x - 3 + np.random.randn(N)x.shape = -1, 1y.shape = -1, 1 模型设置1234567891011121314models = [Pipeline([('poly', PolynomialFeatures()), ('linear', LinearRegression(fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', RidgeCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', LassoCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', ElasticNetCV(alphas=np.logspace(-3, 2, 50), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], fit_intercept=False))])]mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsenp.set_printoptions(suppress=True) 模型训练与图形展示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465plt.figure(figsize=(16, 10), facecolor='w')d_pool = np.arange(1, N, 1) # 阶m = d_pool.sizeclrs = [] # 颜色for c in np.linspace(16711680, 255, m): clrs.append('#%06x' % c)line_width = np.linspace(5, 2, m)titles = u'LinearRegression', u'Ridge', u'LASSO', u'ElasticNet'tss_list = []rss_list = []ess_list = []ess_rss_list = []for t in range(4): model = models[t] plt.subplot(2, 2, t+1) plt.plot(x, y, 'ro', ms=10, zorder=N) for i, d in enumerate(d_pool): model.set_params(poly__degree=d) model.fit(x, y.ravel()) lin = model.get_params('linear')['linear'] output = u'%s：%d阶，系数为：' % (titles[t], d) if hasattr(lin, 'alpha_'): idx = output.find(u'系数') output = output[:idx] + (u'alpha=%.6f，' % lin.alpha_) + output[idx:] if hasattr(lin, 'l1_ratio_'): # 根据交叉验证结果，从输入l1_ratio(list)中选择的最优l1_ratio_(float) idx = output.find(u'系数') output = output[:idx] + (u'l1_ratio=%.6f，' % lin.l1_ratio_) + output[idx:] print output, lin.coef_.ravel() x_hat = np.linspace(x.min(), x.max(), num=100) x_hat.shape = -1, 1 y_hat = model.predict(x_hat) s = model.score(x, y) r2, corr_coef = xss(y, model.predict(x)) # print 'R2和相关系数：', r2, corr_coef # print 'R2：', s, '\n' z = N - 1 if (d == 2) else 0 label = u'%d阶，$R^2$=%.3f' % (d, s) if hasattr(lin, 'l1_ratio_'): label += u'，L1 ratio=%.2f' % lin.l1_ratio_ plt.plot(x_hat, y_hat, color=clrs[i], lw=line_width[i], alpha=0.75, label=label, zorder=z) plt.legend(loc='upper left') plt.grid(True) plt.title(titles[t], fontsize=18) plt.xlabel('X', fontsize=16) plt.ylabel('Y', fontsize=16)plt.tight_layout(1, rect=(0, 0, 1, 0.95))plt.suptitle(u'多项式曲线拟合比较', fontsize=22)plt.show()y_max = max(max(tss_list), max(ess_rss_list)) * 1.05plt.figure(figsize=(9, 7), facecolor='w')t = np.arange(len(tss_list))plt.plot(t, tss_list, 'ro-', lw=2, label=u'TSS(Total Sum of Squares)')plt.plot(t, ess_list, 'mo-', lw=1, label=u'ESS(Explained Sum of Squares)')plt.plot(t, rss_list, 'bo-', lw=1, label=u'RSS(Residual Sum of Squares)')plt.plot(t, ess_rss_list, 'go-', lw=2, label=u'ESS+RSS')plt.ylim((0, y_max))plt.legend(loc='center right')plt.xlabel(u'LinearRegression/Ridge/LASSO/Elastic Net', fontsize=15)plt.ylabel(u'XSS值', fontsize=15)plt.title(u'总平方和TSS=？', fontsize=18)plt.grid(True)plt.show() 输出如下： 123456789101112131415161718192021222324252627282930313233线性回归：1阶，系数为： [-12.12113792 3.05477422]线性回归：2阶，系数为： [-3.23812184 -3.36390661 0.90493645]线性回归：3阶，系数为： [-3.90207326 -2.61163034 0.66422328 0.02290431]线性回归：4阶，系数为： [-8.20599769 4.20778207 -2.85304163 0.73902338 -0.05008557]线性回归：5阶，系数为： [ 21.59733285 -54.12232017 38.43116219 -12.68651476 1.98134176 -0.11572371]线性回归：6阶，系数为： [ 14.73304785 -37.87317494 23.67462342 -6.07037979 0.42536833 0.06803132 -0.00859246]线性回归：7阶，系数为： [ 314.30344622 -827.89446924 857.33293186 -465.46543638 144.21883851 -25.67294678 2.44658612 -0.09675941]线性回归：8阶，系数为： [-1189.50149198 3643.69109456 -4647.92941149 3217.22814712 -1325.87384337 334.32869072 -50.57119119 4.21251817 -0.148521 ]Ridge回归：1阶，alpha=0.109854，系数为： [-11.21592213 2.85121516]Ridge回归：2阶，alpha=0.138950，系数为： [-2.90423989 -3.49931368 0.91803171]Ridge回归：3阶，alpha=0.068665，系数为： [-3.47165245 -2.85078293 0.69245987 0.02314415]Ridge回归：4阶，alpha=0.222300，系数为： [-2.84560266 -1.99887417 -0.40628792 0.33863868 -0.02674442]Ridge回归：5阶，alpha=1.151395，系数为： [-1.68160373 -1.52726943 -0.8382036 0.2329258 0.03934251 -0.00663323]Ridge回归：6阶，alpha=0.001000，系数为： [ 0.53724068 -6.00552086 -3.75961826 5.64559118 -2.21569695 0.36872911 -0.02221343]Ridge回归：7阶，alpha=0.033932，系数为： [-2.38021238 -2.26383055 -1.47715232 0.00763115 1.12242917 -0.52769633 0.09199201 -0.00560199]Ridge回归：8阶，alpha=0.138950，系数为： [-2.19299093 -1.91896884 -1.21608489 -0.19314178 0.49300277 0.05452898 -0.09690455 0.02114435 -0.00140196]LASSO：1阶，alpha=0.222300，系数为： [-10.41556797 2.66199326]LASSO：2阶，alpha=0.001000，系数为： [-3.29932625 -3.31989869 0.89878903]LASSO：3阶，alpha=0.013257，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]LASSO：4阶，alpha=0.002560，系数为： [-5.08513199 -1.41147772 0.3380565 0.0440427 0.00099807]LASSO：5阶，alpha=0.042919，系数为： [-4.11853758 -1.8643949 0.2618319 0.07954732 0.00257481 -0.00069093]LASSO：6阶，alpha=0.001000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]LASSO：7阶，alpha=0.001000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]LASSO：8阶，alpha=0.001000，系数为： [-4.62623251 -1.37717809 0.17183854 0.04307765 0.00629505 0.00069171 0.0000355 -0.00000875 -0.00000386]ElasticNet：1阶，alpha=0.021210，l1_ratio=0.100000，系数为： [-10.74762959 2.74580662]ElasticNet：2阶，alpha=0.013257，l1_ratio=0.100000，系数为： [-2.95099269 -3.48472703 0.91705013]ElasticNet：3阶，alpha=0.013257，l1_ratio=1.000000，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]ElasticNet：4阶，alpha=0.010481，l1_ratio=0.950000，系数为： [-4.8799192 -1.5317438 0.3452403 0.04825571 0.00049763]ElasticNet：5阶，alpha=0.004095，l1_ratio=0.100000，系数为： [-4.07916291 -2.18606287 0.44650232 0.05102669 0.00239164 -0.00048279]ElasticNet：6阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]ElasticNet：7阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]ElasticNet：8阶，alpha=0.001000，l1_ratio=0.500000，系数为： [-4.53761647 -1.45230301 0.18829714 0.0427561 0.00619739 0.00068209 0.00003506 -0.00000869 -0.00000384]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>ElasticNet</tag>
        <tag>Pipeline</tag>
        <tag>Ridge</tag>
        <tag>Lasso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读取文件数据的几种方式]]></title>
    <url>%2F2017%2F11%2F13%2Fpython%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[方式一：f = file(path)1234567891011121314151617f = file(path)x = []y = []for i, d in enumerate(f): if i == 0: continue d = d.strip() if not d: continue d = map(float, d.split(',')) x.append(d[1:-1]) y.append(d[-1])pprint(x)pprint(y)x = np.array(x)y = np.array(y) 方式二：Python自带库csv12345678import csv f = file(path, 'rb') print f d = csv.reader(f) for line in d: print line f.close() 方式三：Python自带库numpy123import numpy as npp = np.loadtxt(path, delimiter=',', skiprows=1) 方式四：Python自带库pandas123import pandas as pddata = pd.read_csv(path)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM]]></title>
    <url>%2F2017%2F11%2F12%2FHMM%2F</url>
    <content type="text"><![CDATA[HMM定义 隐马尔科夫模型的贝叶斯网络 HMM的参数HMM的确定 HMM的参数 HMM的参数总结 HMM的两个基本性质 HMM举例 示例的各个参数 示例的思考 HMM的3个基本问题 概率计算问题 直接计算法 直接计算法分析 借鉴算法的优化思想 前向算法定义：前向概率-后向概率 前向算法定义 前向算法过程 后向算法后向算法定义 后向算法过程 后向算法的说明 前后向关系 单个状态的概率 r的意义 两个状态的联合概率 期望 学习算法 大数定理 监督学习方法 Baum-Welch算法 EM过程 极大化 初始状态概率 转移概率和观测概率 预测算法近似算法 算法：走棋盘/格子取数 Viterbi算法 Viterbi算法举例 总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>Baum-Welch</tag>
        <tag>Viterbi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型]]></title>
    <url>%2F2017%2F11%2F12%2F%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[引子 Beta分布 Beta分布的期望 Beta分布图形 先验分布-共轭分布共轭先验分布的定义 二项分布的最大似然估计 二项分布与先验举例 上述过程的理论解释 先验概率和后验概率的关系 伪计数 共轭先验的直接推广 Beta分布-Dirichlet分布Dirichlet分布 Dirichlet分布的期望 Dirichlet分布分析 对称Dirichlet分布 对称Dirichlet分布的参数分析 参数a对Dirichlet分布的影响 参数选择对对称Dirichlet分布的影响 多项分布的共轭分布是Dirichlet分布 三层贝叶斯网络模型LDALDA的解释 参数的学习 似然概率 Gibbs采样和更新规则Gibbs Sampling 联合分布 计算因子 Gibbs updating rule 词分布和主题分布 LDA总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Beta分布</tag>
        <tag>先验分布</tag>
        <tag>共轭先验分布</tag>
        <tag>二项分布</tag>
        <tag>Dirichlet分布</tag>
        <tag>多项分布</tag>
        <tag>Gibbs采样</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯网络]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯 贝叶斯网络的表达贝叶斯网络定义 一个简单的贝叶斯网络 全连接贝叶斯网络 一个“正常”的贝叶斯网络 对一个实际贝叶斯网络的分析 贝叶斯网络的形式化定义 马尔科夫模型一个特殊的贝叶斯网络。 D-separation条件独立的三种类型通过贝叶斯网络判定条件独立1 通过贝叶斯网络判定条件独立2 通过贝叶斯网络判定条件独立3 举例说明这三种情况 将上述结点推广到结点集 有向分离的举例 Markov Blanket再次分析链式网络 HMM 贝叶斯网络的用途分类预测 转移概率矩阵 贝叶斯网络的构建 混合（离散+连续）网络 孩子结点是连续的 孩子结点是离散的，父节点是连续的 孩子结点是离散的，父节点是连续的 贝叶斯网络的推导 无向环 原贝叶斯网络的近似树结构 将两图的相对熵转换成变量的互信息 Chou-Liu算法 最大权生成树MSWT的建立过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>朴素贝叶斯</tag>
        <tag>马尔科夫</tag>
        <tag>D-separation</tag>
        <tag>Markov Blanket</tag>
        <tag>Chou-Liu</tag>
        <tag>MSWT</tag>
        <tag>转移概率矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PLSA]]></title>
    <url>%2F2017%2F11%2F12%2FPLSA%2F</url>
    <content type="text"><![CDATA[PLSA模型PLSA模型概述 PLSA模型过程 最大似然估计 目标函数分析 求隐含变量主题zk的后验概率 分析似然函数期望 完成目标函数的建立 目标函数的求解 分析第一等式 同理分析第二等式 PLSA总结 PLSA进一步思考]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
        <tag>PLSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM]]></title>
    <url>%2F2017%2F11%2F12%2FEM%2F</url>
    <content type="text"><![CDATA[引子k-Means算法 对K-Means的思考 Jensen不等式：若f是凸函数 最大似然估计 二项分布的最大似然估计 进一步考察 按照MLE的过程分析 化简对数似然函数 参数估计的结论 符合直观想象 问题：随机变量无法直接（完全）观察到 从直观理解猜测GMM的参数估计 建立目标函数 第一步：估算数据来自哪个组份 第二步：估计每个组份的参数 EM算法EM算法的提出 通过最大似然估计建立目标函数 问题的提出 Jensen不等式 寻找尽量紧的下界 进一步分析 EM算法整体框架 坐标上升 从理论公式推导GMM E-step M-step 对均值求偏导 高斯分布的均值 高斯分布的方差：求偏导，等于0 多项分布的参数 拉格朗日乘子法 求偏导，等于0 结论]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一些概念聚类的定义聚类就是对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。 相似度/距离计算方法总结 Hellinger distance 余弦相似度与Pearson相似系数 聚类的基本思想 聚类的衡量指标 ARI AMI 轮廓系数(Silhouette) k-Meansk-Means算法 对k-Means的思考 k-Means的公式化解释 如果使用其他相似度/距离度量 Mini-batch k-Means算法描述 k-Means聚类总结 CanopyCanopy算法 层次聚类方法 AGNES和DIANA算法 密度聚类方法 DBSCAN算法 DBSCAN算法的若干概念 DBSCAN算法流程 密度最大值聚类 高局部密度点距离 簇中心的识别 密度最大值聚类过程 边界和噪声的重新认识 Affinity Propagation算法概述基本概念 Exemplar范例：即聚类族中心点； s(i,j)：数据点i与数据点j的相似度值，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力； 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵； Preference参考度或称为偏好参数：是相似度矩阵中横轴纵轴索引相同的点，如s(i,i)，若按欧氏距离计算其值应为0，但在AP聚类中其表示数据点i作为聚类中心的程度，因此不能为0。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； Responsibility，r(i,k)：吸引度信息，表示数据点k适合作为数据点i的聚类中心的程度；公式如下： 其中a(i,k’)表示除k外其他点对i点的归属度值，初始为0；s(i,k’)表示除k外其他点对i的吸引度，即i外其他点都在争夺i点的 所有权；r(i,k)表示数据点k成为数据点i的聚类中心的累积证明，r(i,k)值大于0，则表示数据点k成为聚类中心的能力强。说明：此时只考虑哪个点k成为点i的聚类中心的可能性最大，但是没考虑这个吸引度最大的k是否也经常成为其他点的聚类中心（即归属度），若点k只是点i的聚类中心，不是其他任何点的聚类中心，则会造成最终聚类中心个数大于实际的中心个数。 Availability，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的合适程度，公式如下： 其中r(i’,k)表示点k作为除i外其他点的聚类中心的相似度值，取所有大于等于0的吸引度值，加上k作为聚类中心的可能程。即点k在这些吸引度值大于0的数据点的支持下，数据点i选择k作为其聚类中心的累积证明。 Damping factor阻尼系数：为防止数据震荡，引入地衰减系数，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得1-λ倍，其中λ在0-1之间，默认为0.5。 算法流程 更新相似度矩阵中每个点的吸引度信息，计算归属度信息； 更新归属度信息，计算吸引度信息； 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 s(i,k)就相当于i对选k这个人的一个固有的偏好程度 r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） a(i,k)：从公式里可以看到，所有r(i’,k)&gt;0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)&gt;0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。 r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。 优点 不需要制定最终聚类族的个数 已有的数据点作为最终的聚类中心，而不是新生成一个族中心。 模型对数据的初始值不敏感。 对初始相似度矩阵数据的对称性没有要求。 相比与k-centers聚类方法，其结果的平方差误差较小。缺点 虽然AP算法不用提前设置聚类中心的个数，但是需要事先设置参考度，而参考度的大小与聚类中心的个数正相关； 由于AP算法每次迭代都需要更新每个数据点的吸引度值和归属度值，算法复杂度较高，在大数据量下运行时间较长。 谱和谱聚类 谱分析的整体过程 一些概念 相似度图G的建立方法 权值比较 拉普拉斯矩阵的定义 拉普拉斯矩阵及其性质 谱聚类算法：未正则拉普拉斯矩阵 谱聚类算法：随机游走拉普拉斯矩阵 谱聚类算法：对称拉普拉斯矩阵 进一步思考 随机游走和拉普拉斯矩阵的关系 标签传递算法 via Affinity Propagation: AP聚类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>k-Means</tag>
        <tag>Canopy</tag>
        <tag>AGNES</tag>
        <tag>DIANA</tag>
        <tag>DBSCAN</tag>
        <tag>AP</tag>
        <tag>谱聚类</tag>
        <tag>层次聚类</tag>
        <tag>密度最大值聚类</tag>
        <tag>拉普拉斯</tag>
        <tag>边界</tag>
        <tag>噪声</tag>
        <tag>标签传递</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2017%2F11%2F12%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[Boosting的思想 AdaBoost算法 AdaBoost误差上限 前向分步算法 前向分步算法的含义 前向分步算法的算法框架 前向分步算法与AdaBoost 证明 基分类器 权值的计算 分类错误率 权值的更新 权值和错误率的关键解释 AdaBoost总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>AdaBoost</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2017%2F11%2F12%2FSVM%2F</url>
    <content type="text"><![CDATA[支持向量机SVM的原理和目标线性可分支持向量机硬间隔最大化hard margin maximization;硬间隔支持向量机 分割超平面 分割超平面的思考 线性支持向量机软间隔最大化soft margin maximization;软间隔支持向量机 线性分类问题 输入数据 线性可分支持向量机 非线性支持向量机核函数kernel function 支持向量机的计算过程和算法步骤推导目标函数 最大间隔分离超平面 函数间隔和几何间隔 建立目标函数 线性可分支持向量机学习算法 线性SVM的目标函数 带松弛因子的SVM拉格朗日函数 代入目标函数 最终的目标函数 线性支持向量机 线性支持向量机学习算法 损失函数分析 核函数 高斯核 SVM中系数的求解：SMO SMO:序列最小最优化 二变量优化问题 SMO的迭代公式 退出条件 SVM总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2017%2F11%2F12%2FXGBoost%2F</url>
    <content type="text"><![CDATA[考虑使用二阶导信息 决策树的描述 正则项的定义 目标函数计算 构造决策树的结构 XGBoost小结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升GBDT]]></title>
    <url>%2F2017%2F11%2F11%2F%E6%8F%90%E5%8D%87GBDT%2F</url>
    <content type="text"><![CDATA[由决策树和随机森林的关系的思考 随机森林的决策树分别采样建立，相对独立。 思考： 假定当前已经得到了m-1棵决策树，是否可以通过现有样本和决策树的信息，对第m棵决策树的建立产生有益的影响呢？ 各个决策树组成随机森林后，最后的投票过程可否在建立决策树时即确定呢？ 提升的概念 提升是一个机器学习技术，可以用于回归和分类问题，它每一步产生一个弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升（Gradient Boosting） 梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合（基函数）；提升算法通过迭代的选择一个负梯度方向上的基函数来逐渐逼近局部极小值。这种在函数域的梯度提升观点对机器学习的很多领域有深刻影响。 提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。 提升算法 提升算法推导 提升算法 梯度提升决策树GBDT 参数设置和正则化 衰减因子、降采样 GBDT总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2017%2F11%2F11%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[BootstrapingBootstraping的名称来自成语“pull up by your own bootstraps”,意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法。本意是通过拉靴子让自己上升，不可能发生的事情。后来意思发生了转变，隐喻不需要外界帮助，仅依靠自身力量让自己变得更好。 Bagging的策略 bootstrap aggregation 从样本集中重采样（有重复的）选出n个样本 在所有属性上，对这n个样本建立分类器（ID3，C4.5，CART,SVM,Logistic回归等） 重复以上两部m次，即获得了m个分类器 将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类 Another description of Bagging 随机森林随机森林在Bagging基础上做了修改： 从样本集中用Bootstrap采样选出n个样本； 从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树； 重复以上两步m次，即建立了m棵CART决策树 这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类 随机森林/Bagging和决策树的关系 当然可以使用决策树作为基本分类器 但也可以使用SVM、Logistic回归等其他分类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 投票机制简单投票机制 一票否决（一致表决） 少数服从多数 有效多数（加权） 阈值表决贝叶斯投票机制一种可能的方案 样本不均衡的常用处理方法 使用RF建立计算样本间相似度 使用RF计算特征重要度 使用RF生成特征 总结 决策树、随机森林的代码清洗，逻辑简单，在胜任分类问题的同时，往往也可以作为对数据分布探索的首要尝试算法。 随机森林的集成思想也可以用在其他分类器的设计中。 如果通过随机森林做样本的异常值检测 统计样本间位于相同决策树的叶节点的个数，形成样本相似度矩阵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树的概念决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。决策树学习是以实例为基础的归纳学习。决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。 决策树学习的生成算法建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法： ID3(Iterative Dichotomiser) C4.5 CART(Classification And Regression Tree) 信息增益概念当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。 信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。 定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差，即：g(D,A)=H(D)-H(D/A)，显然，这即为训练数据集D和特征A的互信息。 基本记号： 信息增益的计算方法 经验条件熵H(D/A) 其他目标 关于Gini系数的讨论 Gini系数的第二定义决策树中的Gini系数和社会学上的Gini系数并不相等。 三种决策树学习算法的比较 ID3:使用信息增益/互信息g(D,A)进行特征选择 取值多的属性，更容易使数据更纯，其信息增益更大 训练得到的是一棵庞大且深度浅的树：不合理。 C4.5：信息增益率gr(D,A)=g(D,A)/H(A) CART：基尼指数一个属性的信息增益（率）/Gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强。 决策树的评价 决策树的过拟合决策树对训练属于有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。 剪枝 随机森林 剪枝三种决策树的剪枝过程算法相同，区别仅是对于当前树的评价标准不同。 信息增益、信息增益率、基尼系数剪枝总体思路： 由完全树T0开始，剪枝部分结点得到T1，再次剪枝部分结点得到T2…直到仅剩树根的树Tk 在验证数据集上对这k个树分别评价，选择损失函数最小的树Ti 剪枝系数的确定 剪枝算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2017%2F11%2F11%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic回归分类问题的首选算法; 线性回归：最大似然估计+高斯分布； Logistic回归：最大似然估计+伯努利分布； Logistic回归是事件发生几率取对数下的线性回归。高斯分布和伯努利分布都是属于指数族分布，所以说逻辑回归是广义线性模型GLM回归。 Logistic/Sigmoid函数 Logistic回归参数估计 对数似然函数 参数的迭代 Logistic回归的损失函数A： ### Logistic回归的损失函数B： 广义线性模型GLM 多分类：Softmax回归Softmax名字由来 ## Softmax回归 信息熵定义信息量 熵熵是随机变量不确定性的度量，不确定性越大，熵值越大。若随机变量退化成定值，熵最小，为0；若随机分布为均匀分布，熵最大。 熵的定义 熵的公式推导 均匀分布的信息熵 联合熵、条件熵、相对熵联合熵、条件熵 推导条件熵的定义公式 相对熵、KL散度 互信息互信息的定义 互信息与条件熵的关系 公式]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>Logistic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归与特征选择]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[线性回归可以对样本是非线性的，只要对参数线性。 高斯分布在做特征选择的时候，可以看回归的残差图，真正理想状态下，经过模型回归后的残差应该是服从正态分布的，而不是均匀的随机分布；此时可以画个单变量与残差的关系图，直观的看一下是不是有什么明显的一阶、高阶趋势在里面，则需要进一步特征加工。 大数定理指的是事件发生的概率在N取无穷大的时候趋近于事件发生的概率。频率的极限是概率。 使用极大似然估计解释最小二乘 最大似然估计MLE最大似然函数的假设似然函数假设每个样本出现的概率是独立的，因此整体出现的概率就是各个样本出现的概率的累计。 似然函数 高斯的对数似然与最小二乘误差服从高斯分布的情况下的对数似然函数： 解析式的求解过程 最小二乘法的本质误差服从高斯分布的情况下的对数似然函数最大化的过程等价于最小二乘。 最小二乘意义下的参数最优解 加入lamda扰动后 线性回归的复杂度惩罚因子惩罚因子的直观理解，阶数越高，模型越复杂，系数振荡的越厉害，系数的取值也越大，同时呢，系数又会有正有负，所以直接用系数的平方作为模型复杂度的惩罚因子。 Ridge岭回归 是加了L2正则项的最小二乘。LASSO 是L1正则。 Ridge岭回归与LASSO：两个都差不多，单看模型系数LASSO要稍稍比Ridge稳定一些，但如果给定模型的评价指标的化，Ridge稍微比LASSO好。LASSO可以具有特征选择的功能。 正则项与防止过拟合 广义逆矩阵（伪逆） 梯度下降算法 梯度方向 批量梯度下降算法 随机梯度下降算法 mini-batch如果不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。 模型评价TSS &gt;= ESS + RSSR方的取值范围：(-inf,1] 局部加权回归 局部加权线性回归 权值的设置]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础]]></title>
    <url>%2F2017%2F11%2F08%2FPython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[intro 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npimport matplotlib as mplfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmimport timefrom scipy.optimize import leastsqfrom scipy import statsimport scipy.optimize as optimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poisson# from scipy.interpolate import BarycentricInterpolator# from scipy.interpolate import CubicSplineimport math# import seaborndef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return yif __name__ == "__main__": # # 开场白： # numpy是非常好用的数据包，如：可以这样得到这个二维数组 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # 正式开始 -:) # 标准Python的列表(list)中，元素本质是对象。 # 如：L = [1, 2, 3]，需要3个指针和三个整数对象，对于数值运算比较浪费内存和CPU。 # 因此，Numpy提供了ndarray(N-dimensional array object)对象：存储单一数据类型的多维数组。 # # 1.使用array创建 # 通过array函数传递list对象 # L = [1, 2, 3, 4, 5, 6] # print "L = ", L # a = np.array(L) # print "a = ", a # print type(a) # # # 若传递的是多层嵌套的list，将创建多维数组 # b = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) # print b # # # # # # # 数组大小可以通过其shape属性获得 # print a.shape # print b.shape # # # # # # 也可以强制修改shape # b.shape = 4, 3 # print b # # # # 注：从(3,4)改为(4,3)并不是对数组进行转置，而只是改变每个轴的大小，数组元素在内存中的位置并没有改变 # # # # # # 当某个轴为-1时，将根据数组元素的个数自动计算此轴的长度 # b.shape = 2, -1 # print b # print b.shape # # # # b.shape = 3, 4 # # # 使用reshape方法，可以创建改变了尺寸的新数组，原数组的shape保持不变 # c = b.reshape((4, -1)) # print "b = \n", b # print 'c = \n', c # # # # # 数组b和c共享内存，修改任意一个将影响另外一个 # b[0][1] = 20 # print "b = \n", b # print "c = \n", c # # # # # # 数组的元素类型可以通过dtype属性获得 # print a.dtype # print b.dtype # # # # # # # 可以通过dtype参数在创建时指定元素类型 # d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.float) # f = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.complex) # print d # print f # # # # # # 如果更改元素类型，可以使用astype安全的转换 # f = d.astype(np.int) # print f # # # # # 但不要强制仅修改元素类型，如下面这句，将会以int来解释单精度float类型 # d.dtype = np.int # print d # # 2.使用函数创建 # 如果生成一定规则的数据，可以使用NumPy提供的专门函数 # arange函数类似于python的range函数：指定起始值、终止值和步长来创建数组 # 和Python的range类似，arange同样不包括终值；但arange可以生成浮点类型，而range只能是整数类型 # a = np.arange(1, 10, 0.5) # print a # # # # # # linspace函数通过指定起始值、终止值和元素个数来创建数组，缺省包括终止值 # b = np.linspace(1, 10, 10) # print 'b = ', b # # # # # 可以通过endpoint关键字指定是否包括终值 # c = np.linspace(1, 10, 10, endpoint=False) # print 'c = ', c # # # # # 和linspace类似，logspace可以创建等比数列 # # 下面函数创建起始值为10^1，终止值为10^2，有20个数的等比数列 # d = np.logspace(1, 2, 10, endpoint=True) # print d # # # # # # 下面创建起始值为2^0，终止值为2^10(包括)，有10个数的等比数列 # f = np.logspace(0, 10, 11, endpoint=True, base=2) # print f # # # # # # 使用 frombuffer, fromstring, fromfile等函数可以从字节序列创建数组 # s = 'abcd' # g = np.fromstring(s, dtype=np.int8) # print g # # # 3.存取 # 3.1常规办法：数组元素的存取方法和Python的标准方法相同 # a = np.arange(10) # print a # # # 获取某个元素 # print a[3] # # # # 切片[3,6)，左闭右开 # print a[3:6] # # # # 省略开始下标，表示从0开始 # print a[:5] # # # # 下标为负表示从后向前数 # print a[3:] # # # # 步长为2 # print a[1:9:2] # # # # 步长为-1，即翻转 # print a[::-1] # # # # 切片数据是原数组的一个视图，与原数组共享内容空间，可以直接修改元素值 # a[1:4] = 10, 20, 30 # print a # # # # 因此，在实践中，切实注意原始数据是否被破坏，如： # b = a[2:5] # b[0] = 200 # print a # # # 3.2 整数/布尔数组存取 # # 3.2.1 # 根据整数数组存取：当使用整数序列对数组元素进行存取时， # 将使用整数序列中的每个元素作为下标，整数序列可以是列表(list)或者数组(ndarray)。 # 使用整数序列作为下标获得的数组不和原始数组共享数据空间。 # a = np.logspace(0, 9, 10, base=2) # print a # i = np.arange(0, 10, 2) # print i # # # # 利用i取a中的元素 # b = a[i] # print b # # # b的元素更改，a中元素不受影响 # b[2] = 1.6 # print b # print a # # 3.2.2 # 使用布尔数组i作为下标存取数组a中的元素：返回数组a中所有在数组b中对应下标为True的元素 # # 生成10个满足[0,1)中均匀分布的随机数 # a = np.random.rand(10) # print a # # # 大于0.5的元素索引 # print a &gt; 0.5 # # # # 大于0.5的元素 # b = a[a &gt; 0.5] # print b # # # # 将原数组中大于0.5的元素截取成0.5 # a[a &gt; 0.5] = 0.5 # print a # # # # b不受影响 # print b # 3.3 二维数组的切片 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10) # 行向量 # print 'a = ', a # b = a.reshape((-1, 1)) # 转换成列向量 # print b # c = np.arange(6) # print c # f = b + c # 行 + 列 # print f # # 合并上述代码： # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # # 二维数组的切片 # print a[[0, 1, 2], [2 ,3, 4]] # print a[4, [2, 3, 4]] # print a[4:, [2, 3, 4]] # i = np.array([True, False, True, False, False, True]) # print a[i] # print a[i, 3] # # 4.1 numpy与Python数学库的时间比较 # for j in np.logspace(0, 7, 10): # j = int(j) # x = np.linspace(0, 10, j) # start = time.clock() # y = np.sin(x) # t1 = time.clock() - start # # x = x.tolist() # start = time.clock() # for i, t in enumerate(x): # x[i] = math.sin(t) # t2 = time.clock() - start # print j, ": ", t1, t2, t2/t1 # 4.2 元素去重 # 4.2.1直接使用库函数 # a = np.array((1, 2, 3, 4, 5, 5, 7, 3, 2, 2, 8, 8)) # print '原始数组：', a # # 使用库函数unique # b = np.unique(a) # print '去重后：', b # # 4.2.2 二维数组的去重，结果会是预期的么？ # c = np.array(((1, 2), (3, 4), (5, 6), (1, 3), (3, 4), (7, 6))) # print '二维数组', c # print '去重后：', np.unique(c) # # 4.2.3 方案1：转换为虚数 # # r, i = np.split(c, (1, ), axis=1) # # x = r + i * 1j # x = c[:, 0] + c[:, 1] * 1j # print '转换成虚数：', x # print '虚数去重后：', np.unique(x) # print np.unique(x, return_index=True) # 思考return_index的意义 # idx = np.unique(x, return_index=True)[1] # print '二维数组去重：\n', c[idx] # # 4.2.3 方案2：利用set # print '去重方案2：\n', np.array(list(set([tuple(t) for t in c]))) # # 4.3 stack and axis # a = np.arange(1, 10).reshape((3, 3)) # b = np.arange(11, 20).reshape((3, 3)) # c = np.arange(101, 110).reshape((3, 3)) # print 'a = \n', a # print 'b = \n', b # print 'c = \n', c # print 'axis = 0 \n', np.stack((a, b, c), axis=0) # print 'axis = 1 \n', np.stack((a, b, c), axis=1) # print 'axis = 2 \n', np.stack((a, b, c), axis=2) # 5.绘图 # 5.1 绘制正态分布概率密度函数 # mu = 0 # sigma = 1 # x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 50) # y = np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (math.sqrt(2 * math.pi) * sigma) # print x.shape # print 'x = \n', x # print y.shape # print 'y = \n', y # # plt.plot(x, y, 'ro-', linewidth=2) # plt.plot(x, y, 'r-', x, y, 'go', linewidth=2, markersize=8) # plt.grid(True) # plt.show() mpl.rcParams['font.sans-serif'] = [u'SimHei'] #FangSong/黑体 FangSong/KaiTi mpl.rcParams['axes.unicode_minus'] = False # # 5.2 损失函数：Logistic损失(-1,1)/SVM Hinge损失/ 0/1损失 # x = np.array(np.linspace(start=-2, stop=3, num=1001, dtype=np.float)) # y_logit = np.log(1 + np.exp(-x)) / math.log(2) # y_boost = np.exp(-x) # y_01 = x &lt; 0 # y_hinge = 1.0 - x # y_hinge[y_hinge &lt; 0] = 0 # plt.plot(x, y_logit, 'r-', label='Logistic Loss', linewidth=2) # plt.plot(x, y_01, 'g-', label='0/1 Loss', linewidth=2) # plt.plot(x, y_hinge, 'b-', label='Hinge Loss', linewidth=2) # plt.plot(x, y_boost, 'm--', label='Adaboost Loss', linewidth=2) # plt.grid() # plt.legend(loc='upper right') # # plt.savefig('1.png') # plt.show() # # 5.3 x^x # x = np.linspace(-1.3, 1.3, 101) # y = f(x) # plt.plot(x, y, 'g-', label='x^x', linewidth=2) # plt.grid() # plt.legend(loc='upper left') # plt.show() # # 5.4 胸型线 # x = np.arange(1, 0, -0.001) # y = (-3 * x * np.log(x) + np.exp(-(40 * (x - 1 / np.e)) ** 4) / 25) / 2 # plt.figure(figsize=(5,7)) # plt.plot(y, x, 'r-', linewidth=2) # plt.grid(True) # plt.show() # 5.5 心形线 # t = np.linspace(0, 7, 100) # x = 16 * np.sin(t) ** 3 # y = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid(True) # plt.show() # # 5.6 渐开线 # t = np.linspace(0, 50, num=1000) # x = t*np.sin(t) + np.cos(t) # y = np.sin(t) - t*np.cos(t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid() # plt.show() # # Bar # mpl.rcParams['font.sans-serif'] = [u'SimHei'] #黑体 FangSong/KaiTi # mpl.rcParams['axes.unicode_minus'] = False # x = np.arange(0, 10, 0.1) # y = np.sin(x) # plt.bar(x, y, width=0.04, linewidth=0.2) # plt.plot(x, y, 'r--', linewidth=2) # plt.title(u'Sin曲线') # plt.xticks(rotation=-60) # plt.xlabel('X') # plt.ylabel('Y') # plt.grid() # plt.show() # # 6. 概率分布 # # 6.1 均匀分布 # x = np.random.rand(10000) # t = np.arange(len(x)) # plt.hist(x, 30, color='m', alpha=0.5, label=u'均匀分布') # # plt.plot(t, x, 'r-', label=u'均匀分布') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 6.2 验证中心极限定理 # t = 1000 # a = np.zeros(10000) # for i in range(t): # a += np.random.uniform(-5, 5, 10000) # a /= t # plt.hist(a, bins=30, color='g', alpha=0.5, normed=True, label=u'均匀分布叠加') # plt.legend(loc='upper left') # plt.grid() # plt.show() # 6.21 其他分布的中心极限定理 # lamda = 10 # p = stats.poisson(lamda) # y = p.rvs(size=1000) # mx = 30 # r = (0, mx) # bins = r[1] - r[0] # plt.figure(figsize=(10, 8), facecolor='w') # plt.subplot(121) # plt.hist(y, bins=bins, range=r, color='g', alpha=0.8, normed=True) # t = np.arange(0, mx+1) # plt.plot(t, p.pmf(t), 'ro-', lw=2) # plt.grid(True) # # N = 1000 # M = 10000 # plt.subplot(122) # a = np.zeros(M, dtype=np.float) # p = stats.poisson(lamda) # for i in np.arange(N): # y = p.rvs(size=M) # a += y # a /= N # plt.hist(a, bins=20, color='g', alpha=0.8, normed=True) # plt.grid(b=True) # plt.show() # # 6.3 Poisson分布 # x = np.random.poisson(lam=5, size=10000) # print x # pillar = 15 # a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) # plt.grid() # # plt.show() # print a # print a[0].sum() # # 6.4 直方图的使用 # mu = 2 # sigma = 3 # data = mu + sigma * np.random.randn(1000) # h = plt.hist(data, 30, normed=1, color='#a0a0ff') # x = h[1] # y = norm.pdf(x, loc=mu, scale=sigma) # plt.plot(x, y, 'r--', x, y, 'ro', linewidth=2, markersize=4) # plt.grid() # plt.show() # # 6.5 插值 # rv = poisson(5) # x1 = a[1] # y1 = rv.pmf(x1) # itp = BarycentricInterpolator(x1, y1) # 重心插值 # x2 = np.linspace(x.min(), x.max(), 50) # y2 = itp(x2) # cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 # plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 # plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 # plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 # plt.legend(loc='upper right') # plt.grid() # plt.show() # 7. 绘制三维图像 # x, y = np.ogrid[-3:3:100j, -3:3:100j] # # u = np.linspace(-3, 3, 101) # # x, y = np.meshgrid(u, u) # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # fig = plt.figure() # ax = fig.add_subplot(111, projection='3d') # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.coolwarm, linewidth=0.1) # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.Accent, linewidth=0.5) # plt.show() # # cmaps = [('Perceptually Uniform Sequential', # # ['viridis', 'inferno', 'plasma', 'magma']), # # ('Sequential', ['Blues', 'BuGn', 'BuPu', # # 'GnBu', 'Greens', 'Greys', 'Oranges', 'OrRd', # # 'PuBu', 'PuBuGn', 'PuRd', 'Purples', 'RdPu', # # 'Reds', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd']), # # ('Sequential (2)', ['afmhot', 'autumn', 'bone', 'cool', # # 'copper', 'gist_heat', 'gray', 'hot', # # 'pink', 'spring', 'summer', 'winter']), # # ('Diverging', ['BrBG', 'bwr', 'coolwarm', 'PiYG', 'PRGn', 'PuOr', # # 'RdBu', 'RdGy', 'RdYlBu', 'RdYlGn', 'Spectral', # # 'seismic']), # # ('Qualitative', ['Accent', 'Dark2', 'Paired', 'Pastel1', # # 'Pastel2', 'Set1', 'Set2', 'Set3']), # # ('Miscellaneous', ['gist_earth', 'terrain', 'ocean', 'gist_stern', # # 'brg', 'CMRmap', 'cubehelix', # # 'gnuplot', 'gnuplot2', 'gist_ncar', # # 'nipy_spectral', 'jet', 'rainbow', # # 'gist_rainbow', 'hsv', 'flag', 'prism'])] # 8.1 scipy # 线性回归例1 # x = np.linspace(-2, 2, 50) # A, B, C = 2, 3, -1 # y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75 # # t = leastsq(residual, [0, 0, 0], args=(x, y)) # theta = t[0] # print '真实值：', A, B, C # print '预测值：', theta # y_hat = theta[0] * x ** 2 + theta[1] * x + theta[2] # plt.plot(x, y, 'r-', linewidth=2, label=u'Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 线性回归例2 # x = np.linspace(0, 5, 100) # A = 5 # w = 1.5 # y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5 # # t = leastsq(residual2, [3, 1], args=(x, y)) # theta = t[0] # print '真实值：', A, w # print '预测值：', theta # y_hat = theta[0] * np.sin(theta[1] * x) # plt.plot(x, y, 'r-', linewidth=2, label='Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict') # plt.legend(loc='lower left') # plt.grid() # plt.show() # # 8.2 使用scipy计算函数极值 # a = opt.fmin(f, 1) # b = opt.fmin_cg(f, 1) # c = opt.fmin_bfgs(f, 1) # print a, 1/a, math.e # print b # print c # marker description # ”.” point # ”,” pixel # “o” circle # “v” triangle_down # “^” triangle_up # “&lt;” triangle_left # “&gt;” triangle_right # “1” tri_down # “2” tri_up # “3” tri_left # “4” tri_right # “8” octagon # “s” square # “p” pentagon # “*” star # “h” hexagon1 # “H” hexagon2 # “+” plus # “x” x # “D” diamond # “d” thin_diamond # “|” vline # “_” hline # TICKLEFT tickleft # TICKRIGHT tickright # TICKUP tickup # TICKDOWN tickdown # CARETLEFT caretleft # CARETRIGHT caretright # CARETUP caretup # CARETDOWN caretdown calc_e123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_e_small(x): n = 10 f = np.arange(1, n+1).cumprod() b = np.array([x]*n).cumprod() return np.sum(b / f) + 1def calc_e(x): reverse = False if x &lt; 0: # 处理负数 x = -x reverse = True ln2 = 0.69314718055994530941723212145818 c = x / ln2 a = int(c+0.5) b = x - a*ln2 y = (2 ** a) * calc_e_small(b) if reverse: return 1/y return yif __name__ == "__main__": t1 = np.linspace(-2, 0, 10, endpoint=False) t2 = np.linspace(0, 3, 20) t = np.concatenate((t1, t2)) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_e(x) print 'e^', x, ' = ', y[i], '(近似值)\t', math.exp(x), '(真实值)' # print '误差：', y[i] - math.exp(x) plt.figure(facecolor='w') mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 指数函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('exp(X)', fontsize=15) plt.grid(True) plt.show() calc_sin123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_sin_small(x): x2 = -x ** 2 t = x f = 1 sum = 0 for i in range(10): sum += t / f t *= x2 f *= ((2*i+2)*(2*i+3)) return sumdef calc_sin(x): a = x / (2*np.pi) k = np.floor(a) a = x - k*2*np.pi return calc_sin_small(a)if __name__ == "__main__": t = np.linspace(-2*np.pi, 2*np.pi, 100, endpoint=False) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_sin(x) print 'sin(', x, ') = ', y[i], '(近似值)\t', math.sin(x), '(真实值)' # print '误差：', y[i] - math.exp(x) mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.figure(facecolor='w') plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 正弦函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('sin(X)', fontsize=15) plt.xlim((-7, 7)) plt.ylim((-1.1, 1.1)) plt.grid(True) plt.show() class_intro123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-class People: def __init__(self, n, a, s): self.name = n self.age = a self.__score = s self.print_people() # self.__print_people() # 私有函数的作用 def print_people(self): str = u'%s的年龄：%d，成绩为：%.2f' % (self.name, self.age, self.__score) print str __print_people = print_peopleclass Student(People): def __init__(self, n, a, w): People.__init__(self, n, a, w) self.name = 'Student ' + self.name def print_people(self): str = u'%s的年龄：%d' % (self.name, self.age) print strdef func(p): p.age = 11if __name__ == '__main__': p = People('Tom', 10, 3.14159) func(p) # p传入的是引用类型 p.print_people() print # 注意分析下面语句的打印结果，是否觉得有些“怪异”？ j = Student('Jerry', 12, 2.71828) print # 成员函数 p.print_people() j.print_people() print People.print_people(p) People.print_people(j) stat12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport mathimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmdef calc_statistics(x): n = x.shape[0] # 样本个数 # 手动计算 m = 0 m2 = 0 m3 = 0 m4 = 0 for t in x: m += t m2 += t*t m3 += t**3 m4 += t**4 m /= n m2 /= n m3 /= n m4 /= n mu = m sigma = np.sqrt(m2 - mu*mu) skew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3 kurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3 print '手动计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 使用系统函数验证 mu = np.mean(x, axis=0) sigma = np.std(x, axis=0) skew = stats.skew(x) kurtosis = stats.kurtosis(x) return mu, sigma, skew, kurtosisif __name__ == '__main__': d = np.random.randn(100000) print d mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 一维直方图 mpl.rcParams[u'font.sans-serif'] = 'SimHei' mpl.rcParams[u'axes.unicode_minus'] = False y1, x1, dummy = plt.hist(d, bins=50, normed=True, color='g', alpha=0.75) t = np.arange(x1.min(), x1.max(), 0.05) y = np.exp(-t**2 / 2) / math.sqrt(2*math.pi) plt.plot(t, y, 'r-', lw=2) plt.title(u'高斯分布，样本个数：%d' % d.shape[0]) plt.grid(True) plt.show() d = np.random.randn(100000, 2) mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 二维图像 N = 30 density, edges = np.histogramdd(d, bins=[N, N]) print '样本总数：', np.sum(density) density /= density.max() x = y = np.arange(N) t = np.meshgrid(x, y) fig = plt.figure(facecolor='w') ax = fig.add_subplot(111, projection='3d') ax.scatter(t[0], t[1], density, c='r', s=15*density, marker='o', depthshade=True) ax.plot_surface(t[0], t[1], density, cmap=cm.Accent, rstride=2, cstride=2, alpha=0.9, lw=0.75) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.title(u'二元高斯分布，样本个数：%d' % d.shape[0], fontsize=20) plt.tight_layout(0.1) plt.show() MultiGuass12345678910111213141516171819202122232425262728293031#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': x1, x2 = np.mgrid[-5:5:51j, -5:5:51j] x = np.stack((x1, x2), axis=2) plt.figure(figsize=(9, 8), facecolor='w') sigma = (np.identity(2), np.diag((3,3)), np.diag((2,5)), np.array(((2,1), (2,5)))) for i in np.arange(4): ax = plt.subplot(2, 2, i+1, projection='3d') norm = stats.multivariate_normal((0, 0), sigma[i]) y = norm.pdf(x) ax.plot_surface(x1, x2, y, cmap=cm.Accent, rstride=1, cstride=1, alpha=0.9, lw=0.3) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.suptitle(u'二元高斯分布方差比较', fontsize=18) plt.tight_layout(1.5) plt.show() gamma12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom scipy.special import gammafrom scipy.special import factorialmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': N = 5 x = np.linspace(0, N, 50) y = gamma(x+1) plt.figure(facecolor='w') plt.plot(x, y, 'r-', x, y, 'm*', lw=2) z = np.arange(0, N+1) f = factorial(z, exact=True) # 阶乘 print f plt.plot(z, f, 'go', markersize=8) plt.grid(b=True) plt.xlim(-0.1,N+0.1) plt.ylim(0.5, np.max(y)*1.05) plt.xlabel(u'X', fontsize=15) plt.ylabel(u'Gamma(X) - 阶乘', fontsize=15) plt.title(u'阶乘和Gamma函数', fontsize=16) plt.show() Benford123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom time import timefrom scipy.special import factorialimport mathmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def top1(number, a): number /= a while number &gt;= 10: number /= 10 a *= 10 return number, adef top2(number, N2): while number &gt;= N2: number /= 10 n = number while number &gt;= 10: number /= 10 return n, numberdef top3(number): number -= int(number) return int(10 ** number)def top4(number): number -= int(number) frequency[int(10 ** number) - 1] += 1if __name__ == '__main__': N = 100000 x = range(1, N+1) frequency = np.zeros(9, dtype=np.int) f = 1 print '开始计算...' t0 = time() # top1 # a = 1 # for t in x: # f *= t # i, a = top1(f, a) # # print t, i, f, a # frequency[i-1] += 1 # top2 # N2 = N ** 3 # for t in x: # f *= t # f, i = top2(f, N2) # frequency[i-1] += 1 # Top 3：实现1 # f = 0 # for t in x: # f += math.log10(t) # frequency[top3(f) - 1] += 1 # Top 3：实现2 # y = np.cumsum(np.log10(x)) # for t in y: # frequency[top3(t) - 1] += 1 # Top 4：本质与Top3相同 y = np.cumsum(np.log10(x)) map(top4, y) t1 = time() print '耗时：', t1 - t0 print frequency t = np.arange(1, 10) plt.plot(t, frequency, 'r-', t, frequency, 'go', lw=2, markersize=8) for x,y in enumerate(frequency): plt.text(x+1.1, y, frequency[x], verticalalignment='top', fontsize=15) plt.title(u'%d!首位数字出现频率' % N, fontsize=18) plt.xlim(0.5, 9.5) plt.ylim(0, max(frequency)*1.03) plt.grid(b=True) plt.show() # 使用numpy # N = 170 # x = np.arange(1, N+1) # f = np.zeros(9, dtype=np.int) # t1 = time() # y = factorial(x, exact=False) # z = map(top, y) # t2 = time() # print '耗时 = \t', t2 - t1 # for t in z: # f[t-1] += 1 # print f Pearson123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltimport warningsmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def calc_pearson(x, y): std1 = np.std(x) # np.sqrt(np.mean(x**2) - np.mean(x)**2) std2 = np.std(y) cov = np.cov(x, y, bias=True)[0,1] return cov / (std1 * std2)def intro(): N = 10 x = np.random.rand(N) y = 2 * x + np.random.randn(N) * 0.1 print x print y print '系统计算：', stats.pearsonr(x, y)[0] print '手动计算：', calc_pearson(x, y)def rotate(x, y, theta=45): data = np.vstack((x, y)) # print data mu = np.mean(data, axis=1) mu = mu.reshape((-1, 1)) # print mu data -= mu # print data theta *= (np.pi / 180) c = np.cos(theta) s = np.sin(theta) m = np.array(((c, -s), (s, c))) return m.dot(data) + mudef pearson(x, y, tip): clrs = list('rgbmycrgbmycrgbmycrgbmyc') plt.figure(figsize=(10, 8), facecolor='w') for i, theta in enumerate(np.linspace(0, 90, 6)): xr, yr = rotate(x, y, theta) p = stats.pearsonr(xr, yr)[0] # print calc_pearson(xr, yr) print '旋转角度：', theta, 'Pearson相关系数：', p str = u'相关系数：%.3f' % p plt.scatter(xr, yr, s=40, alpha=0.9, linewidths=0.5, c=clrs[i], marker='o', label=str) plt.legend(loc='upper left', shadow=True) plt.xlabel(u'X') plt.ylabel(u'Y') plt.title(u'Pearson相关系数与数据分布：%s' % tip, fontsize=18) plt.grid(b=True) plt.show()if __name__ == '__main__': # warnings.filterwarnings(action='ignore', category=RuntimeWarning) np.random.seed(0) # intro() N = 1000 # tip = u'一次函数关系' # x = np.random.rand(N) # y = np.zeros(N) + np.random.randn(N)*0.001 # tip = u'二次函数关系' # x = np.random.rand(N) # y = x ** 2 #+ np.random.randn(N)*0.002 # tip = u'正切关系' # x = np.random.rand(N) * 1.4 # y = np.tan(x) # tip = u'二次函数关系' # x = np.linspace(-1, 1, 101) # y = x ** 2 tip = u'椭圆' x, y = np.random.rand(2, N) * 60 - 30 y /= 5 idx = (x**2 / 900 + y**2 / 36 &lt; 1) x = x[idx] y = y[idx] pearson(x, y, tip) candle123456789101112131415161718192021222324252627282930#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom matplotlib.finance import candlestick_ohlcif __name__ == "__main__": mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False np.set_printoptions(suppress=True, linewidth=100, edgeitems=5) data = np.loadtxt('7.SH600000.txt', dtype=np.float, delimiter='\t', skiprows=2, usecols=(1, 2, 3, 4)) data = data[:30] N = len(data) t = np.arange(1, N+1).reshape((-1, 1)) data = np.hstack((t, data)) fig, ax = plt.subplots(facecolor='w') fig.subplots_adjust(bottom=0.2) candlestick_ohlc(ax, data, width=0.6, colorup='r', colordown='g', alpha=0.9) plt.xlim((0, N+1)) plt.grid(b=True) plt.title(u'股票K线图', fontsize=18) plt.tight_layout(2) plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化]]></title>
    <url>%2F2017%2F11%2F08%2F%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[凸集基本概念凸集保凸运算凸集和凸函数凸函数图像的上方区域，一定是凸集；一个函数图像的上方区域为凸集，则该函数是凸函数。 仿射集（Affine Set）定义：通过集合C中任意两个不同点的直线仍然在集合C内，则称集合C为仿射集。 仿射集的例子：直线、平面、超平面 凸集集合C内任意两点间的线段均在集合C内，则称集合C为凸集。 两个点和k个点的表达版本内涵是一样的。 仿射集和凸集的关系因为仿射集的条件比凸集的条件强，所以，仿射集必然是凸集。 凸包集合C的所有点的凸组合形成的集合，叫做集合c的凸包。集合C的凸包是能够包含C的最小的凸集。 超平面和半空间 保持凸性的运算 集合交运算 仿射变换 函数f=ax+b的形式，称函数是仿射的，即线性函数加常数的形式。 透视变换 透视函数对向量进行伸缩（规范化），使得最后一维的分量为1并舍弃之。 凸集的透视变换仍然是凸集。 投射变换（线性分式变换） 分割超平面分割超平面的定义 分割超平面的构造两个集合的距离，定义为两个集合间元素的最短距离。 做集合A和集合B最短线段的垂直平分线。 支撑超平面 凸函数基本概念一般化定义 一阶可微 二阶可微 凸函数举例 Jensen不等式：若f是凸函数 凸函数保凸运算保持函数凸性的算子 共轭函数共轭函数的定义 共轭函数的理解 共轭函数的求解举例 凸优化一般提法对偶函数 强对偶KKT条件强对偶条件 Karush-Kuhn-Tucker(KKT)条件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode 调试适配器进程已意外终止]]></title>
    <url>%2F2017%2F10%2F24%2Fvscode%20%E8%B0%83%E8%AF%95%E9%80%82%E9%85%8D%E5%99%A8%E8%BF%9B%E7%A8%8B%E5%B7%B2%E6%84%8F%E5%A4%96%E7%BB%88%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[隔了N天以后，重新打开，一进来就重新下载C依赖，然后提示reload。然后莫名的就可以用了。我晕。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李德荃关于偏度与峰度的讲解]]></title>
    <url>%2F2017%2F10%2F24%2F%E6%9D%8E%E5%BE%B7%E8%8D%83%E5%85%B3%E4%BA%8E%E5%81%8F%E5%BA%A6%E4%B8%8E%E5%B3%B0%E5%BA%A6%E7%9A%84%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[偏度这一指标，又称偏斜系数、偏态系数，是用来帮助判断数据序列的分布规律性的指标。 在数据序列呈对称分布（正态分布）的状态下，其均值、中位数和众数重合。且在这三个数的两侧，其它所有的数据完全以对称的方式左右分布。 如果数据序列的分布不对称，则均值、中位数和众数必定分处不同的位置。这时，若以均值为参照点，则要么位于均值左侧的数据较多，称之为右偏；要么位于均值右侧的数据较多，称之为左偏；除此无它。考虑到所有数据与均值之间的离差之和应为零这一约束，则当均值左侧数据较多的时候，均值的右侧必定存在数值较大的“离群”数据；同理，当均值右侧数据较多的时候，均值的左侧必定存在数值较小的“离群”数据。 一般将偏度定义为三阶中心矩与标准差的三次幂之比。 在上述定义下，偏度系数的取值无非三种情景： 1.当数据序列呈正态分布的时候，由于均值两侧的数据完全对称分布，其三阶中心矩必定为零，于是满足正态分布的数据序列的偏度系数必定等于零。 2.当数据序列非对称分布的时候，如果均值的左侧数据较多，则其右侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取正值。因此，当数据的分布呈右偏的时候，其偏度系数将大于零。 3.当数据序列非对称分布的时候，如果均值的右侧数据较多，则其左侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取负值。因此，当数据的分布呈左偏的时候，偏度系数将小于零。在右偏的分布中，由于大部分数据都在均值的左侧，且均值的右侧存在“离群”数据，这就使得分布曲线的右侧出现一个长长的拖尾；而在左偏的分布中，由于大部分数据都在均值的右侧，且均值的左侧存在“离群”数据，从而造成分布曲线的左侧出现一个长长的拖尾。 可见，在偏度系数的绝对值较大的时候，最有可能的含义是“离群”数据离群的程度很高（很大或很小），亦即分布曲线某侧的拖尾很长。 但“拖尾很长”与“分布曲线很偏斜”不完全等价。例如，也不能排除在数据较少的那一侧，只是多数数据的离差相对于另一侧较大，但不存在明显“离群”数据的情景。所以，为准确判断分布函数的偏斜程度，最好的办法是直接观察分布曲线的几何图形。 与偏度（系数）一样，峰度（系数）也是一个用于评价数据系列分布特征的指标。根据这两个指标，我们可以判断数据系列的分布是否满足正态性，进而评价平均数指标的使用价值。一般地，对于一个偏态分布、肥尾分布特征很明显的数据序列来说，平均数这个指标极易令人误解数据序列分布的集中位置及其集中程度，故此使用起来要极其谨慎。 峰度（系数）等于数据序列的四阶中心矩与标准差的四次幂之比。设若先将数据标准化，则峰度（系数）相当于标准化数据序列的四阶中心矩。 显然，一个数据距离均值越远，其对四阶中心矩计算结果的影响越大。是故，峰度（系数）是一个用于衡量离群数据离群度的指标。峰度（系数）越大，说明该数据系列中的极端值越多。这在数据序列的分布曲线图中来看，体现为存在明显的“肥尾”。当然，峰度（系数）较大也可能说明离群数据取值的极端性很严重，或者各数据距离均值的距离普遍较远。可见，峰度（系数）的大小到底能说明什么问题，最好还是看图确定。 根据Jensen不等式，可以确定出峰度（系数）的取值范围：它的下限不会低于1，上限不会高于数据的个数。 有一些典型分布的峰度（系数）值得特别关注。例如，正态分布的峰度（系数）为常数3，均匀分布的峰度（系数）为常数1.6。在统计实践中，我们经常把这两个典型的分布曲线作为评价样本数据序列分布性态的参照。 在金融学中，峰度这个指标具有一定的意义。一项金融资产，设若其预期收益率的峰度较高，则说明该项资产的预期收益率有相对较高的概率取极端值。换句话说，该项资产未来行市发生剧烈波动的概率相对较高。 这个讲解从实用角度解释了偏度与峰度，即这两个指标侧重于对图形的描述，在计算出具体的偏度与峰度后，更主要是要参考图形来分类分析，而不是单纯依照数值简单判断。 其二，根据这两个指标的分类，使用者可以按照自己的需求在原公式的基础上编程时再细化，即把图形数据化（因为如果数据量很大的话每个都要附上图形，计算机能受得了，但分析者还不得累死），这样得出的结果更有助于分析归纳，否则单单依靠原公式，分析归纳的难度有些大。 第三，这两个指标都是基于均值、标准差而来的，所以分析时可以根据均值与标准差来判别长尾属于哪种类型，从而确定其影响。 via http://bbs.pinggu.org/thread-3417092-1-1.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>偏度</tag>
        <tag>峰度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次python数据处理性能调优]]></title>
    <url>%2F2017%2F10%2F21%2F%E8%AE%B0%E4%B8%80%E6%AC%A1python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Python垃圾回收机制根据官方的描叙，Python中，有2中方式将会触发垃圾回收：1、用户显示调用gc.collect()2、每次Python为新对象分配内存时，检查threshold阀值，当对象数量超过threshold设置的阀值就开始进行垃圾回收。 因为数据量太大，处理过程中留下太多暂时不能清除的变量，而python的垃圾回收却在一遍一遍地扫这些不断在增长的列表，导致程序受到的影响越来越大。赶紧证实一下，import gc，然后在数据载入模块前gc.disable（）,结束后再gc.enable()。结果原来要跑将近两个小时的程序，这下不用5分钟就跑完了。cool~！用gc.get_count()也证明了之前的猜想，在第一次运行之后临时变量数目就从几百上升到百万，并一直在涨。 如果你的python程序在处理大数据量的问题，并且出现某个子程序在做同样量的工作，却越跑越慢的情况。 程序代码： 12345678910111213141516171819202122def series_trans(dataset): # 一行转多行 dataset_trans = pd.DataFrame(&#123;'user_id':dataset['user_id'], 'shop_id':dataset['shop_id'], 'time_stamp':dataset['time_stamp'], 'longitude':dataset['longitude'], 'latitude':dataset['latitude'], 'wifi_infos':dataset['wifi_splits']&#125;) return dataset_transprint time.asctime( time.localtime(time.time()) )dataset_trans = series_trans(dataset.loc[0,:])for i in xrange(1,dataset.shape[0]): if i % 10000 == 0: print i print time.asctime( time.localtime(time.time()) ) dataset_trans = dataset_trans.append(series_trans(dataset.loc[i,:])) 程序的性能表现如下： 1234567891011121314151617181920212223242526272829303132333435363738Sat Oct 21 11:01:30 201710000Sat Oct 21 11:02:07 201720000Sat Oct 21 11:03:41 201730000Sat Oct 21 11:06:12 201740000Sat Oct 21 11:09:37 201750000Sat Oct 21 11:13:55 201760000Sat Oct 21 11:19:10 201770000Sat Oct 21 11:25:26 201780000Sat Oct 21 11:33:03 201790000Sat Oct 21 11:41:21 2017100000Sat Oct 21 11:50:33 2017110000Sat Oct 21 12:00:41 2017120000Sat Oct 21 12:11:45 2017130000Sat Oct 21 12:23:43 2017140000Sat Oct 21 12:36:37 2017150000Sat Oct 21 12:50:25 2017160000Sat Oct 21 13:05:10 2017170000Sat Oct 21 13:20:46 2017180000Sat Oct 21 13:37:18 2017 采用：dataset_trans.loc[10:19,] = dataset_trans会提示错误：info_idx = indexer[info_axis]IndexError: tuple index out of range DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.所以一般说来dataframe就是a set of columns, each column is an array of values. In pandas, the array is one way or another a (maybe variant of) numpy ndarray. 而ndarray本身不存在一种in place append的操作。。。因为它实际上是一段连续内存。。。任何需要改变ndarray长度的操作都涉及分配一段长度合适的新的内存，然后copy。。。这是这类操作慢的原因。。。如果pandas dataframe没有用其他设计减少copy的话，我相信Bren说的”That’s probably as efficient as any”是很对的。。。所以in general, 正如Bren说的。。。Pandas/numpy structures are fundamentally not suited for efficiently growing.Matti 和 arynaq说的是两种常见的对付这个问题的方法。。。我想Matti实际的意思是把要加的rows收集成起来然后concatenate, 这样只copy一次。arynaq的方法就是预先分配内存比较好理解。。。如果你真的需要incrementally build a dataframe的话，估计你需要实际测试一下两种方法。。。我的建议是，如有可能，尽力避免incrementally build a dataframe, 比如用其他data structure 收集齐所有data然后转变成dataframe做分析。。。 via Python垃圾回收(gc)拖累了程序执行性能？ python pandas 怎样高效地添加一行数据？]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio 无法编译新打开的C文件]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20%E6%97%A0%E6%B3%95%E7%BC%96%E8%AF%91%E6%96%B0%E6%89%93%E5%BC%80%E7%9A%84C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[用 Visual Studio打开有c文件的文件夹，发现里面的c文件都无法编译，导航栏里面连生成按钮都没有，调试按钮也是灰色的。 需在项目中单个添加c文件。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>Visual Studio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Code工具上手体验]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20Code%E5%B7%A5%E5%85%B7%E4%B8%8A%E6%89%8B%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[下载安装Visual Studio Code,地址：https://code.visualstudio.com/Download 在VSCode内安装c++插件：打开vscode，Ctrl+P之后输入ext install c++ 弹出： 12扩展：商店 错误 不用管它，重新再来一次，Ctrl+P之后输入ext install c++ 安装第一个官方的。 重启vscode。然后发现重新启动，vscode开始自动下载插件依赖。依赖自动下载安装完成。 抬头，提示reload。 查看本机gcc环境： 123456789E:\Code\solar-correlation-map&gt;gcc -vUsing built-in specs.COLLECT_GCC=gccCOLLECT_LTO_WRAPPER=D:/TDM-GCC-64/bin/../libexec/gcc/x86_64-w64-mingw32/5.1.0/lto-wrapper.exeTarget: x86_64-w64-mingw32Configured with: ../../../src/gcc-5.1.0/configure --build=x86_64-w64-mingw32 --enable-targets=all --enable-languages=ada,c,c++,fortran,lto,objc,obj-c++ --enable-libgomp --enable-lto --enable-graphite --enable-cxx-flags=-DWINPTHREAD_STATIC --disable-build-with-cxx --disable-build-poststage1-with-cxx --enable-libstdcxx-debug --enable-threads=posix --enable-version-specific-runtime-libs --enable-fully-dynamic-string --enable-libstdcxx-threads --enable-libstdcxx-time --with-gnu-ld --disable-werror --disable-nls --disable-win32-registry --prefix=/mingw64tdm --with-local-prefix=/mingw64tdm --with-pkgversion=tdm64-1 --with-bugurl=http://tdm-gcc.tdragon.net/bugsThread model: posixgcc version 5.1.0 (tdm64-1) 新建文件夹，在文件夹内新建文件hello.cpp：123456789\#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 0; cout&lt;&lt;a; return 0;&#125; 按下F5，启动调试，提示需要配置。将launch.json的文件内容替换成如下： 123456789101112131415161718192021&#123; "version": "0.2.0", "configurations": [ &#123; "name": "C++ Launch (GDB)", // 配置名称，将会在启动配置的下拉菜单中显示 "type": "cppdbg", // 配置类型，这里只能为cppdbg "request": "launch", // 请求配置类型，可以为launch（启动）或attach（附加） "launchOptionType": "Local", // 调试器启动类型，这里只能为Local "targetArchitecture": "x86", // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 "program": "$&#123;file&#125;.exe", // 将要进行调试的程序的路径 // "miDebuggerPath":"c:\\MinGW\\bin\\gdb.exe", // miDebugger的路径，注意这里要与MinGw的路径对应 "miDebuggerPath":"D:\\TDM-GCC-64\\bin\\gdb.exe", "args": ["blackkitty", "1221", "# #"], // 程序调试时传递给程序的命令行参数，一般设为空即可 "stopAtEntry": false, // 设为true时程序将暂停在程序入口处，一般设置为false "cwd": "$&#123;workspaceRoot&#125;", // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 "externalConsole": true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 "preLaunchTask": "g++" // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; 注意miDebuggerPath要与MinGw的路径对应 替换后保存，然后切换至hello.cpp，按F5进行调试，此时会弹出一个信息框要求你配置任务运行程序，点击它~ 选择任务运行程序，点击Others，跳出tasks.json的配置文件。替换成如下代码: 123456789101112131415161718&#123; "version": "0.1.0", "command": "g++", "args": ["-g","$&#123;file&#125;","-o","$&#123;file&#125;.exe"], // 编译命令参数 "problemMatcher": &#123; "owner": "cpp", "fileLocation": ["relative", "$&#123;workspaceRoot&#125;"], "pattern": &#123; "regexp": "^(.*):(\\d+):(\\d+):\\s+(warning|error):\\s+(.*)$", "file": 1, "line": 2, "column": 3, "severity": 4, "message": 5 &#125; &#125;&#125; 保存一下，然后切换至hello.cpp，再次按F5启动调试: 控制台输出： 12345678910111213141516171819202122=thread-group-added,id="i1"GNU gdb (GDB) 7.9.1Copyright (C) 2015 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-w64-mingw32".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".=cmd-param-changed,param="pagination",value="off"[New Thread 16288.0x3d10]Breakpoint 1, main () at e:\Code\C_playground\hello.cpp:55 int a = 0;[Inferior 1 (process 16288) exited normally]The program 'e:\Code\C_playground\hello.cpp.exe' has exited with code 0 (0x00000000). 文档目录如下：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pydotplus绘决策树]]></title>
    <url>%2F2017%2F10%2F17%2F%E4%BD%BF%E7%94%A8pydotplus%E7%BB%98%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[首先安装pydotplus pip install pydotplus 然后打印训练好的clf: 123456789import pydotplusdot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_dummy.columns, class_names='target', filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") 发现报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-27-2c78351c7fea&gt;", line 7, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; prog=self.prog: self.write(path, format=f, prog=prog) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write fobj.write(self.create(prog, format)) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create 'GraphViz\'s executables not found')InvocationException: GraphViz's executables not found 提示没有找到&#39;GraphViz\&#39;s executables&#39;: 下载安装graphviz-2.38.msiurl：http://www.graphviz.org/pub/graphviz/stable/windows/graphviz-2.38.msi 源地址下载比较慢或者打不开，可以从这里下载：http://download.csdn.net/download/boredbird32/10025853 添加安装路径到系统Path环境变量：D:\Program Files (x86)\Graphviz2.38\;D:\Program Files (x86)\Graphviz2.38\bin\ 重新启动Python环境发现还是报同样的错误。 点开文件&quot;D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py&quot; 找到 Method 3 (Windows only)那一段，修改里面的path为当时安装graphviz的地址。 12345678910111213141516# Method 3 (Windows only)if os.sys.platform == 'win32': # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" 重启，又报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-7-3735ccaa5368&gt;", line 5, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; lambda path, File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create if self.progs is None:InvocationException: GraphViz's executables not found 继续点进去，修改源文件：加上调试语句： 123456789101112131415161718192021222324252627282930313233343536373839# Method 3 (Windows only)if os.sys.platform == 'win32': print('come inside method 3') # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" print(path) progs = __find_executables(path) print(progs) if progs is not None: print("Used default install location") return progsfor path in ( '/usr/bin', '/usr/local/bin', '/opt/local/bin', '/opt/bin', '/sw/bin', '/usr/share', '/Applications/Graphviz.app/Contents/MacOS/'): progs = __find_executables(path) if progs is not None: # print("Used path") return progs# Failed to find GraphVizreturn None 再次运行，发现： 12345InvocationException: GraphViz's executables not foundcome inside method 3C:\Program Files\ATT\GraphViz\binNone 问题就出在这个path上。那干脆我们就直接在判断条件外面指定path： 12345path = r"D:\Program Files (x86)\Graphviz2.38\bin"# print(path)progs = __find_executables(path)# print(progs) 再次运行，问题解决了： 1234567come inside method 3D:\Program Files (x86)\Graphviz2.38\bin&#123;'twopi': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\twopi.exe', 'fdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\fdp.exe', 'circo': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\circo.exe', 'neato': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\neato.exe', 'dot': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe', 'sfdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\sfdp.exe'&#125;Used default install locationOut[3]: True 最后注释掉测试语句。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次诡异的Python中文乱码]]></title>
    <url>%2F2017%2F10%2F17%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E8%AF%A1%E5%BC%82%E7%9A%84Python%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[有一天突然出现了这样的中文乱码： 我的第一反应就是检查：# -*- coding:utf-8 -*- 然后检查： 12345import syssys.getdefaultencoding()Out[5]: 'utf-8' 这，，是中文编码的啊，再检查： 12345678from chardet import detectcfg.dataset_train['white_list_type'][0]Out[8]: '\xb7\xc7\xb0\xd7\xc3\xfb\xb5\xa5'detect(cfg.dataset_train['white_list_type'][0])Out[9]: &#123;'confidence': 0.0, 'encoding': None&#125; 终于发现问题了，&#39;encoding&#39;: None 然后，那就在read_csv的时候指定编码： 123456789101112131415161718192021222324dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8')Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-13-a9c949155761&gt;", line 1, in &lt;module&gt; dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8') File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 646, in parser_f return _read(filepath_or_buffer, kwds) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 401, in _read data = parser.read() File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 939, in read ret = self._engine.read(nrows) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 1508, in read data = self._reader.read(nrows) File "pandas\parser.pyx", line 848, in pandas.parser.TextReader.read (pandas\parser.c:10415) File "pandas\parser.pyx", line 870, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:10691) File "pandas\parser.pyx", line 947, in pandas.parser.TextReader._read_rows (pandas\parser.c:11728) File "pandas\parser.pyx", line 1049, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:13162) File "pandas\parser.pyx", line 1108, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:14116) File "pandas\parser.pyx", line 1206, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:16172) File "pandas\parser.pyx", line 1222, in pandas.parser.TextReader._string_convert (pandas\parser.c:16400) File "pandas\parser.pyx", line 1458, in pandas.parser._string_box_utf8 (pandas\parser.c:22072)UnicodeDecodeError: 'utf8' codec can't decode byte 0xb7 in position 0: invalid start byte 什么情况，一阵百度，无解。 然后用notepad++打开看下我的csv文件编码： 居然没有一种格式是选中的。。。怎么会出现这种情况，我从数据库导出的时候明明已经指定了编码格式为utf-8，而且后面又打开指定了编码格式为utf-8，怎么现在会是未指定的状态。。 好吧，重新指定csv文件编码为utf-8，再重新试下： 12345678910111213dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv')detect(dataset['white_list_type'][0])Out[17]: &#123;'confidence': 0.938125, 'encoding': 'utf-8'&#125;dataset['white_list_type'][0]Out[18]: '\xe9\x9d\x9e\xe7\x99\xbd\xe5\x90\x8d\xe5\x8d\x95'dataset['white_list_type']Out[19]: 0 非白名单1 普通白名单2 普通白名单 一切又正常了。诡异。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Django REST Framework创建API服务]]></title>
    <url>%2F2017%2F10%2F15%2F%E5%9F%BA%E4%BA%8EDjango%20REST%20Framework%E5%88%9B%E5%BB%BAAPI%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Django REST Framework 安装 123pip install djangorestframeworkpip install markdown # Markdown support for the browsable API.pip install django-filter # Filtering support 看下我的环境：1234567(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;pip freezeDjango==1.11.6django-bootstrap3==9.0.0django-filter==1.0.4djangorestframework==3.7.0Markdown==2.6.9pytz==2017.2 创建一个新的项目django_rest，在项目下创建名为api的应用。 cmd.exe 12345678910E:\Code\virtualenvs\myenvs&gt;.\Scripts\activate.bat(myenvs) E:\Code\virtualenvs\myenvs&gt;python .\Scripts\django-admin.py startproject django_rest(myenvs) E:\Code\virtualenvs\myenvs&gt;cd django_rest\(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py startapp api(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt; 打开settings.py文件，添加应用。 settings.py 12345678910111213141516171819202122# Application definitionINSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'api',]......# 在文件末尾添加REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES':( 'rest_framework.permissions.IsAuthenticated', )&#125; “rest_framework”为Django REST Framework应用，”api”为我们自己创建的应用。默认的权限策略可以设置在全局范围内，通过DEFAULT_PERMISSION_CLASSES设置。 通过migrate命令执行数据库迁移。 cmd.exe 1234(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py makemigrations(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py migrate 通过createsuperuser命令创建超级管理员账户admin/admin123456。 cmd.exe 123456(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py createsuperuserUsername (leave blank to use 'maomaochong'): adminEmail address: admin@email.comPassword:Password (again):Superuser created successfully. 创建数据序列化，在api应用下创建serializers.py文件。 serializers.py 1234567891011121314from django.contrib.auth.models import User,Groupfrom rest_framework import serializersclass UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups')class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') Serializers用于定义API的表现形式，如返回哪些字段、返回怎样的格式等。这里序列化Django自带的User和Group。 编写视图文件，打开api应用下的views.py文件，编写如下代码。 views.py 123456789101112131415161718192021222324# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.contrib.auth.models import User,Groupfrom rest_framework import viewsetsfrom api.serializers import UserSerializer,GroupSerializer# ViewSets 定义视图的展现形式class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializerclass GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all().order_by('-date_joined') serializer_class = GroupSerializer 在Django REST framework 中，ViewSets用于定义视图的展现形式，例如返回哪些内容，需要做哪些权限处理。 在URL中会定义相应的规则到ViewSet。ViewSet则通过serializer_class 找到对应的Serializers。这里讲User和Group的所有对象赋予queryset，并返回这些值。在UserSerializer和GroupSerializer中定义要返回的字段。 打开…/django_rest/urls.py文件，添加api的路由配置。 urls.py 12345678910111213141516171819from django.conf.urls import url, includefrom django.contrib import adminfrom rest_framework import routersfrom api import views# Routers provide an easy way of automatically determining the URL conf.router = routers.DefaultRouter()router.register(r'users',views.UserViewSet)router.register(r'groups',views.GroupViewSet)# Wire up out API using automatic URL routing.# Additionally,we include login URLs for the browsable API.urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework'))] 因为使用的是ViewSets,所以可以使用routers类自动生成URL conf。 通过runserver命令启动服务。 cmd.exe 12345678(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py runserverPerforming system checks...System check identified no issues (0 silenced).October 15, 2017 - 12:32:14Django version 1.11.6, using settings 'django_rest.settings'Starting development server at http://127.0.0.1:8000/Quit the server with CTRL-BREAK. 用超级管理员admin/admin123456登录。 接下来在django_rest项目的基础上，创建模型，打开api应用下的models.py文件。 models.py 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.db import models# Create your models here.# 发布会class Event(models.Model): name = models.CharField(max_length=100) # 发布会标题 limit = models.IntegerField() # 限制人数 status = models.BooleanField() # 状态 address = models.CharField(max_length=200) # 地址 start_time = models.DateTimeField('events time') # 发布会时间 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） def __str__(self): return self.name# 嘉宾class Guest(models.Model): event = models.ForeignKey(Event) # 关联发布会id realname = models.CharField(max_length=64) # 姓名 phone = models.CharField(max_length=16) # 手机号 email = models.EmailField() # 邮箱 sign = models.BooleanField() # 签到状态 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） class Meta: unique_together = ('phone', 'event') def __str__(self): return self.realname 然后执行数据库迁移。 cmd.exe 123456789(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py makemigrations apiMigrations for 'api': api\migrations\0001_initial.py - Create model Event - Create model Guest - Alter unique_together for guest (1 constraint(s))(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py migrate 添加发布会数据序列化，打开api应用下的serializers.py文件（上面创建的）。 serializers.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 from django.contrib.auth.models import User,Group from rest_framework import serializers from api.models import Event,Guest class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups') class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') class EventSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Event fields = ('url','name','address','start_time','limit','status') class GuestSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Guest fields = ('url','realname','phone','email','sign','event')``` 打开api应用下的views.py文件，定义发布会和嘉宾视图类。&gt; views.py``` python # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.shortcuts import render # Create your views here. from django.contrib.auth.models import User,Group from rest_framework import viewsets from api.serializers import UserSerializer,GroupSerializer, EventSerializer, GuestSerializer from api.models import Event, Guest # ViewSets 定义视图的展现形式 class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializer class GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all() serializer_class = GroupSerializer class EventViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Event.objects.all() serializer_class = EventSerializer class GuestViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Guest.objects.all() serializer_class = GuestSerializer``` 打开.../django_rest/urls.py文件，添加URL配置。&gt; urls.py``` python from django.conf.urls import url, include from django.contrib import admin from rest_framework import routers from api import views # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r'users',views.UserViewSet) router.register(r'groups',views.GroupViewSet) router.register(r'events',views.EventViewSet) router.register(r'guests',views.GuestViewSet) # Wire up out API using automatic URL routing. # Additionally,we include login URLs for the browsable API. urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework')) ]]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python类属性和方法的封装]]></title>
    <url>%2F2017%2F10%2F14%2Fpython%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95%E7%9A%84%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[封装是一种限制直接访问目标属性和方法的机制，但同时它又有利于对数据（对象的方法）进行操作。 封装是一种将抽象性函数接口的实现细节部分包装、隐藏起来的方法。同时，它也是一种防止外界调用端，去访问对象内部实现细节的手段，这个手段是由编程语言本身来提供的。 对象所有的内部表征对于外部来说都是隐藏的，只有对象能直接与内部数据交互。首先，我们需要理解公开（public）和私有（non-public）实例变量和方法。 公开实例变量对于 Python 的类，我们可以使用 constructor 方法初始化公开实例变量： 123class Person: def __init__(self,first_name): self.first_name = first_name 下面我们应用 first_name 的值作为公开实例变量的变元。 12a = Person('zhang')print a.first_name # -&gt; zhang 在类别内： 12class Person: first_name = 'zhang' 现在我们不需要再对 first_name 赋值，所有赋值到 a 的目标都将有类的属性： 12a = Person()print a.first_name # -&gt; zhang 现在我们已经学会如何使用公开实例变量和类属性。除此之外，我们还能管理公开部分的变量值，即对象可以管理其变量的值：Get 和 Set 变量值。保留 Person 类，我们希望能给 first_name 变量赋另外一个值： 123a = Person('zhang')a.first_name = 'Z'print a.first_name # -&gt; Z 如上我们将另外一个值赋予了 first_name 实例变量，因为它又是一个公开变量，所以会更新变量值。 私有实例变量和公开实例变量一样，我们可以使用 constructor 方法或在类的内部声明而定义一个私有实例变量。语法上的不同在于私有实例变量在变量名前面加一个下划线： 1234class Person: def __init__(self,first_name,email): self.first_name = first_name self._email = email 上述定义的 email 变量就是私有变量。12a = Person('zhang','zhang@mail.com')print a._email # -&gt; zhang@mail.com 我们可以访问并且更新它，私有变量仅是一个约定，即他们需要被视为 API 非公开的部分。所以我们可以使用方法在类的定义中完成操作，例如使用两种方法展示私有实例的值与更新实例的值： 12345678910class Person: def __init__(self,first_name,email): self.first_name = first_name self.__email = email def update_email(self,new_email): self.__email = new_email def email(self): return self.__email 现在我们可以使用方法更新或访问私有变量。 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma.__email = 'new_email@mail.com'print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; zhang@mail.coma.update_email('update_email@mail.com')print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; update_email@mail.com 从上面可见，以双下划线打头的名称会导致出现名称重整的行为。具体来说就是上面的这个类中的私有属性会被分别重命名为_C__private 和 _C__private_name。但是为啥两个值不一样？？ 豁然开朗！ 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma._email = 'new_email@mail.com'print a._email # -&gt; new_email@mail.comprint a.email() # -&gt; new_email@mail.coma.update_email('update_email@mail.com')print a._email # -&gt; update_email@mail.comprint a.email() # -&gt; update_email@mail.com]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django自带的server局域网访问]]></title>
    <url>%2F2017%2F10%2F11%2Fdjango%E8%87%AA%E5%B8%A6%E7%9A%84server%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[问题描述用(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver启动服务，对应的访问地址为： 12Starting development server at http://127.0.0.1:8000/ 然后，我把127.0.0.1改为本机的地址发现居然不能访问。 解决方法使用python .\manage.py runserver 0.0.0.0:8000启动服务： 又出现新的错误： 12345678910111213141516171819202122232425DisallowedHost at /post/act/Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Request Method: GETRequest URL: http://10.83.2.132:8000/post/act/?eid=1&amp;status=%27F%27Django Version: 1.11.6Exception Type: DisallowedHostException Value: Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Exception Location: E:\Code\virtualenvs\myenvs\lib\site-packages\django\http\request.py in get_host, line 113Python Executable: E:\Code\virtualenvs\myenvs\Scripts\python.exePython Version: 2.7.13Python Path: ['E:\\Code\\virtualenvs\\myenvs\\src', 'E:\\Code\\virtualenvs\\myenvs\\Scripts\\python27.zip', 'E:\\Code\\virtualenvs\\myenvs\\DLLs', 'E:\\Code\\virtualenvs\\myenvs\\lib', 'E:\\Code\\virtualenvs\\myenvs\\lib\\plat-win', 'E:\\Code\\virtualenvs\\myenvs\\lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs\\Scripts', 'D:\\ProgramData\\Anaconda2\\Lib', 'D:\\ProgramData\\Anaconda2\\DLLs', 'D:\\ProgramData\\Anaconda2\\Lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs', 'E:\\Code\\virtualenvs\\myenvs\\lib\\site-packages']Server time: 星期三, 11 十月 2017 15:59:19 +0800 解决方法去django-admin.py startproject project-name创建的项目中去修改 setting.py 文件： 12 ALLOWED_HOSTS = [‘*’] ＃在这里请求的host添加了＊ 又又出现新的错误：报错SyntaxError: invalid syntax: 这尼玛，这是什么鬼，我的引号呢？？？终端上为什么没有引号，于是我有尝试了&quot;*&quot;双引号，依然不行。然后无穷的百度（动词），居然没一个类似我的问题。老天干嘛要为难我这个小白啊。 解决方法一不做，二不休： 123# ALLOWED_HOSTS = ['*']ALLOWED_HOSTS = list('*') 居然works！ 加逗号也是可以的： 12ALLOWED_HOSTS = ['*',] 效果 参考： django自带的server 让外网主机访问]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Forbidden (CSRF cookie not set.)]]></title>
    <url>%2F2017%2F10%2F11%2FForbidden%20(CSRF%20cookie%20not%20set.)%2F</url>
    <content type="text"><![CDATA[问题描述用postman提交post请求时，终端打印错误Forbidden (CSRF cookie not set.)： 1234567[11/Oct/2017 14:55:00] "GET /post/post/ HTTP/1.1" 200 47Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 14:55:16] "POST /post/post/ HTTP/1.1" 403 2829Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 15:00:06] "POST /post/post/ HTTP/1.1" 403 2829Performing system checks... 情况如下图所示： 解决方法修改settings.py文件，注释掉 django.middleware.csrf.CsrfViewMiddleware&#39;, 效果post请求处理正常。 参考： (django1.10)访问url报错Forbidden (CSRF cookie not set.): xxx]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>CSRF</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore规则不生效的解决办法]]></title>
    <url>%2F2017%2F10%2F11%2Fgitignore%E8%A7%84%E5%88%99%E4%B8%8D%E7%94%9F%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题描述发现添加.gitignore文件后，本地做的修改仍被push到GitHub。.gitignore规则并没有生效。 解决方法原因是如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交： git rm -r --cached . git add . git commit -m &#39;update .gitignore&#39; 如何避免 忽略规则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 ##最后需要强调的一点是，如果你不慎在创建.gitignore文件之前就push了项目，那么即使你在.gitignore文件中写入新的过滤规则，这些规则也不会起作用，Git仍然会对所有文件进行版本管理。简单来说，出现这种问题的原因就是Git已经开始管理这些文件了，所以你无法再通过过滤规则过滤它们。因此一定要养成在项目开始就创建.gitignore文件的习惯，否则一旦push，处理起来会非常麻烦。 参考： Git忽略规则.gitignore梳理 Git忽略文件.gitignore的使用 Git忽略规则和.gitignore规则不生效的解决办法 github官方给的Python项目.gitignore文件模板]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python环境设置默认编码为utf8的方法]]></title>
    <url>%2F2017%2F10%2F11%2FPython%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE%E9%BB%98%E8%AE%A4%E7%BC%96%E7%A0%81%E4%B8%BAutf8%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[从网上fork了虫师的代码，发现每个文件都没有加头注释指定文件编码，我这一个一个改得什么时候。想着虫师也是老司机了，不可能连IDE自动添加头注释都不知道，应该是哪边统一设置的。果然他是直接修改的Python环境配置。 设置之前： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 52853 52854Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'ascii' 设置之后： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 55595 55596Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'utf-8' 如何设置： 可以在Python安装目录下的Lib/site-packages目录中，新建一个sitecustomize.py文件（也可以建在其它地方，然后手工导入，建在这里，每次启动Python的时候设置将自动生效），内容如下： 123import syssys.setdefaultencoding('utf-8') #set default encoding to utf-8 参考： Python设置默认编码为utf8的方法 python错误：AttributeError: ‘module’ object has no attribute ‘setdefaultencoding’问题的解决方法]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API 设计指南-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FRESTful%20API%20Design%20Guide%2F</url>
    <content type="text"><![CDATA[这篇文章写的也很好，很上一篇有很大重复，甚至有些矛盾的地方。但是读下来，依然还是有启发的。 一、协议API与用户的通信协议，总是使用HTTPs协议。 #二、域名应该尽量将API部署在专用域名之下。12https://api.example.com 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。12https://example.org/api/ 三、版本（Versioning）应该将API的版本号放入URL。12https://api.example.com/v1/ 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观 四、路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。123456https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。1234&#123; error: "Invalid API key"&#125; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。1234567&#123;"link": &#123; "rel": "collection https://www.example.com/zoos", "href": "https://api.example.com/zoos", "title": "List of zoos", "type": "application/vnd.yourformat+json"&#125;&#125; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 123456&#123; "current_user_url": "https://api.github.com/user", "authorizations_url": "https://api.github.com/authorizations", // ...&#125; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 12345&#123; "message": "Requires authentication", "documentation_url": "https://developer.github.com/v3"&#125; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他 API的身份认证应该使用OAuth 2.0框架。 服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 via RESTful API 设计指南]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解RESTful架构-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FUnderstanding%20RESTful%20Framework%2F</url>
    <content type="text"><![CDATA[最近在研究Python开发接口微服务，以便用Python开发出来的模型与服务能对接上，提供复杂模型上线的方案。看到了阮一峰的《理解RESTful架构》这篇文章，写的真好，虽然我是还没入门的外外行。 什么是RESTful架构一、起源REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。 文中写到： 123"本文研究计算机科学两大前沿----软件和网络----的交叉点。长期以来，软件研究主要关注软件设计的分类、设计方法的演化，很少客观地评估不同的设计选择对系统行为的影响。而相反地，网络研究主要关注系统之间通信行为的细节、如何改进特定通信机制的表现，常常忽视了一个事实，那就是改变应用程序的互动风格比改变互动协议，对整体表现有更大的影响。我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。"(This dissertation explores a junction on the frontiers of two research disciplines in computer science: software and networking. Software research has long been concerned with the categorization of software designs and the development of design methodologies, but has rarely been able to objectively evaluate the impact of various design choices on system behavior. Networking research, in contrast, is focused on the details of generic communication behavior between systems and improving the performance of particular communication techniques, often ignoring the fact that changing the interaction style of an application can have more impact on performance than the communication protocols used for that interaction. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. ) 二、名称Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。我对这个词组的翻译是”表现层状态转化”。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 三、资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 四、表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 五、状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 六、综述综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 七、误区RESTful架构有一些典型的设计误区。最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：12POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：12345 POST /transaction HTTP/1.1 Host: 127.0.0.1 from=1&amp;to=2&amp;amount=500.00 另一个设计误区，就是在URI中加入版本号： 123456 http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分参见Versioning REST Services：123456 Accept: vnd.example-com.foo+json; version=1.0 Accept: vnd.example-com.foo+json; version=1.1 Accept: vnd.example-com.foo+json; version=2.0 via 理解RESTful架构]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django处理HTTP请求流程]]></title>
    <url>%2F2017%2F10%2F10%2Fdjango%20flow%2F</url>
    <content type="text"><![CDATA[Based on @FULLSTACKCTO’s understanding of Django, this is how a user request is responded to. 1: User requests a page 2: Request reaches Request Middlewares, which could manipulate or answer the request 3: The URLConffinds the related View using urls.py 4: View Middlewares are called, which could manipulate or answer the request 5: The view function is invoked 6: The view could optionally access data through models 7: All model-to-DB interactions are done via a manager 8: Views could use a special context if needed 9: The context is passed to the Template for rendering a: Template uses Filters and Tags to render the output b: Output is returned to the view c: HTTPResponse is sent to the Response Middlerwares d: Any of the response middlewares can enrich the response or return a completely new response e: The response is sent to the user’s browser. via here Django Flowchart]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POST和GET区别]]></title>
    <url>%2F2017%2F10%2F10%2Fget%20or%20post%2F</url>
    <content type="text"><![CDATA[HTTP协议定义了很多与服务器交互的方法，最基本的有4种，分别是GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作，其中最常见请求方式是GET和POST，并且现在浏览器一般只支持GET和POST方法。 GET一般用于获取/查询资源信息，而POST一般用于更新资源信息，他们之间主要区别如下： 1）根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的，这里安全是指该操作用于获取信息而非修改信息，幂等是指对同一URL的多个请求应该返回同样的结果（这一点在实质实现时，可能并不满足）；POST表示可能修改变服务器上的资源的请求。 2）GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64编码；POST把提交的数据则放置在是HTTP包的包体中。 3）因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系，理论上URL长度是没有限制的，即HTTP协议没有规定URL的长度，但在实质中，特定的浏览器可能对这个长度做了限制；理论上POST也是没有大小限制的，HTTP协议规范也没有进行大小限制，但在服务端通常会对这个大小做一个限制，当然这个限制比GET宽松的多，即使用POST可以提交的数据量比GET大得多。 最后，网上有人说，POST的安全性要比GET的安全性高，实质上POST跟GET都是明文传输，这可以通过类似WireShark工具看到。总之，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求。 via 浅析HTTP中POST和GET区别并用Python模拟其响应和请求 浅谈HTTP中Get与Post的区别 GET和POST的区别]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[understanding-cnn]]></title>
    <url>%2F2017%2F10%2F10%2Funderstanding-cnn%2F</url>
    <content type="text"><![CDATA[(this page is currently in draft form) Visualizing what ConvNets learnSeveral approaches for understanding and visualizing Convolutional Networks have been developed in the literature, partly as a response the common criticism that the learned features in a Neural Network are not interpretable. In this section we briefly survey some of these approaches and related work. Visualizing the activations and first-layer weightsLayer Activations. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates. Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local. Conv/FC Filters. The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasn’t been trained for long enough, or possibly a very low regularization strength that may have led to overfitting. Typical-looking filters on the first CONV layer (left), and the 2nd CONV layer (right) of a trained AlexNet. Notice that the first-layer weights are very nice and smooth, indicating nicely converged network. The color/grayscale features are clustered because the AlexNet contains two separate streams of processing, and an apparent consequence of this architecture is that one stream develops high-frequency grayscale features and the other low-frequency color features. The 2nd CONV layer weights are not as interpretable, but it is apparent that they are still smooth, well-formed, and absent of noisy patterns. Retrieving images that maximally activate a neuronAnother visualization technique is to take a large dataset of images, feed them through the network and keep track of which images maximally activate some neuron. We can then visualize the images to get an understanding of what the neuron is looking for in its receptive field. One such visualization (among others) is shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.: Maximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the receptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a function of a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies, text, or specular highlights. One problem with this approach is that ReLU neurons do not necessarily have any semantic meaning by themselves. Rather, it is more appropriate to think of multiple ReLU neurons as the basis vectors of some space that represents in image patches. In other words, the visualization is showing the patches at the edge of the cloud of representations, along the (arbitrary) axes that correspond to the filter weights. This can also be seen by the fact that neurons in a ConvNet operate linearly over the input space, so any arbitrary rotation of that space is a no-op. This point was further argued in Intriguing properties of neural networks by Szegedy et al., where they perform a similar visualization along arbitrary directions in the representation space. Embedding the codes with t-SNEConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier. We can get a rough idea about the topology of this space by embedding images into two dimensions so that their low-dimensional representation has approximately equal distances than their high-dimensional representation. There are many embedding methods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of the best-known methods that consistently produces visually-pleasing results. To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid: t-SNE embedding of a set of images based on their CNN codes. Images that are nearby each other are also close in the CNN representation space, which implies that the CNN “sees” them as being very similar. Notice that the similarities are more often class-based and semantic rather than pixel and color-based. For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to t-SNE visualization of CNN codes. Occluding parts of the imageSuppose that a ConvNet classifies an image as a dog. How can we be certain that it’s actually picking up on the dog in the image as opposed to some contextual cues from the background or some other miscellaneous object? One way of investigating which part of the image some classification prediction is coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of the position of an occluder object. That is, we iterate over regions of the image, set a patch of the image to be all zero, and look at the probability of the class. We can visualize the probability as a 2-dimensional heat map. This approach has been used in Matthew Zeiler’s Visualizing and Understanding Convolutional Networks: Three input images (top). Notice that the occluder region is shown in grey. As we slide the occluder over the image we record the probability of the correct class and then visualize it as a heatmap (shown below each image). For instance, in the left-most image we see that the probability of Pomeranian plummets when the occluder covers the face of the dog, giving us some level of confidence that the dog’s face is primarily responsible for the high classification score. Conversely, zeroing out other parts of the image is seen to have relatively negligible impact. Visualizing the data gradient and friendsData Gradient. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps DeconvNet. Visualizing and Understanding Convolutional Networks Guided Backpropagation. Striving for Simplicity: The All Convolutional Net Reconstructing original images based on CNN CodesUnderstanding Deep Image Representations by Inverting Them How much spatial information is preserved?Do ConvNets Learn Correspondence? (tldr: yes) Plotting performance as a function of image attributesImageNet Large Scale Visual Recognition Challenge Fooling ConvNetsExplaining and Harnessing Adversarial Examples Comparing ConvNets to Human labelersWhat I learned from competing against a ConvNet on ImageNet]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ConvNet</tag>
        <tag>ReLU</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-2]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-2%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Simple expressions, interpreting the gradient Compound expressions, chain rule, backpropagation Intuitive understanding of backpropagation Modularity: Sigmoid example Backprop in practice: Staged computation Patterns in backward flow Gradients for vectorized operations Summary IntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks. Problem statement. The core problem studied in this section is as follows: We are given some function \(f(x)\) where \(x\) is a vector of inputs and we are interested in computing the gradient of \(f\) at \(x\) (i.e. \(\nabla f(x)\) ). Motivation. Recall that the primary reason we are interested in this problem is that in the specific case of Neural Networks, \(f\) will correspond to the loss function ( \(L\) ) and the inputs \(x\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \((x_i,y_i), i=1 \ldots N\) and the weights and biases \(W,b\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples \(x_i\), in practice we usually only compute the gradient for the parameters (e.g. \(W,b\)) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on \(x_i\) can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing. If you are coming to this class and you’re comfortable with deriving gradients with chain rule, we would still like to encourage you to at least skim this section, since it presents a rarely developed view of backpropagation as backward flow in real-valued circuits and any insights you’ll gain may help you throughout the class. Simple expressions and interpretation of the gradientLets start simple so that we can develop the notation and conventions for more complex expressions. Consider a simple multiplication function of two numbers \(f(x,y) = x y\). It is a matter of simple calculus to derive the partial derivative for either input: $$f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x$$ Interpretation. Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ A technical note is that the division sign on the left-hand sign is, unlike the division sign on the right-hand sign, not a division. Instead, this notation indicates that the operator \( \frac{d}{dx} \) is being applied to the function \(f\), and returns a different function (the derivative). A nice way to think about the expression above is that when \(h\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, if \(x = 4, y = -3\) then \(f(x,y) = -12\) and the derivative on \(x\) \(\frac{\partial f}{\partial x} = -3\). This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount. This can be seen by rearranging the above equation ( \( f(x + h) = f(x) + h \frac{df(x)}{dx} \) ). Analogously, since \(\frac{\partial f}{\partial y} = 4\), we expect that increasing the value of \(y\) by some very small amount \(h\) would also increase the output of the function (due to the positive sign), and by \(4h\). The derivative on each variable tells you the sensitivity of the whole expression on its value. As mentioned, the gradient \(\nabla f\) is the vector of partial derivatives, so we have that \(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]\). Even though the gradient is technically a vector, we will often use terms such as “the gradient on x” instead of the technically correct phrase “the partial derivative on x” for simplicity. We can also derive the derivatives for the addition operation: $$f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$$ that is, the derivative on both \(x,y\) is one regardless of what the values of \(x,y\) are. This makes sense, since increasing either \(x,y\) would increase the output of \(f\), and the rate of that increase would be independent of what the actual values of \(x,y\) are (unlike the case of multiplication above). The last function we’ll use quite a bit in the class is the max operation: $$f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x &gt;= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y &gt;= x)$$ That is, the (sub)gradient is 1 on the input that was larger and 0 on the other input. Intuitively, if the inputs are \(x = 4,y = 2\), then the max is 4, and the function is not sensitive to the setting of \(y\). That is, if we were to increase it by a tiny amount \(h\), the function would keep outputting 4, and therefore the gradient is zero: there is no effect. Of course, if we were to change \(y\) by a large amount (e.g. larger than 2), then the value of \(f\) would change, but the derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the \(\lim_{h \rightarrow 0}\) in its definition. Compound expressions with chain ruleLets now start to consider more complicated expressions that involve multiple composed functions, such as \(f(x,y,z) = (x + y) z\). This expression is still simple enough to differentiate directly, but we’ll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: \(q = x + y\) and \(f = q z\). Moreover, we know how to compute the derivatives of both expressions separately, as seen in the previous section. \(f\) is just multiplication of \(q\) and \(z\), so \(\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q\), and \(q\) is addition of \(x\) and \(y\) so \( \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1 \). However, we don’t necessarily care about the gradient on the intermediate value \(q\) - the value of \(\frac{\partial f}{\partial q}\) is not useful. Instead, we are ultimately interested in the gradient of \(f\) with respect to its inputs \(x,y,z\). The chain rule tells us that the correct way to “chain” these gradient expressions together is through multiplication. For example, \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x} \). In practice this is simply a multiplication of the two numbers that hold the two gradients. Lets see this with an example: 1234567891011121314# set some inputsx = -2; y = 5; z = -4# perform the forward passq = x + y # q becomes 3f = q * z # f becomes -12# perform the backward pass (backpropagation) in reverse order:# first backprop through f = q * zdfdz = q # df/dz = q, so gradient on z becomes 3dfdq = z # df/dq = z, so gradient on q becomes -4# now backprop through q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!dfdy = 1.0 * dfdq # dq/dy = 1 At the end we are left with the gradient in the variables [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of backpropagation. Going forward, we will want to use a more concise notation so that we don’t have to keep writing the df part. That is, for example instead of dfdq we would simply write dq, and always assume that the gradient is with respect to the final output. This computation can also be nicely visualized with a circuit diagram: -2-4x5-4y-43z3-4q+-121f* The real-valued “circuit” on left shows the visual representation of the computation. The forward pass computes values from inputs to output (shown in green). The backward pass then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit. Intuitive understanding of backpropagationNotice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs. This extra multiplication (for each input) due to the chain rule can turn a single and relatively useless gate into a cog in a complex circuit such as an entire neural network. Lets get an intuition for how this works by referring again to the example. The add gate received inputs [-2, 5] and computed output 3. Since the gate is computing the addition operation, its local gradient for both of its inputs is +1. The rest of the circuit computed the final value, which is -12. During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was -4. If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as “wanting” the output of the add gate to be lower (due to negative sign), and with a force of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 * -4 = -4). Notice that this has the desired effect: If x,y were to decrease (responding to their negative gradient) then the add gate’s output would decrease, which in turn makes the multiply gate’s output increase. Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher. Modularity: Sigmoid exampleThe gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point: $$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$ as we will see later in the class, this expression describes a 2-dimensional neuron (with inputs x and weights w) that uses the sigmoid activation function. But for now lets think of this very simply as just a function from inputs w,x to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more: $$f(x) = \frac{1}{x}\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = -1/x^2\\f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = 1\\f(x) = e^x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = e^x\\f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = a$$ Where the functions \(f_c, f_a\) translate the input by a constant of \(c\) and scale the input by a constant of \(a\), respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do need the gradients for the constants. \(c,a\). The full circuit then looks as follows: 2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.206.000.204.000.20+1.000.20+-1.00-0.20*-10.37-0.53exp1.37-0.53+10.731.001/x Example circuit for a 2D neuron with a sigmoid activation function. The inputs are [x0,x1] and the (learnable) weights of the neuron are [w0,w1,w2]. As we will see later, the neuron computes a dot product with the input and then its activation is softly squashed by the sigmoid function to be in range from 0 to 1. In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function \(\sigma(x)\). It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator): $$\sigma(x) = \frac{1}{1+e^{-x}} \\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)= \left( 1 - \sigma(x) \right) \sigma(x)$$ As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code: 123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuit Implementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables. The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort. Backprop in practice: Staged computationLets see this with another example. Suppose that we have a function of the form: $$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$ To be clear, this function is completely useless and it’s not clear why you would ever want to compute its gradient, except for the fact that it is a good example of backpropagation in practice. It is very important to stress that if you were to launch into performing the differentiation with respect to either \(x\) or \(y\), you would end up with very large and complex expressions. However, it turns out that doing so is completely unnecessary because we don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the forward pass of such expression: 123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8) Phew, by the end of the expression we have computed the forward pass. Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass (sigy, num, sigx, xpy, xpysqr, den, invden) we will have the same variable, but one that begins with a d, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to: 123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phew Notice a few things: Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them. Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add. Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit: 3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.002.002.00max-10.002.00+-20.001.002 An example circuit demonstrating the intuition behind the operations that backpropagation performs during the backward pass in order to compute the gradients on the inputs. Sum operation distributes gradients equally to all its inputs. Max operation routes the gradient to the higher input. Multiply gate takes the input activations, swaps them and multiplies by its gradient. Looking at the diagram above as an example, we can see that: The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged. The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero. The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00. Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted \(w^Tx_i\) (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples \(x_i\) by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases. Gradients for vectorized operationsThe above sections were concerned with single variables, but all concepts extend in a straight-forward manner to matrix and vector operations. However, one must pay closer attention to dimensions and transpose operations. Matrix-Matrix multiply gradient. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations: 123456789# forward passW = np.random.randn(5, 10)X = np.random.randn(10, 3)D = W.dot(X)# now suppose we had the gradient on D from above in the circuitdD = np.random.randn(*D.shape) # same shape as DdW = dD.dot(X.T) #.T gives the transpose of the matrixdX = W.T.dot(dD) Tip: use dimension analysis! Note that you do not need to remember the expressions for dW and dX because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights dW must be of the same size as W after it is computed, and that it must depend on matrix multiplication of X and dD (as is the case when both X,W are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, X is of size [10 x 3] and dD of size [5 x 3], so if we want dW and W has shape [5 x 10], then the only way of achieving this is with dD.dot(X.T), as shown above. Work with small, explicit examples. Some people may find it difficult at first to derive the gradient updates for some vectorized expressions. Our recommendation is to explicitly write out a minimal vectorized example, derive the gradient on paper and then generalize the pattern to its efficient, vectorized form. Erik Learned-Miller has also written up a longer related document on taking matrix/vector derivatives which you might find helpful. Find it here. Summary We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher. We discussed the importance of staged computation for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time. In the next section we will start to define Neural Networks, and backpropagation will allow us to efficiently compute the gradients on the connections of the neural network, with respect to a loss function. In other words, we’re now ready to train Neural Nets, and the most conceptually difficult part of this class is behind us! ConvNets will then be a small step away. References Automatic differentiation in machine learning: a survey]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-1]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-1%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Visualizing the loss function Optimization Strategy #1: Random Search Strategy #2: Random Local Search Strategy #3: Following the gradient Computing the gradient Numerically with finite differences Analytically with calculus Gradient descent Summary IntroductionIn the previous section we introduced two key components in context of the image classification task: A (parameterized) score function mapping the raw image pixels to class scores (e.g. a linear function) A loss function that measured the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM). Concretely, recall that the linear function had the form \( f(x_i, W) = W x_i \) and the SVM we developed was formulated as: $$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)$$ We saw that a setting of the parameters \(W\) that produced predictions for examples \(x_i\) consistent with their ground truth labels \(y_i\) would also have a very low loss \(L\). We are now going to introduce the third and last key component: optimization. Optimization is the process of finding the set of parameters \(W\) that minimize the loss function. Foreshadowing: Once we understand how these three core components interact, we will revisit the first component (the parameterized function mapping) and extend it to functions much more complicated than a linear mapping: First entire Neural Networks, and then Convolutional Neural Networks. The loss functions and the optimization process will remain relatively unchanged. Visualizing the loss functionThe loss functions we’ll look at in this class are usually defined over very high-dimensional spaces (e.g. in CIFAR-10 a linear classifier weight matrix is of size [10 x 3073] for a total of 30,730 parameters), making them difficult to visualize. However, we can still gain some intuitions about one by slicing through the high-dimensional space along rays (1 dimension), or along planes (2 dimensions). For example, we can generate a random weight matrix \(W\) (which corresponds to a single point in the space), then march along a ray and record the loss function value along the way. That is, we can generate a random direction \(W_1\) and compute the loss along this direction by evaluating \(L(W + a W_1)\) for different values of \(a\). This process generates a simple plot with the value of \(a\) as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss \( L(W + a W_1 + b W_2) \) as we vary \(a, b\). In a plot, \(a, b\) could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color: Loss function landscape for the Multiclass SVM (without regularization) for one single example (left,middle) and for a hundred examples (right) in CIFAR-10. Left: one-dimensional loss by only varying a. Middle, Right: two-dimensional loss slice, Blue = low loss, Red = high loss. Notice the piecewise-linear structure of the loss function. The losses for multiple examples are combined with average, so the bowl shape on the right is the average of many piece-wise linear bowls (such as the one in the middle). We can explain the piecewise-linear structure of the loss function by examining the math. For a single example we have: $$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]$$ It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the \(\max(0,-)\) function) linear functions of \(W\). Moreover, each row of \(W\) (i.e. \(w_j\)) sometimes has a positive sign in front of it (when it corresponds to a wrong class for an example), and sometimes a negative sign (when it corresponds to the correct class for that example). To make this more explicit, consider a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regularization) becomes: $$\begin{align}L_0 = &amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\L_1 = &amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\L_2 = &amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\L = &amp; (L_0 + L_1 + L_2)/3\end{align}$$ Since these examples are 1-dimensional, the data \(x_i\) and weights \(w_j\) are numbers. Looking at, for instance, \(w_0\), some terms above are linear functions of \(w_0\) and each is clamped at zero. We can visualize this as follows: 1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 30,730-dimensional version of this shape. As an aside, you may have guessed from its bowl-shaped appearance that the SVM cost function is an example of a convex function There is a large amount of literature devoted to efficiently minimizing these types of functions, and you can also take a Stanford class on the topic ( convex optimization ). Once we extend our score functions \(f\) to Neural Networks our objective functions will become non-convex, and the visualizations above will not feature bowls but complex, bumpy terrains. Non-differentiable loss functions. As a technical note, you can also see that the kinks in the loss function (due to the max operation) technically make the loss function non-differentiable because at these kinks the gradient is not defined. However, the subgradient still exists and is commonly used instead. In this class will use the terms subgradient and gradient interchangeably. OptimizationTo reiterate, the loss function lets us quantify the quality of any particular set of weights W. The goal of optimization is to find W that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. For those of you coming to this class with previous experience, this section might seem odd since the working example we’ll use (the SVM loss) is a convex problem, but keep in mind that our goal is to eventually optimize Neural Networks where we can’t easily use any of the tools developed in the Convex Optimization literature. Strategy #1: A first very bad idea solution: Random searchSince it is so simple to check how good a given set of parameters W is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows: 12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines) In the code above, we see that we tried out several random weight vectors W, and some of them work better than others. We can take the best weights W found by this search and try it out on the test set: 1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555 With the best W this gives an accuracy of about 15.5%. Given that guessing classes completely at random achieves only 10%, that’s not a very bad outcome for a such a brain-dead random search solution! Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time. Our strategy will be to start with random weights and iteratively refine them over time to get lower loss Blindfolded hiker analogy. One analogy that you may find helpful going forward is to think of yourself as hiking on a hilly terrain with a blindfold on, and trying to reach the bottom. In the example of CIFAR-10, the hills are 30,730-dimensional, since the dimensions of W are 3073 x 10. At every point on the hill we achieve a particular loss (the height of the terrain). Strategy #2: Random Local SearchThe first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random \(W\), generate random perturbations \( \delta W \) to it and if the loss at the perturbed \(W + \delta W\) is lower, we will perform an update. The code for this procedure is as follows: 12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss) Using the same number of loss function evaluations as before (1000), this approach achieves test set classification accuracy of 21.4%. This is better, but still wasteful and computationally expensive. Strategy #3: Following the GradientIn the previous section we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). This direction will be related to the gradient of the loss function. In our hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest. In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is a generalization of slope for functions that don’t take a single number but a vector of numbers. Additionally, the gradient is just a vector of slopes (more commonly referred to as derivatives) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension. Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both. Computing the gradient numerically with finite differencesThe formula given above allows us to compute the gradient numerically. Here is a generic function that takes a function f, a vector x to evaluate the gradient on, and returns the gradient of f at x: 123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return grad Following the gradient formula we gave above, the code above iterates over all dimensions one by one, makes a small change h along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable grad holds the full gradient in the end. Practical considerations. Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: \( [f(x+h) - f(x-h)] / 2 h \) . See wiki for details. We can use the function given above to compute the gradient at any point and for any function. Lets compute the gradient for the CIFAR-10 loss function at some random point in the weight space: 12345678# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient The gradient tells us the slope of the loss function along every dimension, which we can use to make an update: 12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036 Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase. Effect of step size. The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. As we will see later in the course, choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network. In our blindfolded hill-descent analogy, we feel the hill below our feet sloping in some direction, but the step length we should take is uncertain. If we shuffle our feet carefully we can expect to make consistent but very small progress (this corresponds to having a small step size). Conversely, we can choose to make a large, confident step in an attempt to descend faster, but this may not pay off. As you can see in the code example above, at some point taking a bigger step gives a higher loss as we “overstep”. Visualizing the effect of step size. We start at some particular spot W and evaluate the gradient (or rather its negative - the white arrow) which tells us the direction of the steepest decrease in the loss function. Small steps are likely to lead to consistent but slow progress. Large steps can lead to better progress but are more risky. Note that eventually, for a large step size we will overshoot and make the loss worse. The step size (or as we will later call it - the learning rate) will become one of the most important hyperparameters that we will have to carefully tune. A problem of efficiency. You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. In our example we had 30730 parameters in total and therefore had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better. Computing the gradient analytically with CalculusThe numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of h, while the true gradient is defined as the limit as h goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check. Lets use the example of the SVM loss function for a single datapoint: $$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]$$ We can differentiate the function with respect to the weights. For example, taking the gradient with respect to \(w_{y_i}\) we obtain: $$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$ where \(\mathbb{1}\) is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector \(x_i\) scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of \(W\) that corresponds to the correct class. For the other rows where \(j \neq y_i \) the gradient is: $$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$ Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update. Gradient DescentNow that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows: 12345# Vanilla Gradient Descentwhile True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # perform parameter update This simple loop is at the core of all Neural Network libraries. There are other ways of performing the optimization (e.g. LBFGS), but Gradient Descent is currently by far the most common and established way of optimizing Neural Network loss functions. Throughout the class we will put some bells and whistles on the details of this loop (e.g. the exact details of the update equation), but the core idea of following the gradient until we’re happy with the results will remain the same. Mini-batch gradient descent. In large-scale applications (such as the ILSVRC challenge), the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update: 123456# Vanilla Minibatch Gradient Descentwhile True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter update The reason this works well is that the examples in the training data are correlated. To see this, consider the extreme case where all 1.2 million images in ILSVRC are in fact made up of exact duplicates of only 1000 unique images (one for each class, or in other words 1200 identical copies of each image). Then it is clear that the gradients we would compute for all 1200 identical copies would all be the same, and when we average the data loss over all 1.2 million images we would get the exact same loss as if we only evaluated on a small subset of 1000. In practice of course, the dataset would not contain duplicate images, the gradient from a mini-batch is a good approximation of the gradient of the full objective. Therefore, much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates. The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent). This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times. Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent (i.e. mentions of MGD for “Minibatch Gradient Descent”, or BGD for “Batch gradient descent” are rare to see), where it is usually assumed that mini-batches are used. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2. Summary Summary of the information flow. The dataset of pairs of (x,y) is given and fixed. The weights start out as random numbers and can change. During the forward pass the score function computes class scores, stored in vector f. The loss function contains two components: The data loss computes the compatibility between the scores f and the labels y. The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent. In this section, We developed the intuition of the loss function as a high-dimensional optimization landscape in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped. We motivated the idea of optimizing the loss function withiterative refinement, where we start with a random set of weights and refine them step by step until the loss is minimized. We saw that the gradient of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of h used in computing the numerical gradient). We saw that the parameter update requires a tricky setting of the step size (or the learning rate) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections. We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient. We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop. Coming up: The core takeaway from this section is that the ability to compute the gradient of a loss function with respect to its weights (and have some intuitive understanding of it) is the most important skill needed to design, train and understand neural networks. In the next section we will develop proficiency in computing the gradient analytically using the chain rule, otherwise also referred to as backpropagation. This will allow us to efficiently optimize relatively arbitrary loss functions that express all kinds of Neural Networks, including Convolutional Neural Networks.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的-m参数]]></title>
    <url>%2F2017%2F10%2F09%2Fpython%E7%9A%84-m%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[单个文件创建测试文件：E:\Code\ScoreCard&gt;路径下创建testm.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module ') 在终端上运行： 查看帮助文档： 1-m mod : run library module as a script (terminates option list) 创建模块测试-m作用文档目录结构： E:\Code\ScoreCard\package1&gt;路径下创建testm1.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module 1') E:\Code\ScoreCard\package2&gt;路径下创建testm2.py文件，内容如下：1234567import sysfrom package1 import testm1print(sys.path)if __name__ == "__main__": print ('This is main of module 2') 在终端上测试,效果如下: 结论123456-m 是把模块当作脚本来启动；直接启动是把run.py文件，所在的目录放到了sys.path属性中，见sys.path输出列表的第一个；模块启动是把你输入命令的目录（也就是当前路径），放到了sys.path属性中，见sys.path输出列表的第一个；当需要启动的py文件引用了一个模块。你需要注意：在启动的时候需要考虑sys.path中有没有你import的模块的路径！这个时候，到底是使用直接启动，还是以模块的启动？目的就是把import的那个模块的路径放到sys.path中。 参考here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linear-classify]]></title>
    <url>%2F2017%2F10%2F08%2FLinear%20Classification%2F</url>
    <content type="text"><![CDATA[Table of Contents: Intro to Linear classification Linear score function Interpreting a linear classifier Loss function Multiclass SVM Softmax classifier SVM vs Softmax Interactive Web Demo of Linear Classification Summary Linear ClassificationIn the last section we introduced the problem of Image Classification, which is the task of assigning a single label to an image from a fixed set of categories. Morever, we described the k-Nearest Neighbor (kNN) classifier which labels images by comparing them to (annotated) images from the training set. As we saw, kNN has a number of disadvantages: The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size. Classifying a test image is expensive since it requires a comparison to all training images. Overview. We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function. Parameterized mapping from images to label scoresThe first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. As before, let’s assume a training dataset of images \( x_i \in R^D \), each associated with a label \( y_i \). Here \( i = 1 \dots N \) and \( y_i \in { 1 \dots K } \). That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \(f: R^D \mapsto R^K\) that maps the raw image pixels to class scores. Linear classifier. In this module we will start out with arguably the simplest possible function, a linear mapping: $$f(x_i, W, b) = W x_i + b$$ In the above equation, we are assuming that the image \(x_i\) has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, \(x_i\) contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data \(x_i\). However, you will often hear people use the terms weights and parameters interchangeably. There are a few things to note: First, note that the single matrix multiplication \(W x_i\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of W. Notice also that we think of the input data \( (x_i, y_i) \) as given and fixed, but we have control over the setting of the parameters W,b. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes. An advantage of this approach is that the training data is used to learn the parameters W,b, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores. Lastly, note that to classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images. Foreshadowing: Convolutional Neural Networks will map image pixels to scores exactly as shown above, but the mapping ( f ) will be more complex and will contain more parameters. Interpreting a linear classifierNotice that a linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. Depending on precisely what values we set for these weights, the function has the capacity to like or dislike (depending on the sign of each weight) certain colors at certain positions in the image. For instance, you can imagine that the “ship” class might be more likely if there is a lot of blue on the sides of an image (which could likely correspond to water). You might expect that the “ship” classifier would then have a lot of positive weights across its blue channel weights (presence of blue increases score of ship), and negative weights in the red/green channels (presence of red/green decreases the score of ship). An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it’s looking at a dog. Analogy of images as high-dimensional points. Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points. Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing: Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores. As we saw above, every row of \(W\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \(W\), the corresponding line in the pixel space will rotate in different directions. The biases \(b\), on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in \( x_i = 0 \) would always give score of zero regardless of the weights, so all lines would be forced to cross the origin. Interpretation of linear classifiers as template matching.Another interpretation for the weights \(W\) is that each row of \(W\) corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance. Skipping ahead a bit: Example learned weights at the end of learning for CIFAR-10. Note that, for example, the ship template contains a lot of blue pixels as expected. This template will therefore give a high score once it is matched against images of ships on the ocean with an inner product. Additionally, note that the horse template seems to contain a two-headed horse, which is due to both left and right facing horses in the dataset. The linear classifier merges these two modes of horses in the data into a single template. Similarly, the car classifier seems to have merged several modes into a single template which has to identify cars from all sides, and of all colors. In particular, this template ended up being red, which hints that there are more red cars in the CIFAR-10 dataset than of any other color. The linear classifier is too weak to properly account for different-colored cars, but as we will see later neural networks will allow us to perform this task. Looking ahead a bit, a neural network will be able to develop intermediate neurons in its hidden layers that could detect specific car types (e.g. green car facing left, blue car facing front, etc.), and neurons on the next layer could combine these into a more accurate car score through a weighted sum of the individual car detectors. Bias trick. Before moving on we want to mention a common simplifying trick to representing the two parameters \(W,b\) as one. Recall that we defined the score function as: $$f(x_i, W, b) = W x_i + b$$ As we proceed through the material it is a little cumbersome to keep track of two sets of parameters (the biases \(b\) and weights \(W\)) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \(x_i\) with one additional dimension that always holds the constant \(1\) - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f(x_i, W) = W x_i$$ With our CIFAR-10 example, \(x_i\) is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and \(W\) is now [10 x 3073] instead of [10 x 3072]. The extra column that \(W\) now corresponds to the bias \(b\). An illustration might help clarify: Illustration of the bias trick. Doing a matrix multiplication and then adding a bias vector (left) is equivalent to adding a bias dimension with a constant of 1 to all input vectors and extending the weight matrix by 1 column - a bias column (right). Thus, if we preprocess our data by appending ones to all vectors we only have to learn a single matrix of weights instead of two matrices that hold the weights and the biases. Image data preprocessing. As a quick note, in the examples above we used the raw pixel values (which range from [0…255]). In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to center your data by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 … 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1]. Of these, zero mean centering is arguably more important but we will have to wait for its justification until we understand the dynamics of gradient descent. Loss functionIn the previous section we defined a function from the pixel values to class scores, which was parameterized by a set of weights \(W\). Moreover, we saw that we don’t have control over the data \( (x_i,y_i) \) (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data. For example, going back to the example image of a cat and its scores for the classes “cat”, “dog” and “ship”, we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well. Multiclass Support Vector Machine lossThere are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin \(\Delta\). Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good). Let’s now get more precise. Recall that for the i-th example we are given the pixels of image \( x_i \) and the label \( y_i \) that specifies the index of the correct class. The score function takes the pixels and computes the vector \( f(x_i, W) \) of class scores, which we will abbreviate to \(s\) (short for scores). For example, the score for the j-th class is the j-th element: \( s_j = f(x_i, W)_j \). The Multiclass SVM loss for the i-th example is then formalized as follows: $$L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)$$ Example. Lets unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \( s = [13, -7, 11]\), and that the first class is the true class (i.e. \(y_i = 0\)). Also assume that \(\Delta\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (\(j \neq y_i\)), so we get two terms: $$L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)$$ You can see that the first term gives zero since [-7 - 13 + 10] gives a negative number, which is then thresholded to zero with the \(max(0,-)\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; Any additional difference above the margin is clamped at zero with the max operation. The second term computes [11 - 13 + 10] which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \(y_i\) to be larger than the incorrect class scores by at least by \(\Delta\) (delta). If this is not the case, we will accumulate loss. Note that in this particular module we are working with linear score functions ( \( f(x_i; W) = W x_i \) ), so we can also rewrite the loss function in this equivalent form: $$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$$ where \(w_j\) is the j-th row of \(W\) reshaped as a column. However, this will not necessarily be the case once we start to consider more complex forms of the score function \(f\). A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero \(max(0,-)\) function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form \(max(0,-)^2\) that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation. The loss function quantifies our unhappiness with predictions on the training set The Multiclass Support Vector Machine “wants” the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible. Regularization. There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and \(L_i = 0\) for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters \( \lambda W \) where \( \lambda &gt; 1 \) will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30. In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty \(R(W)\). The most common regularization penalty is the L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters: $$R(W) = \sum_k\sum_l W_{k,l}^2$$ In the expression above, we are summing up all the squared elements of \(W\). Notice that the regularization function is not a function of the data, it is only based on the weights. Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss \(L_i\) over all examples) and the regularization loss. That is, the full Multiclass SVM loss becomes: $$L = \underbrace{ \frac{1}{N} \sum_i L_i }\text{data loss} + \underbrace{ \lambda R(W) }\text{regularization loss} \\$$ Or expanding this out in its full form: $$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$$ Where \(N\) is the number of training examples. As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter \(\lambda\). There is no simple way of setting this hyperparameter and it is usually determined by cross-validation. In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested). The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector \(x = [1,1,1,1] \) and two weight vectors \(w_1 = [1,0,0,0]\), \(w_2 = [0.25,0.25,0.25,0.25] \). Then \(w_1^Tx = w_2^Tx = 1\) so both weight vectors lead to the same dot product, but the L2 penalty of \(w_1\) is 1.0 while the L2 penalty of \(w_2\) is only 0.25. Therefore, according to the L2 penalty the weight vector \(w_2\) would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in \(w_2\) are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting. Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights \(W\) but not the biases \(b\). However, in practice this often turns out to have a negligible effect. Lastly, note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of \(W = 0\). Code. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment The takeaway from this section is that the SVM loss takes one particular approach to measuring how consistent the predictions on training data are with the ground truth labels. Additionally, making good predictions on the training set is equivalent to minimizing the loss. All we have to do now is to come up with a way to find the weights that minimize the loss. Practical ConsiderationsSetting Delta. Note that we brushed over the hyperparameter \(\Delta\) and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to \(\Delta = 1.0\) in all cases. The hyperparameters \(\Delta\) and \(\lambda\) seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights \(W\) has direct effect on the scores (and hence also their differences): As we shrink all values inside \(W\) the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. \(\Delta = 1\), or \(\Delta = 100\)) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength \(\lambda\)). Relation to Binary Support Vector Machine. You may be coming to this class with previous experience with Binary Support Vector Machines, where the loss for the i-th example can be written as: $$L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)$$ where \(C\) is a hyperparameter, and \(y_i \in \{ -1,1 \} \). You can convince yourself that the formulation we presented in this section contains the binary SVM as a special case when there are only two classes. That is, if we only had two classes then the loss reduces to the binary SVM shown above. Also, \(C\) in this formulation and \(\lambda\) in our formulation control the same tradeoff and are related through reciprocal relation \(C \propto \frac{1}{\lambda}\). Aside: Optimization in primal. If you’re coming to this class with previous knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm, etc. In this class (as is the case with Neural Networks in general) we will always work with the optimization objectives in their unconstrained primal form. Many of these objectives are technically not differentiable (e.g. the max(x,y) function isn’t because it has a kink when x=y), but in practice this is not a problem and it is common to use a subgradient. Aside: Other Multiclass SVM formulations. It is worth noting that the Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes. Another commonly used form is the One-Vs-All (OVA) SVM which trains an independent binary SVM for each class vs. all other classes. Related, but less common to see in practice is also the All-vs-All (AVA) strategy. Our formulation follows the Weston and Watkins 1999 (pdf) version, which is a more powerful version than OVA (in the sense that you can construct multiclass datasets where this version can achieve zero data loss, but OVA cannot. See details in the paper if interested). The last formulation you may see is a Structured SVM, which maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class. Understanding the differences between these formulations is outside of the scope of the class. The version presented in these notes is a safe bet to use in practice, but the arguably simplest OVA strategy is likely to work just as well (as also argued by Rikin et al. 2004 in In Defense of One-Vs-All Classification (pdf)). Softmax classifierIt turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the Softmax classifier, which has a different loss function. If you’ve heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes. Unlike the SVM which treats the outputs \(f(x_i,W)\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \(f(x_i; W) = W x_i\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: $$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$$ where we are using the notation \(f_j\) to mean the j-th element of the vector of class scores \(f\). As before, the full loss for the dataset is the mean of \(L_i\) over all training examples together with a regularization term \(R(W)\). The function \(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the softmax function: It takes a vector of arbitrary real-valued scores (in \(z\)) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate. Information theory view. The cross-entropy between a “true” distribution \(p\) and an estimated distribution \(q\) is defined as: $$H(p,q) = - \sum_x p(x) \log q(x)$$ The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( \(q = e^{f_{y_i}} / \sum_j e^{f_j} \) as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \(p = [0, \ldots 1, \ldots, 0]\) contains a single 1 at the \(y_i\) -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as \(H(p,q) = H(p) + D_{KL}(p||q)\), and the entropy of the delta function \(p\) is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer. Probabilistic interpretation. Looking at the expression, we see that $$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }$$ can be interpreted as the (normalized) probability assigned to the correct label \(y_i\) given the image \(x_i\) and parameterized by \(W\). To see this, remember that the Softmax classifier interprets the scores inside the output vector \(f\) as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term \(R(W)\) in the full loss function as coming from a Gaussian prior over the weight matrix \(W\), where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class. Practical issues: Numeric stability. When you’re writing code for computing the Softmax function in practice, the intermediate terms \(e^{f_{y_i}}\) and \(\sum_j e^{f_j}\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \(C\) and push it into the sum, we get the following (mathematically equivalent) expression: $$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}$$ We are free to choose the value of \(C\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \(C\) is to set \(\log C = -\max_j f_j \). This simply states that we should shift the values inside the vector \(f\) so that the highest value is zero. In code: 123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer Possibly confusing naming conventions. To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses the cross-entropy loss. The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. In particular, note that technically it doesn’t make sense to talk about the “softmax loss”, since softmax is just the squashing function, but it is a relatively commonly used shorthand. SVM vs. SoftmaxA picture might help clarify the distinction between the Softmax and SVM classifiers: Example of the difference between the SVM and Softmax classifiers for one datapoint. In both cases we compute the same score vector f (e.g. by matrix multiplication in this section). The difference is in the interpretation of the scores in f: The SVM interprets these as class scores and its loss function encourages the correct class (class 2, in blue) to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low). The final loss for this example is 1.58 for the SVM and 1.04 (note this is 1.04 using the natural logarithm, not base 2 or base 10) for the Softmax classifier, but note that these numbers are not comparable; They are only meaningful in relation to loss computed within the same classifier and with the same data. Softmax classifier provides “probabilities” for each class. Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute “probabilities” for all labels. For example, given an image the SVM classifier might give you scores [12.5, 0.6, -23.0] for the classes “cat”, “dog” and “ship”. The softmax classifier can instead compute the probabilities of the three labels as [0.9, 0.09, 0.01], which allows you to interpret its confidence in each class. The reason we put the word “probabilities” in quotes, however, is that how peaky or diffuse these probabilities are depends directly on the regularization strength \(\lambda\) - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute: $$[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]$$ Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength \(\lambda\) was higher, the weights \(W\) would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute: $$[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]$$ where the probabilites are now more diffuse. Moreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength \(\lambda\), the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. In practice, SVM and Softmax are usually comparable. The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better. Compared to the Softmax classifier, the SVM is a more local objective, which could be thought of either as a bug or a feature. Consider an example that achieves the scores [10, -2, 3] and where the first class is correct. An SVM (e.g. with desired margin of \(\Delta = 1\)) will see that the correct class already has a score higher than the margin compared to the other classes and it will compute loss of zero. The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero. However, these scenarios are not equivalent to a Softmax classifier, which would accumulate a much higher loss for the scores [10, 9, 9] than for [10, -100, -100]. In other words, the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud. Interactive web demo We have written an interactive web demo to help your intuitions with linear classifiers. The demo visualizes the loss functions discussed in this section using a toy 3-way classification on 2D data. The demo also jumps ahead a bit and performs the optimization, which we will discuss in full detail in the next section. SummaryIn summary, We defined a score function from image pixels to class scores (in this section, a linear function that depends on weights W and biases b). Unlike kNN classifier, the advantage of this parametric approach is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with W, not an exhaustive comparison to every single training example. We introduced the bias trick, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix. We defined a loss function (we introduced two commonly used losses for linear classifiers: the SVM and the Softmax) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss. We now saw one way to take a dataset of images and map each one to class scores based on a set of parameters, and we saw two examples of loss functions that we can use to measure the quality of the predictions. But how do we efficiently determine the parameters that give the best (lowest) loss? This process is optimization, and it is the topic of the next section. Further ReadingThese readings are optional and contain pointers of interest. Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>linear</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image Classification With KNN]]></title>
    <url>%2F2017%2F10%2F08%2FImage%20Classification%20With%20KNN%2F</url>
    <content type="text"><![CDATA[This is an introductory lecture designed to introduce people from outside of Computer Vision to the Image Classification problem, and the data-driven approach. The Table of Contents: Intro to Image Classification, data-driven approach, pipeline Nearest Neighbor Classifier k-Nearest Neighbor Validation sets, Cross-validation, hyperparameter tuning Pros/Cons of Nearest Neighbor Summary Summary: Applying kNN in practice Further Reading Image ClassificationMotivation. In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification. Example. For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as “cat”. Challenges. Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values: Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible. Illumination conditions. The effects of illumination are drastic on the pixel level. Background clutter. The objects of interest may blend into their environment, making them hard to identify. Intra-class variation. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance. A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations. Data-driven approach. How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like: The image classification pipeline. We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows: Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set. Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model. Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth). Nearest Neighbor ClassifierAs our first approach, we will develop what we call a Nearest Neighbor Classifier. This classifier has nothing to do with Convolutional Neural Networks and it is very rarely used in practice, but it will allow us to get an idea about the basic approach to an image classification problem. Example image classification dataset: CIFAR-10. One popular toy image classification dataset is the CIFAR-10 dataset. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example “airplane, automobile, bird, etc”). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below you can see 10 random example images from each one of the 10 classes: Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right you can see an example result of such a procedure for 10 example test images. Notice that in only about 3 out of 10 examples an image of the same class is retrieved, while in the other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car. You may have noticed that we left unspecified the details of exactly how we compare two images, which in this case are just two blocks of 32 x 32 x 3. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors \( I_1, I_2 \) , a reasonable choice for comparing them might be the L1 distance: $$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$ Where the sum is taken over all pixels. Here is the procedure visualized: Let’s also look at how we might implement the classifier in code. First, let’s load the CIFAR-10 data into memory as 4 arrays: the training data/labels and the test data/labels. In the code below, Xtr (of size 50,000 x 32 x 32 x 3) holds all the images in the training set, and a corresponding 1-dimensional array Ytr (of length 50,000) holds the training labels (from 0 to 9): 1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 Now that we have all images stretched out as rows, here is how we could train and evaluate a classifier: 123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) Notice that as an evaluation criterion, it is common to use the accuracy, which measures the fraction of predictions that were correct. Notice that all classifiers we will build satisfy this one common API: they have a train(X,y) function that takes the data and the labels to learn from. Internally, the class should build some kind of model of the labels and how they can be predicted from the data. And then there is a predict(X) function, which takes new data and predicts the labels. Of course, we’ve left out the meat of things - the actual classifier itself. Here is an implementation of a simple Nearest Neighbor classifier with the L1 distance that satisfies this template: 123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred If you ran this code, you would see that this classifier only achieves 38.6% on CIFAR-10. That’s more impressive than guessing at random (which would give 10% accuracy since there are 10 classes), but nowhere near human performance (which is estimated at about 94%) or near state-of-the-art Convolutional Neural Networks that achieve about 95%, matching human accuracy (see the leaderboard of a recent Kaggle competition on CIFAR-10). The choice of distance.There are many other ways of computing distances between vectors. Another common choice could be to instead use the L2 distance, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form: $$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$ In other words we would be computing the pixelwise difference as before, but this time we square all of them, add them up and finally take the square root. In numpy, using the code from above we would need to only replace a single line of code. The line that computes the distances: 1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) Note that I included the np.sqrt call above, but in a practical nearest neighbor application we could leave out the square root operation because square root is a monotonic function. That is, it scales the absolute sizes of the distances but it preserves the ordering, so the nearest neighbors with or without it are identical. If you ran the Nearest Neighbor classifier on CIFAR-10 with this distance, you would obtain 35.4% accuracy (slightly lower than our L1 distance result). L1 vs. L2. It is interesting to consider differences between the two metrics. In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. That is, the L2 distance prefers many medium disagreements to one big one. L1 and L2 distances (or equivalently the L1/L2 norms of the differences between a pair of images) are the most commonly used special cases of a p-norm. k - Nearest Neighbor ClassifierYou may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what’s called a k-Nearest Neighbor Classifier. The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers: In practice, you will almost always want to use k-Nearest Neighbor. But what value of k should you use? We turn to this problem next. Validation sets for Hyperparameter tuningThe k-nearest neighbor classifier requires a setting for k. But what number works best? Additionally, we saw that there are many different distance functions we could have used: L1 norm, L2 norm, there are many other choices we didn’t even consider (e.g. dot products). These choices are called hyperparameters and they come up very often in the design of many Machine Learning algorithms that learn from data. It’s often not obvious what values/settings one should choose. You might be tempted to suggest that we should try out many different values and see what works best. That is a fine idea and that’s indeed what we will do, but this must be done very carefully. In particular, we cannot use the test set for the purpose of tweaking hyperparameters. Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end. Otherwise, the very real danger is that you may tune your hyperparameters to work well on the test set, but if you were to deploy your model you could see a significantly reduced performance. In practice, we would say that you overfit to the test set. Another way of looking at it is that if you tune your hyperparameters on the test set, you are effectively using the test set as the training set, and therefore the performance you achieve on it will be too optimistic with respect to what you might actually observe when you deploy your model. But if you only use the test set once at end, it remains a good proxy for measuring the generalization of your classifier (we will see much more discussion surrounding generalization later in the class). Evaluate on the test set only a single time, at the very end. Luckily, there is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a validation set. Using CIFAR-10 as an example, we could for example use 49,000 of the training images for training, and leave 1,000 aside for validation. This validation set is essentially used as a fake test set to tune the hyper-parameters. Here is what this might look like in the case of CIFAR-10: 123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) By the end of this procedure, we could plot a graph that shows which values of k work best. We would then stick with this value and evaluate once on the actual test set. Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance. Cross-validation.In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called cross-validation. Working with our previous example, the idea is that instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of k works by iterating over different validation sets and averaging the performance across these. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds. In practice. In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive. The splits people tend to use is between 50%-90% of the training data for training and rest for validation. However, this depends on multiple factors: For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation. Pros and Cons of Nearest Neighbor classifier. It is worth considering some advantages and drawbacks of the Nearest Neighbor classifier. Clearly, one advantage is that it is very simple to implement and understand. Additionally, the classifier takes no time to train, since all that is required is to store and possibly index the training data. However, we pay that computational cost at test time, since classifying a test example requires a comparison to every single training example. This is backwards, since in practice we often care about the test time efficiency much more than the efficiency at training time. In fact, the deep neural networks we will develop later in this class shift this tradeoff to the other extreme: They are very expensive to train, but once the training is finished it is very cheap to classify a new test example. This mode of operation is much more desirable in practice. As an aside, the computational complexity of the Nearest Neighbor classifier is an active area of research, and several Approximate Nearest Neighbor (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. FLANN). These algorithms allow one to trade off the correctness of the nearest neighbor retrieval with its space/time complexity during retrieval, and usually rely on a pre-processing/indexing stage that involves building a kdtree, or running the k-means algorithm. The Nearest Neighbor Classifier may sometimes be a good choice in some settings (especially if the data is low-dimensional), but it is rarely appropriate for use in practical image classification settings. One problem is that images are high-dimensional objects (i.e. they often contain many pixels), and distances over high-dimensional spaces can be very counter-intuitive. The image below illustrates the point that the pixel-based L2 similarities we developed above are very different from perceptual similarities: Here is one more visualization to convince you that using pixel differences to compare images is inadequate. We can use a visualization technique called t-SNE to take the CIFAR-10 images and embed them in two dimensions so that their (local) pairwise distances are best preserved. In this visualization, images that are shown nearby are considered to be very near according to the L2 pixelwise distance we developed above: In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity. For example, a dog can be seen very near a frog since both happen to be on white background. Ideally we would like images of all of the 10 classes to form their own clusters, so that images of the same class are nearby to each other regardless of irrelevant characteristics and variations (such as the background). However, to get this property we will have to go beyond raw pixels. SummaryIn summary: We introduced the problem of Image Classification, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. We introduced a simple classifier called the Nearest Neighbor classifier. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them. We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call validation set. We try different hyperparameter values and keep the values that lead to the best performance on the validation set. If the lack of training data is a concern, we discussed a procedure called cross-validation, which can help reduce noise in estimating which hyperparameters work best. Once the best hyperparameters are found, we fix them and perform a single evaluation on the actual test set. We saw that Nearest Neighbor can get us about 40% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image. Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content. In next lectures we will embark on addressing these challenges and eventually arrive at solutions that give 90% accuracies, allow us to completely discard the training set once learning is complete, and they will allow us to evaluate a test image in less than a millisecond. Summary: Applying kNN in practiceIf you wish to apply kNN in practice (hopefully not on images, or perhaps as only a baseline) proceed as follows: Preprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization. If your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA (wiki ref, CS229ref, blog ref) or even Random Projections. Split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive). Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of k (e.g. the more the better) and across different distance types (L1 and L2 are good candidates) If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. FLANN) to accelerate the retrieval (at cost of some accuracy). Take note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be burned on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data. Further ReadingHere are some (optional) links you may find interesting for further reading: A Few Useful Things to Know about Machine Learning, where especially section 6 is related but the whole paper is a warmly recommended reading. Recognizing and Learning Object Categories, a short course of object categorization at ICCV 2005.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Image Classification</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPython Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fipython-tutorial%2F</url>
    <content type="text"><![CDATA[In this class, we will use IPython notebooks for theprogramming assignments. An IPython notebook lets you write and execute Pythoncode in your web browser. IPython notebooks make it very easy to tinker withcode and execute it in bits and pieces; for this reason IPython notebooks arewidely used in scientific computing. Installing and running IPython is easy. From the command line, the followingwill install IPython: 1pip install "ipython[notebook]" Once you have IPython installed, start it with this command: 1ipython notebook Once IPython is running, point your web browser at http://localhost:8888 tostart using IPython notebooks. If everything worked correctly, you shouldsee a screen like this, showing all available IPython notebooks in the currentdirectory: If you click through to a notebook file, you will see a screen like this: An IPython notebook is made up of a number of cells. Each cell can containPython code. You can execute a cell by clicking on it and pressing Shift-Enter.When you do so, the code in the cell will run, and the output of the cellwill be displayed beneath the cell. For example, after running the first cellthe notebook looks like this: Global variables are shared between cells. Executing the second cell thus givesthe following result: By convention, IPython notebooks are expected to be run from top to bottom.Failing to execute some cells or executing cells out of order can result inerrors: After you have modified an IPython notebook for one of the assignments bymodifying or executing some of its cells, remember to save your changes! This has only been a brief introduction to IPython notebooks, but it shouldbe enough to get you up and running on the assignments for this course.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>IPython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fpython-numpy-tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial was contributed by Justin Johnson. We will use the Python programming language for all assignments in this course.Python is a great general-purpose programming language on its own, but with thehelp of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerfulenvironment for scientific computing. We expect that many of you will have some experience with Python and numpy;for the rest of you, this section will serve as a quick crash course both onthe Python programming language and on the use of Python for scientificcomputing. Some of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page. You can also find an IPython notebook version of this tutorial here created by Volodymyr Kuleshov and Isaac Caswell for CS 228. Table of contents: Python Basic data types Containers Lists Dictionaries Sets Tuples Functions Classes Numpy Arrays Array indexing Datatypes Array math Broadcasting SciPy Image operations MATLAB files Distance between points Matplotlib Plotting Subplots Images PythonPython is a high-level, dynamically typed multiparadigm programming language.Python code is often said to be almost like pseudocode, since it allows youto express very powerful ideas in very few lines of code while being veryreadable. As an example, here is an implementation of the classic quicksortalgorithm in Python: 1234567891011def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) / 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right) print quicksort([3,6,8,10,1,2,1])# Prints "[1, 1, 2, 3, 6, 8, 10]" Python versionsThere are currently two different supported versions of Python, 2.7 and 3.4.Somewhat confusingly, Python 3.0 introduced many backwards-incompatible changesto the language, so code written for 2.7 may not work under 3.4 and vice versa.For this class all code will use Python 2.7. You can check your Python version at the command line by runningpython --version. Basic data typesLike most languages, Python has a number of basic types including integers,floats, booleans, and strings. These data types behave in ways that arefamiliar from other programming languages. Numbers: Integers and floats work as you would expect from other languages: 1234567891011121314x = 3print type(x) # Prints "&lt;type 'int'&gt;"print x # Prints "3"print x + 1 # Addition; prints "4"print x - 1 # Subtraction; prints "2"print x * 2 # Multiplication; prints "6"print x ** 2 # Exponentiation; prints "9"x += 1print x # Prints "4"x *= 2print x # Prints "8"y = 2.5print type(y) # Prints "&lt;type 'float'&gt;"print y, y + 1, y * 2, y ** 2 # Prints "2.5 3.5 5.0 6.25" Note that unlike many languages, Python does not have unary increment (x++)or decrement (x--) operators. Python also has built-in types for long integers and complex numbers;you can find all of the detailsin the documentation. Booleans: Python implements all of the usual operators for Boolean logic,but uses English words rather than symbols (&amp;&amp;, ||, etc.): 1234567t = Truef = Falseprint type(t) # Prints "&lt;type 'bool'&gt;"print t and f # Logical AND; prints "False"print t or f # Logical OR; prints "True"print not t # Logical NOT; prints "False"print t != f # Logical XOR; prints "True" Strings: Python has great support for strings: 12345678hello = 'hello' # String literals can use single quotesworld = "world" # or double quotes; it does not matter.print hello # Prints "hello"print len(hello) # String length; prints "5"hw = hello + ' ' + world # String concatenationprint hw # prints "hello world"hw12 = '%s %s %d' % (hello, world, 12) # sprintf style string formattingprint hw12 # prints "hello world 12" String objects have a bunch of useful methods; for example: 12345678s = "hello"print s.capitalize() # Capitalize a string; prints "Hello"print s.upper() # Convert a string to uppercase; prints "HELLO"print s.rjust(7) # Right-justify a string, padding with spaces; prints " hello"print s.center(7) # Center a string, padding with spaces; prints " hello "print s.replace('l', '(ell)') # Replace all instances of one substring with another; # prints "he(ell)(ell)o"print ' world '.strip() # Strip leading and trailing whitespace; prints "world" You can find a list of all string methods in the documentation. ContainersPython includes several built-in container types: lists, dictionaries, sets, and tuples. ListsA list is the Python equivalent of an array, but is resizeableand can contain elements of different types: 123456789xs = [3, 1, 2] # Create a listprint xs, xs[2] # Prints "[3, 1, 2] 2"print xs[-1] # Negative indices count from the end of the list; prints "2"xs[2] = 'foo' # Lists can contain elements of different typesprint xs # Prints "[3, 1, 'foo']"xs.append('bar') # Add a new element to the end of the listprint xs # Prints "[3, 1, 'foo', 'bar']"x = xs.pop() # Remove and return the last element of the listprint x, xs # Prints "bar [3, 1, 'foo']" As usual, you can find all the gory details about listsin the documentation. Slicing:In addition to accessing list elements one at a time, Python providesconcise syntax to access sublists; this is known as slicing: 123456789nums = range(5) # range is a built-in function that creates a list of integersprint nums # Prints "[0, 1, 2, 3, 4]"print nums[2:4] # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"print nums[2:] # Get a slice from index 2 to the end; prints "[2, 3, 4]"print nums[:2] # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"print nums[:] # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"print nums[:-1] # Slice indices can be negative; prints ["0, 1, 2, 3]"nums[2:4] = [8, 9] # Assign a new sublist to a sliceprint nums # Prints "[0, 1, 8, 9, 4]" We will see slicing again in the context of numpy arrays. Loops: You can loop over the elements of a list like this: 1234animals = ['cat', 'dog', 'monkey']for animal in animals: print animal# Prints "cat", "dog", "monkey", each on its own line. If you want access to the index of each element within the body of a loop,use the built-in enumerate function: 1234animals = ['cat', 'dog', 'monkey']for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line List comprehensions:When programming, frequently we want to transform one type of data into another.As a simple example, consider the following code that computes square numbers: 12345nums = [0, 1, 2, 3, 4]squares = []for x in nums: squares.append(x ** 2)print squares # Prints [0, 1, 4, 9, 16] You can make this code simpler using a list comprehension: 123nums = [0, 1, 2, 3, 4]squares = [x ** 2 for x in nums]print squares # Prints [0, 1, 4, 9, 16] List comprehensions can also contain conditions: 123nums = [0, 1, 2, 3, 4]even_squares = [x ** 2 for x in nums if x % 2 == 0]print even_squares # Prints "[0, 4, 16]" DictionariesA dictionary stores (key, value) pairs, similar to a Map in Java oran object in Javascript. You can use it like this: 12345678910d = &#123;'cat': 'cute', 'dog': 'furry'&#125; # Create a new dictionary with some dataprint d['cat'] # Get an entry from a dictionary; prints "cute"print 'cat' in d # Check if a dictionary has a given key; prints "True"d['fish'] = 'wet' # Set an entry in a dictionaryprint d['fish'] # Prints "wet"# print d['monkey'] # KeyError: 'monkey' not a key of dprint d.get('monkey', 'N/A') # Get an element with a default; prints "N/A"print d.get('fish', 'N/A') # Get an element with a default; prints "wet"del d['fish'] # Remove an element from a dictionaryprint d.get('fish', 'N/A') # "fish" is no longer a key; prints "N/A" You can find all you need to know about dictionariesin the documentation. Loops: It is easy to iterate over the keys in a dictionary: 12345d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal in d: legs = d[animal] print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" If you want access to keys and their corresponding values, use the iteritems method: 1234d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal, legs in d.iteritems(): print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" Dictionary comprehensions:These are similar to list comprehensions, but allow you to easily constructdictionaries. For example: 123nums = [0, 1, 2, 3, 4]even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;print even_num_to_square # Prints "&#123;0: 0, 2: 4, 4: 16&#125;" SetsA set is an unordered collection of distinct elements. As a simple example, considerthe following: 12345678910animals = &#123;'cat', 'dog'&#125;print 'cat' in animals # Check if an element is in a set; prints "True"print 'fish' in animals # prints "False"animals.add('fish') # Add an element to a setprint 'fish' in animals # Prints "True"print len(animals) # Number of elements in a set; prints "3"animals.add('cat') # Adding an element that is already in the set does nothingprint len(animals) # Prints "3"animals.remove('cat') # Remove an element from a setprint len(animals) # Prints "2" As usual, everything you want to know about sets can be foundin the documentation. Loops:Iterating over a set has the same syntax as iterating over a list;however since sets are unordered, you cannot make assumptions about the orderin which you visit the elements of the set: 1234animals = &#123;'cat', 'dog', 'fish'&#125;for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: fish", "#2: dog", "#3: cat" Set comprehensions:Like lists and dictionaries, we can easily construct sets using set comprehensions: 123from math import sqrtnums = &#123;int(sqrt(x)) for x in range(30)&#125;print nums # Prints "set([0, 1, 2, 3, 4, 5])" TuplesA tuple is an (immutable) ordered list of values.A tuple is in many ways similar to a list; one of the most important differences is thattuples can be used as keys in dictionaries and as elements of sets, while lists cannot.Here is a trivial example: 12345d = &#123;(x, x + 1): x for x in range(10)&#125; # Create a dictionary with tuple keyst = (5, 6) # Create a tupleprint type(t) # Prints "&lt;type 'tuple'&gt;"print d[t] # Prints "5"print d[(1, 2)] # Prints "1" The documentation has more information about tuples. FunctionsPython functions are defined using the def keyword. For example: 1234567891011def sign(x): if x &gt; 0: return 'positive' elif x &lt; 0: return 'negative' else: return 'zero'for x in [-1, 0, 1]: print sign(x)# Prints "negative", "zero", "positive" We will often define functions to take optional keyword arguments, like this: 12345678def hello(name, loud=False): if loud: print 'HELLO, %s!' % name.upper() else: print 'Hello, %s' % namehello('Bob') # Prints "Hello, Bob"hello('Fred', loud=True) # Prints "HELLO, FRED!" There is a lot more information about Python functionsin the documentation. ClassesThe syntax for defining classes in Python is straightforward: 12345678910111213141516class Greeter(object): # Constructor def __init__(self, name): self.name = name # Create an instance variable # Instance method def greet(self, loud=False): if loud: print 'HELLO, %s!' % self.name.upper() else: print 'Hello, %s' % self.name g = Greeter('Fred') # Construct an instance of the Greeter classg.greet() # Call an instance method; prints "Hello, Fred"g.greet(loud=True) # Call an instance method; prints "HELLO, FRED!" You can read a lot more about Python classesin the documentation. NumpyNumpy is the core library for scientific computing in Python.It provides a high-performance multidimensional array object, and tools for working with thesearrays. If you are already familiar with MATLAB, you might findthis tutorial useful to get started with Numpy. ArraysA numpy array is a grid of values, all of the same type, and is indexed by a tuple ofnonnegative integers. The number of dimensions is the rank of the array; the shapeof an array is a tuple of integers giving the size of the array along each dimension. We can initialize numpy arrays from nested Python lists,and access elements using square brackets: 123456789101112import numpy as npa = np.array([1, 2, 3]) # Create a rank 1 arrayprint type(a) # Prints "&lt;type 'numpy.ndarray'&gt;"print a.shape # Prints "(3,)"print a[0], a[1], a[2] # Prints "1 2 3"a[0] = 5 # Change an element of the arrayprint a # Prints "[5, 2, 3]"b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 arrayprint b.shape # Prints "(2, 3)"print b[0, 0], b[0, 1], b[1, 0] # Prints "1 2 4" Numpy also provides many functions to create arrays: 1234567891011121314151617181920import numpy as npa = np.zeros((2,2)) # Create an array of all zerosprint a # Prints "[[ 0. 0.] # [ 0. 0.]]" b = np.ones((1,2)) # Create an array of all onesprint b # Prints "[[ 1. 1.]]"c = np.full((2,2), 7) # Create a constant arrayprint c # Prints "[[ 7. 7.] # [ 7. 7.]]"d = np.eye(2) # Create a 2x2 identity matrixprint d # Prints "[[ 1. 0.] # [ 0. 1.]]" e = np.random.random((2,2)) # Create an array filled with random valuesprint e # Might print "[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]" You can read about other methods of array creationin the documentation. Array indexingNumpy offers several ways to index into arrays. Slicing:Similar to Python lists, numpy arrays can be sliced.Since arrays may be multidimensional, you must specify a slice for each dimensionof the array: 12345678910111213141516171819import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Use slicing to pull out the subarray consisting of the first 2 rows# and columns 1 and 2; b is the following array of shape (2, 2):# [[2 3]# [6 7]]b = a[:2, 1:3]# A slice of an array is a view into the same data, so modifying it# will modify the original array.print a[0, 1] # Prints "2"b[0, 0] = 77 # b[0, 0] is the same piece of data as a[0, 1]print a[0, 1] # Prints "77" You can also mix integer indexing with slice indexing.However, doing so will yield an array of lower rank than the original array.Note that this is quite different from the way that MATLAB handles arrayslicing: 123456789101112131415161718192021222324import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Two ways of accessing the data in the middle row of the array.# Mixing integer indexing with slices yields an array of lower rank,# while using only slices yields an array of the same rank as the# original array:row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of aprint row_r1, row_r1.shape # Prints "[5 6 7 8] (4,)"print row_r2, row_r2.shape # Prints "[[5 6 7 8]] (1, 4)"# We can make the same distinction when accessing columns of an array:col_r1 = a[:, 1]col_r2 = a[:, 1:2]print col_r1, col_r1.shape # Prints "[ 2 6 10] (3,)"print col_r2, col_r2.shape # Prints "[[ 2] # [ 6] # [10]] (3, 1)" Integer array indexing:When you index into numpy arrays using slicing, the resulting array viewwill always be a subarray of the original array. In contrast, integer arrayindexing allows you to construct arbitrary arrays using the data from anotherarray. Here is an example: 1234567891011121314151617import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])# An example of integer array indexing.# The returned array will have shape (3,) and print a[[0, 1, 2], [0, 1, 0]] # Prints "[1 4 5]"# The above example of integer array indexing is equivalent to this:print np.array([a[0, 0], a[1, 1], a[2, 0]]) # Prints "[1 4 5]"# When using integer array indexing, you can reuse the same# element from the source array:print a[[0, 0], [1, 1]] # Prints "[2 2]"# Equivalent to the previous integer array indexing exampleprint np.array([a[0, 1], a[0, 1]]) # Prints "[2 2]" One useful trick with integer array indexing is selecting or mutating oneelement from each row of a matrix: 1234567891011121314151617181920212223import numpy as np# Create a new array from which we will select elementsa = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])print a # prints "array([[ 1, 2, 3], # [ 4, 5, 6], # [ 7, 8, 9], # [10, 11, 12]])"# Create an array of indicesb = np.array([0, 2, 0, 1])# Select one element from each row of a using the indices in bprint a[np.arange(4), b] # Prints "[ 1 6 7 11]"# Mutate one element from each row of a using the indices in ba[np.arange(4), b] += 10print a # prints "array([[11, 2, 3], # [ 4, 5, 16], # [17, 8, 9], # [10, 21, 12]]) Boolean array indexing:Boolean array indexing lets you pick out arbitrary elements of an array.Frequently this type of indexing is used to select the elements of an arraythat satisfy some condition. Here is an example: 1234567891011121314151617181920import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2. print bool_idx # Prints "[[False False] # [ True True] # [ True True]]"# We use boolean array indexing to construct a rank 1 array# consisting of the elements of a corresponding to the True values# of bool_idxprint a[bool_idx] # Prints "[3 4 5 6]"# We can do all of the above in a single concise statement:print a[a &gt; 2] # Prints "[3 4 5 6]" For brevity we have left out a lot of details about numpy array indexing;if you want to know more you shouldread the documentation. DatatypesEvery numpy array is a grid of elements of the same type.Numpy provides a large set of numeric datatypes that you can use to construct arrays.Numpy tries to guess a datatype when you create an array, but functions that constructarrays usually also include an optional argument to explicitly specify the datatype.Here is an example: 12345678910import numpy as npx = np.array([1, 2]) # Let numpy choose the datatypeprint x.dtype # Prints "int64"x = np.array([1.0, 2.0]) # Let numpy choose the datatypeprint x.dtype # Prints "float64"x = np.array([1, 2], dtype=np.int64) # Force a particular datatypeprint x.dtype # Prints "int64" You can read all about numpy datatypesin the documentation. Array mathBasic mathematical functions operate elementwise on arrays, and are availableboth as operator overloads and as functions in the numpy module: 123456789101112131415161718192021222324252627282930313233import numpy as npx = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the array# [[ 6.0 8.0]# [10.0 12.0]]print x + yprint np.add(x, y)# Elementwise difference; both produce the array# [[-4.0 -4.0]# [-4.0 -4.0]]print x - yprint np.subtract(x, y)# Elementwise product; both produce the array# [[ 5.0 12.0]# [21.0 32.0]]print x * yprint np.multiply(x, y)# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print x / yprint np.divide(x, y)# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print np.sqrt(x) Note that unlike MATLAB, * is elementwise multiplication, not matrixmultiplication. We instead use the dot function to compute innerproducts of vectors, to multiply a vector by a matrix, and tomultiply matrices. dot is available both as a function in the numpymodule and as an instance method of array objects: 123456789101112131415161718192021import numpy as npx = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print v.dot(w)print np.dot(v, w)# Matrix / vector product; both produce the rank 1 array [29 67]print x.dot(v)print np.dot(x, v)# Matrix / matrix product; both produce the rank 2 array# [[19 22]# [43 50]]print x.dot(y)print np.dot(x, y) Numpy provides many useful functions for performing computations onarrays; one of the most useful is sum: 1234567import numpy as npx = np.array([[1,2],[3,4]])print np.sum(x) # Compute sum of all elements; prints "10"print np.sum(x, axis=0) # Compute sum of each column; prints "[4 6]"print np.sum(x, axis=1) # Compute sum of each row; prints "[3 7]" You can find the full list of mathematical functions provided by numpyin the documentation. Apart from computing mathematical functions using arrays, we frequentlyneed to reshape or otherwise manipulate data in arrays. The simplest exampleof this type of operation is transposing a matrix; to transpose a matrix,simply use the T attribute of an array object: 123456789101112import numpy as npx = np.array([[1,2], [3,4]])print x # Prints "[[1 2] # [3 4]]"print x.T # Prints "[[1 3] # [2 4]]"# Note that taking the transpose of a rank 1 array does nothing:v = np.array([1,2,3])print v # Prints "[1 2 3]"print v.T # Prints "[1 2 3]" Numpy provides many more functions for manipulating arrays; you can see the full listin the documentation. BroadcastingBroadcasting is a powerful mechanism that allows numpy to work with arrays of differentshapes when performing arithmetic operations. Frequently we have a smaller array and alarger array, and we want to use the smaller array multiple times to perform some operationon the larger array. For example, suppose that we want to add a constant vector to eachrow of a matrix. We could do it like this: 123456789101112131415161718import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = np.empty_like(x) # Create an empty matrix with the same shape as x# Add the vector v to each row of the matrix x with an explicit loopfor i in range(4): y[i, :] = x[i, :] + v# Now y is the following# [[ 2 2 4]# [ 5 5 7]# [ 8 8 10]# [11 11 13]]print y This works; however when the matrix x is very large, computing an explicit loopin Python could be slow. Note that adding the vector v to each row of the matrixx is equivalent to forming a matrix vv by stacking multiple copies of v vertically,then performing elementwise summation of x and vv. We could implement thisapproach like this: 12345678910111213141516import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])vv = np.tile(v, (4, 1)) # Stack 4 copies of v on top of each otherprint vv # Prints "[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]"y = x + vv # Add x and vv elementwiseprint y # Prints "[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" Numpy broadcasting allows us to perform this computation without actuallycreating multiple copies of v. Consider this version, using broadcasting: 1234567891011import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint y # Prints "[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" The line y = x + v works even though x has shape (4, 3) and v has shape(3,) due to broadcasting; this line works as if v actually had shape (4, 3),where each row was a copy of v, and the sum was performed elementwise. Broadcasting two arrays together follows these rules: If the arrays do not have the same rank, prepend the shape of the lower rank arraywith 1s until both shapes have the same length. The two arrays are said to be compatible in a dimension if they have the samesize in the dimension, or if one of the arrays has size 1 in that dimension. The arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwisemaximum of shapes of the two input arrays. In any dimension where one array had size 1 and the other array had size greater than 1,the first array behaves as if it were copied along that dimension If this explanation does not make sense, try reading the explanationfrom the documentationor this explanation. Functions that support broadcasting are known as universal functions. You can findthe list of all universal functionsin the documentation. Here are some applications of broadcasting: 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:# [[ 4 5]# [ 8 10]# [12 15]]print np.reshape(v, (3, 1)) * w# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:# [[2 4 6]# [5 7 9]]print x + v# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:# [[ 5 6 7]# [ 9 10 11]]print (x.T + w).T# Another solution is to reshape w to be a row vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print x + np.reshape(w, (2, 1))# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:# [[ 2 4 6]# [ 8 10 12]]print x * 2 Broadcasting typically makes your code more concise and faster, so youshould strive to use it where possible. Numpy DocumentationThis brief overview has touched on many of the important things that you need toknow about numpy, but is far from complete. Check out thenumpy referenceto find out much more about numpy. SciPyNumpy provides a high-performance multidimensional array and basic tools tocompute with and manipulate these arrays.SciPybuilds on this, and providesa large number of functions that operate on numpy arrays and are useful fordifferent types of scientific and engineering applications. The best way to get familiar with SciPy is tobrowse the documentation.We will highlight some parts of SciPy that you might find useful for this class. Image operationsSciPy provides some basic functions to work with images.For example, it has functions to read images from disk into numpy arrays,to write numpy arrays to disk as images, and to resize images.Here is a simple example that showcases these functions: 12345678910111213141516171819from scipy.misc import imread, imsave, imresize# Read an JPEG image into a numpy arrayimg = imread('assets/cat.jpg')print img.dtype, img.shape # Prints "uint8 (400, 248, 3)"# We can tint the image by scaling each of the color channels# by a different scalar constant. The image has shape (400, 248, 3);# we multiply it by the array [1, 0.95, 0.9] of shape (3,);# numpy broadcasting means that this leaves the red channel unchanged,# and multiplies the green and blue channels by 0.95 and 0.9# respectively.img_tinted = img * [1, 0.95, 0.9]# Resize the tinted image to be 300 by 300 pixels.img_tinted = imresize(img_tinted, (300, 300))# Write the tinted image back to diskimsave('assets/cat_tinted.jpg', img_tinted) Left: The original image. Right: The tinted and resized image. MATLAB filesThe functions scipy.io.loadmat and scipy.io.savemat allow you to read andwrite MATLAB files. You can read about themin the documentation. Distance between pointsSciPy defines some useful functions for computing distances between sets of points. The function scipy.spatial.distance.pdist computes the distance between all pairsof points in a given set: 123456789101112131415161718import numpy as npfrom scipy.spatial.distance import pdist, squareform# Create the following array where each row is a point in 2D space:# [[0 1]# [1 0]# [2 0]]x = np.array([[0, 1], [1, 0], [2, 0]])print x# Compute the Euclidean distance between all rows of x.# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],# and d is the following array:# [[ 0. 1.41421356 2.23606798]# [ 1.41421356 0. 1. ]# [ 2.23606798 1. 0. ]]d = squareform(pdist(x, 'euclidean'))print d You can read all the details about this functionin the documentation. A similar function (scipy.spatial.distance.cdist) computes the distance between all pairsacross two sets of points; you can read about itin the documentation. MatplotlibMatplotlib is a plotting library.In this section give a brief introduction to the matplotlib.pyplot module,which provides a plotting system similar to that of MATLAB. PlottingThe most important function in matplotlib is plot,which allows you to plot 2D data. Here is a simple example: 12345678910import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)plt.show() # You must call plt.show() to make graphics appear. Running this code produces the following plot: With just a little bit of extra work we can easily plot multiple linesat once, and add a title, legend, and axis labels: 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])plt.show() You can read much more about the plot functionin the documentation. SubplotsYou can plot different things in the same figure using the subplot function.Here is an example: 1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show() You can read much more about the subplot functionin the documentation. ImagesYou can use the imshow function to show images. Here is an example: 12345678910111213141516171819import numpy as npfrom scipy.misc import imread, imresizeimport matplotlib.pyplot as pltimg = imread('assets/cat.jpg')img_tinted = img * [1, 0.95, 0.9]# Show the original imageplt.subplot(1, 2, 1)plt.imshow(img)# Show the tinted imageplt.subplot(1, 2, 2)# A slight gotcha with imshow is that it might give strange results# if presented with data that is not uint8. To work around this, we# explicitly cast the image to uint8 before displaying it.plt.imshow(np.uint8(img_tinted))plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[15]反向解析路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B15%5D%E5%8F%8D%E5%90%91%E8%A7%A3%E6%9E%90%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[给url起一个名字1url(r'^(?P&lt;id&gt;\d+)/$', views.posts_detail, name="detail"), Template中 1&lt;a href="&#123;% url 'detail' id=obj.id %&#125;"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt; Python代码中 123456from django.shortcuts import redirect # 调转请求from django.urls import reverse # 反向解析url的...return redirect(reverse("detail", kwargs=&#123;"id": 3&#125;)) Model中 get_absolute_url() 12345def get_absolute_url(self): return reverse("detail", kwargs=&#123;"id": self.id&#125;) # return "/post/&#123;&#125;/".format(self.id) # return "/post/%s/" % (self.id) url的命令空间 namespace在一级的url设置的，用于区分不用的app，因为不同的app下面可能存在同名的 url1234567url(r'^post/', include(posts_urls, namespace="post")),&lt;a href="&#123;% url 'post:detail' id=obj.id %&#125;"&gt;def get_absolute_url(self): return reverse("post:detail", kwargs=&#123;"id": self.id&#125;) via Django1.10教程 -15 -反向解析路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[14]动态路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B14%5D%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[Django中的URL介绍/bbs/urls.py 1234567from posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^post/', include(posts_urls)),] /posts/urls.py 1234567from . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^detail/$', views.posts_detail), ... http://127.0.0.1:8000/post/detail/ 动态路由和参数传递页面点击是通过设置a标签来调转的 后端view中通过id来查询不同的帖子对象1url(r'^detail/(?P&lt;id&gt;\d+)/$', views.posts_detail,), 在模板中组合起来1&lt;h2 class="blog-post-title"&gt;&lt;a href="/post/detail/&#123;&#123;obj.id&#125;&#125;/"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt;&lt;/h2&gt; via Django1.10教程 -14 -动态路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[13]从数据库中获取某个对象]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B13%5D%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E8%8E%B7%E5%8F%96%E6%9F%90%E4%B8%AA%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[获取对象123456models.Post.objects.get(id=3)models.Post.objects.get(title="隔壁老王是谁？") # 获取 标题是隔壁老王是谁？的帖子models.Post.objects.get(title__icontains="隔壁") # 获取 标题中包含隔壁两个字 的帖子 404页面，get_object_or_4041234from django.shortcuts import get_object_or_404get_object_or_404(models.Post, id=3) 新建了一个detail.html页面用于展示的是帖子详情 12345&lt;div class="row"&gt;&lt;h1&gt;&#123;&#123;obj.title&#125;&#125;&lt;/h1&gt;&gt;&lt;p&gt;&#123;&#123;obj.content&#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;!-- /.row --&gt; 修改views.py123456789def posts_detail(request): # obj = models.Post.objects.get(id=3) obj = get_object_or_404(models.Post,id=1) data = &#123; "obj":obj &#125; return render(request,"detail.html",data) 效果 via Django1.10教程 -13 -从数据库中获取某个对象]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[12]Queryset介绍以及Template context补充]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B12%5DQueryset%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8ATemplate%20context%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[Django shell1python manage.py shell 进入django shell,可以在命令行做一些操作。 12345678910111213141516#查询出所有的post对象，for循环遍历queryset = models.Post.objects.all()for obj in queryset: print(obj.title) print(obj.content) print(obj.update) print(obj.timestamp) print(obj.id) print(obj.pk)#创建一条新的post记录models.Post.objects.create(title="abc", content="abc abc abc")#获取所有的记录条数models.Post.objects.all().count() queryset介绍queryset 是一个可以遍历取值的结果集 queryset 里面都是 对象，可以通过对象.字段名的形式取值 一个对象就对应了数据库里面的一条记录 数据库里面的一条记录 Python中的对象 如何在前端使用queryset1. 从数据库里取出数据1queryset = models.Post.objects.all() 2. 把渠道的数据塞进 data1data = &#123;"queryset": queryset&#125; 即，修改views.py 12345678910from . import modelsdef posts_home(request): queryset = models.Post.objects.all() data = &#123; "queryset": queryset, "name": "home", "age": "18", &#125; return render(request,"base.html",data) 3. 用data去填充前端的页面1render(request, "xxx.html", data) 123456789&#123;% for obj in queryset %&#125; &lt;div class="blog-post"&gt; &lt;h2 class="blog-post-title"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/h2&gt; &lt;p class="blog-post-meta"&gt;&#123;&#123; obj.timestamp &#125;&#125;&lt;a href="#"&gt;Mark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&#123;&#123; obj.content &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;!-- /.blog-post --&gt;&#123;% endfor %&#125; 效果 补充 Django的模板语言语法12345&#123;% for x in xx %&#125;&#123;% endfor %&#125;&#123;&#123; val &#125;&#125; via Django1.10教程 -12 -Queryset介绍以及Template context补充]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[11]模板Template context和Bootstrap使用]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B11%5D%E6%A8%A1%E6%9D%BFTemplate%20context%E5%92%8CBootstrap%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Template context视图views.py中：1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): context = &#123; "title"： "home" &#125; return render(request,"index.html",context)def posts_create(request): context = &#123; "title"： "create" &#125; return render(request,"index.html",context)def posts_detail(request): context = &#123; "title"： "detail" &#125; return render(request,"index.html",context) index.html中用 context去填充模板 index.html，然后再返回 index.html中添加如下代码：1&lt;h1&gt;Hello &#123;&#123; title &#125;&#125;&#125;&lt;/h1&gt; context是一个字典,存放要渲染到页面的数据 Bootstrap模板使用1. 下载模板文件模板文件链接 2. 把html文件拷贝到了templates文件夹下面3. 新建一个statics文件夹，用于存放css文件和js文件4. 把css和js文件拷贝到statics文件夹下5. 在settings.py中配置statics文件夹1234STATICFILES_DIRS = [ os.path.join(BASE_DIR, "statics")] 6. 将html文件中指定位置的css和js文件的路径修改为/static/…文档结构如下： 效果如图： via Django1.10教程 -11 -模板Template context和Bootstrap使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[10]模板Template的配置]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B10%5D%E6%A8%A1%E6%9D%BFTemplate%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[BASE_DIR 和 os.path.join(xx, xxx)12345import os# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 创建一个templates目录 在settings.py中配置templates路径123456789101112131415TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR,"templates")], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 在templates文件夹下创建html文件12345678910&lt;!DOCTYPE html&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;try django&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello World! Django!&lt;/h1&gt;&gt;&lt;/body&gt;&lt;/html&gt; 在views.py中使用templates目录下的html文件12345678910# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return render(request,"index.html") 查看效果 via Django1.10教程 -10 -模板Template的配置]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[09]配置URL]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B09%5D%E9%85%8D%E7%BD%AEURL%2F</url>
    <content type="text"><![CDATA[bbs URL ConfigurationThe urlpatterns list routes URLs to views. For more information please see: https://docs.djangoproject.com/en/1.11/topics/http/urls/ Examples:Function views1. Add an import: from my_app import views 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, views.home, name=&apos;home&apos;) Class-based views1. Add an import: from other_app.views import Home 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, Home.as_view(), name=&apos;home&apos;) Including another URLconf1. Import the include() function: from django.conf.urls import url, include 2. Add a URL to urlpatterns: url(r&apos;^blog/&apos;, include(&apos;blog.urls&apos;)) project(project/urls.py)中的urls.py常见的几种写法1、基于function的view详见上回 2、基于class的view1、修改posts[app]下views.py123456from django.views.generic import ListViewclass PostList(ListView): """post的视图类""" def get(self,request): return HttpResponse("&lt;h1&gt;post的视图类&lt;/h1&gt;") 2、修改bbs[project]下urls.py1234567from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), url(r'^home/', views.PostList.as_view()),] 3、效果 3、view别名12345678from app01 import views as app01_viewsfrom app02 import views as app02_viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', app01_views.posts_home), url(r'^home/', app02_views.PostList.as_view()),] 4、在app中的urls.py的使用1、posts[app]下增加urls.py：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 2、在project下的urls.py 中设置第一级的路由：12345678910from django.conf.urls import url,includefrom django.contrib import adminfrom posts import viewsfrom posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), # url(r'^home/', views.PostList.as_view()), url(r'^post/',include(posts_urls))] 3、在app下的urls.py中设置第二级路由，同第一步：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 多视图示例1、app下的urls.py1234567891011from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^create/$', views.posts_create), url(r'^update/$', views.posts_update), url(r'^detail/$', views.posts_detail), url(r'^delete/$', views.posts_delete), url(r'^', views.posts_home),] 2、app下的view.py12345678910111213141516171819202122# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!")def posts_create(request): return HttpResponse("&lt;h1&gt;posts_create&lt;/h1&gt;")def posts_update(request): return HttpResponse("&lt;h1&gt;posts_update&lt;/h1&gt;")def posts_detail(request): return HttpResponse("&lt;h1&gt;posts_detail&lt;/h1&gt;")def posts_delete(request): return HttpResponse("&lt;h1&gt;posts_delete&lt;/h1&gt;") 注： 1、如果匹配到排在上面的正则表达式那么就不会适配下面的正则表达式了 2、不要忘记加’^’和’$’ via Class-based views via Django1.10教程 -09 -配置URL]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[08]第一个view（视图）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B08%5D%E7%AC%AC%E4%B8%80%E4%B8%AAview%EF%BC%88%E8%A7%86%E5%9B%BE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[定义我们第一个视图修改posts[app]下views.py12345# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!") url建立映射到view修改bbs[project]下urls.py123456from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', views.posts_home),] 效果 via Django1.10教程 -08 -第一个view（视图）]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[07]增删改查（CRUD）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B07%5D%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%EF%BC%88CRUD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CRUD 缩写 动作名称 SQL HTTP 实际动作 C Create INSERT PUT/POST 增加 R Retrieve SELECT GET 查询 U Update UPDATE POST/PUT/PATCH 更新 D Delete DELETE DELETE 删除 HTTP请求方法 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 via Django1.10教程 -07 -增删改查（CRUD） via HTTP请求方法]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[06]定制admin]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B06%5D%E5%AE%9A%E5%88%B6admin%2F</url>
    <content type="text"><![CDATA[在post/admin.py文件中添加如下代码：1234567891011class PostAdmin(admin.ModelAdmin): list_display = [,] # 控制页面展示哪些字段 list_display_links = [,] # 控制哪些字段是超链接 list_filter = [,] # 支持在右侧过滤的字段 search_fields = [,] # 支持搜索的字段 list_editable = [,] # 支持直接编辑的字段，注意！不能与list_display_links重复！ class Meta: model = model.Post 补充如何修改admin中显示的app名字？ 1. 在posts/apps.py中123PostsConfig类中添加verbose_name = "帖子" 2. posts/init.py12default_app_config = "posts.apps.PostsConfig" 示例1、修改posts[app]下admin.py1234567891011121314151617181920# Register your models here.from posts import modelsclass PostAdmin(admin.ModelAdmin): """docstring for PostAdmin""" list_display = ["title","content"] list_display_links = ["title"] list_filter = ["timestamp","content"] search_fields = ["title","content"] list_editable = ["content"] # def __init__(self, arg): # super(PostAdmin, self).__init__() # self.arg = arg class Meta: model = models.Post admin.site.register(models.Post,PostAdmin) 2、修改posts[app]下apps.py1234class PostsConfig(AppConfig): name = 'posts' verbose_name = "帖子" 3、修改posts[app]下init.py12default_app_config = "posts.apps.PostsConfig" 4、查看效果 via Django1.10教程[06]_定制admin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[05]model与admin的关系]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B05%5Dmodel%E4%B8%8Eadmin%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[修改app的models中class名称尝试去掉models.py里面 把class Posts改成 class Post 一行代码让model在admin中可见1(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver via Django1.10教程[05]_model与admin的关系]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[04]App和Model]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B04%5DApp%E5%92%8CModel%2F</url>
    <content type="text"><![CDATA[Django中project和app分别是什么？ Project：python manage.py startproject [你的project名] app: python manage.py startapp [你的app名字] project是项目，project下面分一个或多个app 我们的第一个App12python manage.py startapp [app的名字] 一定要注意：把你的app在project的settings.py里面的INSTALLED_APPS加上 Modelmodels.py里面创建类（与数据库建立联系的） 1234python manage.py makemigrations（告诉Django我设计了一些表结构，你去准备一下）python manage.py migrate（告诉Django去数据库里操作一下刚才的动作） MVC:Model View Controllers MTV:Model View Template 1、创建一个app 2、修改app目录下的models.py文件12345678910111213from django.db import modelsclass Posts(models.Model): title = models.CharField(max_length=256) # 标题，存文字的 content = models.TextField() # 内容 update = models.DateTimeField(auto_now=True,auto_now_add=False) # 更新时间 timestamp = models.DateTimeField(auto_now=False,auto_now_add=True) #创建时间 def __str__(): # python3 return self.title def __unicode__(): #python2 return self.title 3、修改project目录下的setting.py文件INSTALLED_APPS 中添加你刚创建的app名称。 4、提交修改至数据库 via Django1.10教程[04]之App和Model via Django1.10教程[04]_APPandModel补录]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[02]第一个django程序]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B02%5D%E7%AC%AC%E4%B8%80%E4%B8%AAdjango%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[创建一个bbs项目 bbs可以作为一个独立的app部署，重命名文件名为src 同步数据表：python .\manage.py migrate 修改setting.py配置文件 创建超级用户：python .\manage.py createsuperuser 启动服务：python .\manage.py runserver 浏览器中输入http://127.0.0.1:8000/admin/进入django的管理后台 via Django1.10教程[02]之第一个Django程序]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[01]Virtualenv&Django]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B01%5DVirtualenv%26Django%2F</url>
    <content type="text"><![CDATA[配置虚拟环境安装virtualenv:12pip install virtualenv 使用virtualenv创建虚拟环境：virtualenv [环境（文件夹）名] 启用虚拟环境：.\Scripts\activate 退出虚拟环境：deactivate 安装Django12pip install django 第一个Django项目1、python manage.py startproject my_first(我们的项目名) 2、python manage.py runserver 127.0.0.1:8000 补充： 1、PowerShell里面执行activate失败？ 输入： set-executionpolicy RemoteSigned 2、pip freeze命令使用 1、pip freeze &gt; requirements.txt (保存依赖包到requirements.txt) 2、pip install -r requirements.txt (批量安装项目需要的所有依赖包) via Django1.10教程[01]之virtualenv使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[03]DjangoAdmin]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B03%5DDjangoAdmin%2F</url>
    <content type="text"><![CDATA[WSGIWSGI(Web server gateway interface)：Web服务器网关接口是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。自从WSGI被开发出来以后，许多其它语言中也出现了类似接口。 web app web server(nginx/tomcat) createsuperuser注意事项注：至少八个字符，不能是简单的数字 Django admin使用的介绍 python manage.py runserver 127.0.0.1:8000 浏览器里面输入：http://127.0.0.1:8000/admin/ 输入用户名、密码登录即可。 urls.py使用的介绍django的路由是通过正则表达式来匹配的。 via Django1.10教程[03]之DjangoAdmin via Django1.10教程[03]之DjangoAdmin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error Analysis (Post-Modeling)]]></title>
    <url>%2F2017%2F10%2F07%2FError%20Analysis%20(Post-Modeling)%2F</url>
    <content type="text"><![CDATA[Error analysis is a broad term that refers to analyzing the misclassified or high error observations from your model and deciding on your next steps for improvement. This is performed after training your first model. Possible next steps include collecting more data, splitting the problem apart, or engineering new features that address the errors. To use error analysis for feature engineering, you’ll need to understand why your model missed its mark. Here’s how: Start with larger errors: Error analysis is typically a manual process. You won’t have time to scrutinize every observation. We recommend starting with those that had higher error scores. Look for patterns that you can formalize into new features. Segment by classes: Another technique is to segment your observations and compare the average error within each segment. You can try creating indicator variables for the segments with the highest errors. Unsupervised clustering: If you have trouble spotting patterns, you can run an unsupervised clustering algorithm on the misclassified observations. We don’t recommend blindly using those clusters as a new feature, but they can make it easier to spot patterns. Remember, the goal is to understand why observations were misclassified. Ask colleagues or domain experts: This is a great complement to any of the other three techniques. Asking a domain expert is especially useful if you’ve identified a pattern of poor performance (e.g. through segmentations) but don’t yet understand why.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 Tips for Writing Better Python]]></title>
    <url>%2F2017%2F10%2F07%2F5%20Tips%20for%20Writing%20Better%20Python%2F</url>
    <content type="text"><![CDATA[1. Make your code a PIP-installable PackageWhen you come across a new Python package, it’s always easier to start using it if all you have to do is run “pip install” followed by the package name or location. There are a number of ways to do this, my “go to” being to create a setup.py file for my project. Assume we have a simple Flask program in “flask_example.py”: 12345678910111213from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello, World!'def main(): app.run()if __name__ == ‘__main__’: main() We can turn this into an installable Python package by first moving it into a separate folder (let’s call this “flask_example/”. Then, we can make a setup.py file in the root project folder that looks like this: 123456789101112131415from distutils.core import setupsetup( name='flask_example', version='1.0', description='Hello, World! in flask.', packages=['flask_example'], install_requires=[ 'Flask==0.12.2' ], entry_points = &#123; 'console_scripts': 'runserver=flask_example.flask_example:main' &#125;) This has a few advantages that comes with it. First, you can now install your app locally using “pip install -e .” This makes it easier for developers to clone and install your project because the setup.py file will take care of all the heavy lifting. Second, with the setup.py file comes dependency management. The install_requires variable allows you to define packages and specific versions to use. If you’re not sure what packages and versions you are using, run “pip freeze” to view them. Lastly, this allows you to define entry points for your package, which allows you to now execute the code on the command line by simply running “runserver”. 2. Lint Your Code in a Pre-Commit HookUsing a linter can fix so many problems in code. PyLint is a great linter for Python, and if you’re using a version control system like Git, you can make Git run your code through a linter before it lets you commit your code. To do this, install the PyLint package. 1pip install pylint Then, add the following code to .git/hooks/pre-commit. If you already have a pre-commit hook doing something, simple append the pylint command to the end of your file. 123#!/bin/shpylint &lt;your_package_name&gt; This will catch all kinds of mistakes before they even make it into your Git repository. You’ll be able to say goodbye to accidentally pushing code with syntax errors, along with the many other things a good linter catches. 3. Use Absolute Imports over Relative ImportsIn python, there are very few situations where you should be using relative module paths in your import statements (e.g. from . import module_name). If you’ve gone through the process of creating a setup.py (or similar mechanism) for your Python project, then you can simply reference submodules by their full module path. Absolute imports are recommended by PEP-8, the Python style guide. This is because they’re more informative in their names and, according to the Python Software Foundation, are “better behaved.” I’ve been in positions where using relative imports has quickly become a nightmare. It’s fine when you first start coding, but once you start moving modules around and doing significant refactoring, they can really cause quite the headache. 4. Context ManagersWhenever you’re opening a file, stream, or connection, you’re usually working with a context manager. Context managers are great because when used properly they can handle the closing of your file should an exception be thrown. In order to do this, simply use the with keyword. The following is how most beginner Python programmers would probably write to a file. 1234f = open(‘newfile.txt’, ‘w’)f.write(‘Hello, World!’)f.close() This is pretty straightforward. But imagine this: You’re writing thousands of lines to a file. At some point, an exception is thrown. After that happens, your file isn’t properly closed, and all the data you thought you had already written to the file is corrupt or non-existent. Don’t worry though, with some simple refactoring we can ensure the file closes properly, even if an exception is encountered. We can do this as shown below. 123with open(‘file’, ‘w’) as file: file.write(‘Hello, World!’) Volla! It really is that simple. Additionally, the code looks much cleaner like this, and is more concise. You can also open multiple context managers with a single “with” statement, eliminating the need to have nested “with” statements. 1234with open(‘file1’, ‘w’) as f1, open(‘file2’, ‘w’) as f2: f1.write(‘Hello’) f2.write(‘World’) 5. Use Well-Named Functions and VariablesIn Python, and untyped languages especially, it can easily become unclear what functions are returning what values. Especially when you’re just a consumer of some functions in a library. If you can save the developer the 5 minutes it takes to look up the function in your documentation, than that’s actually a really valuable improvement. But how do we do this? How can doing something as simple as changing the name of a variable save development time? There are 3 main things I like to take into consideration when naming a function or variable: What the function or variable does Any units associated with the function or variable The data type the function or variable evaluates to For example, if I want to create a function to calculate the area of a rectangle, I might name it “calc_rect_area”. But this doesn’t really let the user know much. Is it going to return the value, or is it going to store it somewhere? Is the value in feet? Meters? To enhance the name of this function, I would change it to “get_rect_area_sq_ft”. This makes it clear to the user that the function gets and returns the area. It also lets the user know that the area is in square feet. If you can save the developer 5 minutes here and there with some nicely named functions and variables, that time starts to add up, and they’ll appreciate your code all the more. ConclusionThese tips are ones that I have found to be helpful over my years as a Python programmer. Some of them I figured out on my own over time, some of them others had to teach me so I could learn them. I hope this list helps you in your effort to write better Python. via here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可视化—matploblib 解决中文显示的问题]]></title>
    <url>%2F2017%2F10%2F07%2F%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94matploblib%20%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[导入相关包123from matplotlib import mplimport matplotlib.pyplot as plt 指定字体123mpl.rcParams['font.sans-serif'] = ['SimHei']mpl.rcParams['axes.unicode_minus'] = False 测试：使用中文12plt.title(u'我是中文')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（六）之模型融合]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到增加高阶特征并没有带来模型效果显著提升。考虑到sigmoid函数在0.5处附近区分度比较大，在远离0.5处概率值变换不是很明显，考虑sigmoid梯度平缓的地方，拆分成&lt;0.3和&gt;0.7分别单独领出来，重新建模，期望能提升模型效果。 按照0.3和0.7两个分割点划分，需要重新再训练出三个模型，然后再把这三个模型整体的效果与未重新训练前的效果进行对比。 首先定义几个函数，省得写一些重复的代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240 def plot_ks(proba,target,axistype=0,out_path=False): a = pd.DataFrame(np.array([proba,target]).T,columns=['proba','target']) a.sort_values(by='proba',ascending=False,inplace=True) a['sum_Times']=a['target'].cumsum() total_1 = a['target'].sum() total_0 = len(a) - a['target'].sum() a['temp'] = 1 a['Times']=a['temp'].cumsum() a['cdf1'] = a['sum_Times']/total_1 a['cdf0'] = (a['Times'] - a['sum_Times'])/total_0 a['ks'] = a['cdf1'] - a['cdf0'] a['percent'] = a['Times']*1.0/len(a) idx = np.argmax(a['ks']) # print a.loc[idx] if axistype == 0: ''' KS曲线,横轴为按照输出的概率值排序后的观察样本比例 ''' plt.figure() plt.plot(a['percent'],a['cdf1'], label="CDF_positive") plt.plot(a['percent'],a['cdf0'],label="CDF_negative") plt.plot(a['percent'],a['ks'],label="K-S") sx = np.linspace(0,1,10) sy = sx plt.plot(sx,sy,linestyle='--',color='darkgrey',linewidth=1.2) plt.legend() plt.grid(True) ymin, ymax = plt.ylim() plt.xlabel('Sample percent') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') # 虚线 t = a.loc[idx]['percent'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) else: ''' 改变横轴,横轴为模型输出的概率值 ''' plt.figure() plt.grid(True) plt.plot(1-a['proba'],a['cdf1'], label="CDF_bad") plt.plot(1-a['proba'],a['cdf0'],label="CDF_good") plt.plot(1-a['proba'],a['ks'],label="ks") plt.legend() ymin, ymax = plt.ylim() plt.xlabel('1-[Predicted probability]') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') plt.show() # 虚线 t = 1 - a.loc[idx]['proba'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) if out_path: file_name = out_path if isinstance(out_path, str) else None plt.savefig(file_name) else: plt.show() return a.loc[idx] ''' 搜索 最优超参数c ''' def grid_search_lr_c(X_train,y_train,df_coef_path=False ,pic_coefpath_title='Logistic Regression Path',pic_coefpath=False ,pic_performance_title='Logistic Regression Performance',pic_performance=False): # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01,class_weight='balanced') cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df['ks'] = ks coef_cv_df['c'] = cs # df_coef_path = 'E:\\Code\\ScoreCard\\agr_coef_cv_df_balanced_nextrain_beta.csv' if df_coef_path: file_name = df_coef_path if isinstance(df_coef_path, str) else None coef_cv_df.to_csv(file_name) coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') # pic_coefpath_title = 'Logistic Regression Path Class_weight Balanced Nextrain_beta' plt.title(pic_coefpath_title) plt.axis('tight') if pic_coefpath: file_name = pic_coefpath if isinstance(pic_coefpath, str) else None plt.savefig(file_name) else: plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') # pic_performance_title = 'Logistic Regression Performance Class_weight balanced Nextrain_beta' plt.title(pic_performance_title) plt.axis('tight') if pic_performance: file_name = pic_performance if isinstance(pic_performance, str) else None plt.savefig(file_name) else: plt.show() flag = coefs_&lt;0 idx = np.array(ks)[flag.sum(axis=1) == 0].argmax() return (cs[idx],ks[idx])``` **然后拆分模型，合并计算ks,将一个已经预测后的模型，根据输出的概率值，给定两个分割点，划分区间重新用LR训练。**``` python def lr_sub_train(X_train,y_train,idx): index = idx sub_xtrain = X_train.loc[index,:] sub_ytrain = y_train.loc[index] c,ks = grid_search_lr_c(sub_xtrain,sub_ytrain) clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced') clf_l1_LR.fit(sub_xtrain, sub_ytrain) sub_proba = clf_l1_LR.predict_proba(sub_xtrain)[:,1] return c,ks,sub_proba,sub_ytrain def train_segmentation_model(X_train,y_train,proba_train,left_point,right_point): idx1 = (proba_train &lt;= left_point) idx2 = (proba_train &gt; left_point)&amp;(proba_train &lt;= right_point) idx3 = (proba_train &gt; right_point) c1,ks1,sub_proba1,sub_ytrain1 = lr_sub_train(X_train,y_train,idx1) c2,ks2,sub_proba2,sub_ytrain2 = lr_sub_train(X_train,y_train,idx2) c3,ks3,sub_proba3,sub_ytrain3 = lr_sub_train(X_train,y_train,idx3) print 'c1:',c1,' ks1:',ks1 print 'c2:',c2,' ks2:',ks2 print 'c3:',c3,' ks3:',ks3 proba = [] proba.extend(sub_proba1) proba.extend(sub_proba2) proba.extend(sub_proba3) y = [] y.extend(sub_ytrain1) y.extend(sub_ytrain2) y.extend(sub_ytrain3) return get_ks(np.array(proba),np.array(y)) # 0.29072674001489612 输出 1234567891011121314151617'''Computing regularization path ...2017-10-03 16:04:45.2290002017-10-03 16:07:12.977000('This took ', datetime.timedelta(0, 147, 748000))Computing regularization path ...2017-10-03 16:07:28.1180002017-10-03 16:08:59.740000('This took ', datetime.timedelta(0, 91, 622000))Computing regularization path ...2017-10-03 16:09:03.4750002017-10-03 16:09:31.128000('This took ', datetime.timedelta(0, 27, 653000))c1: 0.00663966525938 ks1: 0.208779933516c2: 0.00189750388742 ks2: 0.201006811034c3: 0.000415735598049 ks3: 0.169741554117''' 结果并没有按照我想象的那样，反而分割后的效果没有单一模型的效果好。原因是woe变换没有重新训练?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（五）之增加高阶特征]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8B%E5%A2%9E%E5%8A%A0%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解分割模型带来了模型效果的提升。这节我们尝试通过增加高阶特征来增强模型的表达能力。由于备选变量比较多，盲目的全部生成高阶特征会造成特征数量指数级的爆炸式增长，这个不是我想要的，所以先根据IV值进行单变量特征选择，筛选比较重要的变量，然后再次基础上才生成高阶特征。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp from sklearn.preprocessing import PolynomialFeatures get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') poly_features = [] y_train = dataset_train['target'] poly = PolynomialFeatures(2) X_train = poly.fit_transform(dataset_train[poly_features]) del dataset_train import gc gc.collect() # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df_poly.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path Poly') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance Poly') plt.axis('tight') plt.show() fig2.show() fig1.show() 注：X_train第一列是常数项，就是系数图中的那条蓝色线。 从模型的表现来看，模型效果并没有显著提升，根据奥卡姆剃刀原理，放弃采用高阶特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（四）之分割模型]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解剔除噪声样本带来了模型效果的提升。同时也感受到了“与此相关”变量的重要，在这个字段上不同取值的客户所对应的入模特征分布也许有很大差异，于是我尝试了根据这个字段分割建模，期望能达到提升的效果。 按条件分割数据集： agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 1 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 2 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 3 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 &gt;= 4 在分割出的4个训练集上分布训练WOE转换，然后对测试集进行对应的WOE转换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339__author__ = 'boredbird'import pandas as pdimport woe.config as configimport woe.feature_process as fpimport woe.eval as evalimport numpy as npimport pickleimport matplotlib.pyplot as pltfrom datetime import datetimefrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import l1_min_cfrom scipy.stats import ks_2sampprint '*************************************A****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************B****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv')print '*************************************C****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_test_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************D****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv')print '*************************************E****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_model_var_tbl_validation_20160806_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************F****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv')# 'E:\\Code\\ScoreCard\\whitelist_ext_civ_list_alpha.pkl'# 'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr.csv'def process_train_woe(infile_path=None,outfile_path=None,rst_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = infile_pathcfg = config.config()cfg.load_file(config_path,data_path)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1# change feature dtypesfp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)rst = []# process woe transformation of continuous variablesprint 'cfg.global_bt',cfg.global_btprint 'cfg.global_gt', cfg.global_gtfor var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))# process woe transformation of discrete variablesfor var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'rst.append(fp.proc_woe_discrete(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))eval.eval_feature_detail(rst, outfile_path)output = open(rst_path, 'wb')pickle.dump(rst,output)output.close()print '*************************************G****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl')print '*************************************H****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl')print '*************************************I****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl')print '*************************************J****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl')def process_woe_trans(in_data_path=None,rst_path=None,out_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = in_data_pathcfg = config.config()cfg.load_file(config_path, data_path)fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'output = open(rst_path, 'rb')rst = pickle.load(output)output.close()# Training dataset Woe Transformationfor r in rst:cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name], r)cfg.dataset_train.to_csv(out_path)print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv')print '*************************************O****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv')print '*************************************P****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv')print '*************************************Q****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv')print '*************************************R****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv')print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv')"""Training Model"""get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticdef train_model(infile,outfile,fig1,fig2):dataset_train = pd.read_csv(infile)cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']# init a LogisticRegression modelclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3)print("Computing regularization path ...")start = datetime.now()print startcoefs_ = []ks = []for c in cs:clf_l1_LR.set_params(C=c)clf_l1_LR.fit(X_train, y_train)coefs_.append(clf_l1_LR.coef_.ravel().copy())proba = clf_l1_LR.predict_proba(X_train)[:,1]ks.append(get_ks(proba,y_train))end = datetime.now()print endprint("This took ", end - start)coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns)coef_cv_df['ks'] = kscoef_cv_df['cs'] = cscoef_cv_df.to_csv(outfile)coefs_ = np.array(coefs_)plt.figure()plt.plot(np.log10(cs), coefs_)ymin, ymax = plt.ylim()plt.xlabel('log(C)')plt.ylabel('Coefficients')plt.title('Logistic Regression Path')plt.axis('tight')plt.savefig(fig1)plt.close()plt.figure()plt.plot(np.log10(cs), ks)plt.xlabel('log(C)')plt.ylabel('ks score')plt.title('Logistic Regression Performance')plt.axis('tight')plt.savefig(fig2)plt.close()train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd1.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd1.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd1.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd2.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd2.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd2.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd3.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd3.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd3.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd4.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd4.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd4.png') """Model Performance"""def predict_model(train_file,test_file,dataset_validation_path,c):print '*********************************************************'cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced')dataset_train = pd.read_csv(train_file)dataset_test = pd.read_csv(test_file)dataset_validation = pd.read_csv(dataset_validation_path)# fill nullfor var in candidate_var_list:if dataset_validation[var].isnull().sum()&gt;0:dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean()if dataset_test[var].isnull().sum()&gt;0:dataset_test.loc[dataset_test[var].isnull(), (var)] = dataset_test[var].mean()X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']X_test = dataset_test[candidate_var_list]y_test = dataset_test['target']clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]print get_ks(proba,y_train)proba = clf_l1_LR.predict_proba(X_test)[:,1]print get_ks(proba,y_test)# predictionproba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1]print get_ks(proba,dataset_validation['target']) predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv' ,c=0.00948742173925)'''agr_dd1:0.3913372974170.4017604347210.41978303981'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv' ,c=0.00949207179031)'''agr_dd2:0.428815152590.4253306159710.423628758103'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv' ,c=0.0063130606165)'''agr_dd3:0.4448530527120.4429655932780.429972957194'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv' ,c=0.00479980706543)'''agr_dd4:0.456283776350.4451460672930.464371772806''' 结论由切分成4个数据集分别的ks表现来看，分割后的模型表现差异比较明显，并且agr_dd1，agr_dd2，agr_dd3，agr_dd4的ks逐渐提升，这也不难寻求业务理解。可见，在这个字段上分割模型是有意义。 逻辑回归是线性的且单一的分类器，即使我们把这4个数据集增加一个‘A’,’B’,’C’,’D’的字段标识，然后合并为一个数据集，也不能够达到分割的效果。分割模型相对于只是分割数据集会进一步增强模型的表现能力。 分割数据集 分割模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python本地模块名与系统安装的包名重名冲突问题]]></title>
    <url>%2F2017%2F10%2F04%2F%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9D%97%E5%90%8D%E4%B8%8E%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%E5%90%8D%E9%87%8D%E5%90%8D%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本地修改 woe包中的eval模块时，发现 系统老是import&quot;D:\ProgramData\Anaconda2\lib\site-packages\woe\eval.py&quot;路径下的eval文件。那是因为我已经安装了woe这个包，然后需要调用我修改后的woe源码的本地模块文件，系统自动优先import的是之前安装的woe版本。 解决方法如下： 1234567891011#-*-coding:utf-8-*-__author__='boredbird'# importwoe.configasconfig# importwoe.feature_processasfp# importwoe.evalasevalimport impconfig = imp.load_source('config',r'E:\Code\woe\woe\config.py')fp = imp.load_source('feature_process',r'E:\Code\woe\woe\feature_process.py')eval = imp.load_source('eval',r'E:\Code\woe\woe\eval.py') For Python 3.5+ use: 123456import importlib.utilspec = importlib.util.spec_from_file_location("module.name", "/path/to/file.py")foo = importlib.util.module_from_spec(spec)spec.loader.exec_module(foo)foo.MyClass() For Python 3.3 and 3.4 use: 12345from importlib.machinery import SourceFileLoaderfoo = SourceFileLoader("module.name", "/path/to/file.py").load_module()foo.MyClass()# (Although this has been deprecated in Python 3.4.) For Python 2 use: 1234import impfoo = imp.load_source('module.name', '/path/to/file.py')foo.MyClass() There are equivalent convenience functions for compiled Python files and DLLs.See it here. 另外一种方式，将要导入的模块路径添加到系统路径，并且放在&quot;D:\ProgramData\Anaconda2\lib\site-packages\这个路径前面。比如， 1234import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler']) 然后再讲你刚添加的路径元素调整在sys.path这个list中的位置。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（三）之剔除噪声]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E5%89%94%E9%99%A4%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到正则化带来了模型效果的提升。此时，在对建模宽表的加工过程检查发现，有一部分用户在特征加工时间窗口observe_date之前没有表现期，这部分的用户放在样本中其实是噪声，我们将之剔除，重新woe转换，然后再入模查看效果。 首先，搜索最优正则化参数c： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp import woe.config as config import woe.feature_process as fp import woe.eval as eval get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show() print ks print cs``` ![](https://i.imgur.com/812KAjt.png)![](https://i.imgur.com/dAO4rdb.png)然后，将最优正则化参数c代入，训练模型与预测：``` python c = 0.00211473526246312 clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01) ''' 预测并查看模型效果 ''' clf_l1_LR.fit(X_train, y_train) proba = clf_l1_LR.predict_proba(X_train)[:,1] get_ks(proba,y_train) # 0.42519514636341194 proba = clf_l1_LR.predict_proba(X_test)[:,1] get_ks(proba,y_test) # 0.42014866529356498 dataset_validation = pd.read_csv('E:\\ScoreCard\\pos_validation_20160806_agr_woe_trans.csv') # fill null for var in candidate_var_list: if dataset_validation[var].isnull().sum()&gt;0: dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean() # prediction proba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1] get_ks(proba,dataset_validation['target']) # 0.42007842928689282 可以看出，剔除一些噪声样本之后模型的效果有很大的提升，而且此时，训练集的效果与同时期验证，跨时期验证的效果都很接近，可以认为没有发生过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（二）之正则化]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[在上回的基础上，可以看出特征之间存在多重共线性，自然的联想到L1正则化，这次主要尝试用sklearn.linear_model中正则化的LogisticRegression去提升模型效果。 12345678910111213141516171819X = dataset_train[candidate_var_list]y = dataset_train['target']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)from sklearn.linear_model import LogisticRegressionclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]get_ks(proba,y_train)# 0.40141223793092612proba = clf_l1_LR.predict_proba(X_test)[:,1]get_ks(proba,y_test)# 0.39708735770404702 可以看出此时采用C=0.1正则化的模型已经有了很大的提升。 123456C : float, default: 1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. 可是，c是个超参数，如何设置才能达到最优的效果呢？ 还是以结果为导向，一个一个试。虽然不是全局最优，那也大差不差。 import matplotlib.pyplot as plt from datetime import datetime from sklearn import linear_model from sklearn.svm import l1_min_c cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start clf_l1_LR = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6) coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（一）之数据集初探]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[本系列记录我在工作中针对同一个数据集，运用逻辑回归（评分卡）模型不断尝试去提升模型ks值表现的过程。并不是范例，更谈不上指导意义。主要是为了记录自己的思考与验证过程，怕随便零散的写在代码注释里过后就忘了。 评分卡模型在数据处理过程中常用到woe转换，这部分的工作我采用了Python的包woe来完成。 Woe is a python package containing useful tools for WoE Transformation mostly used in ScoreCard Model for Credit Rating. woe包中树节点的分裂方法，是一种Local近似算法。对于每个特征，只考察分位点，减少计算复杂度；每次分裂前，重新提出候选切分点。 关于树节点分裂方法，这里有简单的介绍。 #数据初步处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170 # -*- coding:utf-8 -*- __author__ = 'boredbird' import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np import copy ''' 导入数据集与配置文件 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) ''' 填充空值 ''' for var in cfg.bin_var_list: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] columns = ['cus_id'] columns.extend(cfg.candidate_var_list) columns.append('target') dataset_train = cfg.dataset_train[columns] ''' 删除不用的变量，导出数据集，减少内存占用 ''' dataset_train.to_csv('E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv') ''' 重新导入数据集 并指定变量类型 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] # process woe transformation of continuous variables for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05)) ''' 保存rst至文件 ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'wb') pickle.dump(rst,output) output.close() ''' 格式化输出feature_detail，查看每个变量的iV值 ''' feature_detail = eval.eval_feature_detail(rst,'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail.csv')``` #Woe 转换 ``` python ''' Woe Transformation ''' ''' 加载用于存储分割点信息的列表rst ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'rb') rst = pickle.load(output) output.close() ''' FILL NULL 因为在生成rst之前做了空值填充处理， 所以在利用rst进行woe转换之前，也要分别对连续变量和离散变量做同样的空值填充。 ''' for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing' #Training dataset Woe Transformation for r in rst: cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name],r) cfg.dataset_train.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206.csv') ''' Validation Dataset Transformation ''' #load dataset and eliminate variables dataset_validation = pd.read_csv('E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_validation_20160806.csv') var_exists_list = [rst[i].var_name for i in range(len(rst))] var_exists_list.extend(['cus_id','target',]) columns_to_drop = [var for var in dataset_validation.columns if var not in var_exists_list] dataset_validation = dataset_validation.drop(columns_to_drop,axis=1) #change variable dtypes before woe transformation for var in [tmp for tmp in cfg.bin_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = 'missing' #Validation dataset Woe Transformation for r in rst: if r.var_name in dataset_validation.columns: dataset_validation[r.var_name] = woe_trans(dataset_validation[r.var_name],r) dataset_validation.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_validation_20160806.csv')``` #跑Logit Regression 基于woe替换后的变量，删除iV值比较低的（删除了iv小于0.01的变量），调用statsmodels跑逻辑回归。``` python import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np dataset_train = pd.read_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206_new.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] import numpy as np from sklearn.model_selection import train_test_split X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) import statsmodels.api as sm logit_mod = sm.Logit(y_train,X_train) logit_res = logit_mod.fit(disp=0) # print('Parameters: ', logit_res.params) print logit_res.summary() 由上表可以看出当前这个模型存在两个问题： 1、存在变量系数显著性检验p值不通过的变量； 2、存在coef系数为负的情况，本身经过了woe转换后的变量的woe值与LR的概率值是正相关关系，系数应该为正，这种情况应该是变量之间的多重共线性导致的。 #模型评价，K-S 1234567891011121314from scipy.stats import ks_2sampget_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticproba = logit_res.predict(X_train)get_ks(proba,y_train)# 0.35121344450518605proba = logit_res.predict(X_test)get_ks(proba,y_test)# 0.34393694879059489proba = logit_res.predict(dataset_validation[features_list])get_ks(proba,dataset_validation['target'])# 0.35956755211806057]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM与XGBoost对比]]></title>
    <url>%2F2017%2F10%2F02%2FLightGBM%E4%B8%8EXGBoost%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[XGBoost的系统设计 更高效的工具包LightGBM LightGBM的改进直方图算法 直方图加速 建树过程的两种方法：Level-wise和Leaf-wise 并行优化（Optimization in parallel learning）特征并行 数据并行 GOSS EFB 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT算法原理与系统设计简介-来自wepon的分享]]></title>
    <url>%2F2017%2F10%2F02%2FGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%AE%80%E4%BB%8B-%E6%9D%A5%E8%87%AAwepon%E7%9A%84%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[一、泰勒公式 定义：泰勒公式是一个用函数在某点的信息描述其附近取值的公式。局部有效性二、最优化方法2.1 梯度下降法（Gradient descend method）2.2 牛顿法（Newton’s method）三、从参数空间到函数空间3.1 从Gradient descend 到 Gradient Boosting3.2 从Newton’s method 到 Newton Boosting四、Gradient Boosting Tree算法原理五、Newton Boosting Tree算法原理：详解XGBoost5.1 模型函数形式5.2 目标函数5.2.1 正则项5.2.2 误差函数的二阶泰勒展开5.3 回归树的学习策略5.3.1 打分函数5.3.2 树节点分裂方法5.3.3 缺失值的处理5.4 其它特性]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池老司机天音的经验分享]]></title>
    <url>%2F2017%2F10%2F02%2F%E5%A4%A9%E6%B1%A0%E8%80%81%E5%8F%B8%E6%9C%BA%E5%A4%A9%E9%9F%B3%E7%9A%84%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[化繁为简，各个突破 一、数据处理 25%1.1 缺失值填充某些场景中缺失值也是一个重要特征，如金融信贷，资料完善度不高的用户是潜在的违约对象 数值型：均值，中位数 类别型：众数 通过关联信息补全1.2 数据清洗 剔除缺失值比率大的字段 离群点剔除1.3 数据处理1.3.1 异常数据的处理 log平滑 局部加权1.3.2 数据划分，样本构造 增：增加样本，滑窗采样 减：去掉异常样本，可以借助模型判断 采样：效率和效果的平衡 根据需求构建可靠的验证集 注：线上线下Label窗口大小要一致；滑窗是为了增加样本量，一般把多个窗口的数据集合在一起训练；各窗口构成的数据集Label Window无重叠，特征窗口可以重叠； 1.3.3 噪声样本剔除用一个树模型去训练你的数据，然后用得到的模型去预测数据，同一个叶子节点下偏离叶子节点均值最大的认为它是一个噪声，然后对它的差值进行排序取出前top多少个，把它去掉，然后重新训练一个比较干净的模型 二、特征工程 50%特征没做好，参数调到老 2.1 特征提取结合具体的业务场景思考问题 特征统计：key拆分，组合（按照提供的数据集粒度，不同的维度去交叉组合出更多的指标） 时间窗口统计：3/5/7/15/30 2.2 特征处理 归一化（加快训练速度并且更可能达到最优解，线性模型，对数值大小敏感的模型） onehot编码（增加哑变量） 排序 2.3 特征选择 计算特征与标签相关度（按照相关性大小排序筛选特征） LR+L1正则 树模型对特征打分（用的最多的是XGBoost，RF去对特征打分，然后筛选重要的变量再放到线性模型里面）2.4 特征组合 任意两两组合：维度大 特征选择再组合 GBDT+LR（GBDT每棵树从根节点到叶子节点的路径是一种最优组合） 三、模型设计 20%3.1 分类-回归问题 Xgboost：速度快，效果好 GBDT：拟合能力强，有过拟合风险 RF：不容易过拟合 LR：对稀疏特征效果较好；常用来做stacker3.2 Factorization Machine3.3 基于统计的方法（规则）3.4 时序问题 回归类模型 规则函数3.5 规则函数 四、模型融合 5%]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>FM</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python连接hive]]></title>
    <url>%2F2017%2F09%2F19%2F%E4%BD%BF%E7%94%A8python%E8%BF%9E%E6%8E%A5hive%2F</url>
    <content type="text"><![CDATA[Python连接hive的包有很多，我没有一一尝试，只是演示了工作当中用到的方法。本文主要采用PyHive来进行对hive的增删改查。PyHive is a collection of Python DB-API and SQLAlchemy interfaces for Presto and Hive. Requirementsinstall using1pip install SQLALchemy 1pip install pyhive[presto] 1pip install pyhive[hive] Presto engineCreate engine123```### Presto Select```df = pd.read_sql(sql=(r&apos;select * from &apos;+ &apos;schema.tablename&apos;) , con=engine) Presto Insertcon1234```### Presto UpdateHive does not support update.### Presto Delete engine.execute(“drop table schema.tablename”)123456## Hive engine### Create engine``` engine = create_engine('hive://url:port/hive/schema') Hive Select123```### Hive Insert```df.to_sql(tablename, con=engine, flavor=None, if_exists=&apos;append&apos;, index=False, chunksize=2000000) Hive UpdateHive does not support update. Hive Delete1engine.execute("drop table schema.tablename")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Fxgboost%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 首先，打开Git Shell，依次执行如下命令： 123456789101112131415161718git clone --recursive https://github.com/dmlc/xgboostcd xgboostgit checkout 9a48a40//新版本这一步可以省略git submodule initgit submodule updatecp make/mingw64.mk config.mkcp make/mingw64.mk dmlc-core/config.mkcd rabitmake lib/librabit_empty.a -j4cd ../dmlc-coremake -j4cd..make -j4 然后，安装到Python包中 123cd python-packagepython setup.py install 最后，导入xgboost包，测试demo 12345cd ..//或者直接进入xgboost目录cd democd guide-pythonpython basic_walkthrough.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172--1--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ git clone --recursive https://github.com/dmlc/xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ cd xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule initchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule updatechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk dmlc-core/config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd rabit/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ make lib/librabit_empty.a -j4chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ cd ../dmlc-corechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/dmlc-core ((b5bec54...))$ make -j4--2--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd python-packagechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ python setup.py install--3--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ cd ..chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd demochunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo (master)$ cd guide-python/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ python basic_walkthrough.py[09:58:28] 6513x127 matrix with 143286 entries loaded from ../data/agaricus.txt.train[09:58:28] 1611x127 matrix with 35442 entries loaded from ../data/agaricus.txt.test[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263[09:58:28] 1611x127 matrix with 35442 entries loaded from dtest.buffererror=0.021726start running example of build DMatrix from scipy.sparse CSR Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from scipy.sparse CSC Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from numpy array[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ via 64位Windows下安装xgboost详细参考指南（支持Python2.x和3.x）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightgbm安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Flightgbm%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 Visual Studio 2017 1、下载源代码git clone –recursive https://github.com/Microsoft/LightGBM 2、用VS编译进入LightGBM目录，用VS打开windows/LightGBM.sln，选择DLL和x64，按Ctrl+Shift+B进行编译，dll文件就会在windows/x64/DLL/目录里 编译成功后对应目录E:\Code\LightGBM\windows\x64\DLL下会生成一些文件，如果存在DLL这个文件夹就说明安装成功了。 3、安装Python包123456chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM (master)$ cd python-package/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ python setup.py install 4、测试12345678910111213141516171819202122232425262728chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ cd ../examples/python-guide/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/examples/python-guide (master )$ python ./simple_example.pyLoad data...Start training...[1] valid_0's auc: 0.764496 valid_0's l2: 0.24288Training until validation scores don't improve for 5 rounds.[2] valid_0's auc: 0.766173 valid_0's l2: 0.239307[3] valid_0's auc: 0.785547 valid_0's l2: 0.235559[4] valid_0's auc: 0.797786 valid_0's l2: 0.230771[5] valid_0's auc: 0.805155 valid_0's l2: 0.226297[6] valid_0's auc: 0.800979 valid_0's l2: 0.223692[7] valid_0's auc: 0.806566 valid_0's l2: 0.220941[8] valid_0's auc: 0.808566 valid_0's l2: 0.217982[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351[10] valid_0's auc: 0.805953 valid_0's l2: 0.213064[11] valid_0's auc: 0.804631 valid_0's l2: 0.211053[12] valid_0's auc: 0.802922 valid_0's l2: 0.209336[13] valid_0's auc: 0.802011 valid_0's l2: 0.207492[14] valid_0's auc: 0.80193 valid_0's l2: 0.206016Early stopping, best iteration is:[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351Save model...Start predicting...('The rmse of prediction is:', 0.4640593794679212) 或者直接通过pip安装。。。 via 在Windows下安装LightGBM的Python包 微软开源分布式高性能GB框架LightGBM安装使用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
