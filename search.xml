<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[手撸AdaBoost]]></title>
    <url>%2F2017%2F11%2F17%2F%E6%89%8B%E6%92%B8AdaBoost%2F</url>
    <content type="text"><![CDATA[AdaBoost算法概述： 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。 缺点：对离群点敏感。 适用数据类型：数值型和标称型数据。 将不同的分类器组合起来，成为集成方法（ensemble method）或者元算法（meta-algorithm)。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。 bagging(bootstrap aggregating)，是在从原始数据集选择s次后得到s个新的数据集的一种技术，新数据集和原始数据集大小相等。有放回的抽样，所以允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。在s个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了s个分类器。当我们要对新数据进行分类时，就可以应用这s个分类器进行分类。选择分类器投票结果中最多的类别作为最后的分类结果。 boosting和bagging类似，所使用的多个分类器类型都是一致的，但是boosting是通过串行训练而获得，每个新分类器都是根据已训练出的分类器的性能来进行训练。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸SVM]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8SVM%2F</url>
    <content type="text"><![CDATA[基于最大间隔分隔数据 优点：泛化错误率低，计算开销不大，结果易解释。 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。 适用数据类型：数值型和标称型数据。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸LogisticRegression]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8LogisticRegression%2F</url>
    <content type="text"><![CDATA[LogisticRegression算法概述：利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。 优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 适用数据类型：数值型和标称型数据。 梯度下降法求最佳参数伪代码： 每个回归系数初始化为1 重复R次： a.计算整个数据集的梯度 b.使用alpha*gradient更新回归系数的向量 c.返回回归系数 梯度下降法求最佳参数代码实现：123456789101112131415161718192021222324# 定义sigmoid函数：def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 梯度下降法def gradAscent(dataMatIn, classLabels): """ 利用梯度下降法求解逻辑回归系数 :param dataMatIn:2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本 :param classLabels: :return: 返回训练好的回归系数 """ dataMatrix = mat(dataMatIn) # convert to NumPy matrix labelMat = mat(classLabels).transpose() # convert to NumPy matrix m, n = shape(dataMatrix) alpha = 0.001 # 步长 maxCycles = 500 # 迭代次数 weights = ones((n, 1)) for k in range(maxCycles): # heavy on matrix operations h = sigmoid(dataMatrix * weights) # matrix mult error = (labelMat - h) # vector subtraction weights = weights + alpha * dataMatrix.transpose() * error # matrix mult return weights 随机梯度下降法求最佳参数伪代码： 随机梯度下降法求最佳参数代码实现：12345678910def stocGradAscent(dataMatrix, classLabels): m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) # initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i] * weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights mini-batch梯度下降法求最佳参数代码实现：1234567891011121314def batchGradAscent(dataMatrix, classLabels, numIter=150): m, n = shape(dataMatrix) weights = ones(n) # initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4 / (1.0 + j + i) + 0.0001 # apha decreases with iteration, does not randIndex = int(random.uniform(0, len(dataIndex))) # go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex] * weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del (dataIndex[randIndex]) return weights LogisticRegression小结Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度下降算法。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LogisticRegression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸DecisionTree]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8DecisionTree%2F</url>
    <content type="text"><![CDATA[DecisionTree算法概述： 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配问题。 适用数据范围：数值型和标称型 DecisionTree伪代码： DecisionTree代码实现：创建数据集12345678910111213from math import logimport operatordef createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] # change to discrete values return dataSet, labels 计算给定数据集的香农熵123456789101112131415def calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = &#123;&#125; # 为所有可能分类创建字典 # 循环遍历数据集的每一行 for featVec in dataSet: # the the number of unique elements and their occurance currentLabel = featVec[-1] # 假设最后一列是label if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob, 2) # log base 2 return shannonEnt 选择最好的数据集划分方式123456789101112131415161718192021222324252627def chooseBestFeatureToSplit(dataSet): """ 选择最好的数据集划分方式 :param dataSet: :return: 最佳分割变量所在的下标 """ numFeatures = len(dataSet[0]) - 1 # the last column is used for the labels baseEntropy = calcShannonEnt(dataSet) #计算分割之前的熵 bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): # iterate over all the features # 笨拙的筛选数据集的某一列 # create a list of all the examples of this feature featList = [example[i] for example in dataSet] # get a set of unique values uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: # 遍历所有取值作为可能的分割点 subDataSet = splitDataSet(dataSet, i, value) # 只能筛选离散取值的变量 prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # calculate the info gain; ie reduction in entropy if (infoGain &gt; bestInfoGain): # compare this to the best gain so far bestInfoGain = infoGain # if better than current best, set to best bestFeature = i return bestFeature # returns an integer 计算出现次数最多的分类：12345678910111213def majorityCnt(classList): """ 采用多数表决得方式确定该叶子节点的分类 :param classList: 叶子节点的所有样本标签 :return: 投票的最终类别 """ classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 创建树的函数代码12345678910111213141516171819def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 标签列 if classList.count(classList[0]) == len(classList): return classList[0] # stop splitting when all of the classes are equal if len(dataSet[0]) == 1: # stop splitting when there are no more features in dataSet return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 最佳分割变量的下标 bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel: &#123;&#125;&#125; del (labels[bestFeat]) # 从候选集中删除当前分割变量 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # copy all of labels, so trees don't mess up existing labels # 在当前节点的所有可能取值上递归继续分割 # 字典的key:特征名称、特征取值；字典的value: 字典本身 或者 叶节点的类别名称 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTree 使用决策树进行分类：12345678910111213141516171819202122def classify(inputTree, featLabels, testVec): """ 递归决策树字典，输出所在叶节点的类别 :param inputTree: 树结构以字典的形式存储 :param featLabels: 特征列表 :param testVec: 待预测的数据集 :return: 节点类别 """ firstStr = inputTree.keys()[0] secondDict = inputTree[firstStr] # 将标签字符串转换为索引 featIndex = featLabels.index(firstStr) key = testVec[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): # 如果当前节点的value仍是一个dict则继续递归 classLabel = classify(valueOfFeat, featLabels, testVec) else: # 叶节点 classLabel = valueOfFeat return classLabel DecisionTree小结构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用Python语言内嵌的数据结构字典存储树节点信息。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸kNN]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8kNN%2F</url>
    <content type="text"><![CDATA[k近邻算法概述：k-近邻算法采用测量不同特征值之间的距离方法进行分类。 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型 kNN伪代码：对未知类别属性的数据集中的每个点依次执行以下操作： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点出现频率最高的类别作为当前点的预测分类。 kNN代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from numpy import *import operatorfrom os import listdirdef createDataSet(): """ 创建数据集 :return:特征数据 array，标注数据 list """ group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labelsdef autoNorm(dataSet): # 参数0使得函数可以从列中选取最小值，而不是选取当前行的最小值 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m, 1)) # element wise divide 矩阵数值相除 normDataSet = normDataSet / tile(ranges, (m, 1)) return normDataSet, ranges, minValsdef knn_classifier(inX, dataSet, labels, k): """ k-近邻算法 :param inX:待分类的输入向量 :param dataSet:训练样本集 :param labels:标签向量 :param k:用于选择最近邻居的数目 :return:k个近邻投票的最终类别 """ dataSetSize = dataSet.shape[0] # 计算两个向量点的距离 # tile: 复制输出，将变量内容复制成输入矩阵同样大小的矩阵 diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() # 选择距离最小的k个点 classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # 排序 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] kNN小结: k-近邻算法是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。 k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。 此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。 k-近邻算法的另一缺陷是他无法给出任何数据的基础结构信息，因此我们无法知晓平均实例样本和典型实例样本具有什么特征。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NumPy</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clustering with sklearn]]></title>
    <url>%2F2017%2F11%2F15%2FClustering%20with%20sklearn%2F</url>
    <content type="text"><![CDATA[生成聚类数据集的方法生成数据集方法：sklearn.datasets.make_blobs(n_samples,n_featurs,centers)可以生成数据集,n_samples表示个数，n_features表示特征个数，centers表示y的种类数: make_blobs函数是为聚类产生数据集产生一个数据集和相应的标签 n_samples:表示数据样本点个数,默认值100 n_features:表示数据的维度，默认值是2 centers:产生数据的中心点，默认值3 cluster_std：数据集的标准差，浮点数或者浮点数序列，默认值1.0 center_box：中心确定之后的数据边界，默认值(-10.0, 10.0) shuffle ：洗乱，默认值是True random_state:官网解释是随机生成器的种子 y3 = np.array([0]100 + [1]50 + [2]20 + [3]5)可以这样建立array数组 k-means对于方差不相等和数据与坐标轴不平行时效果不理想；对于数据大小量纲敏感。 The Influence of Data Distribution on KMeans Clustering123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import KMeansdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 400centers = 4data, y = ds.make_blobs(N, n_features=2, centers=centers, random_state=2)data2, y2 = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=(1,2.5,0.5,2), random_state=2)data3 = np.vstack((data[y == 0][:], data[y == 1][:50], data[y == 2][:20], data[y == 3][:5]))y3 = np.array([0] * 100 + [1] * 50 + [2] * 20 + [3] * 5)cls = KMeans(n_clusters=4, init='k-means++')y_hat = cls.fit_predict(data)y2_hat = cls.fit_predict(data2)y3_hat = cls.fit_predict(data3)m = np.array(((1, 1), (1, 3)))data_r = data.dot(m)y_r_hat = cls.fit_predict(data_r)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falsecm = matplotlib.colors.ListedColormap(list('rgbm'))plt.figure(figsize=(9, 10), facecolor='w')plt.subplot(421)plt.title(u'Raw data')plt.scatter(data[:, 0], data[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data, axis=0)x1_max, x2_max = np.max(data, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(422)plt.title(u'KMeans++ clustering')plt.scatter(data[:, 0], data[:, 1], c=y_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(423)plt.title(u'Data after rotation')plt.scatter(data_r[:, 0], data_r[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data_r, axis=0)x1_max, x2_max = np.max(data_r, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(424)plt.title(u'Data rotated KMeans++ clustering')plt.scatter(data_r[:, 0], data_r[:, 1], c=y_r_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(425)plt.title(u'Unequal variance data')plt.scatter(data2[:, 0], data2[:, 1], c=y2, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data2, axis=0)x1_max, x2_max = np.max(data2, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(426)plt.title(u'Data with unequal variance KMeans++ clustering')plt.scatter(data2[:, 0], data2[:, 1], c=y2_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(427)plt.title(u'Data with unequal numbers')plt.scatter(data3[:, 0], data3[:, 1], s=30, c=y3, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data3, axis=0)x1_max, x2_max = np.max(data3, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(428)plt.title(u'Data with unequal numbers KMeans++ clustering')plt.scatter(data3[:, 0], data3[:, 1], c=y3_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.tight_layout(2)plt.suptitle(u'The Influence of Data Distribution on KMeans Clustering', fontsize=18)# https://github.com/matplotlib/matplotlib/issues/829plt.subplots_adjust(top=0.92)plt.show() 输出 聚类性能的评价指标 均一性sklearn.metrics.homogeneity_score 完整性sklearn.metrics.completeness_score 均一性完整性二者的加权平均v_measure_score ARI（Adjusted Rand index(调整兰德指数)：sklearn.metrics.adjusted_rand_score AMI: sklearn.metrics.adjusted_mutual_info_score 关于指标的定义请看这里 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn import metricsy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cv2 = 2 * c * h / (c + h)v = metrics.v_measure_score(y, y_hat)print u'V-Measure：', v2, vy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 2, 3, 3]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', v# 允许不同值y = [0, 0, 0, 1, 1, 1]y_hat = [1, 1, 1, 0, 0, 0]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', vy = [0, 0, 1, 1]y_hat = [0, 1, 0, 1]ari = metrics.adjusted_rand_score(y, y_hat)print ariy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]ari = metrics.adjusted_rand_score(y, y_hat)print ari 输出123456789101112同一性(Homogeneity)： 0.666666666667完整性(Completeness)： 0.420619835714V-Measure： 0.515803742979 0.515803742979同一性(Homogeneity)： 1.0完整性(Completeness)： 0.521296028614V-Measure： 0.685331478962同一性(Homogeneity)： 1.0完整性(Completeness)： 1.0V-Measure： 1.0-0.50.242424242424 AffinityPropagation12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import AffinityPropagationfrom sklearn.metrics import euclidean_distancesN = 400centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)m = euclidean_distances(data, squared=True)preference = -np.median(m)print 'Preference：', preferencematplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 9), facecolor='w')for i, mul in enumerate(np.linspace(1, 4, 9)): print mul p = mul * preference model = AffinityPropagation(affinity='euclidean', preference=p) af = model.fit(data) center_indices = af.cluster_centers_indices_ n_clusters = len(center_indices) print ('p = %.1f' % mul), p, '聚类簇的个数为：', n_clusters y_hat = af.labels_ plt.subplot(3, 3, i+1) plt.title(u'Preference：%.2f，簇个数：%d' % (p, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') center = data[center_indices[k]] for x in data[cur]: plt.plot([x[0], center[0]], [x[1], center[1]], color=clr, zorder=1) plt.scatter(data[center_indices, 0], data[center_indices, 1], s=100, c=clrs, marker='*', edgecolors='k', zorder=2) plt.grid(True)plt.tight_layout()plt.suptitle(u'AP聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出1234567891011121314151617181920Preference： -5.299145530341.0p = 1.0 -5.29914553034 聚类簇的个数为： 161.375p = 1.4 -7.28632510422 聚类簇的个数为： 121.75p = 1.8 -9.27350467809 聚类簇的个数为： 112.125p = 2.1 -11.260684252 聚类簇的个数为： 102.5p = 2.5 -13.2478638258 聚类簇的个数为： 82.875p = 2.9 -15.2350433997 聚类簇的个数为： 83.25p = 3.2 -17.2222229736 聚类簇的个数为： 753.625p = 3.6 -19.2094025475 聚类簇的个数为： 74.0p = 4.0 -21.1965821214 聚类簇的个数为： 7 MeanShift Clustering12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import MeanShiftfrom sklearn.metrics import euclidean_distancesN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 9), facecolor='w')m = euclidean_distances(data, squared=True)bw = np.median(m)print bwfor i, mul in enumerate(np.linspace(0.1, 0.4, 4)): band_width = mul * bw model = MeanShift(bin_seeding=True, bandwidth=band_width) ms = model.fit(data) centers = ms.cluster_centers_ y_hat = ms.labels_ n_clusters = np.unique(y_hat).size print '带宽：', mul, band_width, '聚类簇的个数为：', n_clusters plt.subplot(2, 2, i+1) plt.title(u'带宽：%.2f，聚类簇的个数为：%d' % (band_width, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) print clrs for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') plt.scatter(centers[:, 0], centers[:, 1], s=150, c=clrs, marker='*', edgecolors='k') plt.grid(True)plt.tight_layout(2)plt.suptitle(u'MeanShift聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出123456789105.31661129692带宽： 0.1 0.531661129692 聚类簇的个数为： 7['#ff0000', '#d4802a', '#aa0055', '#7f807f', '#5500aa', '#2a80d4', '#0000ff']带宽： 0.2 1.06332225938 聚类簇的个数为： 4['#ff0000', '#aa0055', '#5500aa', '#0000ff']带宽： 0.3 1.59498338907 聚类簇的个数为： 3['#ff0000', '#7f807f', '#0000ff']带宽： 0.4 2.12664451877 聚类簇的个数为： 1['#ff0000'] DBSCAN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import DBSCANfrom sklearn.preprocessing import StandardScalerdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)data = StandardScaler().fit_transform(data)params = ((0.2, 5), (0.2, 10), (0.2, 15), (0.3, 5), (0.3, 10), (0.3, 15))matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'DBSCAN聚类', fontsize=20)for i in range(6): eps, min_samples = params[i] model = DBSCAN(eps=eps, min_samples=min_samples) model.fit(data) y_hat = model.labels_ core_indices = np.zeros_like(y_hat, dtype=bool) core_indices[model.core_sample_indices_] = True y_unique = np.unique(y_hat) n_clusters = y_unique.size - (1 if -1 in y_hat else 0) print y_unique, '聚类簇的个数为：', n_clusters # clrs = [] # for c in np.linspace(16711680, 255, y_unique.size): # clrs.append('#%06x' % c) plt.subplot(2, 3, i+1) clrs = plt.cm.Spectral(np.linspace(0, 0.8, y_unique.size)) for k, clr in zip(y_unique, clrs): cur = (y_hat == k) if k == -1: plt.scatter(data[cur, 0], data[cur, 1], s=20, c='k') continue plt.scatter(data[cur, 0], data[cur, 1], s=30, c=clr, edgecolors='k') plt.scatter(data[cur &amp; core_indices][:, 0], data[cur &amp; core_indices][:, 1], s=60, c=clr, marker='o', edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\epsilon$ = %.1f m = %d，聚类数目：%d' % (eps, min_samples, n_clusters), fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出1234567[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3 4] 聚类簇的个数为： 5[-1 0] 聚类簇的个数为： 1[-1 0 1] 聚类簇的个数为： 2[-1 0 1 2 3] 聚类簇的个数为： 4 谱聚类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import spectral_clusteringfrom sklearn.metrics import euclidean_distancesdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dmatplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falset = np.arange(0, 2*np.pi, 0.1)data1 = np.vstack((np.cos(t), np.sin(t))).Tdata2 = np.vstack((2*np.cos(t), 2*np.sin(t))).Tdata3 = np.vstack((3*np.cos(t), 3*np.sin(t))).Tdata = np.vstack((data1, data2, data3))n_clusters = 3m = euclidean_distances(data, squared=True)sigma = np.median(m)plt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'谱聚类', fontsize=20)clrs = plt.cm.Spectral(np.linspace(0, 0.8, n_clusters))for i, s in enumerate(np.logspace(-2, 0, 6)): print s af = np.exp(-m ** 2 / (s ** 2)) + 1e-6 y_hat = spectral_clustering(af, n_clusters=n_clusters, assign_labels='kmeans', random_state=1) plt.subplot(2, 3, i+1) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], s=40, c=clr, edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\sigma$ = %.2f' % s, fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>make_blobs</tag>
        <tag>euclidean_distances</tag>
        <tag>KMeans</tag>
        <tag>KMeans++</tag>
        <tag>homogeneity_score</tag>
        <tag>completeness_score</tag>
        <tag>v_measure_score</tag>
        <tag>adjusted_rand_score</tag>
        <tag>AffinityPropagation</tag>
        <tag>MeanShift</tag>
        <tag>spectral_clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.svm]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.svm%2F</url>
    <content type="text"><![CDATA[SVC:ovr123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import numpy as npfrom sklearn import svmfrom sklearn.model_selection import train_test_splitimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.datasets import load_irisiris = load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b, tip): acc = a.ravel() == b.ravel() print tip + '正确率：', np.mean(acc)# 分类器# clf = svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr')# ‘ovo’ 一对一, ‘ovr’ 多对多 or None 无, default=Noneclf = svm.SVC(C=0.8, kernel='rbf', gamma=20, decision_function_shape='ovr')clf.fit(x_train, y_train.ravel())# 准确率# score(X, y, sample_weight=None) ：Returns the mean accuracy on the given test data and labels.print clf.score(x_train, y_train) # 精度y_hat = clf.predict(x_train)show_accuracy(y_hat, y_train, '训练集')print clf.score(x_test, y_test)y_hat = clf.predict(x_test)show_accuracy(y_hat, y_test, '测试集')# decision_functionprint 'decision_function:\n', clf.decision_function(x_train)print '\npredict:\n', clf.predict(x_train)"""# decision_function(): Distance of the samples X to the separating hyperplane.# 分类归属于距离数值最小的类别，距离有正有负只是平面的两侧而已decision_function:[[-0.25631211 0.80529378 2.45101833] [ 2.35232967 0.82494155 -0.17727121] [ 2.45418203 0.77495649 -0.22913852] [ 2.45421283 0.77518383 -0.22939666] [-0.24854074 2.39614366 0.85239708] [ 2.46621083 0.76927647 -0.23548729] [ 2.45410944 0.77775367 -0.2318631 ] [-0.24677667 0.84905678 2.39771989] [-0.46050688 1.21154683 2.24896005] [-0.26893412 0.80449581 2.46443831]predict:[2 0 0 0 1 0 0 2 2 2"""# 画图x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点# numpy.ndarray.flat : A 1-D iterator over the array.# This is a numpy.flatiter instance, which acts similarly to,# but is not a subclass of, Python’s built-in iterator object.grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点# print 'grid_test = \n', grid_test# Z = clf.decision_function(grid_test) # 样本到决策面的距离# print Zgrid_hat = clf.predict(grid_test) # 预测分类值print grid_hatgrid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本plt.scatter(x_test[:, 0], x_test[:, 1], s=120, facecolors='none', zorder=10) # 圈中测试集样本plt.xlabel(iris_feature[0], fontsize=13)plt.ylabel(iris_feature[1], fontsize=13)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.title(u'鸢尾花SVM二特征分类', fontsize=15)plt.grid()plt.show() 输出： SVC:ovo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npfrom sklearn import svmfrom scipy import statsfrom sklearn.metrics import accuracy_scoreimport matplotlib as mplimport matplotlib.pyplot as pltdef extend(a, b, r): x = a - b m = (a + b) / 2 return m-r*x/2, m+r*x/2np.random.seed(0)N = 20x = np.empty((4*N, 2))means = [(-1, 1), (1, 1), (1, -1), (-1, -1)]sigmas = [np.eye(2), 2*np.eye(2), np.diag((1,2)), np.array(((2,1),(1,2)))]for i in range(4): # scipy.stats.multivariate_normal: # A multivariate normal random variable. # The mean keyword specifies the mean. The cov keyword specifies the covariance matrix. mn = stats.multivariate_normal(means[i], sigmas[i]*0.3) # rvs(mean=None, cov=1) Draw random samples from a multivariate normal distribution. x[i*N:(i+1)*N, :] = mn.rvs(N)a = np.array((0,1,2,3)).reshape((-1, 1))y = np.tile(a, N).flatten()clf = svm.SVC(C=1, kernel='rbf', gamma=1, decision_function_shape='ovo')# clf = svm.SVC(C=1, kernel='linear', decision_function_shape='ovr')clf.fit(x, y)y_hat = clf.predict(x)acc = accuracy_score(y, y_hat)np.set_printoptions(suppress=True)print u'预测正确的样本个数：%d，正确率：%.2f%%' % (round(acc*4*N), 100*acc)# decision_functionprint clf.decision_function(x)# print y_hatx1_min, x2_min = np.min(x, axis=0)x1_max, x2_max = np.max(x, axis=0)x1_min, x1_max = extend(x1_min, x1_max, 1.05)x2_min, x2_max = extend(x2_min, x2_max, 1.05)x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j]x_test = np.stack((x1.flat, x2.flat), axis=1)y_test = clf.predict(x_test)y_test = y_test.reshape(x1.shape)cm_light = mpl.colors.ListedColormap(['#FF8080', '#A0FFA0', '#6060FF', '#F080F0'])cm_dark = mpl.colors.ListedColormap(['r', 'g', 'b', 'm'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_test, cmap=cm_light)plt.scatter(x[:, 0], x[:, 1], s=40, c=y, cmap=cm_dark, alpha=0.7)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(b=True)plt.tight_layout(pad=2.5)plt.title(u'SVM多分类方法：One/One or One/Other', fontsize=18)plt.show() 输出： 参数选择:linear/rbf,c,gamma1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_irisimport numpy as npfrom sklearn import svmimport matplotlib as mplimport matplotlib.colorsimport matplotlib.pyplot as pltiris = load_iris()X = iris.datay = iris.targetX = X[y != 0, :2]y = y[y != 0]x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)# 分类器clf_param = (('linear', 0.1), ('linear', 0.5), ('linear', 1), ('linear', 2), ('rbf', 1, 0.1), ('rbf', 1, 1), ('rbf', 1, 10), ('rbf', 1, 100), ('rbf', 5, 0.1), ('rbf', 5, 1), ('rbf', 5, 10), ('rbf', 5, 100))x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = mpl.colors.ListedColormap(['#77E0A0', '#FFA0A0'])cm_dark = mpl.colors.ListedColormap(['g', 'r'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(14, 10), facecolor='w')for i, param in enumerate(clf_param): clf = svm.SVC(C=param[1], kernel=param[0]) if param[0] == 'rbf': clf.gamma = param[2] title = u'高斯核，C=%.1f，$\gamma$ =%.1f' % (param[1], param[2]) else: title = u'线性核，C=%.1f' % param[1] clf.fit(X, y) y_hat = clf.predict(X) show_accuracy(y_hat, y) # 准确率 # 画图 print title print '支撑向量的数目：', clf.n_support_ print '支撑向量的系数：', clf.dual_coef_ print '支撑向量：', clf.support_ plt.subplot(3, 4, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=40, cmap=cm_dark) # 样本的显示 plt.scatter(X[clf.support_, 0], X[clf.support_, 1], edgecolors='k', facecolors='none', s=100, marker='o') # 支撑向量 z = clf.decision_function(grid_test) # print 'z = \n', z z = z.reshape(x1.shape) plt.contour(x1, x2, z, colors=list('kmrmk'), linestyles=['--', '-.', '-', '-.', '--'], linewidths=[1, 0.5, 1.5, 0.5, 1], levels=[-1, -0.5, 0, 0.5, 1]) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(title, fontsize=14)plt.suptitle(u'SVM不同参数的分类', fontsize=20)plt.tight_layout(1.4)plt.subplots_adjust(top=0.92)plt.savefig('1.png')plt.show() 输出： sklearn.metrics模型评价指标：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import precision_score, recall_score, f1_score, fbeta_scorefrom sklearn.metrics import precision_recall_fscore_support, classification_reporty_true = np.array([1, 1, 1, 1, 0, 0])y_hat = np.array([1, 0, 1, 1, 1, 1])print 'Accuracy：\t', accuracy_score(y_true, y_hat)# The precision is the ratio 'tp / (tp + fp)' where 'tp' is the number of# true positives and 'fp' the number of false positives. The precision is# intuitively the ability of the classifier not to label as positive a sample# that is negative.# The best value is 1 and the worst value is 0.precision = precision_score(y_true, y_hat)print 'Precision:\t', precision# The recall is the ratio 'tp / (tp + fn)' where 'tp' is the number of# true positives and 'fn' the number of false negatives. The recall is# intuitively the ability of the classifier to find all the positive samples.# The best value is 1 and the worst value is 0.recall = recall_score(y_true, y_hat)print 'Recall: \t', recall# F1 score, also known as balanced F-score or F-measure# The F1 score can be interpreted as a weighted average of the precision and# recall, where an F1 score reaches its best value at 1 and worst score at 0.# The relative contribution of precision and recall to the F1 score are# equal. The formula for the F1 score is:# F1 = 2 * (precision * recall) / (precision + recall)print 'f1 score: \t', f1_score(y_true, y_hat)print 2 * (precision * recall) / (precision + recall)# The F-beta score is the weighted harmonic mean of precision and recall,# reaching its optimal value at 1 and its worst value at 0.# The 'beta' parameter determines the weight of precision in the combined# score. 'beta &lt; 1' lends more weight to precision, while 'beta &gt; 1'# favors recall ('beta -&gt; 0' considers only precision, 'beta -&gt; inf' only recall).print 'F-beta：'for beta in np.logspace(-3, 3, num=7, base=10): fbeta = fbeta_score(y_true, y_hat, beta=beta) print '\tbeta=%9.3f\tF-beta=%.5f' % (beta, fbeta) #print (1+beta**2)*precision*recall / (beta**2 * precision + recall)print precision_recall_fscore_support(y_true, y_hat, beta=1)print classification_report(y_true, y_hat) 输出：123456789101112131415161718192021Accuracy： 0.5Precision: 0.6Recall: 0.75f1 score: 0.6666666666670.666666666667F-beta： beta= 0.001 F-beta=0.60000 beta= 0.010 F-beta=0.60001 beta= 0.100 F-beta=0.60119 beta= 1.000 F-beta=0.66667 beta= 10.000 F-beta=0.74815 beta= 100.000 F-beta=0.74998 beta= 1000.000 F-beta=0.75000(array([ 0. , 0.6]), array([ 0. , 0.75]), array([ 0. , 0.66666667]), array([2, 4], dtype=int64)) precision recall f1-score support 0 0.00 0.00 0.00 2 1 0.60 0.75 0.67 4avg / total 0.40 0.50 0.44 6 样本不均衡问题：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import numpy as npfrom sklearn import svmimport matplotlib.colorsimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_scoreimport warningsdef show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)def show_recall(y, y_hat): # print y_hat[y == 1] print '召回率：%.2f%%' % (100 * float(np.sum(y_hat[y == 1] == 1)) / np.extract(y == 1, y).size)warnings.filterwarnings("ignore") # UndefinedMetricWarningnp.random.seed(0) # 保持每次生成的数据相同c1 = 990c2 = 10N = c1 + c2x_c1 = 3*np.random.randn(c1, 2)x_c2 = 0.5*np.random.randn(c2, 2) + (4, 4)x = np.vstack((x_c1, x_c2))y = np.ones(N)y[:c1] = -1# 显示大小s = np.ones(N) * 30s[:c1] = 10# 分类器clfs = [svm.SVC(C=1, kernel='linear'), svm.SVC(C=1, kernel='linear', class_weight=&#123;-1: 1, 1: 50&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 2&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 10&#125;)]titles = 'Linear', 'Linear, Weight=50', 'RBF, Weight=2', 'RBF, Weight=10'x1_min, x1_max = x[:, 0].min(), x[:, 0].max() # 第0列的范围x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = matplotlib.colors.ListedColormap(['#77E0A0', '#FF8080'])cm_dark = matplotlib.colors.ListedColormap(['g', 'r'])matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 8), facecolor='w')for i, clf in enumerate(clfs): clf.fit(x, y) y_hat = clf.predict(x) # show_accuracy(y_hat, y) # 正确率 # show_recall(y, y_hat) # 召回率 print i+1, '次：' print '正确率：\t', accuracy_score(y, y_hat) print ' 精度 ：\t', precision_score(y, y_hat, pos_label=1) print '召回率：\t', recall_score(y, y_hat, pos_label=1) print 'F1-score：\t', f1_score(y, y_hat, pos_label=1) print # 画图 plt.subplot(2, 2, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k', s=s, cmap=cm_dark) # 样本的显示 plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(titles[i]) plt.grid()plt.suptitle(u'不平衡数据的处理', fontsize=18)plt.tight_layout(1.5)plt.subplots_adjust(top=0.92)plt.show() 输出： 1234567891011121314151617181920211 次：正确率： 0.99 精度 ： 0.0召回率： 0.0F1-score： 0.02 次：正确率： 0.94 精度 ： 0.142857142857召回率： 1.0F1-score： 0.253 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.7692307692314 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.769230769231 SVR：123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npfrom sklearn import svmimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', yprint 'SVR - RBF'svr_rbf = svm.SVR(kernel='rbf', gamma=0.2, C=100)svr_rbf.fit(x, y)print 'SVR - Linear'svr_linear = svm.SVR(kernel='linear', C=100)svr_linear.fit(x, y)print 'SVR - Polynomial'svr_poly = svm.SVR(kernel='poly', degree=3, C=100)svr_poly.fit(x, y)print 'Fit OK.'x_test = np.linspace(x.min(), 1.1*x.max(), 100).reshape(-1, 1)y_rbf = svr_rbf.predict(x_test)y_linear = svr_linear.predict(x_test)y_poly = svr_poly.predict(x_test)plt.figure(figsize=(9, 8), facecolor='w')plt.plot(x_test, y_rbf, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x_test, y_linear, 'g-', linewidth=2, label='Linear Kernel')plt.plot(x_test, y_poly, 'b-', linewidth=2, label='Polynomial Kernel')plt.plot(x, y, 'mo', markersize=6)plt.scatter(x[svr_rbf.support_], y[svr_rbf.support_], s=130, c='r', marker='*', label='RBF Support Vectors')plt.legend(loc='lower left')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.tight_layout(2)plt.show() 输出： SVR GridSearchCV example:1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom sklearn import svmfrom sklearn.model_selection import GridSearchCV # 0.17 grid_searchimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', ymodel = svm.SVR(kernel='rbf')c_can = np.logspace(-2, 2, 10)gamma_can = np.logspace(-2, 2, 10)svr = GridSearchCV(model, param_grid=&#123;'C': c_can, 'gamma': gamma_can&#125;, cv=5)svr.fit(x, y)print '验证参数：\n', svr.best_params_x_test = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)y_hat = svr.predict(x_test)sp = svr.best_estimator_.support_plt.figure(facecolor='w')plt.scatter(x[sp], y[sp], s=120, c='r', marker='*', label='Support Vectors', zorder=3)plt.plot(x_test, y_hat, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x, y, 'go', markersize=5)plt.legend(loc='upper right')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.show() 输出： via SVM的两个参数 C 和 gamma]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>train_test_split</tag>
        <tag>GridSearchCV</tag>
        <tag>svm</tag>
        <tag>SVC</tag>
        <tag>SVR</tag>
        <tag>precision_score</tag>
        <tag>recall_score</tag>
        <tag>f1_score</tag>
        <tag>fbeta_score</tag>
        <tag>precision_recall_fscore_support</tag>
        <tag>classification_report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging]]></title>
    <url>%2F2017%2F11%2F14%2FBagging%2F</url>
    <content type="text"><![CDATA[引子假设一个分类器分类正确的概率为0.6，如果有n个独立的分类器，选择其中的k个采用投票的方式，作为预测结果，则预测正确的概率为： 123456789101112131415161718import operator# operator.__mul__(a, b)# Return a * b, for a and b numbers.def c(n, k): # 求组合数 return reduce(operator.mul, range(n-k+1, n+1)) / reduce(operator.mul, range(1, k+1))def bagging(n, p): s = 0 for i in range(n / 2 + 1, n + 1): s += c(n, i) * p ** i * (1 - p) ** (n - i) return sfor t in range(9, 100, 10): # 假设事件发生的概率为0.6 print t, '次采样正确率：', bagging(t, 0.6) 输出12345678910119 次采样正确率： 0.7334323219 次采样正确率： 0.81390797858529 次采样正确率： 0.86378705133639 次采样正确率： 0.89794136871149 次采样正确率： 0.92242443765259 次采样正确率： 0.94044799573269 次采样正确率： 0.95394975650579 次采样正确率： 0.96418969283989 次采样正确率： 0.97202751600799 次采样正确率： 0.97806955787 BaggingRegressor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn.linear_model import RidgeCVfrom sklearn.ensemble import BaggingRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesdef f(x): return 0.5*np.exp(-(x+3) **2) + np.exp(-x**2) + + 0.5*np.exp(-(x-3) ** 2)np.random.seed(0)N = 200x = np.random.rand(N) * 10 - 5 # [-5,5)x = np.sort(x)y = f(x) + 0.05*np.random.randn(N)x.shape = -1, 1ridge = RidgeCV(alphas=np.logspace(-3, 2, 10), fit_intercept=False)ridged = Pipeline([('poly', PolynomialFeatures(degree=10)), ('Ridge', ridge)])bagging_ridged = BaggingRegressor(ridged, n_estimators=100, max_samples=0.3)dtr = DecisionTreeRegressor(max_depth=5)regs = [ ('DecisionTree Regressor', dtr), ('Ridge Regressor(6 Degree)', ridged), ('Bagging Ridge(6 Degree)', bagging_ridged), ('Bagging DecisionTree Regressor', BaggingRegressor(dtr, n_estimators=100, max_samples=0.3))]x_test = np.linspace(1.1*x.min(), 1.1*x.max(), 1000)mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.plot(x, y, 'ro', label=u'训练数据')plt.plot(x_test, f(x_test), color='k', lw=3.5, label=u'真实值')clrs = 'bmyg'for i, (name, reg) in enumerate(regs): reg.fit(x, y) y_test = reg.predict(x_test.reshape(-1, 1)) plt.plot(x_test, y_test.ravel(), color=clrs[i], lw=i+1, label=name, zorder=6-i)plt.legend(loc='upper left')plt.xlabel('X', fontsize=15)plt.ylabel('Y', fontsize=15)plt.title(u'回归曲线拟合', fontsize=21)plt.ylim((-0.2, 1.2))plt.tight_layout(2)plt.grid(True)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>Bagging</tag>
        <tag>BaggingRegressor</tag>
        <tag>reduce</tag>
        <tag>DecisionTreeRegressor</tag>
        <tag>RidgeCV</tag>
        <tag>PolynomialFeatures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeRegressor]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.tree.DecisionTreeRegressor%2F</url>
    <content type="text"><![CDATA[导入包：1234import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressor 生成数据集：12345678N = 100x = np.random.rand(N) * 6 - 3 # [-3,3)x.sort()y = np.sin(x) + np.random.randn(N) * 0.05print yx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的print x DecisionTreeRegressor：12345678910reg = DecisionTreeRegressor(criterion='mse', max_depth=9)dt = reg.fit(x, y)x_test = np.linspace(-3, 3, 50).reshape(-1, 1)y_hat = dt.predict(x_test)plt.plot(x, y, 'ro', ms=6, label='Actual')plt.plot(x_test, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='upper left')plt.grid()plt.show() 比较决策树的深度影响：123456789101112131415161718depth = [2, 4, 6, 8, 10]clr = 'rgbmy'reg = [DecisionTreeRegressor(criterion='mse', max_depth=depth[0]), DecisionTreeRegressor(criterion='mse', max_depth=depth[1]), DecisionTreeRegressor(criterion='mse', max_depth=depth[2]), DecisionTreeRegressor(criterion='mse', max_depth=depth[3]), DecisionTreeRegressor(criterion='mse', max_depth=depth[4])]plt.plot(x, y, 'k^', linewidth=2, label='Actual')x_test = np.linspace(-3, 3, 50).reshape(-1, 1)for i, r in enumerate(reg): dt = r.fit(x, y) y_hat = dt.predict(x_test) plt.plot(x_test, y_hat, '-', color=clr[i], linewidth=2, label='Depth=%d' % depth[i])plt.legend(loc='upper left')plt.grid()plt.show() 多变量决策树回归：12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressorN = 400x = np.random.rand(N) * 8 - 4 # [-4,4)x.sort()y1 = np.sin(x) + 3 + np.random.randn(N) * 0.1y2 = np.cos(0.3*x) + np.random.randn(N) * 0.01y = np.vstack((y1, y2)).Tx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的deep = 5 # 树的深度reg = DecisionTreeRegressor(criterion='mse', max_depth=deep)dt = reg.fit(x, y)x_test = np.linspace(-4, 4, num=1000).reshape(-1, 1)print x_testy_hat = dt.predict(x_test)print y_hatplt.scatter(y[:, 0], y[:, 1], c='r', s=40, label='Actual')plt.scatter(y_hat[:, 0], y_hat[:, 1], c='g', marker='s', s=100, label='Depth=%d' % deep, alpha=1)plt.legend(loc='upper left')plt.xlabel('y1')plt.ylabel('y2')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>DecisionTreeRegressor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeClassifier]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.tree.DecisionTreeClassifier%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn import treefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pydotplus 导入数据集：1234567891011121314def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]from sklearn.datasets import load_irisiris=load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica' 决策树参数估计：123456# min_samples_split = 10：如果该结点包含的样本数目大于10，则(有可能)对其分支# min_samples_leaf = 10：若将某结点分支后，得到的每个子结点样本数目都大于10，则完成分支；否则，不进行分支model = DecisionTreeClassifier(criterion='entropy', max_depth=6)model = model.fit(x_train, y_train)y_test_hat = model.predict(x_test) # 测试数据 决策树保存1234567891011121314# 1、输出with open('iris.dot', 'w') as f: tree.export_graphviz(model, out_file=f)# 2、给定文件名# tree.export_graphviz(model, out_file='iris.dot')# 3、输出为pdf格式dot_data = tree.export_graphviz(model, out_file=None, feature_names=iris_feature_E, class_names=iris_class, filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf('iris.pdf')f = open('iris.png', 'wb')f.write(graph.create_png())f.close() 决策树画图1234567891011121314151617181920212223242526272829303132mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = FalseN, M = 50, 50 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_show = np.stack((x1.flat, x2.flat), axis=1) # 测试点print x_show.shapecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_show_hat = model.predict(x_show) # 预测值print y_show_hat.shapeprint y_show_haty_show_hat = y_show_hat.reshape(x1.shape) # 使之与输入的形状相同print y_show_hatplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light) # 预测值的显示print y_testprint y_test.ravel()plt.scatter(x_test[:, 0], x_test[:, 1], c=y_test.ravel(), edgecolors='k', s=120, cmap=cm_dark, marker='*') # 测试数据plt.scatter(x[:, 0], x[:, 1], c=y.ravel(), edgecolors='k', s=40, cmap=cm_dark) # 全部数据plt.xlabel(iris_feature[0], fontsize=15)plt.ylabel(iris_feature[1], fontsize=15)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid(True)plt.title(u'鸢尾花数据的决策树分类', fontsize=17)plt.show() 决策树树状图 决策分割面 预测1234567y_test = y_test.reshape(-1)print y_test_hatprint y_testresult = (y_test_hat == y_test) # True则预测正确，False则预测错误acc = np.mean(result)print '准确度: %.2f%%' % (100 * acc) 过拟合：错误率12345678910111213141516171819depth = np.arange(1, 15)err_list = []for d in depth: clf = DecisionTreeClassifier(criterion='entropy', max_depth=d) clf = clf.fit(x_train, y_train) y_test_hat = clf.predict(x_test) # 测试数据 result = (y_test_hat == y_test) # True则预测正确，False则预测错误 err = 1 - np.mean(result) err_list.append(err) # print d, ' 准确度: %.2f%%' % (100 * err) print d, ' 错误率: %.2f%%' % (100 * err)plt.figure(facecolor='w')plt.plot(depth, err_list, 'ro-', lw=2)plt.xlabel(u'决策树深度', fontsize=15)plt.ylabel(u'错误率', fontsize=15)plt.title(u'决策树深度与过拟合', fontsize=17)plt.grid(True)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>train_test_split</tag>
        <tag>DecisionTreeClassifier</tag>
        <tag>pydotplus</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插值]]></title>
    <url>%2F2017%2F11%2F13%2F%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[插值的定义插值法又称“内插法”，是利用函数f(x)在某区间中已知的若干点的函数值，作出适当的特定函数，在区间的其他点上用这特定函数的值作为函数f(x)的近似值，这种方法称为插值法。如果这特定函数是多项式，就称它为插值多项式。 插入法的拉丁文原意是“内部插入”，即在已知的函数表中，插入一些表中没有列出的、所需要的中间值。 若函数f(x)在自变数x一些离散值所对应的函数值为已知，则可以作一个适当的特定函数p(x)，使得p(x)在这些离散值所取的函数值，就是f(x)的已知值。从而可以用p(x)来估计f(x)在这些离散值之间的自变数所对应的函数值，这种方法称为插值法。 scipy的插值模块 Lagrange插值Lagrange插值是n次多项式插值，其成功地用构造插值基函数的 方法解决了求n次多项式插值函数问题。 ★基本思想 将待求的n次多项式插值函数pn(x）改写成另一种表示方式，再利用插值条件⑴确定其中的待定函数，从而求出插值多项式。 Newton插值Newton插值也是n次多项式插值，它提出另一种构造插值多项式的方法，与Lagrange插值相比，具有承袭性和易于变动节点的特点。 ★基本思想 将待求的n次插值多项式Pn（x）改写为具有承袭性的形式，然后利用插值条件⑴确定Pn（x）的待定系数，以求出所要的插值函数。 Hermite插值Hermite插值是利用未知函数f(x)在插值节点上的函数值及导数值来构造插值多项式的，其提法为：给定n+1个互异的节点x0,x1，……,xn上的函数值和导数值求一个2n+1次多项式H2n+1(x)满足插值条件 H2n+1(xk)=yk H’2n+1(xk)=y’k k=0,1,2，……，n ⒀ 如上求出的H2n+1(x）称为2n+1次Hermite插值函数，它与被插函数一般有更好的密合度. ★基本思想 利用Lagrange插值函数的构造方法，先设定函数形式，再利用插值条件⒀求出插值函数. 插值举例1234567891011121314151617181920212223242526272829303132#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poissonfrom scipy.interpolate import BarycentricInterpolatorfrom scipy.interpolate import CubicSpline x = np.random.poisson(lam=5, size=10000) print x pillar = 15 a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) plt.grid() # plt.show() print a print a[0].sum() rv = poisson(5) x1 = a[1] y1 = rv.pmf(x1) itp = BarycentricInterpolator(x1, y1) # 重心插值 x2 = np.linspace(x.min(), x.max(), 50) y2 = itp(x2) cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 plt.legend(loc='upper right') plt.grid() plt.show() 拟合与插值的区别插值曲线要过数据点，拟合曲线整体效果更好。 插值是指已知某函数的在若干离散点上的函数值或者导数信息，通过求解该函数中待定形式的插值函数以及待定系数，使得该函数在给定离散点上满足约束。 拟合是指已知某函数的若干离散函数值，通过调整该函数中若干待定系数，使得该函数与已知点集的差别（最小二乘意义）最小。如果待定函数是线性，就叫线性拟合或线性回归，否则叫非线性拟合或非线性回归。表达式也可以是分段函数，这种情况下叫样条拟合。 从几何意义上讲，拟合是给定了空间中的一些点，找到一个已知形式未知参数的连续曲面来最大限度地逼近这些点；而插值是找到一个（或几个分片光滑的）连续曲面来穿过这些点。 随着插值节点的增多，多项式次数也在增高，插值曲线在一些区域出现跳跃，并且越来越偏离原始曲线。 为了解决这个问题，人们发明了分段插值法。分段插值一般不会使用四次以上的多项式，而二次多项式会出现尖点，也是有问题的。所以就剩下线性和三次插值，最后使用最多的还是线性分段插值，这个好处是显而易见的。 via 百度百科-插值法 scipy.interpolate 知乎-拟合与插值的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>插值</tag>
        <tag>重心插值</tag>
        <tag>三次样条插值</tag>
        <tag>Hermite插值</tag>
        <tag>临近点插值</tag>
        <tag>线性插值</tag>
        <tag>Newton插值</tag>
        <tag>Lagrange插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticNetCV]]></title>
    <url>%2F2017%2F11%2F13%2FElasticNetCV%2F</url>
    <content type="text"><![CDATA[导入包：12345678910import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import ElasticNetCVimport sklearn.datasetsfrom sklearn.preprocessing import PolynomialFeatures, StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import mean_squared_error 导入数据集：1234567891011def not_empty(s): return s != ''data = sklearn.datasets.load_boston()x = np.array(data.data)y = np.array(data.target)print x.shapeprint y.shapex_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=0) 初始化模型Pipeline：1234567model = Pipeline([ ('ss', StandardScaler()), ('poly', PolynomialFeatures(degree=3, include_bias=True)), ('linear', ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.99, 1], alphas=np.logspace(-3, 2, 5), fit_intercept=False, max_iter=1e3, cv=3))]) 模型训练：123456model.fit(x_train, y_train.ravel())linear = model.get_params('linear')['linear']print u'超参数：', linear.alpha_print u'L1 ratio：', linear.l1_ratio_print u'系数：', linear.coef_.ravel() 模型预测：123456y_pred = model.predict(x_test)r2 = model.score(x_test, y_test)mse = mean_squared_error(y_test, y_pred)print 'R2:', r2print u'均方误差：', mse 模型效果图形化展示：12345678910111213t = np.arange(len(y_pred))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.plot(t, y_test.ravel(), 'r-', lw=2, label=u'真实值')plt.plot(t, y_pred, 'g-', lw=2, label=u'估计值')plt.legend(loc='best')plt.title(u'波士顿房价预测', fontsize=18)plt.xlabel(u'样本编号', fontsize=15)plt.ylabel(u'房屋价格', fontsize=15)plt.grid()plt.show() 输出1234567(506L, 13L)(506L,)超参数： 0.316227766017L1 ratio： 0.99R2: 0.772203419261均方误差： 18.9675965682]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>PolynomialFeatures</tag>
        <tag>linear_model</tag>
        <tag>ElasticNetCV</tag>
        <tag>ElasticNet</tag>
        <tag>StandardScaler</tag>
        <tag>train_test_split</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model%2F</url>
    <content type="text"><![CDATA[导入数据集12345678910from sklearn import datasetsfrom sklearn.model_selection import train_test_splitboston = datasets.load_boston()X = boston.datay = boston.target"""划分数据集"""x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1) GridSearchCV寻参123456789lr = linear_model.LinearRegression()model = Lasso()# model = Ridge()alpha_can = np.logspace(-3, 2, 10)lasso_model = GridSearchCV(model, param_grid=&#123;'alpha': alpha_can&#125;, cv=5)lasso_model.fit(x_train, y_train)print '超参数：\n', lasso_model.best_params_ 预测12y_hat = lasso_model.predict(np.array(x_test)) 模型评价1234mse = np.average((y_hat - np.array(y_test)) ** 2) # Mean Squared Errorrmse = np.sqrt(mse) # Root Mean Squared Errorprint mse, rmse 或者用scikit-learn计算MSE/RMSE1234from sklearn import metricsprint "MSE:",metrics.mean_squared_error(y_test, y_hat)print "RMSE:",np.sqrt(metrics.mean_squared_error(y_test, y_hat)) cross_val_predict123456789boston = datasets.load_boston()X = boston.datay = boston.targetfrom sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(model, X, y, cv=10)print "MSE:",metrics.mean_squared_error(y, predicted)print "RMSE:",np.sqrt(metrics.mean_squared_error(y, predicted)) 输出图形展示这里画图真实值和预测值的变化关系，离中间的直线y=x直线越近的点代表预测损失越低。 12345678910t = np.arange(len(x_test))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.plot(t, y_test, 'r-', linewidth=2, label=u'真实数据')plt.plot(t, y_hat, 'g-', linewidth=2, label=u'预测数据')plt.title(u'线性回归预测销量', fontsize=18)plt.legend(loc='upper right')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>scipy</tag>
        <tag>GridSearchCV</tag>
        <tag>MSE</tag>
        <tag>RMSE</tag>
        <tag>cross_val_predict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scipy.optimize使用]]></title>
    <url>%2F2017%2F11%2F13%2Fscipy.optimize%2F</url>
    <content type="text"><![CDATA[定义损失函数123456789101112131415161718192021222324252627#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npfrom scipy.optimize import leastsqimport matplotlib.pyplot as pltdef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return y 利用scipy.optimize求解线性回归1123456789101112131415x = np.linspace(-2, 2, 50)A, B, C = 2, 3, -1y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75t = leastsq(residual, [0, 0, 0], args=(x, y))theta = t[0]print '真实值：', A, B, Cprint '预测值：', thetay_hat = theta[0] * x ** 2 + theta[1] * x + theta[2]plt.plot(x, y, 'r-', linewidth=2, label=u'Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict')plt.legend(loc='upper left')plt.grid()plt.show() 利用scipy.optimize求解线性回归1输出图例 利用scipy.optimize求解非线性回归12345678910111213141516x = np.linspace(0, 5, 100)A = 5w = 1.5y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5t = leastsq(residual2, [3, 1], args=(x, y))theta = t[0]print '真实值：', A, wprint '预测值：', thetay_hat = theta[0] * np.sin(theta[1] * x)plt.plot(x, y, 'r-', linewidth=2, label='Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='lower left')plt.grid()plt.show() 利用scipy.optimize求解非线性回归输出图例 使用scipy计算函数极值1234567a = opt.fmin(f, 1)b = opt.fmin_cg(f, 1)c = opt.fmin_bfgs(f, 1)print a, 1/a, math.eprint bprint c 使用scipy计算函数极值输出123456789101112131415161718Optimization terminated successfully. Current function value: 0.692201 Iterations: 16 Function evaluations: 32Optimization terminated successfully. Current function value: 0.692201 Iterations: 4 Function evaluations: 30 Gradient evaluations: 10Optimization terminated successfully. Current function value: 0.692201 Iterations: 5 Function evaluations: 24 Gradient evaluations: 8[ 0.36787109] [ 2.71834351] 2.71828182846[ 0.36787948][ 0.36787942]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimize</tag>
        <tag>scipy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model中的Ridge、Lasso、ElasticNet回归]]></title>
    <url>%2F2017%2F11%2F13%2FRidge_Lasso_ElasticNet%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCVfrom sklearn.preprocessing import PolynomialFeaturesimport matplotlib.pyplot as pltfrom sklearn.pipeline import Pipelineimport matplotlib as mplimport warnings 定义模型评价函数：123456789def xss(y, y_hat): y = y.ravel() y_hat = y_hat.ravel() tss = np.var(y) rss = np.average((y_hat - y) ** 2) r2 = 1 - rss / tss corr_coef = np.corrcoef(y, y_hat)[0, 1] return r2, corr_coef 生成训练数据12345678910warnings.filterwarnings("ignore") # ConvergenceWarningnp.random.seed(0)np.set_printoptions(linewidth=1000)N = 9x = np.linspace(0, 6, N) + np.random.randn(N)x = np.sort(x)y = x**2 - 4*x - 3 + np.random.randn(N)x.shape = -1, 1y.shape = -1, 1 模型设置1234567891011121314models = [Pipeline([('poly', PolynomialFeatures()), ('linear', LinearRegression(fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', RidgeCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', LassoCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', ElasticNetCV(alphas=np.logspace(-3, 2, 50), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], fit_intercept=False))])]mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsenp.set_printoptions(suppress=True) 模型训练与图形展示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465plt.figure(figsize=(16, 10), facecolor='w')d_pool = np.arange(1, N, 1) # 阶m = d_pool.sizeclrs = [] # 颜色for c in np.linspace(16711680, 255, m): clrs.append('#%06x' % c)line_width = np.linspace(5, 2, m)titles = u'LinearRegression', u'Ridge', u'LASSO', u'ElasticNet'tss_list = []rss_list = []ess_list = []ess_rss_list = []for t in range(4): model = models[t] plt.subplot(2, 2, t+1) plt.plot(x, y, 'ro', ms=10, zorder=N) for i, d in enumerate(d_pool): model.set_params(poly__degree=d) model.fit(x, y.ravel()) lin = model.get_params('linear')['linear'] output = u'%s：%d阶，系数为：' % (titles[t], d) if hasattr(lin, 'alpha_'): idx = output.find(u'系数') output = output[:idx] + (u'alpha=%.6f，' % lin.alpha_) + output[idx:] if hasattr(lin, 'l1_ratio_'): # 根据交叉验证结果，从输入l1_ratio(list)中选择的最优l1_ratio_(float) idx = output.find(u'系数') output = output[:idx] + (u'l1_ratio=%.6f，' % lin.l1_ratio_) + output[idx:] print output, lin.coef_.ravel() x_hat = np.linspace(x.min(), x.max(), num=100) x_hat.shape = -1, 1 y_hat = model.predict(x_hat) s = model.score(x, y) r2, corr_coef = xss(y, model.predict(x)) # print 'R2和相关系数：', r2, corr_coef # print 'R2：', s, '\n' z = N - 1 if (d == 2) else 0 label = u'%d阶，$R^2$=%.3f' % (d, s) if hasattr(lin, 'l1_ratio_'): label += u'，L1 ratio=%.2f' % lin.l1_ratio_ plt.plot(x_hat, y_hat, color=clrs[i], lw=line_width[i], alpha=0.75, label=label, zorder=z) plt.legend(loc='upper left') plt.grid(True) plt.title(titles[t], fontsize=18) plt.xlabel('X', fontsize=16) plt.ylabel('Y', fontsize=16)plt.tight_layout(1, rect=(0, 0, 1, 0.95))plt.suptitle(u'多项式曲线拟合比较', fontsize=22)plt.show()y_max = max(max(tss_list), max(ess_rss_list)) * 1.05plt.figure(figsize=(9, 7), facecolor='w')t = np.arange(len(tss_list))plt.plot(t, tss_list, 'ro-', lw=2, label=u'TSS(Total Sum of Squares)')plt.plot(t, ess_list, 'mo-', lw=1, label=u'ESS(Explained Sum of Squares)')plt.plot(t, rss_list, 'bo-', lw=1, label=u'RSS(Residual Sum of Squares)')plt.plot(t, ess_rss_list, 'go-', lw=2, label=u'ESS+RSS')plt.ylim((0, y_max))plt.legend(loc='center right')plt.xlabel(u'LinearRegression/Ridge/LASSO/Elastic Net', fontsize=15)plt.ylabel(u'XSS值', fontsize=15)plt.title(u'总平方和TSS=？', fontsize=18)plt.grid(True)plt.show() 输出如下： 123456789101112131415161718192021222324252627282930313233线性回归：1阶，系数为： [-12.12113792 3.05477422]线性回归：2阶，系数为： [-3.23812184 -3.36390661 0.90493645]线性回归：3阶，系数为： [-3.90207326 -2.61163034 0.66422328 0.02290431]线性回归：4阶，系数为： [-8.20599769 4.20778207 -2.85304163 0.73902338 -0.05008557]线性回归：5阶，系数为： [ 21.59733285 -54.12232017 38.43116219 -12.68651476 1.98134176 -0.11572371]线性回归：6阶，系数为： [ 14.73304785 -37.87317494 23.67462342 -6.07037979 0.42536833 0.06803132 -0.00859246]线性回归：7阶，系数为： [ 314.30344622 -827.89446924 857.33293186 -465.46543638 144.21883851 -25.67294678 2.44658612 -0.09675941]线性回归：8阶，系数为： [-1189.50149198 3643.69109456 -4647.92941149 3217.22814712 -1325.87384337 334.32869072 -50.57119119 4.21251817 -0.148521 ]Ridge回归：1阶，alpha=0.109854，系数为： [-11.21592213 2.85121516]Ridge回归：2阶，alpha=0.138950，系数为： [-2.90423989 -3.49931368 0.91803171]Ridge回归：3阶，alpha=0.068665，系数为： [-3.47165245 -2.85078293 0.69245987 0.02314415]Ridge回归：4阶，alpha=0.222300，系数为： [-2.84560266 -1.99887417 -0.40628792 0.33863868 -0.02674442]Ridge回归：5阶，alpha=1.151395，系数为： [-1.68160373 -1.52726943 -0.8382036 0.2329258 0.03934251 -0.00663323]Ridge回归：6阶，alpha=0.001000，系数为： [ 0.53724068 -6.00552086 -3.75961826 5.64559118 -2.21569695 0.36872911 -0.02221343]Ridge回归：7阶，alpha=0.033932，系数为： [-2.38021238 -2.26383055 -1.47715232 0.00763115 1.12242917 -0.52769633 0.09199201 -0.00560199]Ridge回归：8阶，alpha=0.138950，系数为： [-2.19299093 -1.91896884 -1.21608489 -0.19314178 0.49300277 0.05452898 -0.09690455 0.02114435 -0.00140196]LASSO：1阶，alpha=0.222300，系数为： [-10.41556797 2.66199326]LASSO：2阶，alpha=0.001000，系数为： [-3.29932625 -3.31989869 0.89878903]LASSO：3阶，alpha=0.013257，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]LASSO：4阶，alpha=0.002560，系数为： [-5.08513199 -1.41147772 0.3380565 0.0440427 0.00099807]LASSO：5阶，alpha=0.042919，系数为： [-4.11853758 -1.8643949 0.2618319 0.07954732 0.00257481 -0.00069093]LASSO：6阶，alpha=0.001000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]LASSO：7阶，alpha=0.001000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]LASSO：8阶，alpha=0.001000，系数为： [-4.62623251 -1.37717809 0.17183854 0.04307765 0.00629505 0.00069171 0.0000355 -0.00000875 -0.00000386]ElasticNet：1阶，alpha=0.021210，l1_ratio=0.100000，系数为： [-10.74762959 2.74580662]ElasticNet：2阶，alpha=0.013257，l1_ratio=0.100000，系数为： [-2.95099269 -3.48472703 0.91705013]ElasticNet：3阶，alpha=0.013257，l1_ratio=1.000000，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]ElasticNet：4阶，alpha=0.010481，l1_ratio=0.950000，系数为： [-4.8799192 -1.5317438 0.3452403 0.04825571 0.00049763]ElasticNet：5阶，alpha=0.004095，l1_ratio=0.100000，系数为： [-4.07916291 -2.18606287 0.44650232 0.05102669 0.00239164 -0.00048279]ElasticNet：6阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]ElasticNet：7阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]ElasticNet：8阶，alpha=0.001000，l1_ratio=0.500000，系数为： [-4.53761647 -1.45230301 0.18829714 0.0427561 0.00619739 0.00068209 0.00003506 -0.00000869 -0.00000384]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>ElasticNet</tag>
        <tag>Pipeline</tag>
        <tag>Ridge</tag>
        <tag>Lasso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model.LogisticRegression]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model.LogisticRegression%2F</url>
    <content type="text"><![CDATA[导入数据集12345from sklearn import datasetsiris = datasets.load_iris()X = iris.data[:, :2]y = iris.target 数据处理与模型拟合Pipeline1234567891011121314import numpy as npfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegressionlr = Pipeline([('sc', StandardScaler()), ('clf', LogisticRegression()) ])lr.fit(X, y.ravel())y_hat = lr.predict(X)y_hat_prob = lr.predict_proba(X)np.set_printoptions(suppress=True)print 'y_hat = \n', y_hatprint 'y_hat_prob = \n', y_hat_probprint u'准确度：%.2f%%' % (100*np.mean(y_hat == y.ravel())) 绘图：分割面123456789101112131415161718192021222324252627import matplotlib.pyplot as pltimport matplotlib as mplN, M = 500, 500 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#77E0A0', '#FF8080', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_hat = lr.predict(x_test) # 预测值y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同plt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值的显示plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本的显示plt.xlabel(u'花萼长度', fontsize=14)plt.ylabel(u'花萼宽度', fontsize=14)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid()plt.title(u'鸢尾花Logistic回归分类效果 - 标准化', fontsize=17)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>Pipeline</tag>
        <tag>LogisticRegression</tag>
        <tag>分割面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读取文件数据的几种方式]]></title>
    <url>%2F2017%2F11%2F13%2Fpython%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[方式一：f = file(path)1234567891011121314151617f = file(path)x = []y = []for i, d in enumerate(f): if i == 0: continue d = d.strip() if not d: continue d = map(float, d.split(',')) x.append(d[1:-1]) y.append(d[-1])pprint(x)pprint(y)x = np.array(x)y = np.array(y) 方式二：Python自带库csv12345678import csv f = file(path, 'rb') print f d = csv.reader(f) for line in d: print line f.close() 方式三：Python自带库numpy123import numpy as npp = np.loadtxt(path, delimiter=',', skiprows=1) 方式四：Python自带库pandas123import pandas as pddata = pd.read_csv(path)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>file</tag>
        <tag>csv</tag>
        <tag>loadtxt</tag>
        <tag>read_csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型]]></title>
    <url>%2F2017%2F11%2F12%2F%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[引子 Beta分布 Beta分布的期望 Beta分布图形 先验分布-共轭分布共轭先验分布的定义 二项分布的最大似然估计 二项分布与先验举例 上述过程的理论解释 先验概率和后验概率的关系 伪计数 共轭先验的直接推广 Beta分布-Dirichlet分布Dirichlet分布 Dirichlet分布的期望 Dirichlet分布分析 对称Dirichlet分布 对称Dirichlet分布的参数分析 参数a对Dirichlet分布的影响 参数选择对对称Dirichlet分布的影响 多项分布的共轭分布是Dirichlet分布 三层贝叶斯网络模型LDALDA的解释 参数的学习 似然概率 Gibbs采样和更新规则Gibbs Sampling 联合分布 计算因子 Gibbs updating rule 词分布和主题分布 LDA总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Beta分布</tag>
        <tag>先验分布</tag>
        <tag>共轭先验分布</tag>
        <tag>二项分布</tag>
        <tag>Dirichlet分布</tag>
        <tag>多项分布</tag>
        <tag>Gibbs采样</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM]]></title>
    <url>%2F2017%2F11%2F12%2FHMM%2F</url>
    <content type="text"><![CDATA[HMM定义 隐马尔科夫模型的贝叶斯网络 HMM的参数HMM的确定 HMM的参数 HMM的参数总结 HMM的两个基本性质 HMM举例 示例的各个参数 示例的思考 HMM的3个基本问题 概率计算问题 直接计算法 直接计算法分析 借鉴算法的优化思想 前向算法定义：前向概率-后向概率 前向算法定义 前向算法过程 后向算法后向算法定义 后向算法过程 后向算法的说明 前后向关系 单个状态的概率 r的意义 两个状态的联合概率 期望 学习算法 大数定理 监督学习方法 Baum-Welch算法 EM过程 极大化 初始状态概率 转移概率和观测概率 预测算法近似算法 算法：走棋盘/格子取数 Viterbi算法 Viterbi算法举例 总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>Baum-Welch</tag>
        <tag>Viterbi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯网络]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯 贝叶斯网络的表达贝叶斯网络定义 一个简单的贝叶斯网络 全连接贝叶斯网络 一个“正常”的贝叶斯网络 对一个实际贝叶斯网络的分析 贝叶斯网络的形式化定义 马尔科夫模型一个特殊的贝叶斯网络。 D-separation条件独立的三种类型通过贝叶斯网络判定条件独立1 通过贝叶斯网络判定条件独立2 通过贝叶斯网络判定条件独立3 举例说明这三种情况 将上述结点推广到结点集 有向分离的举例 Markov Blanket再次分析链式网络 HMM 贝叶斯网络的用途分类预测 转移概率矩阵 贝叶斯网络的构建 混合（离散+连续）网络 孩子结点是连续的 孩子结点是离散的，父节点是连续的 孩子结点是离散的，父节点是连续的 贝叶斯网络的推导 无向环 原贝叶斯网络的近似树结构 将两图的相对熵转换成变量的互信息 Chou-Liu算法 最大权生成树MSWT的建立过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>朴素贝叶斯</tag>
        <tag>马尔科夫</tag>
        <tag>D-separation</tag>
        <tag>Markov Blanket</tag>
        <tag>Chou-Liu</tag>
        <tag>MSWT</tag>
        <tag>转移概率矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PLSA]]></title>
    <url>%2F2017%2F11%2F12%2FPLSA%2F</url>
    <content type="text"><![CDATA[PLSA模型PLSA模型概述 PLSA模型过程 最大似然估计 目标函数分析 求隐含变量主题zk的后验概率 分析似然函数期望 完成目标函数的建立 目标函数的求解 分析第一等式 同理分析第二等式 PLSA总结 PLSA进一步思考]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
        <tag>PLSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM]]></title>
    <url>%2F2017%2F11%2F12%2FEM%2F</url>
    <content type="text"><![CDATA[引子k-Means算法 对K-Means的思考 Jensen不等式：若f是凸函数 最大似然估计 二项分布的最大似然估计 进一步考察 按照MLE的过程分析 化简对数似然函数 参数估计的结论 符合直观想象 问题：随机变量无法直接（完全）观察到 从直观理解猜测GMM的参数估计 建立目标函数 第一步：估算数据来自哪个组份 第二步：估计每个组份的参数 EM算法EM算法的提出 通过最大似然估计建立目标函数 问题的提出 Jensen不等式 寻找尽量紧的下界 进一步分析 EM算法整体框架 坐标上升 从理论公式推导GMM E-step M-step 对均值求偏导 高斯分布的均值 高斯分布的方差：求偏导，等于0 多项分布的参数 拉格朗日乘子法 求偏导，等于0 结论]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一些概念聚类的定义聚类就是对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。 相似度/距离计算方法总结 Hellinger distance 余弦相似度与Pearson相似系数 聚类的基本思想 聚类的衡量指标 ARI AMI 轮廓系数(Silhouette) k-Meansk-Means算法 对k-Means的思考 k-Means的公式化解释 如果使用其他相似度/距离度量 Mini-batch k-Means算法描述 k-Means聚类总结 CanopyCanopy算法 层次聚类方法 AGNES和DIANA算法 密度聚类方法 DBSCAN算法 DBSCAN算法的若干概念 DBSCAN算法流程 密度最大值聚类 高局部密度点距离 簇中心的识别 密度最大值聚类过程 边界和噪声的重新认识 Affinity Propagation算法概述基本概念 Exemplar范例：即聚类族中心点； s(i,j)：数据点i与数据点j的相似度值，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力； 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵； Preference参考度或称为偏好参数：是相似度矩阵中横轴纵轴索引相同的点，如s(i,i)，若按欧氏距离计算其值应为0，但在AP聚类中其表示数据点i作为聚类中心的程度，因此不能为0。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； Responsibility，r(i,k)：吸引度信息，表示数据点k适合作为数据点i的聚类中心的程度；公式如下： 其中a(i,k’)表示除k外其他点对i点的归属度值，初始为0；s(i,k’)表示除k外其他点对i的吸引度，即i外其他点都在争夺i点的 所有权；r(i,k)表示数据点k成为数据点i的聚类中心的累积证明，r(i,k)值大于0，则表示数据点k成为聚类中心的能力强。说明：此时只考虑哪个点k成为点i的聚类中心的可能性最大，但是没考虑这个吸引度最大的k是否也经常成为其他点的聚类中心（即归属度），若点k只是点i的聚类中心，不是其他任何点的聚类中心，则会造成最终聚类中心个数大于实际的中心个数。 Availability，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的合适程度，公式如下： 其中r(i’,k)表示点k作为除i外其他点的聚类中心的相似度值，取所有大于等于0的吸引度值，加上k作为聚类中心的可能程。即点k在这些吸引度值大于0的数据点的支持下，数据点i选择k作为其聚类中心的累积证明。 Damping factor阻尼系数：为防止数据震荡，引入地衰减系数，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得1-λ倍，其中λ在0-1之间，默认为0.5。 算法流程 更新相似度矩阵中每个点的吸引度信息，计算归属度信息； 更新归属度信息，计算吸引度信息； 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 s(i,k)就相当于i对选k这个人的一个固有的偏好程度 r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） a(i,k)：从公式里可以看到，所有r(i’,k)&gt;0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)&gt;0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。 r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。 优点 不需要制定最终聚类族的个数 已有的数据点作为最终的聚类中心，而不是新生成一个族中心。 模型对数据的初始值不敏感。 对初始相似度矩阵数据的对称性没有要求。 相比与k-centers聚类方法，其结果的平方差误差较小。缺点 虽然AP算法不用提前设置聚类中心的个数，但是需要事先设置参考度，而参考度的大小与聚类中心的个数正相关； 由于AP算法每次迭代都需要更新每个数据点的吸引度值和归属度值，算法复杂度较高，在大数据量下运行时间较长。 谱和谱聚类 谱分析的整体过程 一些概念 相似度图G的建立方法 权值比较 拉普拉斯矩阵的定义 拉普拉斯矩阵及其性质 谱聚类算法：未正则拉普拉斯矩阵 谱聚类算法：随机游走拉普拉斯矩阵 谱聚类算法：对称拉普拉斯矩阵 进一步思考 随机游走和拉普拉斯矩阵的关系 标签传递算法 via Affinity Propagation: AP聚类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>k-Means</tag>
        <tag>Canopy</tag>
        <tag>AGNES</tag>
        <tag>DIANA</tag>
        <tag>DBSCAN</tag>
        <tag>AP</tag>
        <tag>谱聚类</tag>
        <tag>层次聚类</tag>
        <tag>密度最大值聚类</tag>
        <tag>拉普拉斯</tag>
        <tag>边界</tag>
        <tag>噪声</tag>
        <tag>标签传递</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2017%2F11%2F12%2FSVM%2F</url>
    <content type="text"><![CDATA[支持向量机SVM的原理和目标线性可分支持向量机硬间隔最大化hard margin maximization;硬间隔支持向量机 分割超平面 分割超平面的思考 线性支持向量机软间隔最大化soft margin maximization;软间隔支持向量机 线性分类问题 输入数据 线性可分支持向量机 非线性支持向量机核函数kernel function 支持向量机的计算过程和算法步骤推导目标函数 最大间隔分离超平面 函数间隔和几何间隔 建立目标函数 线性可分支持向量机学习算法 线性SVM的目标函数 带松弛因子的SVM拉格朗日函数 代入目标函数 最终的目标函数 线性支持向量机 线性支持向量机学习算法 损失函数分析 核函数 高斯核 SVM中系数的求解：SMO SMO:序列最小最优化 二变量优化问题 SMO的迭代公式 退出条件 SVM总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2017%2F11%2F12%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[Boosting的思想 AdaBoost算法 AdaBoost误差上限 前向分步算法 前向分步算法的含义 前向分步算法的算法框架 前向分步算法与AdaBoost 证明 基分类器 权值的计算 分类错误率 权值的更新 权值和错误率的关键解释 AdaBoost总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>AdaBoost</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2017%2F11%2F12%2FXGBoost%2F</url>
    <content type="text"><![CDATA[考虑使用二阶导信息 决策树的描述 正则项的定义 目标函数计算 构造决策树的结构 XGBoost小结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升GBDT]]></title>
    <url>%2F2017%2F11%2F11%2F%E6%8F%90%E5%8D%87GBDT%2F</url>
    <content type="text"><![CDATA[由决策树和随机森林的关系的思考 随机森林的决策树分别采样建立，相对独立。 思考： 假定当前已经得到了m-1棵决策树，是否可以通过现有样本和决策树的信息，对第m棵决策树的建立产生有益的影响呢？ 各个决策树组成随机森林后，最后的投票过程可否在建立决策树时即确定呢？ 提升的概念 提升是一个机器学习技术，可以用于回归和分类问题，它每一步产生一个弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升（Gradient Boosting） 梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合（基函数）；提升算法通过迭代的选择一个负梯度方向上的基函数来逐渐逼近局部极小值。这种在函数域的梯度提升观点对机器学习的很多领域有深刻影响。 提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。 提升算法 提升算法推导 提升算法 梯度提升决策树GBDT 参数设置和正则化 衰减因子、降采样 GBDT总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2017%2F11%2F11%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[BootstrapingBootstraping的名称来自成语“pull up by your own bootstraps”,意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法。本意是通过拉靴子让自己上升，不可能发生的事情。后来意思发生了转变，隐喻不需要外界帮助，仅依靠自身力量让自己变得更好。 Bagging的策略 bootstrap aggregation 从样本集中重采样（有重复的）选出n个样本 在所有属性上，对这n个样本建立分类器（ID3，C4.5，CART,SVM,Logistic回归等） 重复以上两部m次，即获得了m个分类器 将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类 Another description of Bagging 随机森林随机森林在Bagging基础上做了修改： 从样本集中用Bootstrap采样选出n个样本； 从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树； 重复以上两步m次，即建立了m棵CART决策树 这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类 随机森林/Bagging和决策树的关系 当然可以使用决策树作为基本分类器 但也可以使用SVM、Logistic回归等其他分类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 投票机制简单投票机制 一票否决（一致表决） 少数服从多数 有效多数（加权） 阈值表决贝叶斯投票机制一种可能的方案 样本不均衡的常用处理方法 使用RF建立计算样本间相似度 使用RF计算特征重要度 使用RF生成特征 总结 决策树、随机森林的代码清洗，逻辑简单，在胜任分类问题的同时，往往也可以作为对数据分布探索的首要尝试算法。 随机森林的集成思想也可以用在其他分类器的设计中。 如果通过随机森林做样本的异常值检测 统计样本间位于相同决策树的叶节点的个数，形成样本相似度矩阵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>随机森林</tag>
        <tag>RF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树的概念决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。决策树学习是以实例为基础的归纳学习。决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。 决策树学习的生成算法建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法： ID3(Iterative Dichotomiser) C4.5 CART(Classification And Regression Tree) 信息增益概念当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。 信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。 定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差，即：g(D,A)=H(D)-H(D/A)，显然，这即为训练数据集D和特征A的互信息。 基本记号： 信息增益的计算方法 经验条件熵H(D/A) 其他目标 关于Gini系数的讨论 Gini系数的第二定义决策树中的Gini系数和社会学上的Gini系数并不相等。 三种决策树学习算法的比较 ID3:使用信息增益/互信息g(D,A)进行特征选择 取值多的属性，更容易使数据更纯，其信息增益更大 训练得到的是一棵庞大且深度浅的树：不合理。 C4.5：信息增益率gr(D,A)=g(D,A)/H(A) CART：基尼指数一个属性的信息增益（率）/Gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强。 决策树的评价 决策树的过拟合决策树对训练属于有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。 剪枝 随机森林 剪枝三种决策树的剪枝过程算法相同，区别仅是对于当前树的评价标准不同。 信息增益、信息增益率、基尼系数剪枝总体思路： 由完全树T0开始，剪枝部分结点得到T1，再次剪枝部分结点得到T2…直到仅剩树根的树Tk 在验证数据集上对这k个树分别评价，选择损失函数最小的树Ti 剪枝系数的确定 剪枝算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2017%2F11%2F11%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic回归分类问题的首选算法; 线性回归：最大似然估计+高斯分布； Logistic回归：最大似然估计+伯努利分布； Logistic回归是事件发生几率取对数下的线性回归。高斯分布和伯努利分布都是属于指数族分布，所以说逻辑回归是广义线性模型GLM回归。 Logistic/Sigmoid函数 Logistic回归参数估计 对数似然函数 参数的迭代 Logistic回归的损失函数A： ### Logistic回归的损失函数B： 广义线性模型GLM 多分类：Softmax回归Softmax名字由来 ## Softmax回归 信息熵定义信息量 熵熵是随机变量不确定性的度量，不确定性越大，熵值越大。若随机变量退化成定值，熵最小，为0；若随机分布为均匀分布，熵最大。 熵的定义 熵的公式推导 均匀分布的信息熵 联合熵、条件熵、相对熵联合熵、条件熵 推导条件熵的定义公式 相对熵、KL散度 互信息互信息的定义 互信息与条件熵的关系 公式]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>Logistic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归与特征选择]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[线性回归可以对样本是非线性的，只要对参数线性。 高斯分布在做特征选择的时候，可以看回归的残差图，真正理想状态下，经过模型回归后的残差应该是服从正态分布的，而不是均匀的随机分布；此时可以画个单变量与残差的关系图，直观的看一下是不是有什么明显的一阶、高阶趋势在里面，则需要进一步特征加工。 大数定理指的是事件发生的概率在N取无穷大的时候趋近于事件发生的概率。频率的极限是概率。 使用极大似然估计解释最小二乘 最大似然估计MLE最大似然函数的假设似然函数假设每个样本出现的概率是独立的，因此整体出现的概率就是各个样本出现的概率的累计。 似然函数 高斯的对数似然与最小二乘误差服从高斯分布的情况下的对数似然函数： 解析式的求解过程 最小二乘法的本质误差服从高斯分布的情况下的对数似然函数最大化的过程等价于最小二乘。 最小二乘意义下的参数最优解 加入lamda扰动后 线性回归的复杂度惩罚因子惩罚因子的直观理解，阶数越高，模型越复杂，系数振荡的越厉害，系数的取值也越大，同时呢，系数又会有正有负，所以直接用系数的平方作为模型复杂度的惩罚因子。 Ridge岭回归 是加了L2正则项的最小二乘。LASSO 是L1正则。 Ridge岭回归与LASSO：两个都差不多，单看模型系数LASSO要稍稍比Ridge稳定一些，但如果给定模型的评价指标的化，Ridge稍微比LASSO好。LASSO可以具有特征选择的功能。 正则项与防止过拟合 广义逆矩阵（伪逆） 梯度下降算法 梯度方向 批量梯度下降算法 随机梯度下降算法 mini-batch如果不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。 模型评价TSS &gt;= ESS + RSSR方的取值范围：(-inf,1] 局部加权回归 局部加权线性回归 权值的设置]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础]]></title>
    <url>%2F2017%2F11%2F08%2FPython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[intro 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npimport matplotlib as mplfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmimport timefrom scipy.optimize import leastsqfrom scipy import statsimport scipy.optimize as optimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poisson# from scipy.interpolate import BarycentricInterpolator# from scipy.interpolate import CubicSplineimport math# import seaborndef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return yif __name__ == "__main__": # # 开场白： # numpy是非常好用的数据包，如：可以这样得到这个二维数组 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # 正式开始 -:) # 标准Python的列表(list)中，元素本质是对象。 # 如：L = [1, 2, 3]，需要3个指针和三个整数对象，对于数值运算比较浪费内存和CPU。 # 因此，Numpy提供了ndarray(N-dimensional array object)对象：存储单一数据类型的多维数组。 # # 1.使用array创建 # 通过array函数传递list对象 # L = [1, 2, 3, 4, 5, 6] # print "L = ", L # a = np.array(L) # print "a = ", a # print type(a) # # # 若传递的是多层嵌套的list，将创建多维数组 # b = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) # print b # # # # # # # 数组大小可以通过其shape属性获得 # print a.shape # print b.shape # # # # # # 也可以强制修改shape # b.shape = 4, 3 # print b # # # # 注：从(3,4)改为(4,3)并不是对数组进行转置，而只是改变每个轴的大小，数组元素在内存中的位置并没有改变 # # # # # # 当某个轴为-1时，将根据数组元素的个数自动计算此轴的长度 # b.shape = 2, -1 # print b # print b.shape # # # # b.shape = 3, 4 # # # 使用reshape方法，可以创建改变了尺寸的新数组，原数组的shape保持不变 # c = b.reshape((4, -1)) # print "b = \n", b # print 'c = \n', c # # # # # 数组b和c共享内存，修改任意一个将影响另外一个 # b[0][1] = 20 # print "b = \n", b # print "c = \n", c # # # # # # 数组的元素类型可以通过dtype属性获得 # print a.dtype # print b.dtype # # # # # # # 可以通过dtype参数在创建时指定元素类型 # d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.float) # f = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.complex) # print d # print f # # # # # # 如果更改元素类型，可以使用astype安全的转换 # f = d.astype(np.int) # print f # # # # # 但不要强制仅修改元素类型，如下面这句，将会以int来解释单精度float类型 # d.dtype = np.int # print d # # 2.使用函数创建 # 如果生成一定规则的数据，可以使用NumPy提供的专门函数 # arange函数类似于python的range函数：指定起始值、终止值和步长来创建数组 # 和Python的range类似，arange同样不包括终值；但arange可以生成浮点类型，而range只能是整数类型 # a = np.arange(1, 10, 0.5) # print a # # # # # # linspace函数通过指定起始值、终止值和元素个数来创建数组，缺省包括终止值 # b = np.linspace(1, 10, 10) # print 'b = ', b # # # # # 可以通过endpoint关键字指定是否包括终值 # c = np.linspace(1, 10, 10, endpoint=False) # print 'c = ', c # # # # # 和linspace类似，logspace可以创建等比数列 # # 下面函数创建起始值为10^1，终止值为10^2，有20个数的等比数列 # d = np.logspace(1, 2, 10, endpoint=True) # print d # # # # # # 下面创建起始值为2^0，终止值为2^10(包括)，有10个数的等比数列 # f = np.logspace(0, 10, 11, endpoint=True, base=2) # print f # # # # # # 使用 frombuffer, fromstring, fromfile等函数可以从字节序列创建数组 # s = 'abcd' # g = np.fromstring(s, dtype=np.int8) # print g # # # 3.存取 # 3.1常规办法：数组元素的存取方法和Python的标准方法相同 # a = np.arange(10) # print a # # # 获取某个元素 # print a[3] # # # # 切片[3,6)，左闭右开 # print a[3:6] # # # # 省略开始下标，表示从0开始 # print a[:5] # # # # 下标为负表示从后向前数 # print a[3:] # # # # 步长为2 # print a[1:9:2] # # # # 步长为-1，即翻转 # print a[::-1] # # # # 切片数据是原数组的一个视图，与原数组共享内容空间，可以直接修改元素值 # a[1:4] = 10, 20, 30 # print a # # # # 因此，在实践中，切实注意原始数据是否被破坏，如： # b = a[2:5] # b[0] = 200 # print a # # # 3.2 整数/布尔数组存取 # # 3.2.1 # 根据整数数组存取：当使用整数序列对数组元素进行存取时， # 将使用整数序列中的每个元素作为下标，整数序列可以是列表(list)或者数组(ndarray)。 # 使用整数序列作为下标获得的数组不和原始数组共享数据空间。 # a = np.logspace(0, 9, 10, base=2) # print a # i = np.arange(0, 10, 2) # print i # # # # 利用i取a中的元素 # b = a[i] # print b # # # b的元素更改，a中元素不受影响 # b[2] = 1.6 # print b # print a # # 3.2.2 # 使用布尔数组i作为下标存取数组a中的元素：返回数组a中所有在数组b中对应下标为True的元素 # # 生成10个满足[0,1)中均匀分布的随机数 # a = np.random.rand(10) # print a # # # 大于0.5的元素索引 # print a &gt; 0.5 # # # # 大于0.5的元素 # b = a[a &gt; 0.5] # print b # # # # 将原数组中大于0.5的元素截取成0.5 # a[a &gt; 0.5] = 0.5 # print a # # # # b不受影响 # print b # 3.3 二维数组的切片 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10) # 行向量 # print 'a = ', a # b = a.reshape((-1, 1)) # 转换成列向量 # print b # c = np.arange(6) # print c # f = b + c # 行 + 列 # print f # # 合并上述代码： # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # # 二维数组的切片 # print a[[0, 1, 2], [2 ,3, 4]] # print a[4, [2, 3, 4]] # print a[4:, [2, 3, 4]] # i = np.array([True, False, True, False, False, True]) # print a[i] # print a[i, 3] # # 4.1 numpy与Python数学库的时间比较 # for j in np.logspace(0, 7, 10): # j = int(j) # x = np.linspace(0, 10, j) # start = time.clock() # y = np.sin(x) # t1 = time.clock() - start # # x = x.tolist() # start = time.clock() # for i, t in enumerate(x): # x[i] = math.sin(t) # t2 = time.clock() - start # print j, ": ", t1, t2, t2/t1 # 4.2 元素去重 # 4.2.1直接使用库函数 # a = np.array((1, 2, 3, 4, 5, 5, 7, 3, 2, 2, 8, 8)) # print '原始数组：', a # # 使用库函数unique # b = np.unique(a) # print '去重后：', b # # 4.2.2 二维数组的去重，结果会是预期的么？ # c = np.array(((1, 2), (3, 4), (5, 6), (1, 3), (3, 4), (7, 6))) # print '二维数组', c # print '去重后：', np.unique(c) # # 4.2.3 方案1：转换为虚数 # # r, i = np.split(c, (1, ), axis=1) # # x = r + i * 1j # x = c[:, 0] + c[:, 1] * 1j # print '转换成虚数：', x # print '虚数去重后：', np.unique(x) # print np.unique(x, return_index=True) # 思考return_index的意义 # idx = np.unique(x, return_index=True)[1] # print '二维数组去重：\n', c[idx] # # 4.2.3 方案2：利用set # print '去重方案2：\n', np.array(list(set([tuple(t) for t in c]))) # # 4.3 stack and axis # a = np.arange(1, 10).reshape((3, 3)) # b = np.arange(11, 20).reshape((3, 3)) # c = np.arange(101, 110).reshape((3, 3)) # print 'a = \n', a # print 'b = \n', b # print 'c = \n', c # print 'axis = 0 \n', np.stack((a, b, c), axis=0) # print 'axis = 1 \n', np.stack((a, b, c), axis=1) # print 'axis = 2 \n', np.stack((a, b, c), axis=2) # 5.绘图 # 5.1 绘制正态分布概率密度函数 # mu = 0 # sigma = 1 # x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 50) # y = np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (math.sqrt(2 * math.pi) * sigma) # print x.shape # print 'x = \n', x # print y.shape # print 'y = \n', y # # plt.plot(x, y, 'ro-', linewidth=2) # plt.plot(x, y, 'r-', x, y, 'go', linewidth=2, markersize=8) # plt.grid(True) # plt.show() mpl.rcParams['font.sans-serif'] = [u'SimHei'] #FangSong/黑体 FangSong/KaiTi mpl.rcParams['axes.unicode_minus'] = False # # 5.2 损失函数：Logistic损失(-1,1)/SVM Hinge损失/ 0/1损失 # x = np.array(np.linspace(start=-2, stop=3, num=1001, dtype=np.float)) # y_logit = np.log(1 + np.exp(-x)) / math.log(2) # y_boost = np.exp(-x) # y_01 = x &lt; 0 # y_hinge = 1.0 - x # y_hinge[y_hinge &lt; 0] = 0 # plt.plot(x, y_logit, 'r-', label='Logistic Loss', linewidth=2) # plt.plot(x, y_01, 'g-', label='0/1 Loss', linewidth=2) # plt.plot(x, y_hinge, 'b-', label='Hinge Loss', linewidth=2) # plt.plot(x, y_boost, 'm--', label='Adaboost Loss', linewidth=2) # plt.grid() # plt.legend(loc='upper right') # # plt.savefig('1.png') # plt.show() # # 5.3 x^x # x = np.linspace(-1.3, 1.3, 101) # y = f(x) # plt.plot(x, y, 'g-', label='x^x', linewidth=2) # plt.grid() # plt.legend(loc='upper left') # plt.show() # # 5.4 胸型线 # x = np.arange(1, 0, -0.001) # y = (-3 * x * np.log(x) + np.exp(-(40 * (x - 1 / np.e)) ** 4) / 25) / 2 # plt.figure(figsize=(5,7)) # plt.plot(y, x, 'r-', linewidth=2) # plt.grid(True) # plt.show() # 5.5 心形线 # t = np.linspace(0, 7, 100) # x = 16 * np.sin(t) ** 3 # y = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid(True) # plt.show() # # 5.6 渐开线 # t = np.linspace(0, 50, num=1000) # x = t*np.sin(t) + np.cos(t) # y = np.sin(t) - t*np.cos(t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid() # plt.show() # # Bar # mpl.rcParams['font.sans-serif'] = [u'SimHei'] #黑体 FangSong/KaiTi # mpl.rcParams['axes.unicode_minus'] = False # x = np.arange(0, 10, 0.1) # y = np.sin(x) # plt.bar(x, y, width=0.04, linewidth=0.2) # plt.plot(x, y, 'r--', linewidth=2) # plt.title(u'Sin曲线') # plt.xticks(rotation=-60) # plt.xlabel('X') # plt.ylabel('Y') # plt.grid() # plt.show() # # 6. 概率分布 # # 6.1 均匀分布 # x = np.random.rand(10000) # t = np.arange(len(x)) # plt.hist(x, 30, color='m', alpha=0.5, label=u'均匀分布') # # plt.plot(t, x, 'r-', label=u'均匀分布') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 6.2 验证中心极限定理 # t = 1000 # a = np.zeros(10000) # for i in range(t): # a += np.random.uniform(-5, 5, 10000) # a /= t # plt.hist(a, bins=30, color='g', alpha=0.5, normed=True, label=u'均匀分布叠加') # plt.legend(loc='upper left') # plt.grid() # plt.show() # 6.21 其他分布的中心极限定理 # lamda = 10 # p = stats.poisson(lamda) # y = p.rvs(size=1000) # mx = 30 # r = (0, mx) # bins = r[1] - r[0] # plt.figure(figsize=(10, 8), facecolor='w') # plt.subplot(121) # plt.hist(y, bins=bins, range=r, color='g', alpha=0.8, normed=True) # t = np.arange(0, mx+1) # plt.plot(t, p.pmf(t), 'ro-', lw=2) # plt.grid(True) # # N = 1000 # M = 10000 # plt.subplot(122) # a = np.zeros(M, dtype=np.float) # p = stats.poisson(lamda) # for i in np.arange(N): # y = p.rvs(size=M) # a += y # a /= N # plt.hist(a, bins=20, color='g', alpha=0.8, normed=True) # plt.grid(b=True) # plt.show() # # 6.3 Poisson分布 # x = np.random.poisson(lam=5, size=10000) # print x # pillar = 15 # a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) # plt.grid() # # plt.show() # print a # print a[0].sum() # # 6.4 直方图的使用 # mu = 2 # sigma = 3 # data = mu + sigma * np.random.randn(1000) # h = plt.hist(data, 30, normed=1, color='#a0a0ff') # x = h[1] # y = norm.pdf(x, loc=mu, scale=sigma) # plt.plot(x, y, 'r--', x, y, 'ro', linewidth=2, markersize=4) # plt.grid() # plt.show() # # 6.5 插值 # rv = poisson(5) # x1 = a[1] # y1 = rv.pmf(x1) # itp = BarycentricInterpolator(x1, y1) # 重心插值 # x2 = np.linspace(x.min(), x.max(), 50) # y2 = itp(x2) # cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 # plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 # plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 # plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 # plt.legend(loc='upper right') # plt.grid() # plt.show() # 7. 绘制三维图像 # x, y = np.ogrid[-3:3:100j, -3:3:100j] # # u = np.linspace(-3, 3, 101) # # x, y = np.meshgrid(u, u) # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # fig = plt.figure() # ax = fig.add_subplot(111, projection='3d') # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.coolwarm, linewidth=0.1) # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.Accent, linewidth=0.5) # plt.show() # # cmaps = [('Perceptually Uniform Sequential', # # ['viridis', 'inferno', 'plasma', 'magma']), # # ('Sequential', ['Blues', 'BuGn', 'BuPu', # # 'GnBu', 'Greens', 'Greys', 'Oranges', 'OrRd', # # 'PuBu', 'PuBuGn', 'PuRd', 'Purples', 'RdPu', # # 'Reds', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd']), # # ('Sequential (2)', ['afmhot', 'autumn', 'bone', 'cool', # # 'copper', 'gist_heat', 'gray', 'hot', # # 'pink', 'spring', 'summer', 'winter']), # # ('Diverging', ['BrBG', 'bwr', 'coolwarm', 'PiYG', 'PRGn', 'PuOr', # # 'RdBu', 'RdGy', 'RdYlBu', 'RdYlGn', 'Spectral', # # 'seismic']), # # ('Qualitative', ['Accent', 'Dark2', 'Paired', 'Pastel1', # # 'Pastel2', 'Set1', 'Set2', 'Set3']), # # ('Miscellaneous', ['gist_earth', 'terrain', 'ocean', 'gist_stern', # # 'brg', 'CMRmap', 'cubehelix', # # 'gnuplot', 'gnuplot2', 'gist_ncar', # # 'nipy_spectral', 'jet', 'rainbow', # # 'gist_rainbow', 'hsv', 'flag', 'prism'])] # 8.1 scipy # 线性回归例1 # x = np.linspace(-2, 2, 50) # A, B, C = 2, 3, -1 # y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75 # # t = leastsq(residual, [0, 0, 0], args=(x, y)) # theta = t[0] # print '真实值：', A, B, C # print '预测值：', theta # y_hat = theta[0] * x ** 2 + theta[1] * x + theta[2] # plt.plot(x, y, 'r-', linewidth=2, label=u'Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 线性回归例2 # x = np.linspace(0, 5, 100) # A = 5 # w = 1.5 # y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5 # # t = leastsq(residual2, [3, 1], args=(x, y)) # theta = t[0] # print '真实值：', A, w # print '预测值：', theta # y_hat = theta[0] * np.sin(theta[1] * x) # plt.plot(x, y, 'r-', linewidth=2, label='Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict') # plt.legend(loc='lower left') # plt.grid() # plt.show() # # 8.2 使用scipy计算函数极值 # a = opt.fmin(f, 1) # b = opt.fmin_cg(f, 1) # c = opt.fmin_bfgs(f, 1) # print a, 1/a, math.e # print b # print c # marker description # ”.” point # ”,” pixel # “o” circle # “v” triangle_down # “^” triangle_up # “&lt;” triangle_left # “&gt;” triangle_right # “1” tri_down # “2” tri_up # “3” tri_left # “4” tri_right # “8” octagon # “s” square # “p” pentagon # “*” star # “h” hexagon1 # “H” hexagon2 # “+” plus # “x” x # “D” diamond # “d” thin_diamond # “|” vline # “_” hline # TICKLEFT tickleft # TICKRIGHT tickright # TICKUP tickup # TICKDOWN tickdown # CARETLEFT caretleft # CARETRIGHT caretright # CARETUP caretup # CARETDOWN caretdown calc_e123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_e_small(x): n = 10 f = np.arange(1, n+1).cumprod() b = np.array([x]*n).cumprod() return np.sum(b / f) + 1def calc_e(x): reverse = False if x &lt; 0: # 处理负数 x = -x reverse = True ln2 = 0.69314718055994530941723212145818 c = x / ln2 a = int(c+0.5) b = x - a*ln2 y = (2 ** a) * calc_e_small(b) if reverse: return 1/y return yif __name__ == "__main__": t1 = np.linspace(-2, 0, 10, endpoint=False) t2 = np.linspace(0, 3, 20) t = np.concatenate((t1, t2)) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_e(x) print 'e^', x, ' = ', y[i], '(近似值)\t', math.exp(x), '(真实值)' # print '误差：', y[i] - math.exp(x) plt.figure(facecolor='w') mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 指数函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('exp(X)', fontsize=15) plt.grid(True) plt.show() calc_sin123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_sin_small(x): x2 = -x ** 2 t = x f = 1 sum = 0 for i in range(10): sum += t / f t *= x2 f *= ((2*i+2)*(2*i+3)) return sumdef calc_sin(x): a = x / (2*np.pi) k = np.floor(a) a = x - k*2*np.pi return calc_sin_small(a)if __name__ == "__main__": t = np.linspace(-2*np.pi, 2*np.pi, 100, endpoint=False) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_sin(x) print 'sin(', x, ') = ', y[i], '(近似值)\t', math.sin(x), '(真实值)' # print '误差：', y[i] - math.exp(x) mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.figure(facecolor='w') plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 正弦函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('sin(X)', fontsize=15) plt.xlim((-7, 7)) plt.ylim((-1.1, 1.1)) plt.grid(True) plt.show() class_intro123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-class People: def __init__(self, n, a, s): self.name = n self.age = a self.__score = s self.print_people() # self.__print_people() # 私有函数的作用 def print_people(self): str = u'%s的年龄：%d，成绩为：%.2f' % (self.name, self.age, self.__score) print str __print_people = print_peopleclass Student(People): def __init__(self, n, a, w): People.__init__(self, n, a, w) self.name = 'Student ' + self.name def print_people(self): str = u'%s的年龄：%d' % (self.name, self.age) print strdef func(p): p.age = 11if __name__ == '__main__': p = People('Tom', 10, 3.14159) func(p) # p传入的是引用类型 p.print_people() print # 注意分析下面语句的打印结果，是否觉得有些“怪异”？ j = Student('Jerry', 12, 2.71828) print # 成员函数 p.print_people() j.print_people() print People.print_people(p) People.print_people(j) stat12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport mathimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmdef calc_statistics(x): n = x.shape[0] # 样本个数 # 手动计算 m = 0 m2 = 0 m3 = 0 m4 = 0 for t in x: m += t m2 += t*t m3 += t**3 m4 += t**4 m /= n m2 /= n m3 /= n m4 /= n mu = m sigma = np.sqrt(m2 - mu*mu) skew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3 kurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3 print '手动计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 使用系统函数验证 mu = np.mean(x, axis=0) sigma = np.std(x, axis=0) skew = stats.skew(x) kurtosis = stats.kurtosis(x) return mu, sigma, skew, kurtosisif __name__ == '__main__': d = np.random.randn(100000) print d mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 一维直方图 mpl.rcParams[u'font.sans-serif'] = 'SimHei' mpl.rcParams[u'axes.unicode_minus'] = False y1, x1, dummy = plt.hist(d, bins=50, normed=True, color='g', alpha=0.75) t = np.arange(x1.min(), x1.max(), 0.05) y = np.exp(-t**2 / 2) / math.sqrt(2*math.pi) plt.plot(t, y, 'r-', lw=2) plt.title(u'高斯分布，样本个数：%d' % d.shape[0]) plt.grid(True) plt.show() d = np.random.randn(100000, 2) mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 二维图像 N = 30 density, edges = np.histogramdd(d, bins=[N, N]) print '样本总数：', np.sum(density) density /= density.max() x = y = np.arange(N) t = np.meshgrid(x, y) fig = plt.figure(facecolor='w') ax = fig.add_subplot(111, projection='3d') ax.scatter(t[0], t[1], density, c='r', s=15*density, marker='o', depthshade=True) ax.plot_surface(t[0], t[1], density, cmap=cm.Accent, rstride=2, cstride=2, alpha=0.9, lw=0.75) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.title(u'二元高斯分布，样本个数：%d' % d.shape[0], fontsize=20) plt.tight_layout(0.1) plt.show() MultiGuass12345678910111213141516171819202122232425262728293031#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': x1, x2 = np.mgrid[-5:5:51j, -5:5:51j] x = np.stack((x1, x2), axis=2) plt.figure(figsize=(9, 8), facecolor='w') sigma = (np.identity(2), np.diag((3,3)), np.diag((2,5)), np.array(((2,1), (2,5)))) for i in np.arange(4): ax = plt.subplot(2, 2, i+1, projection='3d') norm = stats.multivariate_normal((0, 0), sigma[i]) y = norm.pdf(x) ax.plot_surface(x1, x2, y, cmap=cm.Accent, rstride=1, cstride=1, alpha=0.9, lw=0.3) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.suptitle(u'二元高斯分布方差比较', fontsize=18) plt.tight_layout(1.5) plt.show() gamma12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom scipy.special import gammafrom scipy.special import factorialmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': N = 5 x = np.linspace(0, N, 50) y = gamma(x+1) plt.figure(facecolor='w') plt.plot(x, y, 'r-', x, y, 'm*', lw=2) z = np.arange(0, N+1) f = factorial(z, exact=True) # 阶乘 print f plt.plot(z, f, 'go', markersize=8) plt.grid(b=True) plt.xlim(-0.1,N+0.1) plt.ylim(0.5, np.max(y)*1.05) plt.xlabel(u'X', fontsize=15) plt.ylabel(u'Gamma(X) - 阶乘', fontsize=15) plt.title(u'阶乘和Gamma函数', fontsize=16) plt.show() Benford123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom time import timefrom scipy.special import factorialimport mathmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def top1(number, a): number /= a while number &gt;= 10: number /= 10 a *= 10 return number, adef top2(number, N2): while number &gt;= N2: number /= 10 n = number while number &gt;= 10: number /= 10 return n, numberdef top3(number): number -= int(number) return int(10 ** number)def top4(number): number -= int(number) frequency[int(10 ** number) - 1] += 1if __name__ == '__main__': N = 100000 x = range(1, N+1) frequency = np.zeros(9, dtype=np.int) f = 1 print '开始计算...' t0 = time() # top1 # a = 1 # for t in x: # f *= t # i, a = top1(f, a) # # print t, i, f, a # frequency[i-1] += 1 # top2 # N2 = N ** 3 # for t in x: # f *= t # f, i = top2(f, N2) # frequency[i-1] += 1 # Top 3：实现1 # f = 0 # for t in x: # f += math.log10(t) # frequency[top3(f) - 1] += 1 # Top 3：实现2 # y = np.cumsum(np.log10(x)) # for t in y: # frequency[top3(t) - 1] += 1 # Top 4：本质与Top3相同 y = np.cumsum(np.log10(x)) map(top4, y) t1 = time() print '耗时：', t1 - t0 print frequency t = np.arange(1, 10) plt.plot(t, frequency, 'r-', t, frequency, 'go', lw=2, markersize=8) for x,y in enumerate(frequency): plt.text(x+1.1, y, frequency[x], verticalalignment='top', fontsize=15) plt.title(u'%d!首位数字出现频率' % N, fontsize=18) plt.xlim(0.5, 9.5) plt.ylim(0, max(frequency)*1.03) plt.grid(b=True) plt.show() # 使用numpy # N = 170 # x = np.arange(1, N+1) # f = np.zeros(9, dtype=np.int) # t1 = time() # y = factorial(x, exact=False) # z = map(top, y) # t2 = time() # print '耗时 = \t', t2 - t1 # for t in z: # f[t-1] += 1 # print f Pearson123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltimport warningsmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def calc_pearson(x, y): std1 = np.std(x) # np.sqrt(np.mean(x**2) - np.mean(x)**2) std2 = np.std(y) cov = np.cov(x, y, bias=True)[0,1] return cov / (std1 * std2)def intro(): N = 10 x = np.random.rand(N) y = 2 * x + np.random.randn(N) * 0.1 print x print y print '系统计算：', stats.pearsonr(x, y)[0] print '手动计算：', calc_pearson(x, y)def rotate(x, y, theta=45): data = np.vstack((x, y)) # print data mu = np.mean(data, axis=1) mu = mu.reshape((-1, 1)) # print mu data -= mu # print data theta *= (np.pi / 180) c = np.cos(theta) s = np.sin(theta) m = np.array(((c, -s), (s, c))) return m.dot(data) + mudef pearson(x, y, tip): clrs = list('rgbmycrgbmycrgbmycrgbmyc') plt.figure(figsize=(10, 8), facecolor='w') for i, theta in enumerate(np.linspace(0, 90, 6)): xr, yr = rotate(x, y, theta) p = stats.pearsonr(xr, yr)[0] # print calc_pearson(xr, yr) print '旋转角度：', theta, 'Pearson相关系数：', p str = u'相关系数：%.3f' % p plt.scatter(xr, yr, s=40, alpha=0.9, linewidths=0.5, c=clrs[i], marker='o', label=str) plt.legend(loc='upper left', shadow=True) plt.xlabel(u'X') plt.ylabel(u'Y') plt.title(u'Pearson相关系数与数据分布：%s' % tip, fontsize=18) plt.grid(b=True) plt.show()if __name__ == '__main__': # warnings.filterwarnings(action='ignore', category=RuntimeWarning) np.random.seed(0) # intro() N = 1000 # tip = u'一次函数关系' # x = np.random.rand(N) # y = np.zeros(N) + np.random.randn(N)*0.001 # tip = u'二次函数关系' # x = np.random.rand(N) # y = x ** 2 #+ np.random.randn(N)*0.002 # tip = u'正切关系' # x = np.random.rand(N) * 1.4 # y = np.tan(x) # tip = u'二次函数关系' # x = np.linspace(-1, 1, 101) # y = x ** 2 tip = u'椭圆' x, y = np.random.rand(2, N) * 60 - 30 y /= 5 idx = (x**2 / 900 + y**2 / 36 &lt; 1) x = x[idx] y = y[idx] pearson(x, y, tip) candle123456789101112131415161718192021222324252627282930#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom matplotlib.finance import candlestick_ohlcif __name__ == "__main__": mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False np.set_printoptions(suppress=True, linewidth=100, edgeitems=5) data = np.loadtxt('7.SH600000.txt', dtype=np.float, delimiter='\t', skiprows=2, usecols=(1, 2, 3, 4)) data = data[:30] N = len(data) t = np.arange(1, N+1).reshape((-1, 1)) data = np.hstack((t, data)) fig, ax = plt.subplots(facecolor='w') fig.subplots_adjust(bottom=0.2) candlestick_ohlc(ax, data, width=0.6, colorup='r', colordown='g', alpha=0.9) plt.xlim((0, N+1)) plt.grid(b=True) plt.title(u'股票K线图', fontsize=18) plt.tight_layout(2) plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化]]></title>
    <url>%2F2017%2F11%2F08%2F%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[凸集基本概念凸集保凸运算凸集和凸函数凸函数图像的上方区域，一定是凸集；一个函数图像的上方区域为凸集，则该函数是凸函数。 仿射集（Affine Set）定义：通过集合C中任意两个不同点的直线仍然在集合C内，则称集合C为仿射集。 仿射集的例子：直线、平面、超平面 凸集集合C内任意两点间的线段均在集合C内，则称集合C为凸集。 两个点和k个点的表达版本内涵是一样的。 仿射集和凸集的关系因为仿射集的条件比凸集的条件强，所以，仿射集必然是凸集。 凸包集合C的所有点的凸组合形成的集合，叫做集合c的凸包。集合C的凸包是能够包含C的最小的凸集。 超平面和半空间 保持凸性的运算 集合交运算 仿射变换 函数f=ax+b的形式，称函数是仿射的，即线性函数加常数的形式。 透视变换 透视函数对向量进行伸缩（规范化），使得最后一维的分量为1并舍弃之。 凸集的透视变换仍然是凸集。 投射变换（线性分式变换） 分割超平面分割超平面的定义 分割超平面的构造两个集合的距离，定义为两个集合间元素的最短距离。 做集合A和集合B最短线段的垂直平分线。 支撑超平面 凸函数基本概念一般化定义 一阶可微 二阶可微 凸函数举例 Jensen不等式：若f是凸函数 凸函数保凸运算保持函数凸性的算子 共轭函数共轭函数的定义 共轭函数的理解 共轭函数的求解举例 凸优化一般提法对偶函数 强对偶KKT条件强对偶条件 Karush-Kuhn-Tucker(KKT)条件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode 调试适配器进程已意外终止]]></title>
    <url>%2F2017%2F10%2F24%2Fvscode%20%E8%B0%83%E8%AF%95%E9%80%82%E9%85%8D%E5%99%A8%E8%BF%9B%E7%A8%8B%E5%B7%B2%E6%84%8F%E5%A4%96%E7%BB%88%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[隔了N天以后，重新打开，一进来就重新下载C依赖，然后提示reload。然后莫名的就可以用了。我晕。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李德荃关于偏度与峰度的讲解]]></title>
    <url>%2F2017%2F10%2F24%2F%E6%9D%8E%E5%BE%B7%E8%8D%83%E5%85%B3%E4%BA%8E%E5%81%8F%E5%BA%A6%E4%B8%8E%E5%B3%B0%E5%BA%A6%E7%9A%84%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[偏度这一指标，又称偏斜系数、偏态系数，是用来帮助判断数据序列的分布规律性的指标。 在数据序列呈对称分布（正态分布）的状态下，其均值、中位数和众数重合。且在这三个数的两侧，其它所有的数据完全以对称的方式左右分布。 如果数据序列的分布不对称，则均值、中位数和众数必定分处不同的位置。这时，若以均值为参照点，则要么位于均值左侧的数据较多，称之为右偏；要么位于均值右侧的数据较多，称之为左偏；除此无它。考虑到所有数据与均值之间的离差之和应为零这一约束，则当均值左侧数据较多的时候，均值的右侧必定存在数值较大的“离群”数据；同理，当均值右侧数据较多的时候，均值的左侧必定存在数值较小的“离群”数据。 一般将偏度定义为三阶中心矩与标准差的三次幂之比。 在上述定义下，偏度系数的取值无非三种情景： 1.当数据序列呈正态分布的时候，由于均值两侧的数据完全对称分布，其三阶中心矩必定为零，于是满足正态分布的数据序列的偏度系数必定等于零。 2.当数据序列非对称分布的时候，如果均值的左侧数据较多，则其右侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取正值。因此，当数据的分布呈右偏的时候，其偏度系数将大于零。 3.当数据序列非对称分布的时候，如果均值的右侧数据较多，则其左侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取负值。因此，当数据的分布呈左偏的时候，偏度系数将小于零。在右偏的分布中，由于大部分数据都在均值的左侧，且均值的右侧存在“离群”数据，这就使得分布曲线的右侧出现一个长长的拖尾；而在左偏的分布中，由于大部分数据都在均值的右侧，且均值的左侧存在“离群”数据，从而造成分布曲线的左侧出现一个长长的拖尾。 可见，在偏度系数的绝对值较大的时候，最有可能的含义是“离群”数据离群的程度很高（很大或很小），亦即分布曲线某侧的拖尾很长。 但“拖尾很长”与“分布曲线很偏斜”不完全等价。例如，也不能排除在数据较少的那一侧，只是多数数据的离差相对于另一侧较大，但不存在明显“离群”数据的情景。所以，为准确判断分布函数的偏斜程度，最好的办法是直接观察分布曲线的几何图形。 与偏度（系数）一样，峰度（系数）也是一个用于评价数据系列分布特征的指标。根据这两个指标，我们可以判断数据系列的分布是否满足正态性，进而评价平均数指标的使用价值。一般地，对于一个偏态分布、肥尾分布特征很明显的数据序列来说，平均数这个指标极易令人误解数据序列分布的集中位置及其集中程度，故此使用起来要极其谨慎。 峰度（系数）等于数据序列的四阶中心矩与标准差的四次幂之比。设若先将数据标准化，则峰度（系数）相当于标准化数据序列的四阶中心矩。 显然，一个数据距离均值越远，其对四阶中心矩计算结果的影响越大。是故，峰度（系数）是一个用于衡量离群数据离群度的指标。峰度（系数）越大，说明该数据系列中的极端值越多。这在数据序列的分布曲线图中来看，体现为存在明显的“肥尾”。当然，峰度（系数）较大也可能说明离群数据取值的极端性很严重，或者各数据距离均值的距离普遍较远。可见，峰度（系数）的大小到底能说明什么问题，最好还是看图确定。 根据Jensen不等式，可以确定出峰度（系数）的取值范围：它的下限不会低于1，上限不会高于数据的个数。 有一些典型分布的峰度（系数）值得特别关注。例如，正态分布的峰度（系数）为常数3，均匀分布的峰度（系数）为常数1.6。在统计实践中，我们经常把这两个典型的分布曲线作为评价样本数据序列分布性态的参照。 在金融学中，峰度这个指标具有一定的意义。一项金融资产，设若其预期收益率的峰度较高，则说明该项资产的预期收益率有相对较高的概率取极端值。换句话说，该项资产未来行市发生剧烈波动的概率相对较高。 这个讲解从实用角度解释了偏度与峰度，即这两个指标侧重于对图形的描述，在计算出具体的偏度与峰度后，更主要是要参考图形来分类分析，而不是单纯依照数值简单判断。 其二，根据这两个指标的分类，使用者可以按照自己的需求在原公式的基础上编程时再细化，即把图形数据化（因为如果数据量很大的话每个都要附上图形，计算机能受得了，但分析者还不得累死），这样得出的结果更有助于分析归纳，否则单单依靠原公式，分析归纳的难度有些大。 第三，这两个指标都是基于均值、标准差而来的，所以分析时可以根据均值与标准差来判别长尾属于哪种类型，从而确定其影响。 via http://bbs.pinggu.org/thread-3417092-1-1.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>偏度</tag>
        <tag>峰度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次python数据处理性能调优]]></title>
    <url>%2F2017%2F10%2F21%2F%E8%AE%B0%E4%B8%80%E6%AC%A1python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Python垃圾回收机制根据官方的描叙，Python中，有2中方式将会触发垃圾回收：1、用户显示调用gc.collect()2、每次Python为新对象分配内存时，检查threshold阀值，当对象数量超过threshold设置的阀值就开始进行垃圾回收。 因为数据量太大，处理过程中留下太多暂时不能清除的变量，而python的垃圾回收却在一遍一遍地扫这些不断在增长的列表，导致程序受到的影响越来越大。赶紧证实一下，import gc，然后在数据载入模块前gc.disable（）,结束后再gc.enable()。结果原来要跑将近两个小时的程序，这下不用5分钟就跑完了。cool~！用gc.get_count()也证明了之前的猜想，在第一次运行之后临时变量数目就从几百上升到百万，并一直在涨。 如果你的python程序在处理大数据量的问题，并且出现某个子程序在做同样量的工作，却越跑越慢的情况。 程序代码： 12345678910111213141516171819202122def series_trans(dataset): # 一行转多行 dataset_trans = pd.DataFrame(&#123;'user_id':dataset['user_id'], 'shop_id':dataset['shop_id'], 'time_stamp':dataset['time_stamp'], 'longitude':dataset['longitude'], 'latitude':dataset['latitude'], 'wifi_infos':dataset['wifi_splits']&#125;) return dataset_transprint time.asctime( time.localtime(time.time()) )dataset_trans = series_trans(dataset.loc[0,:])for i in xrange(1,dataset.shape[0]): if i % 10000 == 0: print i print time.asctime( time.localtime(time.time()) ) dataset_trans = dataset_trans.append(series_trans(dataset.loc[i,:])) 程序的性能表现如下： 1234567891011121314151617181920212223242526272829303132333435363738Sat Oct 21 11:01:30 201710000Sat Oct 21 11:02:07 201720000Sat Oct 21 11:03:41 201730000Sat Oct 21 11:06:12 201740000Sat Oct 21 11:09:37 201750000Sat Oct 21 11:13:55 201760000Sat Oct 21 11:19:10 201770000Sat Oct 21 11:25:26 201780000Sat Oct 21 11:33:03 201790000Sat Oct 21 11:41:21 2017100000Sat Oct 21 11:50:33 2017110000Sat Oct 21 12:00:41 2017120000Sat Oct 21 12:11:45 2017130000Sat Oct 21 12:23:43 2017140000Sat Oct 21 12:36:37 2017150000Sat Oct 21 12:50:25 2017160000Sat Oct 21 13:05:10 2017170000Sat Oct 21 13:20:46 2017180000Sat Oct 21 13:37:18 2017 采用：dataset_trans.loc[10:19,] = dataset_trans会提示错误：info_idx = indexer[info_axis]IndexError: tuple index out of range DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.所以一般说来dataframe就是a set of columns, each column is an array of values. In pandas, the array is one way or another a (maybe variant of) numpy ndarray. 而ndarray本身不存在一种in place append的操作。。。因为它实际上是一段连续内存。。。任何需要改变ndarray长度的操作都涉及分配一段长度合适的新的内存，然后copy。。。这是这类操作慢的原因。。。如果pandas dataframe没有用其他设计减少copy的话，我相信Bren说的”That’s probably as efficient as any”是很对的。。。所以in general, 正如Bren说的。。。Pandas/numpy structures are fundamentally not suited for efficiently growing.Matti 和 arynaq说的是两种常见的对付这个问题的方法。。。我想Matti实际的意思是把要加的rows收集成起来然后concatenate, 这样只copy一次。arynaq的方法就是预先分配内存比较好理解。。。如果你真的需要incrementally build a dataframe的话，估计你需要实际测试一下两种方法。。。我的建议是，如有可能，尽力避免incrementally build a dataframe, 比如用其他data structure 收集齐所有data然后转变成dataframe做分析。。。 via Python垃圾回收(gc)拖累了程序执行性能？ python pandas 怎样高效地添加一行数据？]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio 无法编译新打开的C文件]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20%E6%97%A0%E6%B3%95%E7%BC%96%E8%AF%91%E6%96%B0%E6%89%93%E5%BC%80%E7%9A%84C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[用 Visual Studio打开有c文件的文件夹，发现里面的c文件都无法编译，导航栏里面连生成按钮都没有，调试按钮也是灰色的。 需在项目中单个添加c文件。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>Visual Studio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pydotplus绘决策树]]></title>
    <url>%2F2017%2F10%2F17%2F%E4%BD%BF%E7%94%A8pydotplus%E7%BB%98%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[首先安装pydotplus pip install pydotplus 然后打印训练好的clf: 123456789import pydotplusdot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_dummy.columns, class_names='target', filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") 发现报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-27-2c78351c7fea&gt;", line 7, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; prog=self.prog: self.write(path, format=f, prog=prog) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write fobj.write(self.create(prog, format)) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create 'GraphViz\'s executables not found')InvocationException: GraphViz's executables not found 提示没有找到&#39;GraphViz\&#39;s executables&#39;: 下载安装graphviz-2.38.msiurl：http://www.graphviz.org/pub/graphviz/stable/windows/graphviz-2.38.msi 源地址下载比较慢或者打不开，可以从这里下载：http://download.csdn.net/download/boredbird32/10025853 添加安装路径到系统Path环境变量：D:\Program Files (x86)\Graphviz2.38\;D:\Program Files (x86)\Graphviz2.38\bin\ 重新启动Python环境发现还是报同样的错误。 点开文件&quot;D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py&quot; 找到 Method 3 (Windows only)那一段，修改里面的path为当时安装graphviz的地址。 12345678910111213141516# Method 3 (Windows only)if os.sys.platform == 'win32': # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" 重启，又报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-7-3735ccaa5368&gt;", line 5, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; lambda path, File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create if self.progs is None:InvocationException: GraphViz's executables not found 继续点进去，修改源文件：加上调试语句： 123456789101112131415161718192021222324252627282930313233343536373839# Method 3 (Windows only)if os.sys.platform == 'win32': print('come inside method 3') # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" print(path) progs = __find_executables(path) print(progs) if progs is not None: print("Used default install location") return progsfor path in ( '/usr/bin', '/usr/local/bin', '/opt/local/bin', '/opt/bin', '/sw/bin', '/usr/share', '/Applications/Graphviz.app/Contents/MacOS/'): progs = __find_executables(path) if progs is not None: # print("Used path") return progs# Failed to find GraphVizreturn None 再次运行，发现： 12345InvocationException: GraphViz's executables not foundcome inside method 3C:\Program Files\ATT\GraphViz\binNone 问题就出在这个path上。那干脆我们就直接在判断条件外面指定path： 12345path = r"D:\Program Files (x86)\Graphviz2.38\bin"# print(path)progs = __find_executables(path)# print(progs) 再次运行，问题解决了： 1234567come inside method 3D:\Program Files (x86)\Graphviz2.38\bin&#123;'twopi': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\twopi.exe', 'fdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\fdp.exe', 'circo': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\circo.exe', 'neato': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\neato.exe', 'dot': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe', 'sfdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\sfdp.exe'&#125;Used default install locationOut[3]: True 最后注释掉测试语句。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Code工具上手体验]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20Code%E5%B7%A5%E5%85%B7%E4%B8%8A%E6%89%8B%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[下载安装Visual Studio Code,地址：https://code.visualstudio.com/Download 在VSCode内安装c++插件：打开vscode，Ctrl+P之后输入ext install c++ 弹出： 12扩展：商店 错误 不用管它，重新再来一次，Ctrl+P之后输入ext install c++ 安装第一个官方的。 重启vscode。然后发现重新启动，vscode开始自动下载插件依赖。依赖自动下载安装完成。 抬头，提示reload。 查看本机gcc环境： 123456789E:\Code\solar-correlation-map&gt;gcc -vUsing built-in specs.COLLECT_GCC=gccCOLLECT_LTO_WRAPPER=D:/TDM-GCC-64/bin/../libexec/gcc/x86_64-w64-mingw32/5.1.0/lto-wrapper.exeTarget: x86_64-w64-mingw32Configured with: ../../../src/gcc-5.1.0/configure --build=x86_64-w64-mingw32 --enable-targets=all --enable-languages=ada,c,c++,fortran,lto,objc,obj-c++ --enable-libgomp --enable-lto --enable-graphite --enable-cxx-flags=-DWINPTHREAD_STATIC --disable-build-with-cxx --disable-build-poststage1-with-cxx --enable-libstdcxx-debug --enable-threads=posix --enable-version-specific-runtime-libs --enable-fully-dynamic-string --enable-libstdcxx-threads --enable-libstdcxx-time --with-gnu-ld --disable-werror --disable-nls --disable-win32-registry --prefix=/mingw64tdm --with-local-prefix=/mingw64tdm --with-pkgversion=tdm64-1 --with-bugurl=http://tdm-gcc.tdragon.net/bugsThread model: posixgcc version 5.1.0 (tdm64-1) 新建文件夹，在文件夹内新建文件hello.cpp：123456789\#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 0; cout&lt;&lt;a; return 0;&#125; 按下F5，启动调试，提示需要配置。将launch.json的文件内容替换成如下： 123456789101112131415161718192021&#123; "version": "0.2.0", "configurations": [ &#123; "name": "C++ Launch (GDB)", // 配置名称，将会在启动配置的下拉菜单中显示 "type": "cppdbg", // 配置类型，这里只能为cppdbg "request": "launch", // 请求配置类型，可以为launch（启动）或attach（附加） "launchOptionType": "Local", // 调试器启动类型，这里只能为Local "targetArchitecture": "x86", // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 "program": "$&#123;file&#125;.exe", // 将要进行调试的程序的路径 // "miDebuggerPath":"c:\\MinGW\\bin\\gdb.exe", // miDebugger的路径，注意这里要与MinGw的路径对应 "miDebuggerPath":"D:\\TDM-GCC-64\\bin\\gdb.exe", "args": ["blackkitty", "1221", "# #"], // 程序调试时传递给程序的命令行参数，一般设为空即可 "stopAtEntry": false, // 设为true时程序将暂停在程序入口处，一般设置为false "cwd": "$&#123;workspaceRoot&#125;", // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 "externalConsole": true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 "preLaunchTask": "g++" // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; 注意miDebuggerPath要与MinGw的路径对应 替换后保存，然后切换至hello.cpp，按F5进行调试，此时会弹出一个信息框要求你配置任务运行程序，点击它~ 选择任务运行程序，点击Others，跳出tasks.json的配置文件。替换成如下代码: 123456789101112131415161718&#123; "version": "0.1.0", "command": "g++", "args": ["-g","$&#123;file&#125;","-o","$&#123;file&#125;.exe"], // 编译命令参数 "problemMatcher": &#123; "owner": "cpp", "fileLocation": ["relative", "$&#123;workspaceRoot&#125;"], "pattern": &#123; "regexp": "^(.*):(\\d+):(\\d+):\\s+(warning|error):\\s+(.*)$", "file": 1, "line": 2, "column": 3, "severity": 4, "message": 5 &#125; &#125;&#125; 保存一下，然后切换至hello.cpp，再次按F5启动调试: 控制台输出： 12345678910111213141516171819202122=thread-group-added,id="i1"GNU gdb (GDB) 7.9.1Copyright (C) 2015 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-w64-mingw32".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".=cmd-param-changed,param="pagination",value="off"[New Thread 16288.0x3d10]Breakpoint 1, main () at e:\Code\C_playground\hello.cpp:55 int a = 0;[Inferior 1 (process 16288) exited normally]The program 'e:\Code\C_playground\hello.cpp.exe' has exited with code 0 (0x00000000). 文档目录如下：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次诡异的Python中文乱码]]></title>
    <url>%2F2017%2F10%2F17%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E8%AF%A1%E5%BC%82%E7%9A%84Python%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[有一天突然出现了这样的中文乱码： 我的第一反应就是检查：# -*- coding:utf-8 -*- 然后检查： 12345import syssys.getdefaultencoding()Out[5]: 'utf-8' 这，，是中文编码的啊，再检查： 12345678from chardet import detectcfg.dataset_train['white_list_type'][0]Out[8]: '\xb7\xc7\xb0\xd7\xc3\xfb\xb5\xa5'detect(cfg.dataset_train['white_list_type'][0])Out[9]: &#123;'confidence': 0.0, 'encoding': None&#125; 终于发现问题了，&#39;encoding&#39;: None 然后，那就在read_csv的时候指定编码： 123456789101112131415161718192021222324dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8')Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-13-a9c949155761&gt;", line 1, in &lt;module&gt; dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8') File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 646, in parser_f return _read(filepath_or_buffer, kwds) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 401, in _read data = parser.read() File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 939, in read ret = self._engine.read(nrows) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 1508, in read data = self._reader.read(nrows) File "pandas\parser.pyx", line 848, in pandas.parser.TextReader.read (pandas\parser.c:10415) File "pandas\parser.pyx", line 870, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:10691) File "pandas\parser.pyx", line 947, in pandas.parser.TextReader._read_rows (pandas\parser.c:11728) File "pandas\parser.pyx", line 1049, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:13162) File "pandas\parser.pyx", line 1108, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:14116) File "pandas\parser.pyx", line 1206, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:16172) File "pandas\parser.pyx", line 1222, in pandas.parser.TextReader._string_convert (pandas\parser.c:16400) File "pandas\parser.pyx", line 1458, in pandas.parser._string_box_utf8 (pandas\parser.c:22072)UnicodeDecodeError: 'utf8' codec can't decode byte 0xb7 in position 0: invalid start byte 什么情况，一阵百度，无解。 然后用notepad++打开看下我的csv文件编码： 居然没有一种格式是选中的。。。怎么会出现这种情况，我从数据库导出的时候明明已经指定了编码格式为utf-8，而且后面又打开指定了编码格式为utf-8，怎么现在会是未指定的状态。。 好吧，重新指定csv文件编码为utf-8，再重新试下： 12345678910111213dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv')detect(dataset['white_list_type'][0])Out[17]: &#123;'confidence': 0.938125, 'encoding': 'utf-8'&#125;dataset['white_list_type'][0]Out[18]: '\xe9\x9d\x9e\xe7\x99\xbd\xe5\x90\x8d\xe5\x8d\x95'dataset['white_list_type']Out[19]: 0 非白名单1 普通白名单2 普通白名单 一切又正常了。诡异。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Django REST Framework创建API服务]]></title>
    <url>%2F2017%2F10%2F15%2F%E5%9F%BA%E4%BA%8EDjango%20REST%20Framework%E5%88%9B%E5%BB%BAAPI%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Django REST Framework 安装 123pip install djangorestframeworkpip install markdown # Markdown support for the browsable API.pip install django-filter # Filtering support 看下我的环境：1234567(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;pip freezeDjango==1.11.6django-bootstrap3==9.0.0django-filter==1.0.4djangorestframework==3.7.0Markdown==2.6.9pytz==2017.2 创建一个新的项目django_rest，在项目下创建名为api的应用。 cmd.exe 12345678910E:\Code\virtualenvs\myenvs&gt;.\Scripts\activate.bat(myenvs) E:\Code\virtualenvs\myenvs&gt;python .\Scripts\django-admin.py startproject django_rest(myenvs) E:\Code\virtualenvs\myenvs&gt;cd django_rest\(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py startapp api(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt; 打开settings.py文件，添加应用。 settings.py 12345678910111213141516171819202122# Application definitionINSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'api',]......# 在文件末尾添加REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES':( 'rest_framework.permissions.IsAuthenticated', )&#125; “rest_framework”为Django REST Framework应用，”api”为我们自己创建的应用。默认的权限策略可以设置在全局范围内，通过DEFAULT_PERMISSION_CLASSES设置。 通过migrate命令执行数据库迁移。 cmd.exe 1234(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py makemigrations(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py migrate 通过createsuperuser命令创建超级管理员账户admin/admin123456。 cmd.exe 123456(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py createsuperuserUsername (leave blank to use 'maomaochong'): adminEmail address: admin@email.comPassword:Password (again):Superuser created successfully. 创建数据序列化，在api应用下创建serializers.py文件。 serializers.py 1234567891011121314from django.contrib.auth.models import User,Groupfrom rest_framework import serializersclass UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups')class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') Serializers用于定义API的表现形式，如返回哪些字段、返回怎样的格式等。这里序列化Django自带的User和Group。 编写视图文件，打开api应用下的views.py文件，编写如下代码。 views.py 123456789101112131415161718192021222324# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.contrib.auth.models import User,Groupfrom rest_framework import viewsetsfrom api.serializers import UserSerializer,GroupSerializer# ViewSets 定义视图的展现形式class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializerclass GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all().order_by('-date_joined') serializer_class = GroupSerializer 在Django REST framework 中，ViewSets用于定义视图的展现形式，例如返回哪些内容，需要做哪些权限处理。 在URL中会定义相应的规则到ViewSet。ViewSet则通过serializer_class 找到对应的Serializers。这里讲User和Group的所有对象赋予queryset，并返回这些值。在UserSerializer和GroupSerializer中定义要返回的字段。 打开…/django_rest/urls.py文件，添加api的路由配置。 urls.py 12345678910111213141516171819from django.conf.urls import url, includefrom django.contrib import adminfrom rest_framework import routersfrom api import views# Routers provide an easy way of automatically determining the URL conf.router = routers.DefaultRouter()router.register(r'users',views.UserViewSet)router.register(r'groups',views.GroupViewSet)# Wire up out API using automatic URL routing.# Additionally,we include login URLs for the browsable API.urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework'))] 因为使用的是ViewSets,所以可以使用routers类自动生成URL conf。 通过runserver命令启动服务。 cmd.exe 12345678(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py runserverPerforming system checks...System check identified no issues (0 silenced).October 15, 2017 - 12:32:14Django version 1.11.6, using settings 'django_rest.settings'Starting development server at http://127.0.0.1:8000/Quit the server with CTRL-BREAK. 用超级管理员admin/admin123456登录。 接下来在django_rest项目的基础上，创建模型，打开api应用下的models.py文件。 models.py 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.db import models# Create your models here.# 发布会class Event(models.Model): name = models.CharField(max_length=100) # 发布会标题 limit = models.IntegerField() # 限制人数 status = models.BooleanField() # 状态 address = models.CharField(max_length=200) # 地址 start_time = models.DateTimeField('events time') # 发布会时间 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） def __str__(self): return self.name# 嘉宾class Guest(models.Model): event = models.ForeignKey(Event) # 关联发布会id realname = models.CharField(max_length=64) # 姓名 phone = models.CharField(max_length=16) # 手机号 email = models.EmailField() # 邮箱 sign = models.BooleanField() # 签到状态 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） class Meta: unique_together = ('phone', 'event') def __str__(self): return self.realname 然后执行数据库迁移。 cmd.exe 123456789(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py makemigrations apiMigrations for 'api': api\migrations\0001_initial.py - Create model Event - Create model Guest - Alter unique_together for guest (1 constraint(s))(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py migrate 添加发布会数据序列化，打开api应用下的serializers.py文件（上面创建的）。 serializers.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 from django.contrib.auth.models import User,Group from rest_framework import serializers from api.models import Event,Guest class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups') class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') class EventSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Event fields = ('url','name','address','start_time','limit','status') class GuestSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Guest fields = ('url','realname','phone','email','sign','event')``` 打开api应用下的views.py文件，定义发布会和嘉宾视图类。&gt; views.py``` python # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.shortcuts import render # Create your views here. from django.contrib.auth.models import User,Group from rest_framework import viewsets from api.serializers import UserSerializer,GroupSerializer, EventSerializer, GuestSerializer from api.models import Event, Guest # ViewSets 定义视图的展现形式 class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializer class GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all() serializer_class = GroupSerializer class EventViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Event.objects.all() serializer_class = EventSerializer class GuestViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Guest.objects.all() serializer_class = GuestSerializer``` 打开.../django_rest/urls.py文件，添加URL配置。&gt; urls.py``` python from django.conf.urls import url, include from django.contrib import admin from rest_framework import routers from api import views # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r'users',views.UserViewSet) router.register(r'groups',views.GroupViewSet) router.register(r'events',views.EventViewSet) router.register(r'guests',views.GuestViewSet) # Wire up out API using automatic URL routing. # Additionally,we include login URLs for the browsable API. urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework')) ]]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python类属性和方法的封装]]></title>
    <url>%2F2017%2F10%2F14%2Fpython%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95%E7%9A%84%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[封装是一种限制直接访问目标属性和方法的机制，但同时它又有利于对数据（对象的方法）进行操作。 封装是一种将抽象性函数接口的实现细节部分包装、隐藏起来的方法。同时，它也是一种防止外界调用端，去访问对象内部实现细节的手段，这个手段是由编程语言本身来提供的。 对象所有的内部表征对于外部来说都是隐藏的，只有对象能直接与内部数据交互。首先，我们需要理解公开（public）和私有（non-public）实例变量和方法。 公开实例变量对于 Python 的类，我们可以使用 constructor 方法初始化公开实例变量： 123class Person: def __init__(self,first_name): self.first_name = first_name 下面我们应用 first_name 的值作为公开实例变量的变元。 12a = Person('zhang')print a.first_name # -&gt; zhang 在类别内： 12class Person: first_name = 'zhang' 现在我们不需要再对 first_name 赋值，所有赋值到 a 的目标都将有类的属性： 12a = Person()print a.first_name # -&gt; zhang 现在我们已经学会如何使用公开实例变量和类属性。除此之外，我们还能管理公开部分的变量值，即对象可以管理其变量的值：Get 和 Set 变量值。保留 Person 类，我们希望能给 first_name 变量赋另外一个值： 123a = Person('zhang')a.first_name = 'Z'print a.first_name # -&gt; Z 如上我们将另外一个值赋予了 first_name 实例变量，因为它又是一个公开变量，所以会更新变量值。 私有实例变量和公开实例变量一样，我们可以使用 constructor 方法或在类的内部声明而定义一个私有实例变量。语法上的不同在于私有实例变量在变量名前面加一个下划线： 1234class Person: def __init__(self,first_name,email): self.first_name = first_name self._email = email 上述定义的 email 变量就是私有变量。12a = Person('zhang','zhang@mail.com')print a._email # -&gt; zhang@mail.com 我们可以访问并且更新它，私有变量仅是一个约定，即他们需要被视为 API 非公开的部分。所以我们可以使用方法在类的定义中完成操作，例如使用两种方法展示私有实例的值与更新实例的值： 12345678910class Person: def __init__(self,first_name,email): self.first_name = first_name self.__email = email def update_email(self,new_email): self.__email = new_email def email(self): return self.__email 现在我们可以使用方法更新或访问私有变量。 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma.__email = 'new_email@mail.com'print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; zhang@mail.coma.update_email('update_email@mail.com')print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; update_email@mail.com 从上面可见，以双下划线打头的名称会导致出现名称重整的行为。具体来说就是上面的这个类中的私有属性会被分别重命名为_Cprivate 和 _Cprivate_name。但是为啥两个值不一样？？ 豁然开朗！ 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma._email = 'new_email@mail.com'print a._email # -&gt; new_email@mail.comprint a.email() # -&gt; new_email@mail.coma.update_email('update_email@mail.com')print a._email # -&gt; update_email@mail.comprint a.email() # -&gt; update_email@mail.com]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django自带的server局域网访问]]></title>
    <url>%2F2017%2F10%2F11%2Fdjango%E8%87%AA%E5%B8%A6%E7%9A%84server%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[问题描述用(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver启动服务，对应的访问地址为： 12Starting development server at http://127.0.0.1:8000/ 然后，我把127.0.0.1改为本机的地址发现居然不能访问。 解决方法使用python .\manage.py runserver 0.0.0.0:8000启动服务： 又出现新的错误： 12345678910111213141516171819202122232425DisallowedHost at /post/act/Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Request Method: GETRequest URL: http://10.83.2.132:8000/post/act/?eid=1&amp;status=%27F%27Django Version: 1.11.6Exception Type: DisallowedHostException Value: Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Exception Location: E:\Code\virtualenvs\myenvs\lib\site-packages\django\http\request.py in get_host, line 113Python Executable: E:\Code\virtualenvs\myenvs\Scripts\python.exePython Version: 2.7.13Python Path: ['E:\\Code\\virtualenvs\\myenvs\\src', 'E:\\Code\\virtualenvs\\myenvs\\Scripts\\python27.zip', 'E:\\Code\\virtualenvs\\myenvs\\DLLs', 'E:\\Code\\virtualenvs\\myenvs\\lib', 'E:\\Code\\virtualenvs\\myenvs\\lib\\plat-win', 'E:\\Code\\virtualenvs\\myenvs\\lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs\\Scripts', 'D:\\ProgramData\\Anaconda2\\Lib', 'D:\\ProgramData\\Anaconda2\\DLLs', 'D:\\ProgramData\\Anaconda2\\Lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs', 'E:\\Code\\virtualenvs\\myenvs\\lib\\site-packages']Server time: 星期三, 11 十月 2017 15:59:19 +0800 解决方法去django-admin.py startproject project-name创建的项目中去修改 setting.py 文件： 12 ALLOWED_HOSTS = [‘*’] ＃在这里请求的host添加了＊ 又又出现新的错误：报错SyntaxError: invalid syntax: 这尼玛，这是什么鬼，我的引号呢？？？终端上为什么没有引号，于是我有尝试了&quot;*&quot;双引号，依然不行。然后无穷的百度（动词），居然没一个类似我的问题。老天干嘛要为难我这个小白啊。 解决方法一不做，二不休： 123# ALLOWED_HOSTS = ['*']ALLOWED_HOSTS = list('*') 居然works！ 加逗号也是可以的： 12ALLOWED_HOSTS = ['*',] 效果 参考： django自带的server 让外网主机访问]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Forbidden (CSRF cookie not set.)]]></title>
    <url>%2F2017%2F10%2F11%2FForbidden%20(CSRF%20cookie%20not%20set.)%2F</url>
    <content type="text"><![CDATA[问题描述用postman提交post请求时，终端打印错误Forbidden (CSRF cookie not set.)： 1234567[11/Oct/2017 14:55:00] "GET /post/post/ HTTP/1.1" 200 47Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 14:55:16] "POST /post/post/ HTTP/1.1" 403 2829Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 15:00:06] "POST /post/post/ HTTP/1.1" 403 2829Performing system checks... 情况如下图所示： 解决方法修改settings.py文件，注释掉 django.middleware.csrf.CsrfViewMiddleware&#39;, 效果post请求处理正常。 参考： (django1.10)访问url报错Forbidden (CSRF cookie not set.): xxx]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>CSRF</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore规则不生效的解决办法]]></title>
    <url>%2F2017%2F10%2F11%2Fgitignore%E8%A7%84%E5%88%99%E4%B8%8D%E7%94%9F%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题描述发现添加.gitignore文件后，本地做的修改仍被push到GitHub。.gitignore规则并没有生效。 解决方法原因是如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交： git rm -r --cached . git add . git commit -m &#39;update .gitignore&#39; 如何避免 忽略规则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 ##最后需要强调的一点是，如果你不慎在创建.gitignore文件之前就push了项目，那么即使你在.gitignore文件中写入新的过滤规则，这些规则也不会起作用，Git仍然会对所有文件进行版本管理。简单来说，出现这种问题的原因就是Git已经开始管理这些文件了，所以你无法再通过过滤规则过滤它们。因此一定要养成在项目开始就创建.gitignore文件的习惯，否则一旦push，处理起来会非常麻烦。 参考： Git忽略规则.gitignore梳理 Git忽略文件.gitignore的使用 Git忽略规则和.gitignore规则不生效的解决办法 github官方给的Python项目.gitignore文件模板]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python环境设置默认编码为utf8的方法]]></title>
    <url>%2F2017%2F10%2F11%2FPython%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE%E9%BB%98%E8%AE%A4%E7%BC%96%E7%A0%81%E4%B8%BAutf8%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[从网上fork了虫师的代码，发现每个文件都没有加头注释指定文件编码，我这一个一个改得什么时候。想着虫师也是老司机了，不可能连IDE自动添加头注释都不知道，应该是哪边统一设置的。果然他是直接修改的Python环境配置。 设置之前： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 52853 52854Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'ascii' 设置之后： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 55595 55596Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'utf-8' 如何设置： 可以在Python安装目录下的Lib/site-packages目录中，新建一个sitecustomize.py文件（也可以建在其它地方，然后手工导入，建在这里，每次启动Python的时候设置将自动生效），内容如下： 123import syssys.setdefaultencoding('utf-8') #set default encoding to utf-8 参考： Python设置默认编码为utf8的方法 python错误：AttributeError: ‘module’ object has no attribute ‘setdefaultencoding’问题的解决方法]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API 设计指南-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FRESTful%20API%20Design%20Guide%2F</url>
    <content type="text"><![CDATA[这篇文章写的也很好，很上一篇有很大重复，甚至有些矛盾的地方。但是读下来，依然还是有启发的。 一、协议API与用户的通信协议，总是使用HTTPs协议。 #二、域名应该尽量将API部署在专用域名之下。12https://api.example.com 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。12https://example.org/api/ 三、版本（Versioning）应该将API的版本号放入URL。12https://api.example.com/v1/ 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观 四、路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。123456https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。1234&#123; error: "Invalid API key"&#125; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。1234567&#123;"link": &#123; "rel": "collection https://www.example.com/zoos", "href": "https://api.example.com/zoos", "title": "List of zoos", "type": "application/vnd.yourformat+json"&#125;&#125; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 123456&#123; "current_user_url": "https://api.github.com/user", "authorizations_url": "https://api.github.com/authorizations", // ...&#125; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 12345&#123; "message": "Requires authentication", "documentation_url": "https://developer.github.com/v3"&#125; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他 API的身份认证应该使用OAuth 2.0框架。 服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 via RESTful API 设计指南]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解RESTful架构-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FUnderstanding%20RESTful%20Framework%2F</url>
    <content type="text"><![CDATA[最近在研究Python开发接口微服务，以便用Python开发出来的模型与服务能对接上，提供复杂模型上线的方案。看到了阮一峰的《理解RESTful架构》这篇文章，写的真好，虽然我是还没入门的外外行。 什么是RESTful架构一、起源REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。 文中写到： 123"本文研究计算机科学两大前沿----软件和网络----的交叉点。长期以来，软件研究主要关注软件设计的分类、设计方法的演化，很少客观地评估不同的设计选择对系统行为的影响。而相反地，网络研究主要关注系统之间通信行为的细节、如何改进特定通信机制的表现，常常忽视了一个事实，那就是改变应用程序的互动风格比改变互动协议，对整体表现有更大的影响。我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。"(This dissertation explores a junction on the frontiers of two research disciplines in computer science: software and networking. Software research has long been concerned with the categorization of software designs and the development of design methodologies, but has rarely been able to objectively evaluate the impact of various design choices on system behavior. Networking research, in contrast, is focused on the details of generic communication behavior between systems and improving the performance of particular communication techniques, often ignoring the fact that changing the interaction style of an application can have more impact on performance than the communication protocols used for that interaction. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. ) 二、名称Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。我对这个词组的翻译是”表现层状态转化”。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 三、资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 四、表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 五、状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 六、综述综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 七、误区RESTful架构有一些典型的设计误区。最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：12POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：12345 POST /transaction HTTP/1.1 Host: 127.0.0.1 from=1&amp;to=2&amp;amount=500.00 另一个设计误区，就是在URI中加入版本号： 123456 http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分参见Versioning REST Services：123456 Accept: vnd.example-com.foo+json; version=1.0 Accept: vnd.example-com.foo+json; version=1.1 Accept: vnd.example-com.foo+json; version=2.0 via 理解RESTful架构]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django处理HTTP请求流程]]></title>
    <url>%2F2017%2F10%2F10%2Fdjango%20flow%2F</url>
    <content type="text"><![CDATA[Based on @FULLSTACKCTO’s understanding of Django, this is how a user request is responded to. 1: User requests a page 2: Request reaches Request Middlewares, which could manipulate or answer the request 3: The URLConffinds the related View using urls.py 4: View Middlewares are called, which could manipulate or answer the request 5: The view function is invoked 6: The view could optionally access data through models 7: All model-to-DB interactions are done via a manager 8: Views could use a special context if needed 9: The context is passed to the Template for rendering a: Template uses Filters and Tags to render the output b: Output is returned to the view c: HTTPResponse is sent to the Response Middlerwares d: Any of the response middlewares can enrich the response or return a completely new response e: The response is sent to the user’s browser. via here Django Flowchart]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POST和GET区别]]></title>
    <url>%2F2017%2F10%2F10%2Fget%20or%20post%2F</url>
    <content type="text"><![CDATA[HTTP协议定义了很多与服务器交互的方法，最基本的有4种，分别是GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作，其中最常见请求方式是GET和POST，并且现在浏览器一般只支持GET和POST方法。 GET一般用于获取/查询资源信息，而POST一般用于更新资源信息，他们之间主要区别如下： 1）根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的，这里安全是指该操作用于获取信息而非修改信息，幂等是指对同一URL的多个请求应该返回同样的结果（这一点在实质实现时，可能并不满足）；POST表示可能修改变服务器上的资源的请求。 2）GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64编码；POST把提交的数据则放置在是HTTP包的包体中。 3）因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系，理论上URL长度是没有限制的，即HTTP协议没有规定URL的长度，但在实质中，特定的浏览器可能对这个长度做了限制；理论上POST也是没有大小限制的，HTTP协议规范也没有进行大小限制，但在服务端通常会对这个大小做一个限制，当然这个限制比GET宽松的多，即使用POST可以提交的数据量比GET大得多。 最后，网上有人说，POST的安全性要比GET的安全性高，实质上POST跟GET都是明文传输，这可以通过类似WireShark工具看到。总之，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求。 via 浅析HTTP中POST和GET区别并用Python模拟其响应和请求 浅谈HTTP中Get与Post的区别 GET和POST的区别]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[assignment2]]></title>
    <url>%2F2017%2F10%2F10%2Fassignment2%2F</url>
    <content type="text"><![CDATA[In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows: understand Neural Networks and how they are arranged in layered architectures understand and be able to implement (vectorized) backpropagation implement the core parameter update loop of mini-batch gradient descent effectively cross-validate and find the best hyperparameters for Neural Network architecture understand the architecture of Convolutional Neural Networks and train gain experience with training these models on data SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal. Working locallyGet the code:Download the starter code here. [Optional] virtual environment:Once you have unzipped the starter code, you might want to create avirtual environmentfor the project. If you choose not to use a virtual environment, it is up to youto make sure that all dependencies for the code are installed on your machine.To set up a virtual environment, run the following: 1234567cd assignment2sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environment You can reuse the virtual environment that you created for the first assignment,but you will need to run pip install -r requirements.txt after activating itto install additional dependencies required by this assignment. Download data:Once you have the starter code, you will need to download the CIFAR-10 dataset.Run the following from the assignment2 directory: 12cd cs231n/datasets./get_datasets.sh Compile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command: 1python setup.py build_ext --inplace Start IPython:After you have the CIFAR-10 data, you should start the IPython notebook server from theassignment2 directory. If you are unfamiliar with IPython, you should read ourIPython tutorial. Working on TerminalWe have created a Terminal snapshot that is preconfigured for this assignment;you can find it here. Terminal allows you to work on the assignment from your browser. You can find a tutorial on how to use it here. Submitting your work:Whether you work on the assignment locally or using Terminal, once you are doneworking run the collectSubmission.sh script; this will produce a file calledassignment2.zip. Upload this file to your dropbox onthe courseworkpage for the course. Q1: Two-layer Neural Network (30 points)The IPython Notebook two_layer_net.ipynb will walk you through implementing a two-layer neural network on CIFAR-10. You will write a hard-coded 2-layer Neural Network, implement its backprop pass, and tune its hyperparameters. Q2: Modular Neural Network (30 points)The IPython Notebook layers.ipynb will walk you through a modular Neural Network implementation. You will implement the forward and backward passes of many different layer types, including convolution and pooling layers. Q3: ConvNet on CIFAR-10 (40 points)The IPython Notebook convnet.ipynb will walk you through the process of training a (shallow) convolutional neural network on CIFAR-10. It wil then be up to you to train the best network that you can. Q4: Do something extra! (up to +20 points)In the process of training your network, you should feel free to implement anything that you want to get better performance. You can modify the solver, implement additional layers, use different types of regularization, use an ensemble of models, or anything else that comes to mind. If you implement these or other ideas not covered in the assignment then you will be awarded some bonus points.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
        <tag>backpropagation</tag>
        <tag>mini-batch gradient descent</tag>
        <tag>cross-validate</tag>
        <tag>Convolutional Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transfer-learning]]></title>
    <url>%2F2017%2F10%2F10%2Ftransfer-learning%2F</url>
    <content type="text"><![CDATA[(These notes are currently in draft form and under development) Table of Contents: Transfer Learning Additional References Transfer LearningIn practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset. Fine-tuning the ConvNet. The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds. Pretrained models. Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights. When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios: New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes. New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network. New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network. New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network. Practical advice. There are a few additional things to keep in mind when performing Transfer Learning: Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0. Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization). Additional References CNN Features off-the-shelf: an Astounding Baseline for Recognition trains SVMs on features from ImageNet-pretrained ConvNet and reports several state of the art results. DeCAF reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-based precursor to the C++ Caffe library. How transferable are features in deep neural networks? studies the transfer learning performance in detail, including some unintuitive findings about layer co-adaptations.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>transfer learning</tag>
        <tag>ConvNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[understanding-cnn]]></title>
    <url>%2F2017%2F10%2F10%2Funderstanding-cnn%2F</url>
    <content type="text"><![CDATA[(this page is currently in draft form) Visualizing what ConvNets learnSeveral approaches for understanding and visualizing Convolutional Networks have been developed in the literature, partly as a response the common criticism that the learned features in a Neural Network are not interpretable. In this section we briefly survey some of these approaches and related work. Visualizing the activations and first-layer weightsLayer Activations. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates. Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local. Conv/FC Filters. The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasn’t been trained for long enough, or possibly a very low regularization strength that may have led to overfitting. Typical-looking filters on the first CONV layer (left), and the 2nd CONV layer (right) of a trained AlexNet. Notice that the first-layer weights are very nice and smooth, indicating nicely converged network. The color/grayscale features are clustered because the AlexNet contains two separate streams of processing, and an apparent consequence of this architecture is that one stream develops high-frequency grayscale features and the other low-frequency color features. The 2nd CONV layer weights are not as interpretable, but it is apparent that they are still smooth, well-formed, and absent of noisy patterns. Retrieving images that maximally activate a neuronAnother visualization technique is to take a large dataset of images, feed them through the network and keep track of which images maximally activate some neuron. We can then visualize the images to get an understanding of what the neuron is looking for in its receptive field. One such visualization (among others) is shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.: Maximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the receptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a function of a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies, text, or specular highlights. One problem with this approach is that ReLU neurons do not necessarily have any semantic meaning by themselves. Rather, it is more appropriate to think of multiple ReLU neurons as the basis vectors of some space that represents in image patches. In other words, the visualization is showing the patches at the edge of the cloud of representations, along the (arbitrary) axes that correspond to the filter weights. This can also be seen by the fact that neurons in a ConvNet operate linearly over the input space, so any arbitrary rotation of that space is a no-op. This point was further argued in Intriguing properties of neural networks by Szegedy et al., where they perform a similar visualization along arbitrary directions in the representation space. Embedding the codes with t-SNEConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier. We can get a rough idea about the topology of this space by embedding images into two dimensions so that their low-dimensional representation has approximately equal distances than their high-dimensional representation. There are many embedding methods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of the best-known methods that consistently produces visually-pleasing results. To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid: t-SNE embedding of a set of images based on their CNN codes. Images that are nearby each other are also close in the CNN representation space, which implies that the CNN “sees” them as being very similar. Notice that the similarities are more often class-based and semantic rather than pixel and color-based. For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to t-SNE visualization of CNN codes. Occluding parts of the imageSuppose that a ConvNet classifies an image as a dog. How can we be certain that it’s actually picking up on the dog in the image as opposed to some contextual cues from the background or some other miscellaneous object? One way of investigating which part of the image some classification prediction is coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of the position of an occluder object. That is, we iterate over regions of the image, set a patch of the image to be all zero, and look at the probability of the class. We can visualize the probability as a 2-dimensional heat map. This approach has been used in Matthew Zeiler’s Visualizing and Understanding Convolutional Networks: Three input images (top). Notice that the occluder region is shown in grey. As we slide the occluder over the image we record the probability of the correct class and then visualize it as a heatmap (shown below each image). For instance, in the left-most image we see that the probability of Pomeranian plummets when the occluder covers the face of the dog, giving us some level of confidence that the dog’s face is primarily responsible for the high classification score. Conversely, zeroing out other parts of the image is seen to have relatively negligible impact. Visualizing the data gradient and friendsData Gradient. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps DeconvNet. Visualizing and Understanding Convolutional Networks Guided Backpropagation. Striving for Simplicity: The All Convolutional Net Reconstructing original images based on CNN CodesUnderstanding Deep Image Representations by Inverting Them How much spatial information is preserved?Do ConvNets Learn Correspondence? (tldr: yes) Plotting performance as a function of image attributesImageNet Large Scale Visual Recognition Challenge Fooling ConvNetsExplaining and Harnessing Adversarial Examples Comparing ConvNets to Human labelersWhat I learned from competing against a ConvNet on ImageNet]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>ConvNet</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[assignment1]]></title>
    <url>%2F2017%2F10%2F10%2Fassignment1%2F</url>
    <content type="text"><![CDATA[In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows: understand the basic Image Classification pipeline and the data-driven approach (train/predict stages) understand the train/val/test splits and the use of validation data for hyperparameter tuning. develop proficiency in writing efficient vectorized code with numpy implement and apply a k-Nearest Neighbor (kNN) classifier implement and apply a Multiclass Support Vector Machine (SVM) classifier implement and apply a Softmax classifier understand the differences and tradeoffs between these classifiers get a basic understanding of performance improvements from using higher-level representations than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features) SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machinethrough Terminal. Working locallyGet the code:Download the starter code here. [Optional] virtual environment:Once you have unzipped the starter code, you might want to create avirtual environmentfor the project. If you choose not to use a virtual environment, it is up to youto make sure that all dependencies for the code are installed on your machine.To set up a virtual environment, run the following: 1234567cd assignment1sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environment Download data:Once you have the starter code, you will need to download the CIFAR-10 dataset.Run the following from the assignment1 directory: 12cd cs231n/datasets./get_datasets.sh Start IPython:After you have the CIFAR-10 data, you should start the IPython notebook server from theassignment1 directory. If you are unfamiliar with IPython, you should read ourIPython tutorial. Working on TerminalWe have created a Terminal snapshot that is preconfigured for this assignment;you can find it here. Terminal allows you to work on the assignment from your browser. You can find a tutorial on how to use it here. Submitting your work:Whether you work on the assignment locally or using Terminal, once you are doneworking run the collectSubmission.sh script; this will produce a file calledassignment1.zip. Upload this file to your dropbox onthe courseworkpage for the course. Q1: k-Nearest Neighbor classifier (30 points)The IPython Notebook knn.ipynb will walk you through implementing the kNN classifier. Q2: Training a Support Vector Machine (30 points)The IPython Notebook svm.ipynb will walk you through implementing the SVM classifier. Q3: Implement a Softmax classifier (30 points)The IPython Notebook softmax.ipynb will walk you through implementing the Softmax classifier. Q4: Higher Level Representations: Image Features (10 points)The IPython Notebook features.ipynb will walk you through this exercise, in which you will examine the improvements gained by using higher-level representations as opposed to using raw pixel values. Q5: Bonus: Design your own features! (+10 points)In this assignment we provide you with Color Histograms and HOG features. To claim these bonus points, implement your own additional features from scratch, and using only numpy or scipy (no external dependencies). You will have to research different feature types to get ideas for what you might want to implement. Your new feature should help you improve the performance beyond what you got in Q4 if you wish to get these bonus points. If you come up with nice features we’ll feature them in the lecture. Q6: Cool Bonus: Do something extra! (+10 points)Implement, investigate or analyze something extra surrounding the topics in this assignment, and using the code you developed. For example, is there some other interesting question we could have asked? Is there any insightful visualization you can plot? Or maybe you can experiment with a spin on the loss function? If you try out something cool we’ll give you points and might feature your results in the lecture.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>SVM</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[assignment3]]></title>
    <url>%2F2017%2F10%2F10%2Fassignment3%2F</url>
    <content type="text"><![CDATA[In the previous assignment, you implemented and trained your own ConvNets.In this assignment, we will explore many of the ideas we have discussed inlectures. Specifically, you will: Reduce overfitting using dropout and data augmentation Combine models into ensembles to boost performance Use transfer learning to adapt a pretrained model to a new dataset Use data gradients to visualize saliency maps and create fooling images SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal. Working locallyGet the code:Download the starter code here. [Optional] virtual environment:Once you have unzipped the starter code, you might want to create avirtual environmentfor the project. If you choose not to use a virtual environment, it is up to youto make sure that all dependencies for the code are installed on your machine.To set up a virtual environment, run the following: 1234567cd assignment3sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environment You can reuse the virtual environment that you created for the first or secondassignment, but you will need to run pip install -r requirements.txt afteractivating it to install additional dependencies required by this assignment. Download data:Once you have the starter code, you will need to download the CIFAR-10 dataset,the TinyImageNet-100-A and TinyImageNet-100-B datasets, and pretrained modelsfor the TinyImageNet-100-A dataset. Run the following from the assignment3 directory: NOTE: After downloading and unpacking, the data and pretrained models willtake about 900MB of disk space. 1234cd cs231n/datasets./get_datasets.sh./get_tiny_imagenet_splits.sh./get_pretrained_models.sh Compile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command: 1python setup.py build_ext --inplace Start IPython:After you have downloaded the data and compiled the Cython extensions,you should start the IPython notebook server from theassignmen3 directory. If you are unfamiliar with IPython, you should read ourIPython tutorial. Working on TerminalWe have created a Terminal snapshot that is preconfigured for this assignment;you can find it here. Terminal allows you to work on the assignment from your browser. You can find a tutorial on how to use it here. Submitting your work:Whether you work on the assignment locally or using Terminal, once you are doneworking run the collectSubmission.sh script; this will produce a file calledassignment3.zip. Upload this file to your dropbox onthe courseworkpage for the course. Q1: Dropout and Data AugmentationIn the IPython notebook q1.ipynb you will implement dropout and several forms of data augmentation. This will allow you to reduce overfitting when training on a small subset of the CIFAR-10 dataset. Q2: TinyImageNet and Model EnsemblesIn the IPython notebook q2.ipynb we will introduce the TinyImageNet-100-Adataset. You will try to classify examples from this dataset by hand, and youwill combine pretrained models into an ensemble to boost your performance onthis dataset. Q3: Transfer LearningIn the IPython notebook q3.ipynb you will implement several forms of transferlearning. You will use adapt a pretrained model for TinyImageNet-100-A to achievereasonable performanc with minimal training time on the TinyImageNet-100-Bdataset. Q4: Visualizing and Breaking ConvNetsIn the IPython notebook q4.ipynb you will visualize data gradients and youwill generate images to fool a pretrained ConvNet.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>dropout</tag>
        <tag>data augmentation</tag>
        <tag>ensembles</tag>
        <tag>transfer learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-1]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-1%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Visualizing the loss function Optimization Strategy #1: Random Search Strategy #2: Random Local Search Strategy #3: Following the gradient Computing the gradient Numerically with finite differences Analytically with calculus Gradient descent Summary IntroductionIn the previous section we introduced two key components in context of the image classification task: A (parameterized) score function mapping the raw image pixels to class scores (e.g. a linear function) A loss function that measured the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM). Concretely, recall that the linear function had the form \( f(x_i, W) = W x_i \) and the SVM we developed was formulated as: $$L = \frac{1}{N} \sumi \sum{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(xi; W){y_i} + 1) \right] + \alpha R(W)$$ We saw that a setting of the parameters \(W\) that produced predictions for examples \(x_i\) consistent with their ground truth labels \(y_i\) would also have a very low loss \(L\). We are now going to introduce the third and last key component: optimization. Optimization is the process of finding the set of parameters \(W\) that minimize the loss function. Foreshadowing: Once we understand how these three core components interact, we will revisit the first component (the parameterized function mapping) and extend it to functions much more complicated than a linear mapping: First entire Neural Networks, and then Convolutional Neural Networks. The loss functions and the optimization process will remain relatively unchanged. Visualizing the loss functionThe loss functions we’ll look at in this class are usually defined over very high-dimensional spaces (e.g. in CIFAR-10 a linear classifier weight matrix is of size [10 x 3073] for a total of 30,730 parameters), making them difficult to visualize. However, we can still gain some intuitions about one by slicing through the high-dimensional space along rays (1 dimension), or along planes (2 dimensions). For example, we can generate a random weight matrix \(W\) (which corresponds to a single point in the space), then march along a ray and record the loss function value along the way. That is, we can generate a random direction \(W_1\) and compute the loss along this direction by evaluating \(L(W + a W_1)\) for different values of \(a\). This process generates a simple plot with the value of \(a\) as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss \( L(W + a W_1 + b W_2) \) as we vary \(a, b\). In a plot, \(a, b\) could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color: Loss function landscape for the Multiclass SVM (without regularization) for one single example (left,middle) and for a hundred examples (right) in CIFAR-10. Left: one-dimensional loss by only varying a. Middle, Right: two-dimensional loss slice, Blue = low loss, Red = high loss. Notice the piecewise-linear structure of the loss function. The losses for multiple examples are combined with average, so the bowl shape on the right is the average of many piece-wise linear bowls (such as the one in the middle). We can explain the piecewise-linear structure of the loss function by examining the math. For a single example we have: $$Li = \sum{j\neq y_i} \left[ \max(0, w_j^Txi - w{y_i}^Tx_i + 1) \right]$$ It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the \(\max(0,-)\) function) linear functions of \(W\). Moreover, each row of \(W\) (i.e. \(w_j\)) sometimes has a positive sign in front of it (when it corresponds to a wrong class for an example), and sometimes a negative sign (when it corresponds to the correct class for that example). To make this more explicit, consider a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regularization) becomes: $$\begin{align}L_0 = &amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\L_1 = &amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\L_2 = &amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\L = &amp; (L_0 + L_1 + L_2)/3\end{align}$$ Since these examples are 1-dimensional, the data \(x_i\) and weights \(w_j\) are numbers. Looking at, for instance, \(w_0\), some terms above are linear functions of \(w_0\) and each is clamped at zero. We can visualize this as follows: 1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 30,730-dimensional version of this shape. As an aside, you may have guessed from its bowl-shaped appearance that the SVM cost function is an example of a convex function There is a large amount of literature devoted to efficiently minimizing these types of functions, and you can also take a Stanford class on the topic ( convex optimization ). Once we extend our score functions \(f\) to Neural Networks our objective functions will become non-convex, and the visualizations above will not feature bowls but complex, bumpy terrains. Non-differentiable loss functions. As a technical note, you can also see that the kinks in the loss function (due to the max operation) technically make the loss function non-differentiable because at these kinks the gradient is not defined. However, the subgradient still exists and is commonly used instead. In this class will use the terms subgradient and gradient interchangeably. OptimizationTo reiterate, the loss function lets us quantify the quality of any particular set of weights W. The goal of optimization is to find W that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. For those of you coming to this class with previous experience, this section might seem odd since the working example we’ll use (the SVM loss) is a convex problem, but keep in mind that our goal is to eventually optimize Neural Networks where we can’t easily use any of the tools developed in the Convex Optimization literature. Strategy #1: A first very bad idea solution: Random searchSince it is so simple to check how good a given set of parameters W is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows: 12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines) In the code above, we see that we tried out several random weight vectors W, and some of them work better than others. We can take the best weights W found by this search and try it out on the test set: 1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555 With the best W this gives an accuracy of about 15.5%. Given that guessing classes completely at random achieves only 10%, that’s not a very bad outcome for a such a brain-dead random search solution! Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time. Our strategy will be to start with random weights and iteratively refine them over time to get lower loss Blindfolded hiker analogy. One analogy that you may find helpful going forward is to think of yourself as hiking on a hilly terrain with a blindfold on, and trying to reach the bottom. In the example of CIFAR-10, the hills are 30,730-dimensional, since the dimensions of W are 3073 x 10. At every point on the hill we achieve a particular loss (the height of the terrain). Strategy #2: Random Local SearchThe first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random \(W\), generate random perturbations \( \delta W \) to it and if the loss at the perturbed \(W + \delta W\) is lower, we will perform an update. The code for this procedure is as follows: 12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss) Using the same number of loss function evaluations as before (1000), this approach achieves test set classification accuracy of 21.4%. This is better, but still wasteful and computationally expensive. Strategy #3: Following the GradientIn the previous section we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). This direction will be related to the gradient of the loss function. In our hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest. In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is a generalization of slope for functions that don’t take a single number but a vector of numbers. Additionally, the gradient is just a vector of slopes (more commonly referred to as derivatives) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension. Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both. Computing the gradient numerically with finite differencesThe formula given above allows us to compute the gradient numerically. Here is a generic function that takes a function f, a vector x to evaluate the gradient on, and returns the gradient of f at x: 123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return grad Following the gradient formula we gave above, the code above iterates over all dimensions one by one, makes a small change h along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable grad holds the full gradient in the end. Practical considerations. Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: \( [f(x+h) - f(x-h)] / 2 h \) . See wiki for details. We can use the function given above to compute the gradient at any point and for any function. Lets compute the gradient for the CIFAR-10 loss function at some random point in the weight space: 12345678# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient The gradient tells us the slope of the loss function along every dimension, which we can use to make an update: 12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036 Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase. Effect of step size. The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. As we will see later in the course, choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network. In our blindfolded hill-descent analogy, we feel the hill below our feet sloping in some direction, but the step length we should take is uncertain. If we shuffle our feet carefully we can expect to make consistent but very small progress (this corresponds to having a small step size). Conversely, we can choose to make a large, confident step in an attempt to descend faster, but this may not pay off. As you can see in the code example above, at some point taking a bigger step gives a higher loss as we “overstep”. Visualizing the effect of step size. We start at some particular spot W and evaluate the gradient (or rather its negative - the white arrow) which tells us the direction of the steepest decrease in the loss function. Small steps are likely to lead to consistent but slow progress. Large steps can lead to better progress but are more risky. Note that eventually, for a large step size we will overshoot and make the loss worse. The step size (or as we will later call it - the learning rate) will become one of the most important hyperparameters that we will have to carefully tune. A problem of efficiency. You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. In our example we had 30730 parameters in total and therefore had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better. Computing the gradient analytically with CalculusThe numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of h, while the true gradient is defined as the limit as h goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check. Lets use the example of the SVM loss function for a single datapoint: $$Li = \sum{j\neq y_i} \left[ \max(0, w_j^Txi - w{y_i}^Tx_i + \Delta) \right]$$ We can differentiate the function with respect to the weights. For example, taking the gradient with respect to \(w_{y_i}\) we obtain: $$\nabla{w{y_i}} Li = - \left( \sum{j\neq y_i} \mathbb{1}(w_j^Txi - w{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$ where \(\mathbb{1}\) is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector \(x_i\) scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of \(W\) that corresponds to the correct class. For the other rows where \(j \neq y_i \) the gradient is: $$\nabla_{w_j} L_i = \mathbb{1}(w_j^Txi - w{y_i}^Tx_i + \Delta &gt; 0) x_i$$ Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update. Gradient DescentNow that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows: 12345# Vanilla Gradient Descentwhile True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # perform parameter update This simple loop is at the core of all Neural Network libraries. There are other ways of performing the optimization (e.g. LBFGS), but Gradient Descent is currently by far the most common and established way of optimizing Neural Network loss functions. Throughout the class we will put some bells and whistles on the details of this loop (e.g. the exact details of the update equation), but the core idea of following the gradient until we’re happy with the results will remain the same. Mini-batch gradient descent. In large-scale applications (such as the ILSVRC challenge), the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update: 123456# Vanilla Minibatch Gradient Descentwhile True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter update The reason this works well is that the examples in the training data are correlated. To see this, consider the extreme case where all 1.2 million images in ILSVRC are in fact made up of exact duplicates of only 1000 unique images (one for each class, or in other words 1200 identical copies of each image). Then it is clear that the gradients we would compute for all 1200 identical copies would all be the same, and when we average the data loss over all 1.2 million images we would get the exact same loss as if we only evaluated on a small subset of 1000. In practice of course, the dataset would not contain duplicate images, the gradient from a mini-batch is a good approximation of the gradient of the full objective. Therefore, much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates. The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent). This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times. Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent (i.e. mentions of MGD for “Minibatch Gradient Descent”, or BGD for “Batch gradient descent” are rare to see), where it is usually assumed that mini-batches are used. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2. Summary Summary of the information flow. The dataset of pairs of (x,y) is given and fixed. The weights start out as random numbers and can change. During the forward pass the score function computes class scores, stored in vector f. The loss function contains two components: The data loss computes the compatibility between the scores f and the labels y. The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent. In this section, We developed the intuition of the loss function as a high-dimensional optimization landscape in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped. We motivated the idea of optimizing the loss function withiterative refinement, where we start with a random set of weights and refine them step by step until the loss is minimized. We saw that the gradient of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of h used in computing the numerical gradient). We saw that the parameter update requires a tricky setting of the step size (or the learning rate) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections. We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient. We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop. Coming up: The core takeaway from this section is that the ability to compute the gradient of a loss function with respect to its weights (and have some intuitive understanding of it) is the most important skill needed to design, train and understand neural networks. In the next section we will develop proficiency in computing the gradient analytically using the chain rule, otherwise also referred to as backpropagation. This will allow us to efficiently optimize relatively arbitrary loss functions that express all kinds of Neural Networks, including Convolutional Neural Networks.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-2]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-2%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Simple expressions, interpreting the gradient Compound expressions, chain rule, backpropagation Intuitive understanding of backpropagation Modularity: Sigmoid example Backprop in practice: Staged computation Patterns in backward flow Gradients for vectorized operations Summary IntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks. Problem statement. The core problem studied in this section is as follows: We are given some function \(f(x)\) where \(x\) is a vector of inputs and we are interested in computing the gradient of \(f\) at \(x\) (i.e. \(\nabla f(x)\) ). Motivation. Recall that the primary reason we are interested in this problem is that in the specific case of Neural Networks, \(f\) will correspond to the loss function ( \(L\) ) and the inputs \(x\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \((x_i,y_i), i=1 \ldots N\) and the weights and biases \(W,b\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples \(x_i\), in practice we usually only compute the gradient for the parameters (e.g. \(W,b\)) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on \(x_i\) can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing. If you are coming to this class and you’re comfortable with deriving gradients with chain rule, we would still like to encourage you to at least skim this section, since it presents a rarely developed view of backpropagation as backward flow in real-valued circuits and any insights you’ll gain may help you throughout the class. Simple expressions and interpretation of the gradientLets start simple so that we can develop the notation and conventions for more complex expressions. Consider a simple multiplication function of two numbers \(f(x,y) = x y\). It is a matter of simple calculus to derive the partial derivative for either input: $$f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x$$ Interpretation. Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ A technical note is that the division sign on the left-hand sign is, unlike the division sign on the right-hand sign, not a division. Instead, this notation indicates that the operator \( \frac{d}{dx} \) is being applied to the function \(f\), and returns a different function (the derivative). A nice way to think about the expression above is that when \(h\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, if \(x = 4, y = -3\) then \(f(x,y) = -12\) and the derivative on \(x\) \(\frac{\partial f}{\partial x} = -3\). This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount. This can be seen by rearranging the above equation ( \( f(x + h) = f(x) + h \frac{df(x)}{dx} \) ). Analogously, since \(\frac{\partial f}{\partial y} = 4\), we expect that increasing the value of \(y\) by some very small amount \(h\) would also increase the output of the function (due to the positive sign), and by \(4h\). The derivative on each variable tells you the sensitivity of the whole expression on its value. As mentioned, the gradient \(\nabla f\) is the vector of partial derivatives, so we have that \(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]\). Even though the gradient is technically a vector, we will often use terms such as “the gradient on x” instead of the technically correct phrase “the partial derivative on x” for simplicity. We can also derive the derivatives for the addition operation: $$f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$$ that is, the derivative on both \(x,y\) is one regardless of what the values of \(x,y\) are. This makes sense, since increasing either \(x,y\) would increase the output of \(f\), and the rate of that increase would be independent of what the actual values of \(x,y\) are (unlike the case of multiplication above). The last function we’ll use quite a bit in the class is the max operation: $$f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x &gt;= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y &gt;= x)$$ That is, the (sub)gradient is 1 on the input that was larger and 0 on the other input. Intuitively, if the inputs are \(x = 4,y = 2\), then the max is 4, and the function is not sensitive to the setting of \(y\). That is, if we were to increase it by a tiny amount \(h\), the function would keep outputting 4, and therefore the gradient is zero: there is no effect. Of course, if we were to change \(y\) by a large amount (e.g. larger than 2), then the value of \(f\) would change, but the derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the \(\lim_{h \rightarrow 0}\) in its definition. Compound expressions with chain ruleLets now start to consider more complicated expressions that involve multiple composed functions, such as \(f(x,y,z) = (x + y) z\). This expression is still simple enough to differentiate directly, but we’ll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: \(q = x + y\) and \(f = q z\). Moreover, we know how to compute the derivatives of both expressions separately, as seen in the previous section. \(f\) is just multiplication of \(q\) and \(z\), so \(\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q\), and \(q\) is addition of \(x\) and \(y\) so \( \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1 \). However, we don’t necessarily care about the gradient on the intermediate value \(q\) - the value of \(\frac{\partial f}{\partial q}\) is not useful. Instead, we are ultimately interested in the gradient of \(f\) with respect to its inputs \(x,y,z\). The chain rule tells us that the correct way to “chain” these gradient expressions together is through multiplication. For example, \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x} \). In practice this is simply a multiplication of the two numbers that hold the two gradients. Lets see this with an example: 1234567891011121314# set some inputsx = -2; y = 5; z = -4# perform the forward passq = x + y # q becomes 3f = q * z # f becomes -12# perform the backward pass (backpropagation) in reverse order:# first backprop through f = q * zdfdz = q # df/dz = q, so gradient on z becomes 3dfdq = z # df/dq = z, so gradient on q becomes -4# now backprop through q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!dfdy = 1.0 * dfdq # dq/dy = 1 At the end we are left with the gradient in the variables [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of backpropagation. Going forward, we will want to use a more concise notation so that we don’t have to keep writing the df part. That is, for example instead of dfdq we would simply write dq, and always assume that the gradient is with respect to the final output. This computation can also be nicely visualized with a circuit diagram: -2-4x5-4y-43z3-4q+-121f* The real-valued “circuit” on left shows the visual representation of the computation. The forward pass computes values from inputs to output (shown in green). The backward pass then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit. Intuitive understanding of backpropagationNotice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs. This extra multiplication (for each input) due to the chain rule can turn a single and relatively useless gate into a cog in a complex circuit such as an entire neural network. Lets get an intuition for how this works by referring again to the example. The add gate received inputs [-2, 5] and computed output 3. Since the gate is computing the addition operation, its local gradient for both of its inputs is +1. The rest of the circuit computed the final value, which is -12. During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was -4. If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as “wanting” the output of the add gate to be lower (due to negative sign), and with a force of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 -4 = -4). Notice that this has the desired effect: If *x,y were to decrease (responding to their negative gradient) then the add gate’s output would decrease, which in turn makes the multiply gate’s output increase. Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher. Modularity: Sigmoid exampleThe gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point: $$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$ as we will see later in the class, this expression describes a 2-dimensional neuron (with inputs x and weights w) that uses the sigmoid activation function. But for now lets think of this very simply as just a function from inputs w,x to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more: $$f(x) = \frac{1}{x}\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = -1/x^2\\f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = 1\\f(x) = e^x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = e^x\\f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = a$$ Where the functions \(f_c, f_a\) translate the input by a constant of \(c\) and scale the input by a constant of \(a\), respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do need the gradients for the constants. \(c,a\). The full circuit then looks as follows: 2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.206.000.204.000.20+1.000.20+-1.00-0.20*-10.37-0.53exp1.37-0.53+10.731.001/x Example circuit for a 2D neuron with a sigmoid activation function. The inputs are [x0,x1] and the (learnable) weights of the neuron are [w0,w1,w2]. As we will see later, the neuron computes a dot product with the input and then its activation is softly squashed by the sigmoid function to be in range from 0 to 1. In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function \(\sigma(x)\). It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator): $$\sigma(x) = \frac{1}{1+e^{-x}} \\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)= \left( 1 - \sigma(x) \right) \sigma(x)$$ As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code: 123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuit Implementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables. The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort. Backprop in practice: Staged computationLets see this with another example. Suppose that we have a function of the form: $$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$ To be clear, this function is completely useless and it’s not clear why you would ever want to compute its gradient, except for the fact that it is a good example of backpropagation in practice. It is very important to stress that if you were to launch into performing the differentiation with respect to either \(x\) or \(y\), you would end up with very large and complex expressions. However, it turns out that doing so is completely unnecessary because we don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the forward pass of such expression: 123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8) Phew, by the end of the expression we have computed the forward pass. Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass (sigy, num, sigx, xpy, xpysqr, den, invden) we will have the same variable, but one that begins with a d, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to: 123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phew Notice a few things: Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them. Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add. Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit: 3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.002.002.00max-10.002.00+-20.001.002 An example circuit demonstrating the intuition behind the operations that backpropagation performs during the backward pass in order to compute the gradients on the inputs. Sum operation distributes gradients equally to all its inputs. Max operation routes the gradient to the higher input. Multiply gate takes the input activations, swaps them and multiplies by its gradient. Looking at the diagram above as an example, we can see that: The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged. The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero. The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00. Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted \(w^Tx_i\) (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples \(x_i\) by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases. Gradients for vectorized operationsThe above sections were concerned with single variables, but all concepts extend in a straight-forward manner to matrix and vector operations. However, one must pay closer attention to dimensions and transpose operations. Matrix-Matrix multiply gradient. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations: 123456789# forward passW = np.random.randn(5, 10)X = np.random.randn(10, 3)D = W.dot(X)# now suppose we had the gradient on D from above in the circuitdD = np.random.randn(*D.shape) # same shape as DdW = dD.dot(X.T) #.T gives the transpose of the matrixdX = W.T.dot(dD) Tip: use dimension analysis! Note that you do not need to remember the expressions for dW and dX because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights dW must be of the same size as W after it is computed, and that it must depend on matrix multiplication of X and dD (as is the case when both X,W are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, X is of size [10 x 3] and dD of size [5 x 3], so if we want dW and W has shape [5 x 10], then the only way of achieving this is with dD.dot(X.T), as shown above. Work with small, explicit examples. Some people may find it difficult at first to derive the gradient updates for some vectorized expressions. Our recommendation is to explicitly write out a minimal vectorized example, derive the gradient on paper and then generalize the pattern to its efficient, vectorized form. Erik Learned-Miller has also written up a longer related document on taking matrix/vector derivatives which you might find helpful. Find it here. Summary We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher. We discussed the importance of staged computation for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time. In the next section we will start to define Neural Networks, and backpropagation will allow us to efficiently compute the gradients on the connections of the neural network, with respect to a loss function. In other words, we’re now ready to train Neural Nets, and the most conceptually difficult part of this class is behind us! ConvNets will then be a small step away. References Automatic differentiation in machine learning: a survey]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的-m参数]]></title>
    <url>%2F2017%2F10%2F09%2Fpython%E7%9A%84-m%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[单个文件创建测试文件：E:\Code\ScoreCard&gt;路径下创建testm.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module ') 在终端上运行： 查看帮助文档： 1-m mod : run library module as a script (terminates option list) 创建模块测试-m作用文档目录结构： E:\Code\ScoreCard\package1&gt;路径下创建testm1.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module 1') E:\Code\ScoreCard\package2&gt;路径下创建testm2.py文件，内容如下：1234567import sysfrom package1 import testm1print(sys.path)if __name__ == "__main__": print ('This is main of module 2') 在终端上测试,效果如下: 结论123456-m 是把模块当作脚本来启动；直接启动是把run.py文件，所在的目录放到了sys.path属性中，见sys.path输出列表的第一个；模块启动是把你输入命令的目录（也就是当前路径），放到了sys.path属性中，见sys.path输出列表的第一个；当需要启动的py文件引用了一个模块。你需要注意：在启动的时候需要考虑sys.path中有没有你import的模块的路径！这个时候，到底是使用直接启动，还是以模块的启动？目的就是把import的那个模块的路径放到sys.path中。 参考here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks[3]]]></title>
    <url>%2F2017%2F10%2F08%2FNeural%20Networks%5B3%5D%2F</url>
    <content type="text"><![CDATA[Table of Contents: Gradient checks Sanity checks Babysitting the learning process Loss function Train/val accuracy Weights:Updates ratio Activation/Gradient distributions per layer Visualization Parameter updates First-order (SGD), momentum, Nesterov momentum Annealing the learning rate Second-order methods Per-parameter adaptive learning rates (Adagrad, RMSProp) Hyperparameter Optimization Evaluation Model Ensembles Summary Additional References LearningIn the previous sections we’ve discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters. Gradient ChecksIn theory, performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for: Use the centered formula. The formula you may have seen for the finite difference approximation when evaluating the numerical gradient looks as follows: $$\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}$$ where \(h\) is a very small number, in practice approximately 1e-5 or so. In practice, it turns out that it is much better to use the centered difference formula of the form: $$\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}$$ This requires you to evaluate the loss function twice to check every single dimension of the gradient (so it is about 2 times as expensive), but the gradient approximation turns out to be much more precise. To see this, you can use Taylor expansion of \(f(x+h)\) and \(f(x-h)\) and verify that the first formula has an error on order of \(O(h)\), while the second formula only has error terms on order of \(O(h^2)\) (i.e. it is a second order approximation). Use relative error for the comparison. What are the details of comparing the numerical gradient \(f’_n\) and analytic gradient \(f’_a\)? That is, how do we know if the two are not compatible? You might be temped to keep track of the difference \(\mid f’_a - f’_n \mid \) or its square and define the gradient check as failed if that difference is above a threshold. However, this is problematic. For example, consider the case where their difference is 1e-4. This seems like a very appropriate difference if the two gradients are about 1.0, so we’d consider the two gradients to match. But if the gradients were both on order of 1e-5 or lower, then we’d consider 1e-4 to be a huge difference and likely a failure. Hence, it is always more appropriate to consider the relative error: $$\frac{\mid f’_a - f’_n \mid}{\max(\mid f’_a \mid, \mid f’_n \mid)}$$ which considers their ratio of the differences to the ratio of the absolute values of both gradients. Notice that normally the relative error formula only includes one of the two terms (either one), but I prefer to max (or add) both to make it symmetric and to prevent dividing by zero in the case where one of the two is zero (which can often happen, especially with ReLUs). However, one must explicitly keep track of the case where both are zero and pass the gradient check in that edge case. In practice: relative error &gt; 1e-2 usually means the gradient is probably wrong 1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable 1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high. 1e-7 and less you should be happy. Also keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way. Conversely, an error of 1e-2 for a single differentiable function likely indicates incorrect gradient. Use double precision. A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. In my experience I’ve sometimes seen my relative errors plummet from 1e-2 to 1e-8 by switching to double precision. Stick around active range of floating point. It’s a good idea to read through “What Every Computer Scientist Should Know About Floating-Point Arithmetic”, as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then additionally dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. This is why I like to always print the raw numerical/analytic gradient, and make sure that the numbers you are comparing are not extremely small (e.g. roughly 1e-10 and smaller in absolute value is worrying). If they are you may want to temporarily scale your loss function up by a constant to bring them to a “nicer” range where floats are more dense - ideally on the order of 1.0, where your float exponent is 0. Kinks in the objective. One source of inaccuracy to be aware of during gradient checking is the problem of kinks. Kinks refer to non-differentiable parts of an objective function, introduced by functions such as ReLU (\(max(0,x)\)), or the SVM loss, Maxout neurons, etc. Consider gradient checking the ReLU function at \(x = -1e6\). Since \(x &lt; 0\), the analytic gradient at this point is exactly zero. However, the numerical gradient would suddenly compute a non-zero gradient because \(f(x+h)\) might cross over the kink (e.g. if \(h &gt; 1e-6\)) and introduce a non-zero contribution. You might think that this is a pathological case, but in fact this case can be very common. For example, an SVM for CIFAR-10 contains up to 450,000 \(max(0,x)\) terms because there are 50,000 examples and each example yields 9 terms to the objective. Moreover, a Neural Network with an SVM classifier will contain many more kinks due to ReLUs. Note that it is possible to know if a kink was crossed in the evaluation of the loss. This can be done by keeping track of the identities of all “winners” in a function of form \(max(x,y)\); That is, was x or y higher during the forward pass. If the identity of at least one winner changes when evaluating \(f(x+h)\) and then \(f(x-h)\), then a kink was crossed and the numerical gradient will not be exact. Use only few datapoints. One fix to the above problem of kinks is to use fewer datapoints, since loss functions that contain kinks (e.g. due to use of ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it is less likely for you to cross one when you perform the finite different approximation. Moreover, if your gradcheck for only ~2 or 3 datapoints then you would almost certainly gradcheck for an entire batch. Using very few datapoints also makes your gradient check faster and more efficient. Be careful with the step size h. It is not necessarily the case that smaller is better, because when \(h\) is much smaller, you may start running into numerical precision problems. Sometimes when the gradient doesn’t check, it is possible that you change \(h\) to be 1e-4 or 1e-6 and suddenly the gradient will be correct. This wikipedia article contains a chart that plots the value of h on the x-axis and the numerical gradient error on the y-axis. Gradcheck during a “characteristic” mode of operation. It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most “characteristic” point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn’t. For instance, an SVM with very small weight initialization will assign almost exactly zero scores to all datapoints and the gradients will exhibit a particular pattern across all datapoints. An incorrect implementation of the gradient could still produce this pattern and not generalize to a more characteristic mode of operation where some scores are larger than others. Therefore, to be safe it is best to use a short burn-in time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient. Don’t let the regularization overwhelm the data. It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently. One way to perform the latter is to hack the code to remove the data loss contribution. Another way is to increase the regularization strength so as to ensure that its effect is non-negligible in the gradient check, and that an incorrect implementation would be spotted. Remember to turn off dropout/augmentations. When performing gradient check, remember to turn off any non-deterministic effects in the network, such as dropout, random data augmentations, etc. Otherwise these can clearly introduce huge errors when estimating the numerical gradient. The downside of turning off these effects is that you wouldn’t be gradient checking them (e.g. it might be that dropout isn’t backpropagated correctly). Therefore, a better solution might be to force a particular random seed before evaluating both \(f(x+h)\) and \(f(x-h)\), and when evaluating the analytic gradient. Check only few dimensions. In practice the gradients can have sizes of million parameters. In these cases it is only practical to check some of the dimensions of the gradient and assume that the others are correct. Be careful: One issue to be careful with is to make sure to gradient check a few dimensions for every separate parameter. In some applications, people combine the parameters into a single large parameter vector for convenience. In these cases, for example, the biases could only take up a tiny number of parameters from the whole vector, so it is important to not sample at random but to take this into account and check that all parameters receive the correct gradients. Before learning: sanity checks Tips/TricksHere are a few sanity checks you might consider running before you plunge into expensive optimization: Look for correct loss at chance performance. Make sure you’re getting the loss you expect when you initialize with small parameters. It’s best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. For The Weston Watkins SVM, we expect all desired margins to be violated (since all scores are approximately zero), and hence expect a loss of 9 (since margin is 1 for each wrong class). If you’re not seeing these losses there might be issue with initialization. As a second sanity check, increasing the regularization strength should increase the loss Overfit a tiny subset of data. Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it’s also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints’ features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset. Babysitting the learning processThere are multiple useful quantities you should monitor during training of a neural network. These plots are the window into the training process and should be utilized to get intuitions about different hyperparameter settings and how they should be changed for more efficient learning. The x-axis of the plots below are always in units of epochs, which measure how many times every example has been seen during training in expectation (e.g. one epoch means that every example has been seen once). It is preferable to track epochs rather than iterations since the number of iterations depends on the arbitrary setting of batch size. Loss functionThe first quantity that is useful to track during training is the loss, as it is evaluated on the individual batches during the forward pass. Below is a cartoon diagram showing the loss over time, and especially what the shape might tell you about the learning rate: Left: A cartoon depicting the effects of different learning rates. With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much “energy” in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. Right: An example of a typical loss function over time, while training a small network on CIFAR-10 dataset. This loss function looks reasonable (it might indicate a slightly too small learning rate based on its speed of decay, but it’s hard to say), and also indicates that the batch size might be a little too low (since the cost is a little too noisy). The amount of “wiggle” in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high). Some people prefer to plot their loss functions in the log domain. Since learning progress generally takes an exponential form shape, the plot appears more as a slightly more interpretable straight line, rather than a hockey stick. Additionally, if multiple cross-validated models are plotted on the same loss graph, the differences between them become more apparent. Sometimes loss functions can look funny lossfunctions.tumblr.com. Train/Val accuracyThe second important quantity to track while training a classifier is the validation/training accuracy. This plot can give you valuable insights into the amount of overfitting in your model: The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it’s possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters. Ratio of weights:updatesThe last quantity you might want to track is the ratio of the update magnitudes to the value magnitudes. Note: updates, not the raw gradients (e.g. in vanilla sgd this would be the gradient multiplied by the learning rate). You might want to evaluate and track this ratio for every set of parameters independently. A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high. Here is a specific example: 123456# assume parameter vector W and its gradient vector dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # simple SGD updateupdate_scale = np.linalg.norm(update.ravel())W += update # the actual updateprint update_scale / param_scale # want ~1e-3 Instead of tracking the min or the max, some people prefer to compute and track the norm of the gradients and their updates instead. These metrics are usually correlated and often give approximately the same results. Activation / Gradient distributions per layerAn incorrect initialization can slow down or even completely stall the learning process. Luckily, this issue can be diagnosed relatively easily. One way to do so is to plot activation/gradient histograms for all layers of the network. Intuitively, it is not a good sign to see any strange distributions - e.g. with tanh neurons we would like to see a distribution of neuron activations between the full range of [-1,1], instead of seeing all neurons outputting zero, or all neurons being completely saturated at either -1 or 1. First-layer VisualizationsLastly, when one is working with image pixels it can be helpful and satisfying to plot the first-layer features visually: Examples of visualized weights for the first layer of a neural network. Left: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. Right: Nice, smooth, clean and diverse features are a good indication that the training is proceeding well. Parameter updatesOnce the analytic gradient is computed with backpropagation, the gradients are used to perform a parameter update. There are several approaches for performing the update, which we discuss next. We note that optimization for deep networks is currently a very active area of research. In this section we highlight some established and common techniques you may see in practice, briefly describe their intuition, but leave a detailed analysis outside of the scope of the class. We provide some further pointers for an interested reader. SGD and bells and whistlesVanilla update. The simplest form of update is to change the parameters along the negative gradient direction (since the gradient indicates the direction of increase, but we usually wish to minimize a loss function). Assuming a vector of parameters x and the gradient dx, the simplest update has the form: 12# Vanilla updatex += - learning_rate * dx where learning_rate is a hyperparameter - a fixed constant. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function. Momentum update is another approach that almost always enjoys better converge rates on deep networks. This update can be motivated from a physical perspective of the optimization problem. In particular, the loss can be interpreted as a the height of a hilly terrain (and therefore also to the potential energy since \(U = mgh\) and therefore \( U \propto h \) ). Initializing the parameters with random numbers is equivalent to setting a particle with zero initial velocity at some location. The optimization process can then be seen as equivalent to the process of simulating the parameter vector (i.e. a particle) as rolling on the landscape. Since the force on the particle is related to the gradient of potential energy (i.e. \(F = - \nabla U \) ), the force felt by the particle is precisely the (negative) gradient of the loss function. Moreover, \(F = ma \) so the (negative) gradient is in this view proportional to the acceleration of the particle. Note that this is different from the SGD update shown above, where the gradient directly integrates the position. Instead, the physics view suggests an update in which the gradient only directly influences the velocity, which in turn has an effect on the position: 123# Momentum updatev = mu * v - learning_rate * dx # integrate velocityx += v # integrate position Here we see an introduction of a v variable that is initialized at zero, and an additional hyperparameter (mu). As an unfortunate misnomer, this variable is in optimization referred to as momentum (its typical value is about 0.9), but its physical meaning is more consistent with the coefficient of friction. Effectively, this variable damps the velocity and reduces the kinetic energy of the system, or otherwise the particle would never come to a stop at the bottom of a hill. When cross-validated, this parameter is usually set to values such as [0.5, 0.9, 0.95, 0.99]. Similar to annealing schedules for learning rates (discussed later, below), optimization can sometimes benefit a little from momentum schedules, where the momentum is increased in later stages of learning. A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs. With Momentum update, the parameter vector will build up velocity in any direction that has consistent gradient. Nesterov Momentum is a slightly different version of the momentum update has recently been gaining popularity. It enjoys stronger theoretical converge guarantees for convex functions and in practice it also consistenly works slightly better than standard momentum. The core idea behind Nesterov momentum is that when the current parameter vector is at some position x, then looking at the momentum update above, we know that the momentum term alone (i.e. ignoring the second term with the gradient) is about to nudge the parameter vector by mu * v. Therefore, if we are about to compute the gradient, we can treat the future approximate position x + mu * v as a “lookahead” - this is a point in the vicinity of where we are soon going to end up. Hence, it makes sense to compute the gradient at x + mu * v instead of at the “old/stale” position x. Nesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this “looked-ahead” position. That is, in a slightly awkward notation, we would like to do the following: 1234x_ahead = x + mu * v# evaluate dx_ahead (the gradient at x_ahead instead of at x)v = mu * v - learning_rate * dx_aheadx += v However, in practice people prefer to express the update to look as similar to vanilla SGD or to the previous momentum update as possible. This is possible to achieve by manipulating the update above with a variable transform x_ahead = x + mu * v, and then expressing the update in terms of x_ahead instead of x. That is, the parameter vector we are actually storing is always the ahead version. The equations in terms of x_ahead (but renaming it back to x) then become: 123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form We recommend this further reading to understand the source of these equations and the mathematical formulation of Nesterov’s Accelerated Momentum (NAG): Advances in optimizing Recurrent Networks by Yoshua Bengio, Section 3.5. Ilya Sutskever’s thesis (pdf) contains a longer exposition of the topic in section 7.2 Annealing the learning rateIn training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function. Knowing when to decay the learning rate can be tricky: Decay it slowly and you’ll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can. There are three common types of implementing the learning rate decay: Step decay: Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving. Exponential decay. has the mathematical form \(\alpha = \alpha_0 e^{-k t}\), where \(\alpha_0, k\) are hyperparameters and \(t\) is the iteration number (but you can also use units of epochs). 1/t decay has the mathematical form \(\alpha = \alpha_0 / (1 + k t )\) where \(a_0, k\) are hyperparameters and \(t\) is the iteration number. In practice, we find that the step decay dropout is slightly preferable because the hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable than the hyperparameter \(k\). Lastly, if you can afford the computational budget, err on the side of slower decay and train for a longer time. Second order methodsA second, popular group of methods for optimization in context of deep learning is based on Newton’s method, which iterates the following update: $$x \leftarrow x - [H f(x)]^{-1} \nabla f(x)$$ Here, \(H f(x)\) is the Hessian matrix, which is a square matrix of second-order partial derivatives of the function. The term \(\nabla f(x)\) is the gradient vector, as seen in Gradient Descent. Intuitively, the Hessian describes the local curvature of the loss function, which allows us to perform a more efficient update. In particular, multiplying by the inverse Hessian leads the optimization to take more aggressive steps in directions of shallow curvature and shorter steps in directions of steep curvature. Note, crucially, the absence of any learning rate hyperparameters in the update formula, which the proponents of these methods cite this as a large advantage over first-order methods. However, the update above is impractical for most deep learning applications because computing (and inverting) the Hessian in its explicit form is a very costly process in both space and time. For instance, a Neural Network with one million parameters would have a Hessian matrix of size [1,000,000 x 1,000,000], occupying approximately 3725 gigabytes of RAM. Hence, a large variety of quasi-Newton methods have been developed that seek to approximate the inverse Hessian. Among these, the most popular is L-BFGS, which uses the information in the gradients over time to form the approximation implicitly (i.e. the full matrix is never computed). However, even after we eliminate the memory concerns, a large downside of a naive application of L-BFGS is that it must be computed over the entire training set, which could contain millions of examples. Unlike mini-batch SGD, getting L-BFGS to work on mini-batches is more tricky and an active area of research. In practice, it is currently not common to see L-BFGS or similar second-order methods applied to large-scale Deep Learning and Convolutional Neural Networks. Instead, SGD variants based on (Nesterov’s) momentum are more standard because they are simpler and scale more easily. Additional references: Large Scale Distributed Deep Networks is a paper from the Google Brain team, comparing L-BFGS and SGD variants in large-scale distributed optimization. SFO algorithm strives to combine the advantages of SGD with advantages of L-BFGS. Per-parameter adaptive learning rate methodsAll previous approaches we’ve discussed so far manipulated the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter. Many of these methods may still require other hyperparameter settings, but the argument is that they are well-behaved for a broader range of hyperparameter values than the raw learning rate. In this section we highlight some common adaptive methods you may encounter in practice: Adagrad is an adaptive learning rate method originally proposed by Duchi et al.. 123# Assume the gradient dx and parameter vector xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) Notice that the variable cache has size equal to the size of the gradient, and keeps track of per-parameter sum of squared gradients. This is then used to normalize the parameter update step, element-wise. Notice that the weights that receive high gradients will have their effective learning rate reduced, while weights that receive small or infrequent updates will have their effective learning rate increased. Amusingly, the square root operation turns out to be very important and without it the algorithm performs much worse. The smoothing term eps (usually set somewhere in range from 1e-4 to 1e-8) avoids division by zero. A downside of Adagrad is that in case of Deep Learning, the monotonic learning rate usually proves too aggressive and stops learning too early. RMSprop. RMSprop is a very effective, but currently unpublished adaptive learning rate method. Amusingly, everyone who uses this method in their work currently cites slide 29 of Lecture 6 of Geoff Hinton’s Coursera class. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. In particular, it uses a moving average of squared gradients instead, giving: 12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) Here, decay_rate is a hyperparameter and typical values are [0.9, 0.99, 0.999]. Notice that the x+= update is identical to Adagrad, but the cache variable is a “leaky”. Hence, RMSProp still modulates the learning rate of each weight based on the magnitudes of its gradients, which has a beneficial equalizing effect, but unlike Adagrad the updates do not get monotonically smaller. Adam. Adam is a recently proposed update that looks a bit like RMSProp with momentum. The (simplified) update looks as follows: 123m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)x += - learning_rate * m / (np.sqrt(v) + eps) Notice that the update looks exactly as RMSProp update, except the “smooth” version of the gradient m is used instead of the raw (and perhaps noisy) gradient vector dx. Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999. In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors m,v are both initialized and therefore biased at zero, before they fully “warm up”. We refer the reader to the paper for the details, or the course slides where this is expanded on. Additional References: Unit Tests for Stochastic Optimization proposes a series of tests as a standardized benchmark for stochastic optimization. Animations that may help your intuitions about the learning process dynamics. Left: Contours of a loss surface and time evolution of different optimization algorithms. Notice the “overshooting” behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill. Right: A visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed. Images credit: Alec Radford. Hyperparameter optimizationAs we’ve seen, training Neural Networks can involve many hyperparameter settings. The most common hyperparameters in context of Neural Networks include: the initial learning rate learning rate decay schedule (such as the decay constant) regularization strength (L2 penalty, dropout strength) But as saw, there are many more relatively less sensitive hyperparameters, for example in per-parameter adaptive learning methods, the setting of momentum and its schedule, etc. In this section we describe some additional tips and tricks for performing the hyperparameter search: Implementation. Larger Neural Networks typically require a long time to train, so performing hyperparameter search can take many days/weeks. It is important to keep this in mind since it influences the design of your code base. One particular design is to have a worker that continuously samples random hyperparameters and performs the optimization. During the training, the worker will keep track of the validation performance after every epoch, and writes a model checkpoint (together with miscellaneous training statistics such as the loss over time) to a file, preferably on a shared file system. It is useful to include the validation performance directly in the filename, so that it is simple to inspect and sort the progress. Then there is a second program which we will call a master, which launches or kills workers across a computing cluster, and may additionally inspect the checkpoints written by workers and plot their training statistics, etc. Prefer one validation fold to cross-validation. In most cases a single validation set of respectable size substantially simplifies the code base, without the need for cross-validation with multiple folds. You’ll hear people say they “cross-validated” a parameter, but many times it is assumed that they still only used a single validation set. Hyperparameter ranges. Search for hyperparameters on log scale. For example, a typical sampling of the learning rate would look as follows: learning_rate = 10 ** uniform(-6, 1). That is, we are generating a random number from a uniform distribution, but then raising it to the power of 10. The same strategy should be used for the regularization strength. Intuitively, this is because learning rate and regularization strength have multiplicative effects on the training dynamics. For example, a fixed change of adding 0.01 to a learning rate has huge effects on the dynamics if the learning rate is 0.001, but nearly no effect if the learning rate when it is 10. This is because the learning rate multiplies the computed gradient in the update. Therefore, it is much more natural to consider a range of learning rate multiplied or divided by some value, than a range of learning rate added or subtracted to by some value. Some parameters (e.g. dropout) are instead usually searched in the original scale (e.g. dropout = uniform(0,1)). Prefer random search to grid search. As argued by Bergstra and Bengio in Random Search for Hyper-Parameter Optimization, “randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid”. As it turns out, this is also usually easier to implement. Core illustration from Random Search for Hyper-Parameter Optimization by Bergstra and Bengio. It is very often the case that some of the hyperparameters matter much more than others (e.g. top hyperparam vs. left one in this figure). Performing random search rather than grid search allows you to much more precisely discover good values for the important ones. Careful with best values on border. Sometimes it can happen that you’re searching for a hyperparameter (e.g. learning rate) in a bad range. For example, suppose we use learning_rate = 10 ** uniform(-6, 1). Once we receive the results, it is important to double check that the final learning rate is not at the edge of this interval, or otherwise you may be missing more optimal hyperparameter setting beyond the interval. Stage your search from coarse to fine. In practice, it can be helpful to first search in coarse ranges (e.g. 10 ** [-6, 1]), and then depending on where the best results are turning up, narrow the range. Also, it can be helpful to perform the initial coarse search while only training for 1 epoch or even less, because many hyperparameter settings can lead the model to not learn at all, or immediately explode with infinite cost. The second stage could then perform a narrower search with 5 epochs, and the last stage could perform a detailed search in the final range for many more epochs (for example). Bayesian Hyperparameter Optimization is a whole area of research devoted to coming up with algorithms that try to more efficiently navigate the space of hyperparameters. The core idea is to appropriately balance the exploration - exploitation trade-off when querying the performance at different hyperparameters. Multiple libraries have been developed based on these models as well, among some of the better known ones are Spearmint, SMAC, and Hyperopt. However, in practical settings with ConvNets it is still relatively difficult to beat random search in a carefully-chosen intervals. See some additional from-the-trenches discussion here. Evaluation Model EnsemblesIn practice, one reliable approach to improving the performance of Neural Networks by a few percent is to train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). Moreover, the improvements are more dramatic with higher model variety in the ensemble. There are a few approaches to forming an ensemble: Same model, different initializations. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization. Top models discovered during cross-validation. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn’t require additional retraining of models after cross-validation Different checkpoints of a single model. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap. Running average of parameters during training. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the network’s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way you’re averaging the state of the network over last several iterations. You will find that this “smoothed” version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode. One disadvantage of model ensembles is that they take longer to evaluate on test example. An interested reader may find the recent work from Geoff Hinton on “Dark Knowledge” inspiring, where the idea is to “distill” a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective. SummaryTo train a Neural Network: Gradient check your implementation with a small batch of data and be aware of the pitfalls. As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights. The two recommended updates to use are either SGD+Nesterov Momentum or Adam. Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off. Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs) Form model ensembles for extra performance Additional References SGD tips and tricks from Leon Bottou Efficient BackProp (pdf) from Yann LeCun Practical Recommendations for Gradient-Based Training of DeepArchitectures from Yoshua Bengio]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linear-classify]]></title>
    <url>%2F2017%2F10%2F08%2FLinear%20Classification%2F</url>
    <content type="text"><![CDATA[Table of Contents: Intro to Linear classification Linear score function Interpreting a linear classifier Loss function Multiclass SVM Softmax classifier SVM vs Softmax Interactive Web Demo of Linear Classification Summary Linear ClassificationIn the last section we introduced the problem of Image Classification, which is the task of assigning a single label to an image from a fixed set of categories. Morever, we described the k-Nearest Neighbor (kNN) classifier which labels images by comparing them to (annotated) images from the training set. As we saw, kNN has a number of disadvantages: The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size. Classifying a test image is expensive since it requires a comparison to all training images. Overview. We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function. Parameterized mapping from images to label scoresThe first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. As before, let’s assume a training dataset of images \( x_i \in R^D \), each associated with a label \( y_i \). Here \( i = 1 \dots N \) and \( y_i \in { 1 \dots K } \). That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \(f: R^D \mapsto R^K\) that maps the raw image pixels to class scores. Linear classifier. In this module we will start out with arguably the simplest possible function, a linear mapping: $$f(x_i, W, b) = W x_i + b$$ In the above equation, we are assuming that the image \(x_i\) has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, \(x_i\) contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data \(x_i\). However, you will often hear people use the terms weights and parameters interchangeably. There are a few things to note: First, note that the single matrix multiplication \(W x_i\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of W. Notice also that we think of the input data \( (x_i, y_i) \) as given and fixed, but we have control over the setting of the parameters W,b. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes. An advantage of this approach is that the training data is used to learn the parameters W,b, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores. Lastly, note that to classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images. Foreshadowing: Convolutional Neural Networks will map image pixels to scores exactly as shown above, but the mapping ( f ) will be more complex and will contain more parameters. Interpreting a linear classifierNotice that a linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. Depending on precisely what values we set for these weights, the function has the capacity to like or dislike (depending on the sign of each weight) certain colors at certain positions in the image. For instance, you can imagine that the “ship” class might be more likely if there is a lot of blue on the sides of an image (which could likely correspond to water). You might expect that the “ship” classifier would then have a lot of positive weights across its blue channel weights (presence of blue increases score of ship), and negative weights in the red/green channels (presence of red/green decreases the score of ship). An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it’s looking at a dog. Analogy of images as high-dimensional points. Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points. Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing: Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores. As we saw above, every row of \(W\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \(W\), the corresponding line in the pixel space will rotate in different directions. The biases \(b\), on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in \( x_i = 0 \) would always give score of zero regardless of the weights, so all lines would be forced to cross the origin. Interpretation of linear classifiers as template matching.Another interpretation for the weights \(W\) is that each row of \(W\) corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance. Skipping ahead a bit: Example learned weights at the end of learning for CIFAR-10. Note that, for example, the ship template contains a lot of blue pixels as expected. This template will therefore give a high score once it is matched against images of ships on the ocean with an inner product. Additionally, note that the horse template seems to contain a two-headed horse, which is due to both left and right facing horses in the dataset. The linear classifier merges these two modes of horses in the data into a single template. Similarly, the car classifier seems to have merged several modes into a single template which has to identify cars from all sides, and of all colors. In particular, this template ended up being red, which hints that there are more red cars in the CIFAR-10 dataset than of any other color. The linear classifier is too weak to properly account for different-colored cars, but as we will see later neural networks will allow us to perform this task. Looking ahead a bit, a neural network will be able to develop intermediate neurons in its hidden layers that could detect specific car types (e.g. green car facing left, blue car facing front, etc.), and neurons on the next layer could combine these into a more accurate car score through a weighted sum of the individual car detectors. Bias trick. Before moving on we want to mention a common simplifying trick to representing the two parameters \(W,b\) as one. Recall that we defined the score function as: $$f(x_i, W, b) = W x_i + b$$ As we proceed through the material it is a little cumbersome to keep track of two sets of parameters (the biases \(b\) and weights \(W\)) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \(x_i\) with one additional dimension that always holds the constant \(1\) - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f(x_i, W) = W x_i$$ With our CIFAR-10 example, \(x_i\) is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and \(W\) is now [10 x 3073] instead of [10 x 3072]. The extra column that \(W\) now corresponds to the bias \(b\). An illustration might help clarify: Illustration of the bias trick. Doing a matrix multiplication and then adding a bias vector (left) is equivalent to adding a bias dimension with a constant of 1 to all input vectors and extending the weight matrix by 1 column - a bias column (right). Thus, if we preprocess our data by appending ones to all vectors we only have to learn a single matrix of weights instead of two matrices that hold the weights and the biases. Image data preprocessing. As a quick note, in the examples above we used the raw pixel values (which range from [0…255]). In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to center your data by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 … 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1]. Of these, zero mean centering is arguably more important but we will have to wait for its justification until we understand the dynamics of gradient descent. Loss functionIn the previous section we defined a function from the pixel values to class scores, which was parameterized by a set of weights \(W\). Moreover, we saw that we don’t have control over the data \( (x_i,y_i) \) (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data. For example, going back to the example image of a cat and its scores for the classes “cat”, “dog” and “ship”, we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well. Multiclass Support Vector Machine lossThere are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin \(\Delta\). Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good). Let’s now get more precise. Recall that for the i-th example we are given the pixels of image \( x_i \) and the label \( y_i \) that specifies the index of the correct class. The score function takes the pixels and computes the vector \( f(x_i, W) \) of class scores, which we will abbreviate to \(s\) (short for scores). For example, the score for the j-th class is the j-th element: \( s_j = f(x_i, W)_j \). The Multiclass SVM loss for the i-th example is then formalized as follows: $$Li = \sum{j\neq y_i} \max(0, sj - s{y_i} + \Delta)$$ Example. Lets unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \( s = [13, -7, 11]\), and that the first class is the true class (i.e. \(y_i = 0\)). Also assume that \(\Delta\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (\(j \neq y_i\)), so we get two terms: $$L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)$$ You can see that the first term gives zero since [-7 - 13 + 10] gives a negative number, which is then thresholded to zero with the \(max(0,-)\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; Any additional difference above the margin is clamped at zero with the max operation. The second term computes [11 - 13 + 10] which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \(y_i\) to be larger than the incorrect class scores by at least by \(\Delta\) (delta). If this is not the case, we will accumulate loss. Note that in this particular module we are working with linear score functions ( \( f(x_i; W) = W x_i \) ), so we can also rewrite the loss function in this equivalent form: $$Li = \sum{j\neq y_i} \max(0, w_j^T xi - w{y_i}^T x_i + \Delta)$$ where \(w_j\) is the j-th row of \(W\) reshaped as a column. However, this will not necessarily be the case once we start to consider more complex forms of the score function \(f\). A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero \(max(0,-)\) function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form \(max(0,-)^2\) that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation. The loss function quantifies our unhappiness with predictions on the training set The Multiclass Support Vector Machine “wants” the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible. Regularization. There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and \(L_i = 0\) for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters \( \lambda W \) where \( \lambda &gt; 1 \) will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30. In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty \(R(W)\). The most common regularization penalty is the L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters: $$R(W) = \sum_k\suml W{k,l}^2$$ In the expression above, we are summing up all the squared elements of \(W\). Notice that the regularization function is not a function of the data, it is only based on the weights. Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss \(L_i\) over all examples) and the regularization loss. That is, the full Multiclass SVM loss becomes: $$L = \underbrace{ \frac{1}{N} \sum_i Li }\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\$$ Or expanding this out in its full form: $$L = \frac{1}{N} \sumi \sum{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(xi; W){y_i} + \Delta) \right] + \lambda \sum_k\suml W{k,l}^2$$ Where \(N\) is the number of training examples. As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter \(\lambda\). There is no simple way of setting this hyperparameter and it is usually determined by cross-validation. In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested). The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector \(x = [1,1,1,1] \) and two weight vectors \(w_1 = [1,0,0,0]\), \(w_2 = [0.25,0.25,0.25,0.25] \). Then \(w_1^Tx = w_2^Tx = 1\) so both weight vectors lead to the same dot product, but the L2 penalty of \(w_1\) is 1.0 while the L2 penalty of \(w_2\) is only 0.25. Therefore, according to the L2 penalty the weight vector \(w_2\) would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in \(w_2\) are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting. Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights \(W\) but not the biases \(b\). However, in practice this often turns out to have a negligible effect. Lastly, note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of \(W = 0\). Code. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment The takeaway from this section is that the SVM loss takes one particular approach to measuring how consistent the predictions on training data are with the ground truth labels. Additionally, making good predictions on the training set is equivalent to minimizing the loss. All we have to do now is to come up with a way to find the weights that minimize the loss. Practical ConsiderationsSetting Delta. Note that we brushed over the hyperparameter \(\Delta\) and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to \(\Delta = 1.0\) in all cases. The hyperparameters \(\Delta\) and \(\lambda\) seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights \(W\) has direct effect on the scores (and hence also their differences): As we shrink all values inside \(W\) the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. \(\Delta = 1\), or \(\Delta = 100\)) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength \(\lambda\)). Relation to Binary Support Vector Machine. You may be coming to this class with previous experience with Binary Support Vector Machines, where the loss for the i-th example can be written as: $$L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)$$ where \(C\) is a hyperparameter, and \(y_i \in \{ -1,1 \} \). You can convince yourself that the formulation we presented in this section contains the binary SVM as a special case when there are only two classes. That is, if we only had two classes then the loss reduces to the binary SVM shown above. Also, \(C\) in this formulation and \(\lambda\) in our formulation control the same tradeoff and are related through reciprocal relation \(C \propto \frac{1}{\lambda}\). Aside: Optimization in primal. If you’re coming to this class with previous knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm, etc. In this class (as is the case with Neural Networks in general) we will always work with the optimization objectives in their unconstrained primal form. Many of these objectives are technically not differentiable (e.g. the max(x,y) function isn’t because it has a kink when x=y), but in practice this is not a problem and it is common to use a subgradient. Aside: Other Multiclass SVM formulations. It is worth noting that the Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes. Another commonly used form is the One-Vs-All (OVA) SVM which trains an independent binary SVM for each class vs. all other classes. Related, but less common to see in practice is also the All-vs-All (AVA) strategy. Our formulation follows the Weston and Watkins 1999 (pdf) version, which is a more powerful version than OVA (in the sense that you can construct multiclass datasets where this version can achieve zero data loss, but OVA cannot. See details in the paper if interested). The last formulation you may see is a Structured SVM, which maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class. Understanding the differences between these formulations is outside of the scope of the class. The version presented in these notes is a safe bet to use in practice, but the arguably simplest OVA strategy is likely to work just as well (as also argued by Rikin et al. 2004 in In Defense of One-Vs-All Classification (pdf)). Softmax classifierIt turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the Softmax classifier, which has a different loss function. If you’ve heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes. Unlike the SVM which treats the outputs \(f(x_i,W)\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \(f(x_i; W) = W x_i\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: $$Li = -\log\left(\frac{e^{f{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} Li = -f{y_i} + \log\sum_j e^{f_j}$$ where we are using the notation \(f_j\) to mean the j-th element of the vector of class scores \(f\). As before, the full loss for the dataset is the mean of \(L_i\) over all training examples together with a regularization term \(R(W)\). The function \(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the softmax function: It takes a vector of arbitrary real-valued scores (in \(z\)) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate. Information theory view. The cross-entropy between a “true” distribution \(p\) and an estimated distribution \(q\) is defined as: $$H(p,q) = - \sum_x p(x) \log q(x)$$ The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( \(q = e^{f_{y_i}} / \sum_j e^{f_j} \) as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \(p = [0, \ldots 1, \ldots, 0]\) contains a single 1 at the \(yi\) -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as \(H(p,q) = H(p) + D{KL}(p||q)\), and the entropy of the delta function \(p\) is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer. Probabilistic interpretation. Looking at the expression, we see that $$P(y_i \mid xi; W) = \frac{e^{f{y_i}}}{\sum_j e^{f_j} }$$ can be interpreted as the (normalized) probability assigned to the correct label \(y_i\) given the image \(x_i\) and parameterized by \(W\). To see this, remember that the Softmax classifier interprets the scores inside the output vector \(f\) as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term \(R(W)\) in the full loss function as coming from a Gaussian prior over the weight matrix \(W\), where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class. Practical issues: Numeric stability. When you’re writing code for computing the Softmax function in practice, the intermediate terms \(e^{f_{y_i}}\) and \(\sum_j e^{f_j}\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \(C\) and push it into the sum, we get the following (mathematically equivalent) expression: $$\frac{e^{f_{y_i}}}{\sum_j e^{fj}}= \frac{Ce^{f{y_i}}}{C\sum_j e^{fj}}= \frac{e^{f{y_i} + \log C}}{\sum_j e^{f_j + \log C}}$$ We are free to choose the value of \(C\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \(C\) is to set \(\log C = -\max_j f_j \). This simply states that we should shift the values inside the vector \(f\) so that the highest value is zero. In code: 123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer Possibly confusing naming conventions. To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses the cross-entropy loss. The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. In particular, note that technically it doesn’t make sense to talk about the “softmax loss”, since softmax is just the squashing function, but it is a relatively commonly used shorthand. SVM vs. SoftmaxA picture might help clarify the distinction between the Softmax and SVM classifiers: Example of the difference between the SVM and Softmax classifiers for one datapoint. In both cases we compute the same score vector f (e.g. by matrix multiplication in this section). The difference is in the interpretation of the scores in f: The SVM interprets these as class scores and its loss function encourages the correct class (class 2, in blue) to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low). The final loss for this example is 1.58 for the SVM and 1.04 (note this is 1.04 using the natural logarithm, not base 2 or base 10) for the Softmax classifier, but note that these numbers are not comparable; They are only meaningful in relation to loss computed within the same classifier and with the same data. Softmax classifier provides “probabilities” for each class. Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute “probabilities” for all labels. For example, given an image the SVM classifier might give you scores [12.5, 0.6, -23.0] for the classes “cat”, “dog” and “ship”. The softmax classifier can instead compute the probabilities of the three labels as [0.9, 0.09, 0.01], which allows you to interpret its confidence in each class. The reason we put the word “probabilities” in quotes, however, is that how peaky or diffuse these probabilities are depends directly on the regularization strength \(\lambda\) - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute: $$[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]$$ Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength \(\lambda\) was higher, the weights \(W\) would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute: $$[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]$$ where the probabilites are now more diffuse. Moreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength \(\lambda\), the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. In practice, SVM and Softmax are usually comparable. The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better. Compared to the Softmax classifier, the SVM is a more local objective, which could be thought of either as a bug or a feature. Consider an example that achieves the scores [10, -2, 3] and where the first class is correct. An SVM (e.g. with desired margin of \(\Delta = 1\)) will see that the correct class already has a score higher than the margin compared to the other classes and it will compute loss of zero. The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero. However, these scenarios are not equivalent to a Softmax classifier, which would accumulate a much higher loss for the scores [10, 9, 9] than for [10, -100, -100]. In other words, the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud. Interactive web demo We have written an interactive web demo to help your intuitions with linear classifiers. The demo visualizes the loss functions discussed in this section using a toy 3-way classification on 2D data. The demo also jumps ahead a bit and performs the optimization, which we will discuss in full detail in the next section. SummaryIn summary, We defined a score function from image pixels to class scores (in this section, a linear function that depends on weights W and biases b). Unlike kNN classifier, the advantage of this parametric approach is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with W, not an exhaustive comparison to every single training example. We introduced the bias trick, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix. We defined a loss function (we introduced two commonly used losses for linear classifiers: the SVM and the Softmax) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss. We now saw one way to take a dataset of images and map each one to class scores based on a set of parameters, and we saw two examples of loss functions that we can use to measure the quality of the predictions. But how do we efficiently determine the parameters that give the best (lowest) loss? This process is optimization, and it is the topic of the next section. Further ReadingThese readings are optional and contain pointers of interest. Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>linear</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convolutional Networks]]></title>
    <url>%2F2017%2F10%2F08%2FConvolutional%20Networks%2F</url>
    <content type="text"><![CDATA[Table of Contents: Architecture Overview ConvNet Layers Convolutional Layer Pooling Layer Normalization Layer Fully-Connected Layer Converting Fully-Connected Layers to Convolutional Layers ConvNet Architectures Layer Patterns Layer Sizing Patterns Case Studies (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet) Computational Considerations Additional References Convolutional Neural Networks (CNNs / ConvNets)Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network. Architecture OverviewRecall: Regular Neural Nets. As we saw in the previous chapter, Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores. Regular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectible size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting. 3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. Here is a visualization: Left: A regular 3-layer Neural Network. Right: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels). A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters. Layers used to build ConvNetsAs we described above, a simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture. Example Architecture: Overview. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail: INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B. CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters. RELU layer will apply an elementwise activation function, such as the \(max(0,x)\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]). POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12]. FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume. In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image. In summary: A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores) There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular) Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t) Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t) The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it’s difficult to visualize 3D volumes, we lay out each volume’s slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full web-based demo is shown in the header of our website. The architecture shown here is a tiny VGG Net, which we will discuss later. We now describe the individual layers and the details of their hyperparameters and their connectivities. Convolutional LayerThe Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting. Overview and intuition without brain stuff. Lets first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume. The brain view. If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter). We now discuss the details of the neuron connectivities, their arrangement in space, and their parameter sharing scheme. Local Connectivity. When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in space (along width and height), but always full along the entire depth of the input volume. Example 1. For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume. Example 2. Suppose an input volume had size [16x16x20]. Then using an example receptive field size of 3x3, every neuron in the Conv Layer would now have a total of 3*3*20 = 180 connections to the input volume. Notice that, again, the connectivity is local in space (e.g. 3x3), but full along the input depth (20). Left: An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input - see discussion of depth columns in text below. Right: The neurons from the Neural Network chapter remain unchanged: They still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially. Spatial arrangement. We have explained the connectivity of each neuron in the Conv Layer to the input volume, but we haven’t yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding. We discuss these next: First, the depth of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edged, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre). Second, we must specify the stride with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially. As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same). We can compute the spatial size of the output volume as a function of the input volume size (\(W\)), the receptive field size of the Conv Layer neurons (\(F\)), the stride with which they are applied (\(S\)), and the amount of zero padding used (\(P\)) on the border. You can convince yourself that the correct formula for calculating how many neurons “fit” is given by \((W - F + 2P)/S + 1\). For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more graphical example: Illustration of spatial arrangement. In this example there is only one spatial dimension (x-axis), one neuron with a receptive field size of F = 3, the input size is W = 5, and there is zero padding of P = 1. Left: The neuron strided across the input in stride of S = 1, giving output of size (5 - 3 + 2)/1+1 = 5. Right: The neuron uses stride of S = 2, giving output of size (5 - 3 + 2)/2+1 = 3. Notice that stride S = 3 could not be used since it wouldn’t fit neatly across the volume. In terms of the equation, this can be determined since (5 - 3 + 2) = 4 is not divisible by 3. The neuron weights are in this example [1,0,-1] (shown on very right), and its bias is zero. These weights are shared across all yellow neurons (see parameter sharing below). Use of zero-padding. In the example above on left, note that the input dimension was 5 and the output dimension was equal: also 5. This worked out so because our receptive fields were 3 and we used zero padding of 1. If there was no zero-padding used, then the output volume would have had spatial dimension of only 3, because that it is how many neurons would have “fit” across the original input. In general, setting zero padding to be \(P = (F - 1)/2\) when the stride is \(S = 1\) ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way and we will discuss the full reasons when we talk more about ConvNet architectures. Constraints on strides. Note again that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size \(W = 10\), no zero-padding is used \(P = 0\), and the filter size is \(F = 3\), then it would be impossible to use stride \(S = 2\), since \((W - F + 2P)/S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5\), i.e. not an integer, indicating that the neurons don’t “fit” neatly and symmetrically across the input. Therefore, this setting of the hyperparameters is considered to be invalid, and a ConvNet library could throw an exception or zero pad the rest to make it fit, or crop the input to make it fit, or something. As we will see in the ConvNet architectures section, sizing the ConvNets appropriately so that all the dimensions “work out” can be a real headache, which the use of zero-padding and some design guidelines will significantly alleviate. Real-world example. The Krizhevsky et al. architecture that won the ImageNet challenge in 2012 accepted images of size [227x227x3]. On the first Convolutional Layer, it used neurons with receptive field size \(F = 11\), stride \(S = 4\) and no zero padding \(P = 0\). Since (227 - 11)/4 + 1 = 55, and since the Conv layer had a depth of \(K = 96\), the Conv layer output volume had size [55x55x96]. Each of the 55*55*96 neurons in this volume was connected to a region of size [11x11x3] in the input volume. Moreover, all 96 neurons in each depth column are connected to the same [11x11x3] region of the input, but of course with different weights. As a fun aside, if you read the actual paper it claims that the input images were 224x224, which is surely incorrect because (224 - 11)/4 + 1 is quite clearly not an integer. This has confused many people in the history of ConvNets and little is known about what happened. My own best guess is that Alex used zero-padding of 3 extra pixels that he does not mention in the paper. Parameter Sharing. Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55*55*96 = 290,400 neurons in the first Conv Layer, and each has 11*11*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high. It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96*11*11*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice. Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a convolution of the neuron’s weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a filter (or a kernel), that is convolved with the input. Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 5555 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 5555 distinct locations in the Conv layer output volume. Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer. Numpy examples. To make the discussion above more concrete, lets express the same ideas but in code and with a specific example. Suppose that the input volume is a numpy array X. Then: A depth column (or a fibre) at position (x,y) would be the activations X[x,y,:]. A depth slice, or equivalently an activation map at depth d would be the activations X[:,:,d]. Conv Layer Example. Suppose that the input volume X has shape X.shape: (11,11,4). Suppose further that we use no zero padding (\(P = 0\)), that the filter size is \(F = 5\), and that the stride is \(S = 2\). The output volume would therefore have spatial size (11-5)/2+1 = 4, giving a volume with width and height of 4. The activation map in the output volume (call it V), would then look as follows (only some of the elements are computed in this example): V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0 V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0 V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0 V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0 Remember that in numpy, the operation * above denotes elementwise multiplication between the arrays. Notice also that the weight vector W0 is the weight vector of that neuron and b0 is the bias. Here, W0 is assumed to be of shape W0.shape: (5,5,4), since the filter size is 5 and the depth of the input volume is 4. Notice that at each point, we are computing the dot product as seen before in ordinary neural networks. Also, we see that we are using the same weight and bias (due to parameter sharing), and where the dimensions along the width are increasing in steps of 2 (i.e. the stride). To construct a second activation map in the output volume, we would have: V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1 V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1 V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1 V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1 V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 (example of going along y) V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 (or along both) where we see that we are indexing into the second depth dimension in V (at index 1) because we are computing the second activation map, and that a different set of parameters (W1) is now used. In the example above, we are for brevity leaving out some of the other operations the Conv Layer would perform to fill the other parts of the output array V. Additionally, recall that these activation maps are often followed elementwise through an activation function such as ReLU, but this is not shown here. Summary. To summarize, the Conv Layer: Accepts a volume of size \(W_1 \times H_1 \times D_1\) Requires four hyperparameters: Number of filters \(K\), their spatial extent \(F\), the stride \(S\), the amount of zero padding \(P\). Produces a volume of size \(W_2 \times H_2 \times D_2\) where: \(W_2 = (W_1 - F + 2P)/S + 1\) \(H_2 = (H_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry) \(D_2 = K\) With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases. In the output volume, the \(d\)-th depth slice (of size \(W_2 \times H_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias. A common setting of the hyperparameters is \(F = 3, S = 1, P = 1\). However, there are common conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures section below. Convolution Demo. Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize, all the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green)) are visualized with each depth slice stacked in rows. The input volume is of size \(W_1 = 5, H_1 = 5, D_1 = 3\), and the CONV layer parameters are \(K = 2, F = 3, S = 2, P = 1\). That is, we have two filters of size \(3 \times 3\), and they are applied with a stride of 2. Therefore, the output volume size has spatial size (5 - 3 + 2)/2 + 1 = 3. Moreover, notice that a padding of \(P = 1\) is applied to the input volume, making the outer border of the input volume zero. The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias. Implementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows: The local regions in the input image are stretched out into columns in an operation commonly called im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix X_col of im2col of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix W_row of size [96 x 363]. The result of a convolution is now equivalent to performing one large matrix multiply np.dot(W_row, X_col), which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location. The result must finally be reshaped back to its proper output dimension [55x55x96]. This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated multiple times in X_col. However, the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col idea can be reused to perform the pooling operation, which we discuss next. Backpropagation. The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters). This is easy to derive in the 1-dimensional case with a toy example (not expanded on for now). 1x1 convolution. As an aside, several papers use 1x1 convolutions, as first investigated by Network in Network. Some people are at first confused to see 1x1 convolutions especially when they come from signal processing background. Normally signals are 2-dimensional so 1x1 convolutions do not make sense (it’s just pointwise scaling). However, in ConvNets this is not the case because one must remember that we operate over 3-dimensional volumes, and that the filters always extend through the full depth of the input volume. For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels). Dilated convolutions. A recent development (e.g. see paper by Fisher Yu and Vladlen Koltun) is to introduce one more hyperparameter to the CONV layer called the dilation. So far we’ve only dicussed CONV filters that are contiguous. However, it’s possible to have filters that have spaces between each cell, called dilation. As an example, in one dimension a filter w of size 3 would compute over input x the following: w[0]*x[0] + w[1]*x[1] + w[2]*x[2]. This is dilation of 0. For dilation 1 the filter would instead compute w[0]*x[0] + w[1]*x[2] + w[2]*x[4]; In other words there is a gap of 1 between the applications. This can be very useful in some settings to use in conjunction with 0-dilated filters because it allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other than you can convince yourself that the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the effective receptive field of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker. Pooling LayerIt is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer: Accepts a volume of size \(W_1 \times H_1 \times D_1\) Requires two hyperparameters: their spatial extent \(F\), the stride \(S\), Produces a volume of size \(W_2 \times H_2 \times D_2\) where: \(W_2 = (W_1 - F)/S + 1\) \(H_2 = (H_1 - F)/S + 1\) \(D_2 = D_1\) Introduces zero parameters since it computes a fixed function of the input Note that it is not common to use zero-padding for Pooling layers It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \(F = 3, S = 2\) (also called overlapping pooling), and more commonly \(F = 2, S = 2\). Pooling sizes with larger receptive fields are too destructive. General pooling. In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice. Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square). Backpropagation. Recall from the backpropagation chapter that the backward pass for a max(x, y) operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called the switches) so that gradient routing is efficient during backpropagation. Getting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, Striving for Simplicity: The All Convolutional Net proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers. Normalization LayerMany types of normalization layers have been proposed for use in ConvNet architectures, sometimes with the intentions of implementing inhibition schemes observed in the biological brain. However, these layers have since fallen out of favor because in practice their contribution has been shown to be minimal, if any. For various types of normalizations, see the discussion in Alex Krizhevsky’s cuda-convnet library API). Fully-connected layerNeurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset. See the Neural Network section of the notes for more information. Converting FC layers to CONV layersIt is worth noting that the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it’s possible to convert between FC and CONV layers: For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing). Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with \(K = 4096\) that is looking at some input volume of size \(7 \times 7 \times 512\) can be equivalently expressed as a CONV layer with \(F = 7, P = 0, S = 1, K = 4096\). In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be \(1 \times 1 \times 4096\) since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer. FC-&gt;CONV conversion. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that we’ll see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above: Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \(F = 7\), giving output volume [1x1x4096]. Replace the second FC layer with a CONV layer that uses filter size \(F = 1\), giving output volume [1x1x4096] Replace the last FC layer similarly, with \(F=1\), giving final output [1x1x1000] Each of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix \(W\) in each FC layer into CONV layer filters. It turns out that this conversion allows us to “slide” the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass. For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], we’re now getting and entire 6x6 array of class scores across the 384x384 image. Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time. Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores. Lastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and height. An IPython Notebook on Net Surgery shows how to perform the conversion in practice, in code (using Caffe) ConvNet ArchitecturesWe have seen that Convolutional Networks are commonly made up of only three layer types: CONV, POOL (we assume Max pool unless stated otherwise) and FC (short for fully-connected). We will also explicitly write the RELU activation function as a layer, which applies elementwise non-linearity. In this section we discuss how these are commonly stacked together to form entire ConvNets. Layer PatternsThe most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern: INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N &gt;= 0 (and usually N &lt;= 3), M &gt;= 0, K &gt;= 0 (and usually K &lt; 3). For example, here are some common ConvNet architectures you may see that follow this pattern: INPUT -&gt; FC, implements a linear classifier. Here N = M = K = 0. INPUT -&gt; CONV -&gt; RELU -&gt; FC INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC. Here we see that there is a single CONV layer between every POOL layer. INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation. Prefer a stack of small filter CONV to one large receptive field CONV layer. Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages. First, the neurons would be computing a linear function over the input, while the three stacks of CONV layers contain non-linearities that make their features more expressive. Second, if we suppose that all the volumes have \(C\) channels, then it can be seen that the single 7x7 CONV layer would contain \(C \times (7 \times 7 \times C) = 49 C^2\) parameters, while the three 3x3 CONV layers would only contain \(3 \times (C \times (3 \times 3 \times C)) = 27 C^2\) parameters. Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation. Recent departures. It should be noted that the conventional paradigm of a linear list of layers has recently been challanged, in Google’s Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures. In practice: use whatever works best on ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t be a hero“: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school. Layer Sizing PatternsUntil now we’ve omitted mentions of common hyperparameters used in each of the layers in a ConvNet. We will first state the common rules of thumb for sizing the architectures and then follow the rules with a discussion of the notation: The input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512. The conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of \(S = 1\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when \(F = 3\), then using \(P = 1\) will retain the original size of the input. When \(F = 5\), \(P = 2\). For a general \(F\), it can be seen that \(P = (F - 1) / 2\) preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image. The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. \(F = 2\)), and with a stride of 2 (i.e. \(S = 2\)). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance. Reducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired. Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise. Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly. Compromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filer sizes of 11x11 and stride of 4. Case studiesThere are several architectures in the field of Convolutional Networks that have a name. The most common are: LeNet. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc. AlexNet. The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer). ZF Net. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler &amp; Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller. GoogLeNet. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4. VGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters. ResNet. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016). VGGNet in detail.Lets break down the VGGNet in more detail as a case study. The whole VGGNet is composed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max pooling with stride 2 (and no padding). We can write out the size of the representation at each step of the processing and keep track of both the representation size and the total number of weights: 12345678910111213141516171819202122232425INPUT: [224x224x3] memory: 224*224*3=150K weights: 0CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864POOL2: [112x112x64] memory: 112*112*64=800K weights: 0CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456POOL2: [56x56x128] memory: 56*56*128=400K weights: 0CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824POOL2: [28x28x256] memory: 28*28*256=200K weights: 0CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296POOL2: [14x14x512] memory: 14*14*512=100K weights: 0CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296POOL2: [7x7x512] memory: 7*7*512=25K weights: 0FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)TOTAL params: 138M parameters As is common with Convolutional Networks, notice that most of the memory (and also compute time) is used in the early CONV layers, and that most of the parameters are in the last FC layers. In this particular case, the first FC layer contains 100M weights, out of a total of 140M. Computational ConsiderationsThe largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about 12GB of memory. There are three major sources of memory to keep track of: From the intermediate volume sizes: These are the raw number of activations at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below. From the parameter sizes: These are the numbers that hold the network parameters, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so. Every ConvNet implementation has to maintain miscellaneous memory, such as the image data batches, perhaps their augmented versions, etc. Once you have a rough estimate of the total number of values (for activations, gradients, and misc), the number should be converted to size in GB. Take the number of values, multiply by 4 to get the raw number of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and then divide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your network doesn’t fit, a common heuristic to “make it fit” is to decrease the batch size, since most of the memory is usually consumed by the activations. Additional ResourcesAdditional resources related to implementation: Soumith benchmarks for CONV performance ConvNetJS CIFAR-10 demo allows you to play with ConvNet architectures and see the results and computations in real time, in the browser. Caffe, one of the popular ConvNet libraries. State of the art ResNets in Torch7]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks[2]]]></title>
    <url>%2F2017%2F10%2F08%2FNeural%20Networks%5B2%5D%2F</url>
    <content type="text"><![CDATA[Table of Contents: Setting up the data and the model Data Preprocessing Weight Initialization Batch Normalization Regularization (L2/L1/Maxnorm/Dropout) Loss functions Summary Setting up the data and the modelIn the previous section we introduced a model of a Neuron, which computes a dot product following a non-linearity, and Neural Networks that arrange neurons into layers. Together, these choices define the new form of the score function, which we have extended from the simple linear mapping that we have seen in the Linear Classification section. In particular, a Neural Network performs a sequence of linear mappings with interwoven non-linearities. In this section we will discuss additional design choices regarding data preprocessing, weight initialization, and loss functions. Data PreprocessingThere are three common forms of data preprocessing a data matrix X, where we will assume that X is of size [N x D] (N is the number of data, D is their dimensionality). Mean subtraction is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. In numpy, this operation would be implemented as: X -= np.mean(X, axis = 0). With images specifically, for convenience it can be common to subtract a single value from all pixels (e.g. X -= np.mean(X)), or to do so separately across the three color channels. Normalization refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered: (X /= np.std(X, axis = 0)). Another form of this preprocessing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm.In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional preprocessing step. Common data preprocessing pipeline. Left: Original toy, 2-dimensional input data. Middle: The data is zero-centered by subtracting the mean in each dimension. The data cloud is now centered around the origin. Right: Each dimension is additionally scaled by its standard deviation. The red lines indicate the extent of the data - they are of unequal length in the middle, but of equal length on the right. PCA and Whitening is another form of preprocessing. In this process, the data is first centered as described above. Then, we can compute the covariance matrix that tells us about the correlation structure in the data: 123# Assume input data matrix X of size [N x D]X -= np.mean(X, axis = 0) # zero-center the data (important)cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix The (i,j) element of the data covariance matrix contains the covariance between i-th and j-th dimension of the data. In particular, the diagonal of this matrix contains the variances. Furthermore, the covariance matrix is symmetric and positive semi-definite. We can compute the SVD factorization of the data covariance matrix: 1U,S,V = np.linalg.svd(cov) where the columns of U are the eigenvectors and S is a 1-D array of the singular values. To decorrelate the data, we project the original (but zero-centered) data into the eigenbasis: 1Xrot = np.dot(X, U) # decorrelate the data Notice that the columns of U are a set of orthonormal vectors (norm of 1, and orthogonal to each other), so they can be regarded as basis vectors. The projection therefore corresponds to a rotation of the data in X so that the new axes are the eigenvectors. If we were to compute the covariance matrix of Xrot, we would see that it is now diagonal. A nice property of np.linalg.svd is that in its returned value U, the eigenvector columns are sorted by their eigenvalues. We can use this to reduce the dimensionality of the data by only using the top few eigenvectors, and discarding the dimensions along which the data has no variance. This is also sometimes refereed to as Principal Component Analysis (PCA) dimensionality reduction: 1Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100] After this operation, we would have reduced the original dataset of size [N x D] to one of size [N x 100], keeping the 100 dimensions of the data that contain the most variance. It is very often the case that you can get very good performance by training linear classifiers or neural networks on the PCA-reduced datasets, obtaining savings in both space and time. The last transformation you may see in practice is whitening. The whitening operation takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale. The geometric interpretation of this transformation is that if the input data is a multivariable gaussian, then the whitened data will be a gaussian with zero mean and identity covariance matrix. This step would take the form: 123# whiten the data:# divide by the eigenvalues (which are square roots of the singular values)Xwhite = Xrot / np.sqrt(S + 1e-5) Warning: Exaggerating noise. Note that we’re adding 1e-5 (or a small constant) to prevent division by zero. One weakness of this transformation is that it can greatly exaggerate the noise in the data, since it stretches all dimensions (including the irrelevant dimensions of tiny variance that are mostly noise) to be of equal size in the input. This can in practice be mitigated by stronger smoothing (i.e. increasing 1e-5 to be a larger number). PCA / Whitening. Left: Original toy, 2-dimensional input data. Middle: After performing PCA. The data is centered at zero and then rotated into the eigenbasis of the data covariance matrix. This decorrelates the data (the covariance matrix becomes diagonal). Right: Each dimension is additionally scaled by the eigenvalues, transforming the data covariance matrix into the identity matrix. Geometrically, this corresponds to stretching and squeezing the data into an isotropic gaussian blob. We can also try to visualize these transformations with CIFAR-10 images. The training set of CIFAR-10 is of size 50,000 x 3072, where every image is stretched out into a 3072-dimensional row vector. We can then compute the [3072 x 3072] covariance matrix and compute its SVD decomposition (which can be relatively expensive). What do the computed eigenvectors look like visually? An image might help: Left:An example set of 49 images. 2nd from Left: The top 144 out of 3072 eigenvectors. The top eigenvectors account for most of the variance in the data, and we can see that they correspond to lower frequencies in the images. 2nd from Right: The 49 images reduced with PCA, using the 144 eigenvectors shown here. That is, instead of expressing every image as a 3072-dimensional vector where each element is the brightness of a particular pixel at some location and channel, every image above is only represented with a 144-dimensional vector, where each element measures how much of each eigenvector adds up to make up the image. In order to visualize what image information has been retained in the 144 numbers, we must rotate back into the “pixel” basis of 3072 numbers. Since U is a rotation, this can be achieved by multiplying by U.transpose()[:144,:], and then visualizing the resulting 3072 numbers as the image. You can see that the images are slightly blurrier, reflecting the fact that the top eigenvectors capture lower frequencies. However, most of the information is still preserved. Right: Visualization of the “white” representation, where the variance along every one of the 144 dimensions is squashed to equal length. Here, the whitened 144 numbers are rotated back to image pixel basis by multiplying by U.transpose()[:144,:]. The lower frequencies (which accounted for most variance) are now negligible, while the higher frequencies (which account for relatively little variance originally) become exaggerated. In practice. We mention PCA/Whitening in these notes for completeness, but these transformations are not used with Convolutional Networks. However, it is very important to zero-center the data, and it is common to see normalization of every pixel as well. Common pitfall. An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test). Weight InitializationWe have seen how to construct a Neural Network architecture, and how to preprocess the data. Before we can begin to train the network we have to initialize its parameters. Pitfall: all zero initialization. Lets start with what we should not do. Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same. Small random numbers. Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for one weight matrix might look like W = 0.01* np.random.randn(D,H), where randn samples from a zero mean, unit standard deviation gaussian. With this formulation, every neuron’s weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice. Warning: It’s not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the “gradient signal” flowing backward through a network, and could become a concern for deep networks. Calibrating the variances with 1/sqrt(n). One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron’s weight vector as: w = np.random.randn(n) / sqrt(n), where n is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence. The sketch of the derivation is as follows: Consider the inner product \(s = \sum_i^n w_i x_i\) between the weights \(w\) and input \(x\), which gives the raw activation of a neuron before the non-linearity. We can examine the variance of \(s\): $$\begin{align}\text{Var}(s) &amp;= \text{Var}(\sum_i^n w_ix_i) \\&amp;= \sum_i^n \text{Var}(w_ix_i) \\&amp;= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\&amp;= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\&amp;= \left( n \text{Var}(w) \right) \text{Var}(x)\end{align}$$ where in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so \(E[x_i] = E[w_i] = 0\). Note that this is not generally the case: For example ReLU units will have a positive mean. In the last step we assumed that all \(w_i, x_i\) are identically distributed. From this derivation we can see that if we want \(s\) to have the same variance as all of its inputs \(x\), then during initialization we should make sure that the variance of every weight \(w\) is \(1/n\). And since \(\text{Var}(aX) = a^2\text{Var}(X)\) for a random variable \(X\) and a scalar \(a\), this implies that we should draw from unit gaussian and then scale it by \(a = \sqrt{1/n}\), to make its variance \(1/n\). This gives the initialization w = np.random.randn(n) / sqrt(n). A similar analysis is carried out in Understanding the difficulty of training deep feedforward neural networks by Glorot et al. In this paper, the authors end up recommending an initialization of the form \( \text{Var}(w) = 2/(n{in} + n {out}) \) where \(n{in}, n{out}\) are the number of units in the previous layer and the next layer. This is motivated by based on a compromise and an equivalent analysis of the backpropagated gradients. A more recent paper on this topic, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by He et al., derives an initialization specifically for ReLU neurons, reaching the conclusion that the variance of neurons in the network should be \(2.0/n\). This gives the initialization w = np.random.randn(n) * sqrt(2.0/n), and is the current recommendation for use in practice in the specific case of neural networks with ReLU neurons. Sparse initialization. Another way to address the uncalibrated variances problem is to set all weight matrices to zero, but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10. Initializing the biases. It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant value such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use 0 bias initialization. In practice, the current recommendation is to use ReLU units and use the w = np.random.randn(n) * sqrt(2.0/n), as discussed in He et al.. Batch Normalization. A recently developed technique by Ioffe and Szegedy called Batch Normalization alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. The core observation is that this is possible because normalization is a simple differentiable operation. In the implementation, applying this technique usually amounts to insert the BatchNorm layer immediately after fully connected layers (or convolutional layers, as we’ll soon see), and before non-linearities. We do not expand on this technique here because it is well described in the linked paper, but note that it has become a very common practice to use Batch Normalization in neural networks. In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. Neat! RegularizationThere are several ways of controlling the capacity of Neural Networks to prevent overfitting: L2 regularization is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight \(w\) in the network, we add the term \(\frac{1}{2} \lambda w^2\) to the objective, where \(\lambda\) is the regularization strength. It is common to see the factor of \(\frac{1}{2}\) in front because then the gradient of this term with respect to the parameter \(w\) is simply \(\lambda w\) instead of \(2 \lambda w\). The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. As we discussed in the Linear Classification section, due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather that some of its inputs a lot. Lastly, notice that during gradient descent parameter update, using the L2 regularization ultimately means that every weight is decayed linearly: W += -lambda * W towards zero. L1 regularization is another relatively common form of regularization, where for each weight \(w\) we add the term \(\lambda \mid w \mid\) to the objective. It is possible to combine the L1 regularization with the L2 regularization: \(\lambda_1 \mid w \mid + \lambda_2 w^2\) (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1. Max norm constraints. Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \(\vec{w}\) of every neuron to satisfy \(\Vert \vec{w} \Vert_2 &lt; c\). Typical values of \(c\) are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded. Dropout is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting (pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability \(p\) (a hyperparameter), or setting it to zero otherwise. Figure taken from the Dropout paper that illustrates the idea. During training, Dropout can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. (However, the exponential number of possible sampled networks are not independent because they share the parameters.) During testing there is no dropout applied, with the interpretation of evaluating an averaged prediction across the exponentially-sized ensemble of all sub-networks (more about ensembles in the next section). Vanilla dropout in an example 3-layer Neural Network would be implemented as follows: 123456789101112131415161718192021222324""" Vanilla Dropout: Not recommended implementation (see notes below) """p = 0.5 # probability of keeping a unit active. higher = less dropoutdef train_step(X): """ X contains the data """ # forward pass for example 3-layer neural network H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &lt; p # first dropout mask H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &lt; p # second dropout mask H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # backward pass: compute gradients... (not shown) # perform parameter update... (not shown) def predict(X): # ensembled forward pass H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations out = np.dot(W3, H2) + b3 In the code above, inside the train_step function we have performed dropout twice: on the first hidden layer and on the second hidden layer. It is also possible to perform dropout right on the input layer, in which case we would also create a binary mask for the input X. The backward pass remains unchanged, but of course has to take into account the generated masks U1,U2. Crucially, note that in the predict function we are not dropping anymore, but we are performing a scaling of both hidden layer outputs by \(p\). This is important because at test time all neurons see all their inputs, so we want the outputs of neurons at test time to be identical to their expected outputs at training time. For example, in case of \(p = 0.5\), the neurons must halve their outputs at test time to have the same output as they had during training time (in expectation). To see this, consider an output of a neuron \(x\) (before dropout). With dropout, the expected output from this neuron will become \(px + (1-p)0\), because the neuron’s output will be set to zero with probability \(1-p\). At test time, when we keep the neuron always active, we must adjust \(x \rightarrow px\) to keep the same expected output. It can also be shown that performing this attenuation at test time can be related to the process of iterating over all the possible binary masks (and therefore all the exponentially many sub-networks) and computing their ensemble prediction. The undesirable property of the scheme presented above is that we must scale the activations by \(p\) at test time. Since test-time performance is so critical, it is always preferable to use inverted dropout, which performs the scaling at train time, leaving the forward pass at test time untouched. Additionally, this has the appealing property that the prediction code can remain untouched when you decide to tweak where you apply dropout, or if at all. Inverted dropout looks as follows: 12345678910111213141516171819202122232425""" Inverted Dropout: Recommended implementation example.We drop and scale at train time and don't do anything at test time."""p = 0.5 # probability of keeping a unit active. higher = less dropoutdef train_step(X): # forward pass for example 3-layer neural network H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # first dropout mask. Notice /p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # second dropout mask. Notice /p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # backward pass: compute gradients... (not shown) # perform parameter update... (not shown) def predict(X): # ensembled forward pass H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 There has a been a large amount of research after the first introduction of dropout that tries to understand the source of its power in practice, and its relation to the other regularization techniques. Recommended further reading for an interested reader includes: Dropout paper by Srivastava et al. 2014. Dropout Training as Adaptive Regularization: “we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix”. Theme of noise in forward pass. Dropout falls into a more general category of methods that introduce stochastic behavior in the forward pass of the network. During testing, the noise is marginalized over analytically (as is the case with dropout when multiplying by \(p\)), or numerically (e.g. via sampling, by performing several forward passes with different random decisions and then averaging over them). An example of other research in this direction includes DropConnect, where a random set of weights is instead set to zero during forward pass. As foreshadowing, Convolutional Neural Networks also take advantage of this theme with methods such as stochastic pooling, fractional pooling, and data augmentation. We will go into details of these methods later. Bias regularization. As we already mentioned in the Linear Classification section, it is not common to regularize the bias parameters because they do not interact with the data through multiplicative interactions, and therefore do not have the interpretation of controlling the influence of a data dimension on the final objective. However, in practical applications (and with proper data preprocessing) regularizing the bias rarely leads to significantly worse performance. This is likely because there are very few bias terms compared to all the weights, so the classifier can “afford to” use the biases if it needs them to obtain a better data loss. Per-layer regularization. It is not very common to regularize different layers to different amounts (except perhaps the output layer). Relatively few results regarding this idea have been published in the literature. In practice: It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of \(p = 0.5\) is a reasonable default, but this can be tuned on validation data. Loss functionsWe have discussed the regularization loss part of the objective, which can be seen as penalizing some measure of complexity of the model. The second part of an objective is the data loss, which in a supervised learning problem measures the compatibility between a prediction (e.g. the class scores in classification) and the ground truth label. The data loss takes the form of an average over the data losses for every individual example. That is, \(L = \frac{1}{N} \sum_i L_i\) where \(N\) is the number of training data. Lets abbreviate \(f = f(x_i; W)\) to be the activations of the output layer in a Neural Network. There are several types of problems you might want to solve in practice: Classification is the case that we have so far discussed at length. Here, we assume a dataset of examples and a single correct label (out of a fixed set) for each example. One of two most commonly seen cost functions in this setting are the SVM (e.g. the Weston Watkins formulation): $$Li = \sum{j\neq y_i} \max(0, fj - f{y_i} + 1)$$ As we briefly alluded to, some people report better performance with the squared hinge loss (i.e. instead using \(\max(0, fj - f{y_i} + 1)^2\)). The second common choice is the Softmax classifier that uses the cross-entropy loss: $$Li = -\log\left(\frac{e^{f{y_i}}}{ \sum_j e^{f_j} }\right)$$ Problem: Large number of classes. When the set of labels is very large (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), it may be helpful to use Hierarchical Softmax (see one explanation here (pdf)). The hierarchical softmax decomposes labels into a tree. Each label is then represented as a path along the tree, and a Softmax classifier is trained at every node of the tree to disambiguate between the left and right branch. The structure of the tree strongly impacts the performance and is generally problem-dependent. Attribute classification. Both losses above assume that there is a single correct answer \(y_i\). But what if \(y_i\) is a binary vector where every example may or may not have a certain attribute, and where the attributes are not exclusive? For example, images on Instagram can be thought of as labeled with a certain subset of hashtags from a large set of all hashtags, and an image may contain multiple. A sensible approach in this case is to build a binary classifier for every single attribute independently. For example, a binary classifier for each category independently would take the form: $$L_i = \sumj \max(0, 1 - y{ij} f_j)$$ where the sum is over all categories \(j\), and \(y_{ij}\) is either +1 or -1 depending on whether the i-th example is labeled with the j-th attribute, and the score vector \(f_j\) will be positive when the class is predicted to be present and negative otherwise. Notice that loss is accumulated if a positive example has score less than +1, or when a negative example has score greater than -1. An alternative to this loss would be to train a logistic regression classifier for every attribute independently. A binary logistic regression classifier has only two classes (0,1), and calculates the probability of class 1 as: $$P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)$$ Since the probabilities of class 1 and 0 sum to one, the probability for class 0 is \(P(y = 0 \mid x; w, b) = 1 - P(y = 1 \mid x; w,b)\). Hence, an example is classified as a positive example (y = 1) if \(\sigma (w^Tx + b) &gt; 0.5\), or equivalently if the score \(w^Tx +b &gt; 0\). The loss function then maximizes the log likelihood of this probability. You can convince yourself that this simplifies to: $$L_i = \sumj y{ij} \log(\sigma(fj)) + (1 - y{ij}) \log(1 - \sigma(f_j))$$ where the labels \(y_{ij}\) are assumed to be either 1 (positive) or 0 (negative), and \(\sigma(\cdot)\) is the sigmoid function. The expression above can look scary but the gradient on \(f\) is in fact extremely simple and intuitive: \(\partial{L_i} / \partial{fj} = y{ij} - \sigma(f_j)\) (as you can double check yourself by taking the derivatives). Regression is the task of predicting real-valued quantities, such as the price of houses or the length of something in an image. For this task, it is common to compute the loss between the predicted quantity and the true answer and then measure the L2 squared norm, or L1 norm of the difference. The L2 norm squared would compute the loss for a single example of the form: $$L_i = \Vert f - y_i \Vert_2^2$$ The reason the L2 norm is squared in the objective is that the gradient becomes much simpler, without changing the optimal parameters since squaring is a monotonic operation. The L1 norm would be formulated by summing the absolute value along each dimension: $$L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$$ where the sum \(\sumj\) is a sum over all dimensions of the desired prediction, if there is more than one quantity being predicted. Looking at only the j-th dimension of the i-th example and denoting the difference between the true and the predicted value by \(\delta{ij}\), the gradient for this dimension (i.e. \(\partial{L_i} / \partial{fj}\)) is easily derived to be either \(\delta{ij}\) with the L2 norm, or \(sign(\delta_{ij})\). That is, the gradient on the score will either be directly proportional to the difference in the error, or it will be fixed and only inherit the sign of the difference. Word of caution: It is important to note that the L2 loss is much harder to optimize than a more stable loss such as Softmax. Intuitively, it requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations). Notice that this is not the case with Softmax, where the precise value of each score is less important: It only matters that their magnitudes are appropriate. Additionally, the L2 loss is less robust because outliers can introduce huge gradients. When faced with a regression problem, first consider if it is absolutely inadequate to quantize the output into bins. For example, if you are predicting star rating for a product, it might work much better to use 5 independent classifiers for ratings of 1-5 stars instead of a regression loss. Classification has the additional benefit that it can give you a distribution over the regression outputs, not just a single output with no indication of its confidence. If you’re certain that classification is not appropriate, use the L2 but be careful: For example, the L2 is more fragile and applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea. When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible. Structured prediction. The structured loss refers to a case where the labels can be arbitrary structures such as graphs, trees, or other complex objects. Usually it is also assumed that the space of structures is very large and not easily enumerable. The basic idea behind the structured SVM loss is to demand a margin between the correct structure \(y_i\) and the highest-scoring incorrect structure. It is not common to solve this problem as a simple unconstrained optimization problem with gradient descent. Instead, special solvers are usually devised so that the specific simplifying assumptions of the structure space can be taken advantage of. We mention the problem briefly but consider the specifics to be outside of the scope of the class. SummaryIn summary: The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature Initialize the weights by drawing them from a gaussian distribution with standard deviation of \(\sqrt{2/n}\), where \(n\) is the number of inputs to the neuron. E.g. in numpy: w = np.random.randn(n) * sqrt(2.0/n). Use L2 regularization and dropout (the inverted version) Use batch normalization We discussed different tasks you might want to perform in practice, and the most common loss functions for each task We’ve now preprocessed the data and set up and initialized the model. In the next section we will look at the learning process and its dynamics.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks[1]]]></title>
    <url>%2F2017%2F10%2F08%2FNeural%20Networks%5B1%5D%2F</url>
    <content type="text"><![CDATA[Table of Contents: Quick intro without brain analogies Modeling one neuron Biological motivation and connections Single neuron as a linear classifier Commonly used activation functions Neural Network architectures Layer-wise organization Example feed-forward computation Representational power Setting number of layers and their sizes Summary Additional references Quick introIt is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula \( s = W x \), where \(W\) was a matrix and \(x\) was an input column vector containing all pixel data of the image. In the case of CIFAR-10, \(x\) is a [3072x1] column vector, and \(W\) is a [10x3072] matrix, so that the output scores is a vector of 10 class scores. An example neural network would instead compute \( s = W_2 \max(0, W_1 x) \). Here, \(W_1\) could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function \(max(0,-) \) is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix \(W_2\) would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters \(W_2, W_1\) are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation). A three-layer neural network could analogously look like \( s = W_3 \max(0, W_2 \max(0, W_1 x)) \), where all of \(W_3, W_2, W_1\) are parameters to be learned. The sizes of the intermediate hidden vectors are hyperparameters of the network and we’ll see how we can set them later. Lets now look into how we can interpret these computations from the neuron/network perspective. Modeling one neuronThe area of Neural Networks has originally been primarily inspired by the goal of modeling biological neural systems, but has since diverged and become a matter of engineering and achieving good results in Machine Learning tasks. Nonetheless, we begin our discussion with a very brief and high-level description of the biological system that a large portion of this area has been inspired by. Biological motivation and connectionsThe basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10^14 - 10^15 synapses. The diagram below shows a cartoon drawing of a biological neuron (left) and a common mathematical model (right). Each neuron receives input signals from its dendrites and produces output signals along its (single) axon. The axon eventually branches out and connects via synapses to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. \(x_0\)) interact multiplicatively (e.g. \(w_0 x_0\)) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. \(w_0\)). The idea is that the synaptic strengths (the weights \(w\)) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function \(f\), which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the sigmoid function \(\sigma\), since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1. We will see details of these activation functions later in this section. A cartoon drawing of a biological neuron (left) and its mathematical model (right). An example code for forward-propagating a single neuron might look as follows: 1234567class Neuron(object): # ... def forward(inputs): """ assume inputs and weights are 1-D numpy arrays and bias is a number """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rate In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function), in this case the sigmoid \(\sigma(x) = 1/(1+e^{-x})\). We will go into more details about different activation functions at the end of this section. Coarse model. It’s important to stress that this model of a biological neuron is very coarse: For example, there are many different types of neurons, each with different properties. The dendrites in biological neurons perform complex nonlinear computations. The synapses are not just a single weight, they’re a complex non-linear dynamical system. The exact timing of the output spikes in many systems in known to be important, suggesting that the rate code approximation may not hold. Due to all these and many other simplifications, be prepared to hear groaning sounds from anyone with some neuroscience background if you draw analogies between Neural Networks and real brains. See this review (pdf), or more recently this review if you are interested. Single neuron as a linear classifierThe mathematical form of the model Neuron’s forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to “like” (activation near one) or “dislike” (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron’s output, we can turn a single neuron into a linear classifier: Binary Softmax classifier. For example, we can interpret \(\sigma(\sum_iw_ix_i + b)\) to be the probability of one of the classes \(P(y_i = 1 \mid x_i; w) \). The probability of the other class would be \(P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) \), since they must sum to one. With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5. Binary SVM classifier. Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine. Regularization interpretation. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights \(w\) towards zero after every parameter update. A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers) Commonly used activation functionsEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice: Left: Sigmoid non-linearity squashes real numbers to range between [0,1] Right: The tanh non-linearity squashes real numbers to range between [-1,1]. Sigmoid. The sigmoid non-linearity has the mathematical form \(\sigma(x) = 1 / (1 + e^{-x})\) and is shown in the image above on the left. As alluded to in the previous section, it takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks: Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn. Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. \(x &gt; 0\) elementwise in \(f = w^Tx + b\))), then the gradient on the weights \(w\) will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression \(f\)). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above. Tanh. The tanh non-linearity is shown on the image above on the right. It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: \( \tanh(x) = 2 \sigma(2x) -1 \). Left: Rectified Linear Unit (ReLU) activation function, which is zero when x &amp;lt 0 and then linear with slope 1 when x &amp;gt 0. Right: A plot from Krizhevsky et al. (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit. ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function \(f(x) = \max(0, x)\). In other words, the activation is simply thresholded at zero (see image above on the left). There are several pros and cons to using the ReLUs: (+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form. (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero. (-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue. Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes \(f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x) \) where \(\alpha\) is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear. Maxout. Other types of units have been proposed that do not have the functional form \(f(w^Tx + b)\) where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function \(\max(w_1^Tx+b_1, w_2^Tx + b_2)\). Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have \(w_1, b_1 = 0\)). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters. This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so. TLDR: “What neuron type should I use?“ Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout. Neural Network architectures Layer-wise organizationNeural Networks as neurons in graphs. Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. In other words, the outputs of some neurons can become inputs to other neurons. Cycles are not allowed since that would imply an infinite loop in the forward pass of a network. Instead of an amorphous blobs of connected neurons, Neural Network models are often organized into distinct layers of neurons. For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections. Below are two example Neural Network topologies that use a stack of fully-connected layers: Left: A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. Right: A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer. Naming conventions. Notice that when we say N-layer neural network, we do not count the input layer. Therefore, a single-layer neural network describes a network with no hidden layers (input directly mapped to output). In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks. You may also hear these networks interchangeably referred to as “Artificial Neural Networks” (ANN) or “Multi-Layer Perceptrons” (MLP). Many people do not like the analogies between Neural Networks and real brains and prefer to refer to neurons as units. Output layer. Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function (or you can think of them as having a linear identity activation function). This is because the last output layer is usually taken to represent the class scores (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression). Sizing neural networks. The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture: The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters. The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters. To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module. Example feed-forward computationRepeated matrix multiplications interwoven with activation function. One of the primary reasons that Neural Networks are organized into layers is that this structure makes it very simple and efficient to evaluate Neural Networks using matrix vector operations. Working with the example three-layer neural network in the diagram above, the input would be a [3x1] vector. All connection strengths for a layer can be stored in a single matrix. For example, the first hidden layer’s weights W1 would be of size [4x3], and the biases for all units would be in the vector b1, of size [4x1]. Here, every single neuron has its weights in a row of W1, so the matrix vector multiplication np.dot(W1,x) evaluates the activations of all neurons in that layer. Similarly, W2 would be a [4x4] matrix that stores the connections of the second hidden layer, and W3 a [1x4] matrix for the last (output) layer. The full forward pass of this 3-layer neural network is then simply three matrix multiplications, interwoven with the application of the activation function: 123456# forward-pass of a 3-layer neural network:f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)x = np.random.randn(3, 1) # random input vector of three numbers (3x1)h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)out = np.dot(W3, h2) + b3 # output neuron (1x1) In the above code, W1,W2,W3,b1,b2,b3 are the learnable parameters of the network. Notice also that instead of having a single input column vector, the variable x could hold an entire batch of training data (where each input example would be a column of x) and then all examples would be efficiently evaluated in parallel. Notice that the final Neural Network layer usually doesn’t have an activation function (e.g. it represents a (real-valued) class score in a classification setting). The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function. Representational powerOne way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network? It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function \(f(x)\) and some \(\epsilon &gt; 0\), there exists a Neural Network \(g(x)\) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that \( \forall x, \mid f(x) - g(x) \mid &lt; \epsilon \). In other words, the neural network can approximate any continuous function. If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the “sum of indicator bumps” function \(g(x) = \sum_i c_i \mathbb{1}(a_i &lt; x &lt; b_i)\) where \(a,b,c\) are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning. Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal. As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain. The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading: Deep Learning book in press by Bengio, Goodfellow, Courville, in practicular Chapter 6.4. Do Deep Nets Really Need to be Deep? FitNets: Hints for Thin Deep Nets Setting number of layers and their sizesHow do we decide on what architecture to use when faced with a practical problem? Should we use no hidden layers? One hidden layer? Two hidden layers? How large should each layer be? First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. For example, suppose we had a binary classification problem in two dimensions. We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers: Larger Neural Networks can represent more complicated functions. The data are shown as circles colored by their class, and the decision regions by a trained neural network are shown underneath. You can play with these examples in this ConvNetsJS demo. In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set. Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting. However, this is incorrect - there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods to control overfitting instead of the number of neurons. The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It’s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization. To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings: The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization. You can play with these examples in this ConvNetsJS demo. The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting. SummaryIn summary, We introduced a very coarse model of a biological neuron We discussed several types of activation functions that are used in practice, with ReLU being the most common choice We introduced Neural Networks where neurons are connected with Fully-Connected layers where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected. We saw that this layered architecture enables very efficient evaluation of Neural Networks based on matrix multiplications interwoven with the application of the activation function. We saw that that Neural Networks are universal function approximators, but we also discussed the fact that this property has little to do with their ubiquitous use. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice. We discussed the fact that larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections. Additional References deeplearning.net tutorial with Theano ConvNetJS demos for intuitions Michael Nielsen’s tutorials]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neural-networks-case-study]]></title>
    <url>%2F2017%2F10%2F08%2FNeural%20Networks%20Case%20Study%2F</url>
    <content type="text"><![CDATA[Table of Contents: Generating some data Training a Softmax Linear Classifier Initialize the parameters Compute the class scores Compute the loss Computing the analytic gradient with backpropagation Performing a parameter update Putting it all together: Training a Softmax Classifier Training a Neural Network Summary In this section we’ll walk through a complete implementation of a toy Neural Network in 2 dimensions. We’ll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network. As we’ll see, this extension is surprisingly simple and very few changes are necessary. Generating some dataLets generate a classification dataset that is not easily linearly separable. Our favorite example is the spiral dataset, which can be generated as follows: 12345678910111213N = 100 # number of points per classD = 2 # dimensionalityK = 3 # number of classesX = np.zeros((N*K,D)) # data matrix (each row = single example)y = np.zeros(N*K, dtype='uint8') # class labelsfor j in xrange(K): ix = range(N*j,N*(j+1)) r = np.linspace(0.0,1,N) # radius t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] y[ix] = j# lets visualize the data:plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral) The toy spiral data consists of three classes (blue, red, yellow) that are not linearly separable. Normally we would want to preprocess the dataset so that each feature has zero mean and unit standard deviation, but in this case the features are already in a nice range from -1 to 1, so we skip this step. Training a Softmax Linear Classifier Initialize the parametersLets first train a Softmax classifier on this classification dataset. As we saw in the previous sections, the Softmax classifier has a linear score function and uses the cross-entropy loss. The parameters of the linear classifier consist of a weight matrix W and a bias vector b for each class. Lets first initialize these parameters to be random numbers: 123# initialize parameters randomlyW = 0.01 * np.random.randn(D,K)b = np.zeros((1,K)) Recall that we D = 2 is the dimensionality and K = 3 is the number of classes. Compute the class scoresSince this is a linear classifier, we can compute all class scores very simply in parallel with a single matrix multiplication: 12# compute class scores for a linear classifierscores = np.dot(X, W) + b In this example we have 300 2-D points, so after this multiplication the array scores will have size [300 x 3], where each row gives the class scores corresponding to the 3 classes (blue, red, yellow). Compute the lossThe second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. Intuitively, we want the correct class to have a higher score than the other classes. When this is the case, the loss should be low and otherwise the loss should be high. There are many ways to quantify this intuition, but in this example lets use the cross-entropy loss that is associated with the Softmax classifier. Recall that if \(f\) is the array of class scores for a single example (e.g. array of 3 numbers here), then the Softmax classifier computes the loss for that example as: $$Li = -\log\left(\frac{e^{f{y_i}}}{ \sum_j e^{f_j} }\right)$$ We can see that the Softmax classifier interprets every element of \(f\) as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class. Note how this expression works: this quantity is always between 0 and 1. When the probability of the correct class is very small (near 0), the loss will go towards (postiive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because \(log(1) = 0\). Hence, the expression for \(L_i\) is low when the correct class probability is high, and it’s very high when it is low. Recall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization: $$L = \underbrace{ \frac{1}{N} \sum_i Li }\text{data loss} + \underbrace{ \frac{1}{2} \lambda \sum_k\suml W{k,l}^2 }_\text{regularization loss} \\$$ Given the array of scores we’ve computed above, we can compute the loss. First, the way to obtain the probabilities is straight forward: 1234# get unnormalized probabilitiesexp_scores = np.exp(scores)# normalize them for each exampleprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) We now have an array probs of size [300 x 3], where each row now contains the class probabilities. In particular, since we’ve normalized them every row now sums to one. We can now query for the log probabilities assigned to the correct classes in each example: 1corect_logprobs = -np.log(probs[range(num_examples),y]) The array correct_logprobs is a 1D array of just the probabilities assigned to the correct classes for each example. The full loss is then the average of these log probabilities and the regularization loss: 1234# compute the loss: average cross-entropy loss and regularizationdata_loss = np.sum(corect_logprobs)/num_examplesreg_loss = 0.5*reg*np.sum(W*W)loss = data_loss + reg_loss In this code, the regularization strength \(\lambda\) is stored inside the reg. The convenience factor of 0.5 multiplying the regularization will become clear in a second. Evaluating this in the beginning (with random parameters) might give us loss = 1.1, which is np.log(1.0/3), since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with loss = 0 as the absolute lower bound. But the lower the loss is, the higher are the probabilities assigned to the correct classes for all examples. Computing the Analytic Gradient with BackpropagationWe have a way of evaluating the loss, and now we have to minimize it. We’ll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Lets introduce the intermediate variable \(p\), which is a vector of the (normalized) probabilities. The loss for one example is: $$p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} Li =-\log\left(p{y_i}\right)$$ We now wish to understand how the computed scores inside \(f\) should change to decrease the loss \(L_i\) that this example contributes to the full objective. In other words, we want to derive the gradient \( \partial L_i / \partial f_k \). The loss \(L_i\) is computed from \(p\), which in turn depends on \(f\). It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out: $$\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)$$ Notice how elegant and simple this expression is. Suppose the probabilities we computed were p = [0.2, 0.3, 0.5], and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be df = [0.2, -0.7, 0.5]. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector f (the scores of the incorrect classes) leads to an increased loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss \(L_i\), which makes sense. All of this boils down to the following code. Recall that probs stores the probabilities of all classes (as rows) for each example. To get the gradient on the scores, which we call dscores, we proceed as follows: 123dscores = probsdscores[range(num_examples),y] -= 1dscores /= num_examples Lastly, we had that scores = np.dot(X, W) + b, so armed with the gradient on scores (stored in dscores), we can now backpropagate into W and b: 123dW = np.dot(X.T, dscores)db = np.sum(dscores, axis=0, keepdims=True)dW += reg*W # don't forget the regularization gradient Where we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. Note that the regularization gradient has the very simple form reg*W since we used the constant 0.5 for its loss contribution (i.e. \(\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda w\). This is a common convenience trick that simplifies the gradient expression. Performing a parameter updateNow that we’ve evaluated the gradient we know how every parameter influences the loss function. We will now perform a parameter update in the negative gradient direction to decrease the loss: 123# perform a parameter updateW += -step_size * dWb += -step_size * db Putting it all together: Training a Softmax ClassifierPutting all of this together, here is the full code for training a Softmax classifier with Gradient descent: 12345678910111213141516171819202122232425262728293031323334353637383940414243#Train a Linear Classifier# initialize parameters randomlyW = 0.01 * np.random.randn(D,K)b = np.zeros((1,K))# some hyperparametersstep_size = 1e-0reg = 1e-3 # regularization strength# gradient descent loopnum_examples = X.shape[0]for i in xrange(200): # evaluate class scores, [N x K] scores = np.dot(X, W) + b # compute the class probabilities exp_scores = np.exp(scores) probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K] # compute the loss: average cross-entropy loss and regularization corect_logprobs = -np.log(probs[range(num_examples),y]) data_loss = np.sum(corect_logprobs)/num_examples reg_loss = 0.5*reg*np.sum(W*W) loss = data_loss + reg_loss if i % 10 == 0: print "iteration %d: loss %f" % (i, loss) # compute the gradient on scores dscores = probs dscores[range(num_examples),y] -= 1 dscores /= num_examples # backpropate the gradient to the parameters (W,b) dW = np.dot(X.T, dscores) db = np.sum(dscores, axis=0, keepdims=True) dW += reg*W # regularization gradient # perform a parameter update W += -step_size * dW b += -step_size * db Running this prints the output: 1234567891011121314151617181920iteration 0: loss 1.096956iteration 10: loss 0.917265iteration 20: loss 0.851503iteration 30: loss 0.822336iteration 40: loss 0.807586iteration 50: loss 0.799448iteration 60: loss 0.794681iteration 70: loss 0.791764iteration 80: loss 0.789920iteration 90: loss 0.788726iteration 100: loss 0.787938iteration 110: loss 0.787409iteration 120: loss 0.787049iteration 130: loss 0.786803iteration 140: loss 0.786633iteration 150: loss 0.786514iteration 160: loss 0.786431iteration 170: loss 0.786373iteration 180: loss 0.786331iteration 190: loss 0.786302 We see that we’ve converged to something after about 190 iterations. We can evaluate the training set accuracy: 1234# evaluate training set accuracyscores = np.dot(X, W) + bpredicted_class = np.argmax(scores, axis=1)print 'training accuracy: %.2f' % (np.mean(predicted_class == y)) This prints 49%. Not very good at all, but also not surprising given that the dataset is constructed so it is not linearly separable. We can also plot the learned decision boundaries: Linear classifier fails to learn the toy spiral dataset. Training a Neural NetworkClearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers): 123456# initialize parameters randomlyh = 100 # size of hidden layerW = 0.01 * np.random.randn(D,h)b = np.zeros((1,h))W2 = 0.01 * np.random.randn(h,K)b2 = np.zeros((1,K)) The forward pass to compute scores now changes form: 123# evaluate class scores with a 2-layer Neural Networkhidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activationscores = np.dot(hidden_layer, W2) + b2 Notice that the only change from before is one extra line of code, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we’ve also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero. Everything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores dscores exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. This looks identical to the code we had for the Softmax classifier, except we’re replacing X (the raw data), with the variable hidden_layer): 1234# backpropate the gradient to the parameters# first backprop into parameters W2 and b2dW2 = np.dot(hidden_layer.T, dscores)db2 = np.sum(dscores, axis=0, keepdims=True) However, unlike before we are not yet done, because hidden_layer is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as: 1dhidden = np.dot(dscores, W2.T) Now we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since \(r = max(0, x)\), we have that \(\frac{dr}{dx} = 1(x &gt; 0) \). Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with: 12# backprop the ReLU non-linearitydhidden[hidden_layer &lt;= 0] = 0 And now we finally continue to the first layer weights and biases: 123# finally into W,bdW = np.dot(X.T, dhidden)db = np.sum(dhidden, axis=0, keepdims=True) We’re done! We have the gradients dW,db,dW2,db2 and can perform the parameter update. Everything else remains unchanged. The full code looks very similar: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# initialize parameters randomlyh = 100 # size of hidden layerW = 0.01 * np.random.randn(D,h)b = np.zeros((1,h))W2 = 0.01 * np.random.randn(h,K)b2 = np.zeros((1,K))# some hyperparametersstep_size = 1e-0reg = 1e-3 # regularization strength# gradient descent loopnum_examples = X.shape[0]for i in xrange(10000): # evaluate class scores, [N x K] hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation scores = np.dot(hidden_layer, W2) + b2 # compute the class probabilities exp_scores = np.exp(scores) probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K] # compute the loss: average cross-entropy loss and regularization corect_logprobs = -np.log(probs[range(num_examples),y]) data_loss = np.sum(corect_logprobs)/num_examples reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) loss = data_loss + reg_loss if i % 1000 == 0: print "iteration %d: loss %f" % (i, loss) # compute the gradient on scores dscores = probs dscores[range(num_examples),y] -= 1 dscores /= num_examples # backpropate the gradient to the parameters # first backprop into parameters W2 and b2 dW2 = np.dot(hidden_layer.T, dscores) db2 = np.sum(dscores, axis=0, keepdims=True) # next backprop into hidden layer dhidden = np.dot(dscores, W2.T) # backprop the ReLU non-linearity dhidden[hidden_layer &lt;= 0] = 0 # finally into W,b dW = np.dot(X.T, dhidden) db = np.sum(dhidden, axis=0, keepdims=True) # add regularization gradient contribution dW2 += reg * W2 dW += reg * W # perform a parameter update W += -step_size * dW b += -step_size * db W2 += -step_size * dW2 b2 += -step_size * db2 This prints: 12345678910iteration 0: loss 1.098744iteration 1000: loss 0.294946iteration 2000: loss 0.259301iteration 3000: loss 0.248310iteration 4000: loss 0.246170iteration 5000: loss 0.245649iteration 6000: loss 0.245491iteration 7000: loss 0.245400iteration 8000: loss 0.245335iteration 9000: loss 0.245292 The training accuracy is now: 12345# evaluate training set accuracyhidden_layer = np.maximum(0, np.dot(X, W) + b)scores = np.dot(hidden_layer, W2) + b2predicted_class = np.argmax(scores, axis=1)print 'training accuracy: %.2f' % (np.mean(predicted_class == y)) Which prints 98%!. We can also visualize the decision boundaries: Neural Network classifier crushes the spiral dataset. SummaryWe’ve worked with a toy 2D dataset and trained both a linear network and a 2-layer Neural Network. We saw that the change from a linear classifier to a Neural Network involves very few changes in the code. The score function changes its form (1 line of code difference), and the backpropagation changes its form (we have to perform one more round of backprop through the hidden layer to the first layer of the network). You may want to look at this IPython Notebook code rendered as HTML. Or download the ipynb file]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[convnet-tips]]></title>
    <url>%2F2017%2F10%2F08%2FConvNet%20Tips%2F</url>
    <content type="text"><![CDATA[Addressing OverfittingData Augmentation Flip the training images over x-axis Sample random crops / scales in the original image Jitter the colors Dropout Dropout is just as effective for Conv layers. Usually people apply less dropout right before early conv layers since there are not that many parameters there compared to later stages of the network (e.g. the fully connected layers).]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>convnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image Classification With KNN]]></title>
    <url>%2F2017%2F10%2F08%2FImage%20Classification%20With%20KNN%2F</url>
    <content type="text"><![CDATA[This is an introductory lecture designed to introduce people from outside of Computer Vision to the Image Classification problem, and the data-driven approach. The Table of Contents: Intro to Image Classification, data-driven approach, pipeline Nearest Neighbor Classifier k-Nearest Neighbor Validation sets, Cross-validation, hyperparameter tuning Pros/Cons of Nearest Neighbor Summary Summary: Applying kNN in practice Further Reading Image ClassificationMotivation. In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification. Example. For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as “cat”. Challenges. Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values: Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible. Illumination conditions. The effects of illumination are drastic on the pixel level. Background clutter. The objects of interest may blend into their environment, making them hard to identify. Intra-class variation. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance. A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations. Data-driven approach. How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like: The image classification pipeline. We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows: Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set. Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model. Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth). Nearest Neighbor ClassifierAs our first approach, we will develop what we call a Nearest Neighbor Classifier. This classifier has nothing to do with Convolutional Neural Networks and it is very rarely used in practice, but it will allow us to get an idea about the basic approach to an image classification problem. Example image classification dataset: CIFAR-10. One popular toy image classification dataset is the CIFAR-10 dataset. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example “airplane, automobile, bird, etc”). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below you can see 10 random example images from each one of the 10 classes: Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right you can see an example result of such a procedure for 10 example test images. Notice that in only about 3 out of 10 examples an image of the same class is retrieved, while in the other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car. You may have noticed that we left unspecified the details of exactly how we compare two images, which in this case are just two blocks of 32 x 32 x 3. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors \( I_1, I_2 \) , a reasonable choice for comparing them might be the L1 distance: $$d_1 (I_1, I2) = \sum{p} \left| I^p_1 - I^p_2 \right|$$ Where the sum is taken over all pixels. Here is the procedure visualized: Let’s also look at how we might implement the classifier in code. First, let’s load the CIFAR-10 data into memory as 4 arrays: the training data/labels and the test data/labels. In the code below, Xtr (of size 50,000 x 32 x 32 x 3) holds all the images in the training set, and a corresponding 1-dimensional array Ytr (of length 50,000) holds the training labels (from 0 to 9): 1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 Now that we have all images stretched out as rows, here is how we could train and evaluate a classifier: 123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) Notice that as an evaluation criterion, it is common to use the accuracy, which measures the fraction of predictions that were correct. Notice that all classifiers we will build satisfy this one common API: they have a train(X,y) function that takes the data and the labels to learn from. Internally, the class should build some kind of model of the labels and how they can be predicted from the data. And then there is a predict(X) function, which takes new data and predicts the labels. Of course, we’ve left out the meat of things - the actual classifier itself. Here is an implementation of a simple Nearest Neighbor classifier with the L1 distance that satisfies this template: 123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred If you ran this code, you would see that this classifier only achieves 38.6% on CIFAR-10. That’s more impressive than guessing at random (which would give 10% accuracy since there are 10 classes), but nowhere near human performance (which is estimated at about 94%) or near state-of-the-art Convolutional Neural Networks that achieve about 95%, matching human accuracy (see the leaderboard of a recent Kaggle competition on CIFAR-10). The choice of distance.There are many other ways of computing distances between vectors. Another common choice could be to instead use the L2 distance, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form: $$d_2 (I_1, I2) = \sqrt{\sum{p} \left( I^p_1 - I^p_2 \right)^2}$$ In other words we would be computing the pixelwise difference as before, but this time we square all of them, add them up and finally take the square root. In numpy, using the code from above we would need to only replace a single line of code. The line that computes the distances: 1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) Note that I included the np.sqrt call above, but in a practical nearest neighbor application we could leave out the square root operation because square root is a monotonic function. That is, it scales the absolute sizes of the distances but it preserves the ordering, so the nearest neighbors with or without it are identical. If you ran the Nearest Neighbor classifier on CIFAR-10 with this distance, you would obtain 35.4% accuracy (slightly lower than our L1 distance result). L1 vs. L2. It is interesting to consider differences between the two metrics. In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. That is, the L2 distance prefers many medium disagreements to one big one. L1 and L2 distances (or equivalently the L1/L2 norms of the differences between a pair of images) are the most commonly used special cases of a p-norm. k - Nearest Neighbor ClassifierYou may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what’s called a k-Nearest Neighbor Classifier. The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers: In practice, you will almost always want to use k-Nearest Neighbor. But what value of k should you use? We turn to this problem next. Validation sets for Hyperparameter tuningThe k-nearest neighbor classifier requires a setting for k. But what number works best? Additionally, we saw that there are many different distance functions we could have used: L1 norm, L2 norm, there are many other choices we didn’t even consider (e.g. dot products). These choices are called hyperparameters and they come up very often in the design of many Machine Learning algorithms that learn from data. It’s often not obvious what values/settings one should choose. You might be tempted to suggest that we should try out many different values and see what works best. That is a fine idea and that’s indeed what we will do, but this must be done very carefully. In particular, we cannot use the test set for the purpose of tweaking hyperparameters. Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end. Otherwise, the very real danger is that you may tune your hyperparameters to work well on the test set, but if you were to deploy your model you could see a significantly reduced performance. In practice, we would say that you overfit to the test set. Another way of looking at it is that if you tune your hyperparameters on the test set, you are effectively using the test set as the training set, and therefore the performance you achieve on it will be too optimistic with respect to what you might actually observe when you deploy your model. But if you only use the test set once at end, it remains a good proxy for measuring the generalization of your classifier (we will see much more discussion surrounding generalization later in the class). Evaluate on the test set only a single time, at the very end. Luckily, there is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a validation set. Using CIFAR-10 as an example, we could for example use 49,000 of the training images for training, and leave 1,000 aside for validation. This validation set is essentially used as a fake test set to tune the hyper-parameters. Here is what this might look like in the case of CIFAR-10: 123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) By the end of this procedure, we could plot a graph that shows which values of k work best. We would then stick with this value and evaluate once on the actual test set. Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance. Cross-validation.In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called cross-validation. Working with our previous example, the idea is that instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of k works by iterating over different validation sets and averaging the performance across these. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds. In practice. In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive. The splits people tend to use is between 50%-90% of the training data for training and rest for validation. However, this depends on multiple factors: For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation. Pros and Cons of Nearest Neighbor classifier. It is worth considering some advantages and drawbacks of the Nearest Neighbor classifier. Clearly, one advantage is that it is very simple to implement and understand. Additionally, the classifier takes no time to train, since all that is required is to store and possibly index the training data. However, we pay that computational cost at test time, since classifying a test example requires a comparison to every single training example. This is backwards, since in practice we often care about the test time efficiency much more than the efficiency at training time. In fact, the deep neural networks we will develop later in this class shift this tradeoff to the other extreme: They are very expensive to train, but once the training is finished it is very cheap to classify a new test example. This mode of operation is much more desirable in practice. As an aside, the computational complexity of the Nearest Neighbor classifier is an active area of research, and several Approximate Nearest Neighbor (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. FLANN). These algorithms allow one to trade off the correctness of the nearest neighbor retrieval with its space/time complexity during retrieval, and usually rely on a pre-processing/indexing stage that involves building a kdtree, or running the k-means algorithm. The Nearest Neighbor Classifier may sometimes be a good choice in some settings (especially if the data is low-dimensional), but it is rarely appropriate for use in practical image classification settings. One problem is that images are high-dimensional objects (i.e. they often contain many pixels), and distances over high-dimensional spaces can be very counter-intuitive. The image below illustrates the point that the pixel-based L2 similarities we developed above are very different from perceptual similarities: Here is one more visualization to convince you that using pixel differences to compare images is inadequate. We can use a visualization technique called t-SNE to take the CIFAR-10 images and embed them in two dimensions so that their (local) pairwise distances are best preserved. In this visualization, images that are shown nearby are considered to be very near according to the L2 pixelwise distance we developed above: In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity. For example, a dog can be seen very near a frog since both happen to be on white background. Ideally we would like images of all of the 10 classes to form their own clusters, so that images of the same class are nearby to each other regardless of irrelevant characteristics and variations (such as the background). However, to get this property we will have to go beyond raw pixels. SummaryIn summary: We introduced the problem of Image Classification, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. We introduced a simple classifier called the Nearest Neighbor classifier. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them. We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call validation set. We try different hyperparameter values and keep the values that lead to the best performance on the validation set. If the lack of training data is a concern, we discussed a procedure called cross-validation, which can help reduce noise in estimating which hyperparameters work best. Once the best hyperparameters are found, we fix them and perform a single evaluation on the actual test set. We saw that Nearest Neighbor can get us about 40% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image. Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content. In next lectures we will embark on addressing these challenges and eventually arrive at solutions that give 90% accuracies, allow us to completely discard the training set once learning is complete, and they will allow us to evaluate a test image in less than a millisecond. Summary: Applying kNN in practiceIf you wish to apply kNN in practice (hopefully not on images, or perhaps as only a baseline) proceed as follows: Preprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization. If your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA (wiki ref, CS229ref, blog ref) or even Random Projections. Split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive). Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of k (e.g. the more the better) and across different distance types (L1 and L2 are good candidates) If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. FLANN) to accelerate the retrieval (at cost of some accuracy). Take note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be burned on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data. Further ReadingHere are some (optional) links you may find interesting for further reading: A Few Useful Things to Know about Machine Learning, where especially section 6 is related but the whole paper is a warmly recommended reading. Recognizing and Learning Object Categories, a short course of object categorization at ICCV 2005.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Image Classification</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPython Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fipython-tutorial%2F</url>
    <content type="text"><![CDATA[In this class, we will use IPython notebooks for theprogramming assignments. An IPython notebook lets you write and execute Pythoncode in your web browser. IPython notebooks make it very easy to tinker withcode and execute it in bits and pieces; for this reason IPython notebooks arewidely used in scientific computing. Installing and running IPython is easy. From the command line, the followingwill install IPython: 1pip install "ipython[notebook]" Once you have IPython installed, start it with this command: 1ipython notebook Once IPython is running, point your web browser at http://localhost:8888 tostart using IPython notebooks. If everything worked correctly, you shouldsee a screen like this, showing all available IPython notebooks in the currentdirectory: If you click through to a notebook file, you will see a screen like this: An IPython notebook is made up of a number of cells. Each cell can containPython code. You can execute a cell by clicking on it and pressing Shift-Enter.When you do so, the code in the cell will run, and the output of the cellwill be displayed beneath the cell. For example, after running the first cellthe notebook looks like this: Global variables are shared between cells. Executing the second cell thus givesthe following result: By convention, IPython notebooks are expected to be run from top to bottom.Failing to execute some cells or executing cells out of order can result inerrors: After you have modified an IPython notebook for one of the assignments bymodifying or executing some of its cells, remember to save your changes! This has only been a brief introduction to IPython notebooks, but it shouldbe enough to get you up and running on the assignments for this course.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>IPython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fpython-numpy-tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial was contributed by Justin Johnson. We will use the Python programming language for all assignments in this course.Python is a great general-purpose programming language on its own, but with thehelp of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerfulenvironment for scientific computing. We expect that many of you will have some experience with Python and numpy;for the rest of you, this section will serve as a quick crash course both onthe Python programming language and on the use of Python for scientificcomputing. Some of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page. You can also find an IPython notebook version of this tutorial here created by Volodymyr Kuleshov and Isaac Caswell for CS 228. Table of contents: Python Basic data types Containers Lists Dictionaries Sets Tuples Functions Classes Numpy Arrays Array indexing Datatypes Array math Broadcasting SciPy Image operations MATLAB files Distance between points Matplotlib Plotting Subplots Images PythonPython is a high-level, dynamically typed multiparadigm programming language.Python code is often said to be almost like pseudocode, since it allows youto express very powerful ideas in very few lines of code while being veryreadable. As an example, here is an implementation of the classic quicksortalgorithm in Python: 1234567891011def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) / 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right) print quicksort([3,6,8,10,1,2,1])# Prints "[1, 1, 2, 3, 6, 8, 10]" Python versionsThere are currently two different supported versions of Python, 2.7 and 3.4.Somewhat confusingly, Python 3.0 introduced many backwards-incompatible changesto the language, so code written for 2.7 may not work under 3.4 and vice versa.For this class all code will use Python 2.7. You can check your Python version at the command line by runningpython --version. Basic data typesLike most languages, Python has a number of basic types including integers,floats, booleans, and strings. These data types behave in ways that arefamiliar from other programming languages. Numbers: Integers and floats work as you would expect from other languages: 1234567891011121314x = 3print type(x) # Prints "&lt;type 'int'&gt;"print x # Prints "3"print x + 1 # Addition; prints "4"print x - 1 # Subtraction; prints "2"print x * 2 # Multiplication; prints "6"print x ** 2 # Exponentiation; prints "9"x += 1print x # Prints "4"x *= 2print x # Prints "8"y = 2.5print type(y) # Prints "&lt;type 'float'&gt;"print y, y + 1, y * 2, y ** 2 # Prints "2.5 3.5 5.0 6.25" Note that unlike many languages, Python does not have unary increment (x++)or decrement (x--) operators. Python also has built-in types for long integers and complex numbers;you can find all of the detailsin the documentation. Booleans: Python implements all of the usual operators for Boolean logic,but uses English words rather than symbols (&amp;&amp;, ||, etc.): 1234567t = Truef = Falseprint type(t) # Prints "&lt;type 'bool'&gt;"print t and f # Logical AND; prints "False"print t or f # Logical OR; prints "True"print not t # Logical NOT; prints "False"print t != f # Logical XOR; prints "True" Strings: Python has great support for strings: 12345678hello = 'hello' # String literals can use single quotesworld = "world" # or double quotes; it does not matter.print hello # Prints "hello"print len(hello) # String length; prints "5"hw = hello + ' ' + world # String concatenationprint hw # prints "hello world"hw12 = '%s %s %d' % (hello, world, 12) # sprintf style string formattingprint hw12 # prints "hello world 12" String objects have a bunch of useful methods; for example: 12345678s = "hello"print s.capitalize() # Capitalize a string; prints "Hello"print s.upper() # Convert a string to uppercase; prints "HELLO"print s.rjust(7) # Right-justify a string, padding with spaces; prints " hello"print s.center(7) # Center a string, padding with spaces; prints " hello "print s.replace('l', '(ell)') # Replace all instances of one substring with another; # prints "he(ell)(ell)o"print ' world '.strip() # Strip leading and trailing whitespace; prints "world" You can find a list of all string methods in the documentation. ContainersPython includes several built-in container types: lists, dictionaries, sets, and tuples. ListsA list is the Python equivalent of an array, but is resizeableand can contain elements of different types: 123456789xs = [3, 1, 2] # Create a listprint xs, xs[2] # Prints "[3, 1, 2] 2"print xs[-1] # Negative indices count from the end of the list; prints "2"xs[2] = 'foo' # Lists can contain elements of different typesprint xs # Prints "[3, 1, 'foo']"xs.append('bar') # Add a new element to the end of the listprint xs # Prints "[3, 1, 'foo', 'bar']"x = xs.pop() # Remove and return the last element of the listprint x, xs # Prints "bar [3, 1, 'foo']" As usual, you can find all the gory details about listsin the documentation. Slicing:In addition to accessing list elements one at a time, Python providesconcise syntax to access sublists; this is known as slicing: 123456789nums = range(5) # range is a built-in function that creates a list of integersprint nums # Prints "[0, 1, 2, 3, 4]"print nums[2:4] # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"print nums[2:] # Get a slice from index 2 to the end; prints "[2, 3, 4]"print nums[:2] # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"print nums[:] # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"print nums[:-1] # Slice indices can be negative; prints ["0, 1, 2, 3]"nums[2:4] = [8, 9] # Assign a new sublist to a sliceprint nums # Prints "[0, 1, 8, 9, 4]" We will see slicing again in the context of numpy arrays. Loops: You can loop over the elements of a list like this: 1234animals = ['cat', 'dog', 'monkey']for animal in animals: print animal# Prints "cat", "dog", "monkey", each on its own line. If you want access to the index of each element within the body of a loop,use the built-in enumerate function: 1234animals = ['cat', 'dog', 'monkey']for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line List comprehensions:When programming, frequently we want to transform one type of data into another.As a simple example, consider the following code that computes square numbers: 12345nums = [0, 1, 2, 3, 4]squares = []for x in nums: squares.append(x ** 2)print squares # Prints [0, 1, 4, 9, 16] You can make this code simpler using a list comprehension: 123nums = [0, 1, 2, 3, 4]squares = [x ** 2 for x in nums]print squares # Prints [0, 1, 4, 9, 16] List comprehensions can also contain conditions: 123nums = [0, 1, 2, 3, 4]even_squares = [x ** 2 for x in nums if x % 2 == 0]print even_squares # Prints "[0, 4, 16]" DictionariesA dictionary stores (key, value) pairs, similar to a Map in Java oran object in Javascript. You can use it like this: 12345678910d = &#123;'cat': 'cute', 'dog': 'furry'&#125; # Create a new dictionary with some dataprint d['cat'] # Get an entry from a dictionary; prints "cute"print 'cat' in d # Check if a dictionary has a given key; prints "True"d['fish'] = 'wet' # Set an entry in a dictionaryprint d['fish'] # Prints "wet"# print d['monkey'] # KeyError: 'monkey' not a key of dprint d.get('monkey', 'N/A') # Get an element with a default; prints "N/A"print d.get('fish', 'N/A') # Get an element with a default; prints "wet"del d['fish'] # Remove an element from a dictionaryprint d.get('fish', 'N/A') # "fish" is no longer a key; prints "N/A" You can find all you need to know about dictionariesin the documentation. Loops: It is easy to iterate over the keys in a dictionary: 12345d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal in d: legs = d[animal] print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" If you want access to keys and their corresponding values, use the iteritems method: 1234d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal, legs in d.iteritems(): print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" Dictionary comprehensions:These are similar to list comprehensions, but allow you to easily constructdictionaries. For example: 123nums = [0, 1, 2, 3, 4]even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;print even_num_to_square # Prints "&#123;0: 0, 2: 4, 4: 16&#125;" SetsA set is an unordered collection of distinct elements. As a simple example, considerthe following: 12345678910animals = &#123;'cat', 'dog'&#125;print 'cat' in animals # Check if an element is in a set; prints "True"print 'fish' in animals # prints "False"animals.add('fish') # Add an element to a setprint 'fish' in animals # Prints "True"print len(animals) # Number of elements in a set; prints "3"animals.add('cat') # Adding an element that is already in the set does nothingprint len(animals) # Prints "3"animals.remove('cat') # Remove an element from a setprint len(animals) # Prints "2" As usual, everything you want to know about sets can be foundin the documentation. Loops:Iterating over a set has the same syntax as iterating over a list;however since sets are unordered, you cannot make assumptions about the orderin which you visit the elements of the set: 1234animals = &#123;'cat', 'dog', 'fish'&#125;for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: fish", "#2: dog", "#3: cat" Set comprehensions:Like lists and dictionaries, we can easily construct sets using set comprehensions: 123from math import sqrtnums = &#123;int(sqrt(x)) for x in range(30)&#125;print nums # Prints "set([0, 1, 2, 3, 4, 5])" TuplesA tuple is an (immutable) ordered list of values.A tuple is in many ways similar to a list; one of the most important differences is thattuples can be used as keys in dictionaries and as elements of sets, while lists cannot.Here is a trivial example: 12345d = &#123;(x, x + 1): x for x in range(10)&#125; # Create a dictionary with tuple keyst = (5, 6) # Create a tupleprint type(t) # Prints "&lt;type 'tuple'&gt;"print d[t] # Prints "5"print d[(1, 2)] # Prints "1" The documentation has more information about tuples. FunctionsPython functions are defined using the def keyword. For example: 1234567891011def sign(x): if x &gt; 0: return 'positive' elif x &lt; 0: return 'negative' else: return 'zero'for x in [-1, 0, 1]: print sign(x)# Prints "negative", "zero", "positive" We will often define functions to take optional keyword arguments, like this: 12345678def hello(name, loud=False): if loud: print 'HELLO, %s!' % name.upper() else: print 'Hello, %s' % namehello('Bob') # Prints "Hello, Bob"hello('Fred', loud=True) # Prints "HELLO, FRED!" There is a lot more information about Python functionsin the documentation. ClassesThe syntax for defining classes in Python is straightforward: 12345678910111213141516class Greeter(object): # Constructor def __init__(self, name): self.name = name # Create an instance variable # Instance method def greet(self, loud=False): if loud: print 'HELLO, %s!' % self.name.upper() else: print 'Hello, %s' % self.name g = Greeter('Fred') # Construct an instance of the Greeter classg.greet() # Call an instance method; prints "Hello, Fred"g.greet(loud=True) # Call an instance method; prints "HELLO, FRED!" You can read a lot more about Python classesin the documentation. NumpyNumpy is the core library for scientific computing in Python.It provides a high-performance multidimensional array object, and tools for working with thesearrays. If you are already familiar with MATLAB, you might findthis tutorial useful to get started with Numpy. ArraysA numpy array is a grid of values, all of the same type, and is indexed by a tuple ofnonnegative integers. The number of dimensions is the rank of the array; the shapeof an array is a tuple of integers giving the size of the array along each dimension. We can initialize numpy arrays from nested Python lists,and access elements using square brackets: 123456789101112import numpy as npa = np.array([1, 2, 3]) # Create a rank 1 arrayprint type(a) # Prints "&lt;type 'numpy.ndarray'&gt;"print a.shape # Prints "(3,)"print a[0], a[1], a[2] # Prints "1 2 3"a[0] = 5 # Change an element of the arrayprint a # Prints "[5, 2, 3]"b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 arrayprint b.shape # Prints "(2, 3)"print b[0, 0], b[0, 1], b[1, 0] # Prints "1 2 4" Numpy also provides many functions to create arrays: 1234567891011121314151617181920import numpy as npa = np.zeros((2,2)) # Create an array of all zerosprint a # Prints "[[ 0. 0.] # [ 0. 0.]]" b = np.ones((1,2)) # Create an array of all onesprint b # Prints "[[ 1. 1.]]"c = np.full((2,2), 7) # Create a constant arrayprint c # Prints "[[ 7. 7.] # [ 7. 7.]]"d = np.eye(2) # Create a 2x2 identity matrixprint d # Prints "[[ 1. 0.] # [ 0. 1.]]" e = np.random.random((2,2)) # Create an array filled with random valuesprint e # Might print "[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]" You can read about other methods of array creationin the documentation. Array indexingNumpy offers several ways to index into arrays. Slicing:Similar to Python lists, numpy arrays can be sliced.Since arrays may be multidimensional, you must specify a slice for each dimensionof the array: 12345678910111213141516171819import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Use slicing to pull out the subarray consisting of the first 2 rows# and columns 1 and 2; b is the following array of shape (2, 2):# [[2 3]# [6 7]]b = a[:2, 1:3]# A slice of an array is a view into the same data, so modifying it# will modify the original array.print a[0, 1] # Prints "2"b[0, 0] = 77 # b[0, 0] is the same piece of data as a[0, 1]print a[0, 1] # Prints "77" You can also mix integer indexing with slice indexing.However, doing so will yield an array of lower rank than the original array.Note that this is quite different from the way that MATLAB handles arrayslicing: 123456789101112131415161718192021222324import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Two ways of accessing the data in the middle row of the array.# Mixing integer indexing with slices yields an array of lower rank,# while using only slices yields an array of the same rank as the# original array:row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of aprint row_r1, row_r1.shape # Prints "[5 6 7 8] (4,)"print row_r2, row_r2.shape # Prints "[[5 6 7 8]] (1, 4)"# We can make the same distinction when accessing columns of an array:col_r1 = a[:, 1]col_r2 = a[:, 1:2]print col_r1, col_r1.shape # Prints "[ 2 6 10] (3,)"print col_r2, col_r2.shape # Prints "[[ 2] # [ 6] # [10]] (3, 1)" Integer array indexing:When you index into numpy arrays using slicing, the resulting array viewwill always be a subarray of the original array. In contrast, integer arrayindexing allows you to construct arbitrary arrays using the data from anotherarray. Here is an example: 1234567891011121314151617import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])# An example of integer array indexing.# The returned array will have shape (3,) and print a[[0, 1, 2], [0, 1, 0]] # Prints "[1 4 5]"# The above example of integer array indexing is equivalent to this:print np.array([a[0, 0], a[1, 1], a[2, 0]]) # Prints "[1 4 5]"# When using integer array indexing, you can reuse the same# element from the source array:print a[[0, 0], [1, 1]] # Prints "[2 2]"# Equivalent to the previous integer array indexing exampleprint np.array([a[0, 1], a[0, 1]]) # Prints "[2 2]" One useful trick with integer array indexing is selecting or mutating oneelement from each row of a matrix: 1234567891011121314151617181920212223import numpy as np# Create a new array from which we will select elementsa = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])print a # prints "array([[ 1, 2, 3], # [ 4, 5, 6], # [ 7, 8, 9], # [10, 11, 12]])"# Create an array of indicesb = np.array([0, 2, 0, 1])# Select one element from each row of a using the indices in bprint a[np.arange(4), b] # Prints "[ 1 6 7 11]"# Mutate one element from each row of a using the indices in ba[np.arange(4), b] += 10print a # prints "array([[11, 2, 3], # [ 4, 5, 16], # [17, 8, 9], # [10, 21, 12]]) Boolean array indexing:Boolean array indexing lets you pick out arbitrary elements of an array.Frequently this type of indexing is used to select the elements of an arraythat satisfy some condition. Here is an example: 1234567891011121314151617181920import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2. print bool_idx # Prints "[[False False] # [ True True] # [ True True]]"# We use boolean array indexing to construct a rank 1 array# consisting of the elements of a corresponding to the True values# of bool_idxprint a[bool_idx] # Prints "[3 4 5 6]"# We can do all of the above in a single concise statement:print a[a &gt; 2] # Prints "[3 4 5 6]" For brevity we have left out a lot of details about numpy array indexing;if you want to know more you shouldread the documentation. DatatypesEvery numpy array is a grid of elements of the same type.Numpy provides a large set of numeric datatypes that you can use to construct arrays.Numpy tries to guess a datatype when you create an array, but functions that constructarrays usually also include an optional argument to explicitly specify the datatype.Here is an example: 12345678910import numpy as npx = np.array([1, 2]) # Let numpy choose the datatypeprint x.dtype # Prints "int64"x = np.array([1.0, 2.0]) # Let numpy choose the datatypeprint x.dtype # Prints "float64"x = np.array([1, 2], dtype=np.int64) # Force a particular datatypeprint x.dtype # Prints "int64" You can read all about numpy datatypesin the documentation. Array mathBasic mathematical functions operate elementwise on arrays, and are availableboth as operator overloads and as functions in the numpy module: 123456789101112131415161718192021222324252627282930313233import numpy as npx = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the array# [[ 6.0 8.0]# [10.0 12.0]]print x + yprint np.add(x, y)# Elementwise difference; both produce the array# [[-4.0 -4.0]# [-4.0 -4.0]]print x - yprint np.subtract(x, y)# Elementwise product; both produce the array# [[ 5.0 12.0]# [21.0 32.0]]print x * yprint np.multiply(x, y)# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print x / yprint np.divide(x, y)# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print np.sqrt(x) Note that unlike MATLAB, * is elementwise multiplication, not matrixmultiplication. We instead use the dot function to compute innerproducts of vectors, to multiply a vector by a matrix, and tomultiply matrices. dot is available both as a function in the numpymodule and as an instance method of array objects: 123456789101112131415161718192021import numpy as npx = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print v.dot(w)print np.dot(v, w)# Matrix / vector product; both produce the rank 1 array [29 67]print x.dot(v)print np.dot(x, v)# Matrix / matrix product; both produce the rank 2 array# [[19 22]# [43 50]]print x.dot(y)print np.dot(x, y) Numpy provides many useful functions for performing computations onarrays; one of the most useful is sum: 1234567import numpy as npx = np.array([[1,2],[3,4]])print np.sum(x) # Compute sum of all elements; prints "10"print np.sum(x, axis=0) # Compute sum of each column; prints "[4 6]"print np.sum(x, axis=1) # Compute sum of each row; prints "[3 7]" You can find the full list of mathematical functions provided by numpyin the documentation. Apart from computing mathematical functions using arrays, we frequentlyneed to reshape or otherwise manipulate data in arrays. The simplest exampleof this type of operation is transposing a matrix; to transpose a matrix,simply use the T attribute of an array object: 123456789101112import numpy as npx = np.array([[1,2], [3,4]])print x # Prints "[[1 2] # [3 4]]"print x.T # Prints "[[1 3] # [2 4]]"# Note that taking the transpose of a rank 1 array does nothing:v = np.array([1,2,3])print v # Prints "[1 2 3]"print v.T # Prints "[1 2 3]" Numpy provides many more functions for manipulating arrays; you can see the full listin the documentation. BroadcastingBroadcasting is a powerful mechanism that allows numpy to work with arrays of differentshapes when performing arithmetic operations. Frequently we have a smaller array and alarger array, and we want to use the smaller array multiple times to perform some operationon the larger array. For example, suppose that we want to add a constant vector to eachrow of a matrix. We could do it like this: 123456789101112131415161718import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = np.empty_like(x) # Create an empty matrix with the same shape as x# Add the vector v to each row of the matrix x with an explicit loopfor i in range(4): y[i, :] = x[i, :] + v# Now y is the following# [[ 2 2 4]# [ 5 5 7]# [ 8 8 10]# [11 11 13]]print y This works; however when the matrix x is very large, computing an explicit loopin Python could be slow. Note that adding the vector v to each row of the matrixx is equivalent to forming a matrix vv by stacking multiple copies of v vertically,then performing elementwise summation of x and vv. We could implement thisapproach like this: 12345678910111213141516import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])vv = np.tile(v, (4, 1)) # Stack 4 copies of v on top of each otherprint vv # Prints "[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]"y = x + vv # Add x and vv elementwiseprint y # Prints "[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" Numpy broadcasting allows us to perform this computation without actuallycreating multiple copies of v. Consider this version, using broadcasting: 1234567891011import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint y # Prints "[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" The line y = x + v works even though x has shape (4, 3) and v has shape(3,) due to broadcasting; this line works as if v actually had shape (4, 3),where each row was a copy of v, and the sum was performed elementwise. Broadcasting two arrays together follows these rules: If the arrays do not have the same rank, prepend the shape of the lower rank arraywith 1s until both shapes have the same length. The two arrays are said to be compatible in a dimension if they have the samesize in the dimension, or if one of the arrays has size 1 in that dimension. The arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwisemaximum of shapes of the two input arrays. In any dimension where one array had size 1 and the other array had size greater than 1,the first array behaves as if it were copied along that dimension If this explanation does not make sense, try reading the explanationfrom the documentationor this explanation. Functions that support broadcasting are known as universal functions. You can findthe list of all universal functionsin the documentation. Here are some applications of broadcasting: 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:# [[ 4 5]# [ 8 10]# [12 15]]print np.reshape(v, (3, 1)) * w# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:# [[2 4 6]# [5 7 9]]print x + v# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:# [[ 5 6 7]# [ 9 10 11]]print (x.T + w).T# Another solution is to reshape w to be a row vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print x + np.reshape(w, (2, 1))# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:# [[ 2 4 6]# [ 8 10 12]]print x * 2 Broadcasting typically makes your code more concise and faster, so youshould strive to use it where possible. Numpy DocumentationThis brief overview has touched on many of the important things that you need toknow about numpy, but is far from complete. Check out thenumpy referenceto find out much more about numpy. SciPyNumpy provides a high-performance multidimensional array and basic tools tocompute with and manipulate these arrays.SciPybuilds on this, and providesa large number of functions that operate on numpy arrays and are useful fordifferent types of scientific and engineering applications. The best way to get familiar with SciPy is tobrowse the documentation.We will highlight some parts of SciPy that you might find useful for this class. Image operationsSciPy provides some basic functions to work with images.For example, it has functions to read images from disk into numpy arrays,to write numpy arrays to disk as images, and to resize images.Here is a simple example that showcases these functions: 12345678910111213141516171819from scipy.misc import imread, imsave, imresize# Read an JPEG image into a numpy arrayimg = imread('assets/cat.jpg')print img.dtype, img.shape # Prints "uint8 (400, 248, 3)"# We can tint the image by scaling each of the color channels# by a different scalar constant. The image has shape (400, 248, 3);# we multiply it by the array [1, 0.95, 0.9] of shape (3,);# numpy broadcasting means that this leaves the red channel unchanged,# and multiplies the green and blue channels by 0.95 and 0.9# respectively.img_tinted = img * [1, 0.95, 0.9]# Resize the tinted image to be 300 by 300 pixels.img_tinted = imresize(img_tinted, (300, 300))# Write the tinted image back to diskimsave('assets/cat_tinted.jpg', img_tinted) Left: The original image. Right: The tinted and resized image. MATLAB filesThe functions scipy.io.loadmat and scipy.io.savemat allow you to read andwrite MATLAB files. You can read about themin the documentation. Distance between pointsSciPy defines some useful functions for computing distances between sets of points. The function scipy.spatial.distance.pdist computes the distance between all pairsof points in a given set: 123456789101112131415161718import numpy as npfrom scipy.spatial.distance import pdist, squareform# Create the following array where each row is a point in 2D space:# [[0 1]# [1 0]# [2 0]]x = np.array([[0, 1], [1, 0], [2, 0]])print x# Compute the Euclidean distance between all rows of x.# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],# and d is the following array:# [[ 0. 1.41421356 2.23606798]# [ 1.41421356 0. 1. ]# [ 2.23606798 1. 0. ]]d = squareform(pdist(x, 'euclidean'))print d You can read all the details about this functionin the documentation. A similar function (scipy.spatial.distance.cdist) computes the distance between all pairsacross two sets of points; you can read about itin the documentation. MatplotlibMatplotlib is a plotting library.In this section give a brief introduction to the matplotlib.pyplot module,which provides a plotting system similar to that of MATLAB. PlottingThe most important function in matplotlib is plot,which allows you to plot 2D data. Here is a simple example: 12345678910import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)plt.show() # You must call plt.show() to make graphics appear. Running this code produces the following plot: With just a little bit of extra work we can easily plot multiple linesat once, and add a title, legend, and axis labels: 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])plt.show() You can read much more about the plot functionin the documentation. SubplotsYou can plot different things in the same figure using the subplot function.Here is an example: 1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show() You can read much more about the subplot functionin the documentation. ImagesYou can use the imshow function to show images. Here is an example: 12345678910111213141516171819import numpy as npfrom scipy.misc import imread, imresizeimport matplotlib.pyplot as pltimg = imread('assets/cat.jpg')img_tinted = img * [1, 0.95, 0.9]# Show the original imageplt.subplot(1, 2, 1)plt.imshow(img)# Show the tinted imageplt.subplot(1, 2, 2)# A slight gotcha with imshow is that it might give strange results# if presented with data that is not uint8. To work around this, we# explicitly cast the image to uint8 before displaying it.plt.imshow(np.uint8(img_tinted))plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[15]反向解析路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B15%5D%E5%8F%8D%E5%90%91%E8%A7%A3%E6%9E%90%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[给url起一个名字1url(r'^(?P&lt;id&gt;\d+)/$', views.posts_detail, name="detail"), Template中 1&lt;a href="&#123;% url 'detail' id=obj.id %&#125;"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt; Python代码中 123456from django.shortcuts import redirect # 调转请求from django.urls import reverse # 反向解析url的...return redirect(reverse("detail", kwargs=&#123;"id": 3&#125;)) Model中 get_absolute_url() 12345def get_absolute_url(self): return reverse("detail", kwargs=&#123;"id": self.id&#125;) # return "/post/&#123;&#125;/".format(self.id) # return "/post/%s/" % (self.id) url的命令空间 namespace在一级的url设置的，用于区分不用的app，因为不同的app下面可能存在同名的 url1234567url(r'^post/', include(posts_urls, namespace="post")),&lt;a href="&#123;% url 'post:detail' id=obj.id %&#125;"&gt;def get_absolute_url(self): return reverse("post:detail", kwargs=&#123;"id": self.id&#125;) via Django1.10教程 -15 -反向解析路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[14]动态路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B14%5D%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[Django中的URL介绍/bbs/urls.py 1234567from posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^post/', include(posts_urls)),] /posts/urls.py 1234567from . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^detail/$', views.posts_detail), ... http://127.0.0.1:8000/post/detail/ 动态路由和参数传递页面点击是通过设置a标签来调转的 后端view中通过id来查询不同的帖子对象1url(r'^detail/(?P&lt;id&gt;\d+)/$', views.posts_detail,), 在模板中组合起来1&lt;h2 class="blog-post-title"&gt;&lt;a href="/post/detail/&#123;&#123;obj.id&#125;&#125;/"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt;&lt;/h2&gt; via Django1.10教程 -14 -动态路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[13]从数据库中获取某个对象]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B13%5D%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E8%8E%B7%E5%8F%96%E6%9F%90%E4%B8%AA%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[获取对象123456models.Post.objects.get(id=3)models.Post.objects.get(title="隔壁老王是谁？") # 获取 标题是隔壁老王是谁？的帖子models.Post.objects.get(title__icontains="隔壁") # 获取 标题中包含隔壁两个字 的帖子 404页面，get_object_or_4041234from django.shortcuts import get_object_or_404get_object_or_404(models.Post, id=3) 新建了一个detail.html页面用于展示的是帖子详情 12345&lt;div class="row"&gt;&lt;h1&gt;&#123;&#123;obj.title&#125;&#125;&lt;/h1&gt;&gt;&lt;p&gt;&#123;&#123;obj.content&#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;!-- /.row --&gt; 修改views.py123456789def posts_detail(request): # obj = models.Post.objects.get(id=3) obj = get_object_or_404(models.Post,id=1) data = &#123; "obj":obj &#125; return render(request,"detail.html",data) 效果 via Django1.10教程 -13 -从数据库中获取某个对象]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[12]Queryset介绍以及Template context补充]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B12%5DQueryset%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8ATemplate%20context%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[Django shell1python manage.py shell 进入django shell,可以在命令行做一些操作。 12345678910111213141516#查询出所有的post对象，for循环遍历queryset = models.Post.objects.all()for obj in queryset: print(obj.title) print(obj.content) print(obj.update) print(obj.timestamp) print(obj.id) print(obj.pk)#创建一条新的post记录models.Post.objects.create(title="abc", content="abc abc abc")#获取所有的记录条数models.Post.objects.all().count() queryset介绍queryset 是一个可以遍历取值的结果集 queryset 里面都是 对象，可以通过对象.字段名的形式取值 一个对象就对应了数据库里面的一条记录 数据库里面的一条记录 &lt;- ORM -&gt; Python中的对象 如何在前端使用queryset1. 从数据库里取出数据1queryset = models.Post.objects.all() 2. 把渠道的数据塞进 data1data = &#123;"queryset": queryset&#125; 即，修改views.py 12345678910from . import modelsdef posts_home(request): queryset = models.Post.objects.all() data = &#123; "queryset": queryset, "name": "home", "age": "18", &#125; return render(request,"base.html",data) 3. 用data去填充前端的页面1render(request, "xxx.html", data) 123456789&#123;% for obj in queryset %&#125; &lt;div class="blog-post"&gt; &lt;h2 class="blog-post-title"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/h2&gt; &lt;p class="blog-post-meta"&gt;&#123;&#123; obj.timestamp &#125;&#125;&lt;a href="#"&gt;Mark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&#123;&#123; obj.content &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;!-- /.blog-post --&gt;&#123;% endfor %&#125; 效果 补充 Django的模板语言语法12345&#123;% for x in xx %&#125;&#123;% endfor %&#125;&#123;&#123; val &#125;&#125; via Django1.10教程 -12 -Queryset介绍以及Template context补充]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[11]模板Template context和Bootstrap使用]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B11%5D%E6%A8%A1%E6%9D%BFTemplate%20context%E5%92%8CBootstrap%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Template context视图views.py中：1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): context = &#123; "title"： "home" &#125; return render(request,"index.html",context)def posts_create(request): context = &#123; "title"： "create" &#125; return render(request,"index.html",context)def posts_detail(request): context = &#123; "title"： "detail" &#125; return render(request,"index.html",context) index.html中用 context去填充模板 index.html，然后再返回 index.html中添加如下代码：1&lt;h1&gt;Hello &#123;&#123; title &#125;&#125;&#125;&lt;/h1&gt; context是一个字典,存放要渲染到页面的数据 Bootstrap模板使用1. 下载模板文件模板文件链接 2. 把html文件拷贝到了templates文件夹下面3. 新建一个statics文件夹，用于存放css文件和js文件4. 把css和js文件拷贝到statics文件夹下5. 在settings.py中配置statics文件夹1234STATICFILES_DIRS = [ os.path.join(BASE_DIR, "statics")] 6. 将html文件中指定位置的css和js文件的路径修改为/static/…文档结构如下： 效果如图： via Django1.10教程 -11 -模板Template context和Bootstrap使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[10]模板Template的配置]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B10%5D%E6%A8%A1%E6%9D%BFTemplate%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[BASE_DIR 和 os.path.join(xx, xxx)12345import os# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 创建一个templates目录 在settings.py中配置templates路径123456789101112131415TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR,"templates")], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 在templates文件夹下创建html文件12345678910&lt;!DOCTYPE html&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;try django&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello World! Django!&lt;/h1&gt;&gt;&lt;/body&gt;&lt;/html&gt; 在views.py中使用templates目录下的html文件12345678910# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return render(request,"index.html") 查看效果 via Django1.10教程 -10 -模板Template的配置]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[09]配置URL]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B09%5D%E9%85%8D%E7%BD%AEURL%2F</url>
    <content type="text"><![CDATA[bbs URL ConfigurationThe urlpatterns list routes URLs to views. For more information please see: https://docs.djangoproject.com/en/1.11/topics/http/urls/ Examples:Function views1. Add an import: from my_app import views 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, views.home, name=&apos;home&apos;) Class-based views1. Add an import: from other_app.views import Home 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, Home.as_view(), name=&apos;home&apos;) Including another URLconf1. Import the include() function: from django.conf.urls import url, include 2. Add a URL to urlpatterns: url(r&apos;^blog/&apos;, include(&apos;blog.urls&apos;)) project(project/urls.py)中的urls.py常见的几种写法1、基于function的view详见上回 2、基于class的view1、修改posts[app]下views.py123456from django.views.generic import ListViewclass PostList(ListView): """post的视图类""" def get(self,request): return HttpResponse("&lt;h1&gt;post的视图类&lt;/h1&gt;") 2、修改bbs[project]下urls.py1234567from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), url(r'^home/', views.PostList.as_view()),] 3、效果 3、view别名12345678from app01 import views as app01_viewsfrom app02 import views as app02_viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', app01_views.posts_home), url(r'^home/', app02_views.PostList.as_view()),] 4、在app中的urls.py的使用1、posts[app]下增加urls.py：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 2、在project下的urls.py 中设置第一级的路由：12345678910from django.conf.urls import url,includefrom django.contrib import adminfrom posts import viewsfrom posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), # url(r'^home/', views.PostList.as_view()), url(r'^post/',include(posts_urls))] 3、在app下的urls.py中设置第二级路由，同第一步：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 多视图示例1、app下的urls.py1234567891011from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^create/$', views.posts_create), url(r'^update/$', views.posts_update), url(r'^detail/$', views.posts_detail), url(r'^delete/$', views.posts_delete), url(r'^', views.posts_home),] 2、app下的view.py12345678910111213141516171819202122# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!")def posts_create(request): return HttpResponse("&lt;h1&gt;posts_create&lt;/h1&gt;")def posts_update(request): return HttpResponse("&lt;h1&gt;posts_update&lt;/h1&gt;")def posts_detail(request): return HttpResponse("&lt;h1&gt;posts_detail&lt;/h1&gt;")def posts_delete(request): return HttpResponse("&lt;h1&gt;posts_delete&lt;/h1&gt;") 注： 1、如果匹配到排在上面的正则表达式那么就不会适配下面的正则表达式了 2、不要忘记加’^’和’$’ via Class-based views via Django1.10教程 -09 -配置URL]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[08]第一个view（视图）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B08%5D%E7%AC%AC%E4%B8%80%E4%B8%AAview%EF%BC%88%E8%A7%86%E5%9B%BE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[定义我们第一个视图修改posts[app]下views.py12345# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!") url建立映射到view修改bbs[project]下urls.py123456from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', views.posts_home),] 效果 via Django1.10教程 -08 -第一个view（视图）]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[07]增删改查（CRUD）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B07%5D%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%EF%BC%88CRUD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CRUD 缩写 动作名称 SQL HTTP 实际动作 C Create INSERT PUT/POST 增加 R Retrieve SELECT GET 查询 U Update UPDATE POST/PUT/PATCH 更新 D Delete DELETE DELETE 删除 HTTP请求方法 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 via Django1.10教程 -07 -增删改查（CRUD） via HTTP请求方法]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[06]定制admin]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B06%5D%E5%AE%9A%E5%88%B6admin%2F</url>
    <content type="text"><![CDATA[在post/admin.py文件中添加如下代码：1234567891011class PostAdmin(admin.ModelAdmin): list_display = [,] # 控制页面展示哪些字段 list_display_links = [,] # 控制哪些字段是超链接 list_filter = [,] # 支持在右侧过滤的字段 search_fields = [,] # 支持搜索的字段 list_editable = [,] # 支持直接编辑的字段，注意！不能与list_display_links重复！ class Meta: model = model.Post 补充如何修改admin中显示的app名字？ 1. 在posts/apps.py中123PostsConfig类中添加verbose_name = "帖子" 2. posts/init.py12default_app_config = "posts.apps.PostsConfig" 示例1、修改posts[app]下admin.py1234567891011121314151617181920# Register your models here.from posts import modelsclass PostAdmin(admin.ModelAdmin): """docstring for PostAdmin""" list_display = ["title","content"] list_display_links = ["title"] list_filter = ["timestamp","content"] search_fields = ["title","content"] list_editable = ["content"] # def __init__(self, arg): # super(PostAdmin, self).__init__() # self.arg = arg class Meta: model = models.Post admin.site.register(models.Post,PostAdmin) 2、修改posts[app]下apps.py1234class PostsConfig(AppConfig): name = 'posts' verbose_name = "帖子" 3、修改posts[app]下init.py12default_app_config = "posts.apps.PostsConfig" 4、查看效果 via Django1.10教程[06]_定制admin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[05]model与admin的关系]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B05%5Dmodel%E4%B8%8Eadmin%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[修改app的models中class名称尝试去掉models.py里面 把class Posts改成 class Post 一行代码让model在admin中可见1(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver via Django1.10教程[05]_model与admin的关系]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[04]App和Model]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B04%5DApp%E5%92%8CModel%2F</url>
    <content type="text"><![CDATA[Django中project和app分别是什么？ Project：python manage.py startproject [你的project名] app: python manage.py startapp [你的app名字] project是项目，project下面分一个或多个app 我们的第一个App12python manage.py startapp [app的名字] 一定要注意：把你的app在project的settings.py里面的INSTALLED_APPS加上 Modelmodels.py里面创建类（与数据库建立联系的） 1234python manage.py makemigrations（告诉Django我设计了一些表结构，你去准备一下）python manage.py migrate（告诉Django去数据库里操作一下刚才的动作） MVC:Model View Controllers MTV:Model View Template 1、创建一个app 2、修改app目录下的models.py文件12345678910111213from django.db import modelsclass Posts(models.Model): title = models.CharField(max_length=256) # 标题，存文字的 content = models.TextField() # 内容 update = models.DateTimeField(auto_now=True,auto_now_add=False) # 更新时间 timestamp = models.DateTimeField(auto_now=False,auto_now_add=True) #创建时间 def __str__(): # python3 return self.title def __unicode__(): #python2 return self.title 3、修改project目录下的setting.py文件INSTALLED_APPS 中添加你刚创建的app名称。 4、提交修改至数据库 via Django1.10教程[04]之App和Model via Django1.10教程[04]_APPandModel补录]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[02]第一个django程序]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B02%5D%E7%AC%AC%E4%B8%80%E4%B8%AAdjango%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[创建一个bbs项目 bbs可以作为一个独立的app部署，重命名文件名为src 同步数据表：python .\manage.py migrate 修改setting.py配置文件 创建超级用户：python .\manage.py createsuperuser 启动服务：python .\manage.py runserver 浏览器中输入http://127.0.0.1:8000/admin/进入django的管理后台 via Django1.10教程[02]之第一个Django程序]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[03]DjangoAdmin]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B03%5DDjangoAdmin%2F</url>
    <content type="text"><![CDATA[WSGIWSGI(Web server gateway interface)：Web服务器网关接口是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。自从WSGI被开发出来以后，许多其它语言中也出现了类似接口。 web app &lt;-WSGI-&gt;web server(nginx/tomcat) createsuperuser注意事项注：至少八个字符，不能是简单的数字 Django admin使用的介绍 python manage.py runserver 127.0.0.1:8000 浏览器里面输入：http://127.0.0.1:8000/admin/ 输入用户名、密码登录即可。 urls.py使用的介绍django的路由是通过正则表达式来匹配的。 via Django1.10教程[03]之DjangoAdmin via Django1.10教程[03]之DjangoAdmin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[01]Virtualenv&Django]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B01%5DVirtualenv%26Django%2F</url>
    <content type="text"><![CDATA[配置虚拟环境安装virtualenv:12pip install virtualenv 使用virtualenv创建虚拟环境：virtualenv [环境（文件夹）名] 启用虚拟环境：.\Scripts\activate 退出虚拟环境：deactivate 安装Django12pip install django 第一个Django项目1、python manage.py startproject my_first(我们的项目名) 2、python manage.py runserver 127.0.0.1:8000 补充： 1、PowerShell里面执行activate失败？ 输入： set-executionpolicy RemoteSigned 2、pip freeze命令使用 1、pip freeze &gt; requirements.txt (保存依赖包到requirements.txt) 2、pip install -r requirements.txt (批量安装项目需要的所有依赖包) via Django1.10教程[01]之virtualenv使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error Analysis (Post-Modeling)]]></title>
    <url>%2F2017%2F10%2F07%2FError%20Analysis%20(Post-Modeling)%2F</url>
    <content type="text"><![CDATA[Error analysis is a broad term that refers to analyzing the misclassified or high error observations from your model and deciding on your next steps for improvement. This is performed after training your first model. Possible next steps include collecting more data, splitting the problem apart, or engineering new features that address the errors. To use error analysis for feature engineering, you’ll need to understand why your model missed its mark. Here’s how: Start with larger errors: Error analysis is typically a manual process. You won’t have time to scrutinize every observation. We recommend starting with those that had higher error scores. Look for patterns that you can formalize into new features. Segment by classes: Another technique is to segment your observations and compare the average error within each segment. You can try creating indicator variables for the segments with the highest errors. Unsupervised clustering: If you have trouble spotting patterns, you can run an unsupervised clustering algorithm on the misclassified observations. We don’t recommend blindly using those clusters as a new feature, but they can make it easier to spot patterns. Remember, the goal is to understand why observations were misclassified. Ask colleagues or domain experts: This is a great complement to any of the other three techniques. Asking a domain expert is especially useful if you’ve identified a pattern of poor performance (e.g. through segmentations) but don’t yet understand why.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 Tips for Writing Better Python]]></title>
    <url>%2F2017%2F10%2F07%2F5%20Tips%20for%20Writing%20Better%20Python%2F</url>
    <content type="text"><![CDATA[1. Make your code a PIP-installable PackageWhen you come across a new Python package, it’s always easier to start using it if all you have to do is run “pip install” followed by the package name or location. There are a number of ways to do this, my “go to” being to create a setup.py file for my project. Assume we have a simple Flask program in “flask_example.py”: 12345678910111213from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello, World!'def main(): app.run()if __name__ == ‘__main__’: main() We can turn this into an installable Python package by first moving it into a separate folder (let’s call this “flask_example/”. Then, we can make a setup.py file in the root project folder that looks like this: 123456789101112131415from distutils.core import setupsetup( name='flask_example', version='1.0', description='Hello, World! in flask.', packages=['flask_example'], install_requires=[ 'Flask==0.12.2' ], entry_points = &#123; 'console_scripts': 'runserver=flask_example.flask_example:main' &#125;) This has a few advantages that comes with it. First, you can now install your app locally using “pip install -e .” This makes it easier for developers to clone and install your project because the setup.py file will take care of all the heavy lifting. Second, with the setup.py file comes dependency management. The install_requires variable allows you to define packages and specific versions to use. If you’re not sure what packages and versions you are using, run “pip freeze” to view them. Lastly, this allows you to define entry points for your package, which allows you to now execute the code on the command line by simply running “runserver”. 2. Lint Your Code in a Pre-Commit HookUsing a linter can fix so many problems in code. PyLint is a great linter for Python, and if you’re using a version control system like Git, you can make Git run your code through a linter before it lets you commit your code. To do this, install the PyLint package. 1pip install pylint Then, add the following code to .git/hooks/pre-commit. If you already have a pre-commit hook doing something, simple append the pylint command to the end of your file. 123#!/bin/shpylint &lt;your_package_name&gt; This will catch all kinds of mistakes before they even make it into your Git repository. You’ll be able to say goodbye to accidentally pushing code with syntax errors, along with the many other things a good linter catches. 3. Use Absolute Imports over Relative ImportsIn python, there are very few situations where you should be using relative module paths in your import statements (e.g. from . import module_name). If you’ve gone through the process of creating a setup.py (or similar mechanism) for your Python project, then you can simply reference submodules by their full module path. Absolute imports are recommended by PEP-8, the Python style guide. This is because they’re more informative in their names and, according to the Python Software Foundation, are “better behaved.” I’ve been in positions where using relative imports has quickly become a nightmare. It’s fine when you first start coding, but once you start moving modules around and doing significant refactoring, they can really cause quite the headache. 4. Context ManagersWhenever you’re opening a file, stream, or connection, you’re usually working with a context manager. Context managers are great because when used properly they can handle the closing of your file should an exception be thrown. In order to do this, simply use the with keyword. The following is how most beginner Python programmers would probably write to a file. 1234f = open(‘newfile.txt’, ‘w’)f.write(‘Hello, World!’)f.close() This is pretty straightforward. But imagine this: You’re writing thousands of lines to a file. At some point, an exception is thrown. After that happens, your file isn’t properly closed, and all the data you thought you had already written to the file is corrupt or non-existent. Don’t worry though, with some simple refactoring we can ensure the file closes properly, even if an exception is encountered. We can do this as shown below. 123with open(‘file’, ‘w’) as file: file.write(‘Hello, World!’) Volla! It really is that simple. Additionally, the code looks much cleaner like this, and is more concise. You can also open multiple context managers with a single “with” statement, eliminating the need to have nested “with” statements. 1234with open(‘file1’, ‘w’) as f1, open(‘file2’, ‘w’) as f2: f1.write(‘Hello’) f2.write(‘World’) 5. Use Well-Named Functions and VariablesIn Python, and untyped languages especially, it can easily become unclear what functions are returning what values. Especially when you’re just a consumer of some functions in a library. If you can save the developer the 5 minutes it takes to look up the function in your documentation, than that’s actually a really valuable improvement. But how do we do this? How can doing something as simple as changing the name of a variable save development time? There are 3 main things I like to take into consideration when naming a function or variable: What the function or variable does Any units associated with the function or variable The data type the function or variable evaluates to For example, if I want to create a function to calculate the area of a rectangle, I might name it “calc_rect_area”. But this doesn’t really let the user know much. Is it going to return the value, or is it going to store it somewhere? Is the value in feet? Meters? To enhance the name of this function, I would change it to “get_rect_area_sq_ft”. This makes it clear to the user that the function gets and returns the area. It also lets the user know that the area is in square feet. If you can save the developer 5 minutes here and there with some nicely named functions and variables, that time starts to add up, and they’ll appreciate your code all the more. ConclusionThese tips are ones that I have found to be helpful over my years as a Python programmer. Some of them I figured out on my own over time, some of them others had to teach me so I could learn them. I hope this list helps you in your effort to write better Python. via here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可视化—matploblib 解决中文显示的问题]]></title>
    <url>%2F2017%2F10%2F07%2F%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94matploblib%20%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[导入相关包123from matplotlib import mplimport matplotlib.pyplot as plt 指定字体123mpl.rcParams['font.sans-serif'] = ['SimHei']mpl.rcParams['axes.unicode_minus'] = False 测试：使用中文12plt.title(u'我是中文')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（六）之模型融合]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到增加高阶特征并没有带来模型效果显著提升。考虑到sigmoid函数在0.5处附近区分度比较大，在远离0.5处概率值变换不是很明显，考虑sigmoid梯度平缓的地方，拆分成0.7分别单独领出来，重新建模，期望能提升模型效果。 按照0.3和0.7两个分割点划分，需要重新再训练出三个模型，然后再把这三个模型整体的效果与未重新训练前的效果进行对比。 首先定义几个函数，省得写一些重复的代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240 def plot_ks(proba,target,axistype=0,out_path=False): a = pd.DataFrame(np.array([proba,target]).T,columns=['proba','target']) a.sort_values(by='proba',ascending=False,inplace=True) a['sum_Times']=a['target'].cumsum() total_1 = a['target'].sum() total_0 = len(a) - a['target'].sum() a['temp'] = 1 a['Times']=a['temp'].cumsum() a['cdf1'] = a['sum_Times']/total_1 a['cdf0'] = (a['Times'] - a['sum_Times'])/total_0 a['ks'] = a['cdf1'] - a['cdf0'] a['percent'] = a['Times']*1.0/len(a) idx = np.argmax(a['ks']) # print a.loc[idx] if axistype == 0: ''' KS曲线,横轴为按照输出的概率值排序后的观察样本比例 ''' plt.figure() plt.plot(a['percent'],a['cdf1'], label="CDF_positive") plt.plot(a['percent'],a['cdf0'],label="CDF_negative") plt.plot(a['percent'],a['ks'],label="K-S") sx = np.linspace(0,1,10) sy = sx plt.plot(sx,sy,linestyle='--',color='darkgrey',linewidth=1.2) plt.legend() plt.grid(True) ymin, ymax = plt.ylim() plt.xlabel('Sample percent') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') # 虚线 t = a.loc[idx]['percent'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) else: ''' 改变横轴,横轴为模型输出的概率值 ''' plt.figure() plt.grid(True) plt.plot(1-a['proba'],a['cdf1'], label="CDF_bad") plt.plot(1-a['proba'],a['cdf0'],label="CDF_good") plt.plot(1-a['proba'],a['ks'],label="ks") plt.legend() ymin, ymax = plt.ylim() plt.xlabel('1-[Predicted probability]') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') plt.show() # 虚线 t = 1 - a.loc[idx]['proba'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) if out_path: file_name = out_path if isinstance(out_path, str) else None plt.savefig(file_name) else: plt.show() return a.loc[idx] ''' 搜索 最优超参数c ''' def grid_search_lr_c(X_train,y_train,df_coef_path=False ,pic_coefpath_title='Logistic Regression Path',pic_coefpath=False ,pic_performance_title='Logistic Regression Performance',pic_performance=False): # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01,class_weight='balanced') cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df['ks'] = ks coef_cv_df['c'] = cs # df_coef_path = 'E:\\Code\\ScoreCard\\agr_coef_cv_df_balanced_nextrain_beta.csv' if df_coef_path: file_name = df_coef_path if isinstance(df_coef_path, str) else None coef_cv_df.to_csv(file_name) coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') # pic_coefpath_title = 'Logistic Regression Path Class_weight Balanced Nextrain_beta' plt.title(pic_coefpath_title) plt.axis('tight') if pic_coefpath: file_name = pic_coefpath if isinstance(pic_coefpath, str) else None plt.savefig(file_name) else: plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') # pic_performance_title = 'Logistic Regression Performance Class_weight balanced Nextrain_beta' plt.title(pic_performance_title) plt.axis('tight') if pic_performance: file_name = pic_performance if isinstance(pic_performance, str) else None plt.savefig(file_name) else: plt.show() flag = coefs_&lt;0 idx = np.array(ks)[flag.sum(axis=1) == 0].argmax() return (cs[idx],ks[idx])``` **然后拆分模型，合并计算ks,将一个已经预测后的模型，根据输出的概率值，给定两个分割点，划分区间重新用LR训练。**``` python def lr_sub_train(X_train,y_train,idx): index = idx sub_xtrain = X_train.loc[index,:] sub_ytrain = y_train.loc[index] c,ks = grid_search_lr_c(sub_xtrain,sub_ytrain) clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced') clf_l1_LR.fit(sub_xtrain, sub_ytrain) sub_proba = clf_l1_LR.predict_proba(sub_xtrain)[:,1] return c,ks,sub_proba,sub_ytrain def train_segmentation_model(X_train,y_train,proba_train,left_point,right_point): idx1 = (proba_train &lt;= left_point) idx2 = (proba_train &gt; left_point)&amp;(proba_train &lt;= right_point) idx3 = (proba_train &gt; right_point) c1,ks1,sub_proba1,sub_ytrain1 = lr_sub_train(X_train,y_train,idx1) c2,ks2,sub_proba2,sub_ytrain2 = lr_sub_train(X_train,y_train,idx2) c3,ks3,sub_proba3,sub_ytrain3 = lr_sub_train(X_train,y_train,idx3) print 'c1:',c1,' ks1:',ks1 print 'c2:',c2,' ks2:',ks2 print 'c3:',c3,' ks3:',ks3 proba = [] proba.extend(sub_proba1) proba.extend(sub_proba2) proba.extend(sub_proba3) y = [] y.extend(sub_ytrain1) y.extend(sub_ytrain2) y.extend(sub_ytrain3) return get_ks(np.array(proba),np.array(y)) # 0.29072674001489612 输出 1234567891011121314151617'''Computing regularization path ...2017-10-03 16:04:45.2290002017-10-03 16:07:12.977000('This took ', datetime.timedelta(0, 147, 748000))Computing regularization path ...2017-10-03 16:07:28.1180002017-10-03 16:08:59.740000('This took ', datetime.timedelta(0, 91, 622000))Computing regularization path ...2017-10-03 16:09:03.4750002017-10-03 16:09:31.128000('This took ', datetime.timedelta(0, 27, 653000))c1: 0.00663966525938 ks1: 0.208779933516c2: 0.00189750388742 ks2: 0.201006811034c3: 0.000415735598049 ks3: 0.169741554117''' 结果并没有按照我想象的那样，反而分割后的效果没有单一模型的效果好。原因是woe变换没有重新训练?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（五）之增加高阶特征]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8B%E5%A2%9E%E5%8A%A0%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解分割模型带来了模型效果的提升。这节我们尝试通过增加高阶特征来增强模型的表达能力。由于备选变量比较多，盲目的全部生成高阶特征会造成特征数量指数级的爆炸式增长，这个不是我想要的，所以先根据IV值进行单变量特征选择，筛选比较重要的变量，然后再次基础上才生成高阶特征。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp from sklearn.preprocessing import PolynomialFeatures get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') poly_features = [] y_train = dataset_train['target'] poly = PolynomialFeatures(2) X_train = poly.fit_transform(dataset_train[poly_features]) del dataset_train import gc gc.collect() # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df_poly.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path Poly') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance Poly') plt.axis('tight') plt.show() fig2.show() fig1.show() 注：X_train第一列是常数项，就是系数图中的那条蓝色线。 从模型的表现来看，模型效果并没有显著提升，根据奥卡姆剃刀原理，放弃采用高阶特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（四）之分割模型]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解剔除噪声样本带来了模型效果的提升。同时也感受到了“与此相关”变量的重要，在这个字段上不同取值的客户所对应的入模特征分布也许有很大差异，于是我尝试了根据这个字段分割建模，期望能达到提升的效果。 按条件分割数据集： agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 1 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 2 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 3 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 &gt;= 4 在分割出的4个训练集上分布训练WOE转换，然后对测试集进行对应的WOE转换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339__author__ = 'boredbird'import pandas as pdimport woe.config as configimport woe.feature_process as fpimport woe.eval as evalimport numpy as npimport pickleimport matplotlib.pyplot as pltfrom datetime import datetimefrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import l1_min_cfrom scipy.stats import ks_2sampprint '*************************************A****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************B****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv')print '*************************************C****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_test_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************D****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv')print '*************************************E****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_model_var_tbl_validation_20160806_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************F****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv')# 'E:\\Code\\ScoreCard\\whitelist_ext_civ_list_alpha.pkl'# 'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr.csv'def process_train_woe(infile_path=None,outfile_path=None,rst_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = infile_pathcfg = config.config()cfg.load_file(config_path,data_path)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1# change feature dtypesfp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)rst = []# process woe transformation of continuous variablesprint 'cfg.global_bt',cfg.global_btprint 'cfg.global_gt', cfg.global_gtfor var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))# process woe transformation of discrete variablesfor var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'rst.append(fp.proc_woe_discrete(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))eval.eval_feature_detail(rst, outfile_path)output = open(rst_path, 'wb')pickle.dump(rst,output)output.close()print '*************************************G****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl')print '*************************************H****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl')print '*************************************I****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl')print '*************************************J****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl')def process_woe_trans(in_data_path=None,rst_path=None,out_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = in_data_pathcfg = config.config()cfg.load_file(config_path, data_path)fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'output = open(rst_path, 'rb')rst = pickle.load(output)output.close()# Training dataset Woe Transformationfor r in rst:cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name], r)cfg.dataset_train.to_csv(out_path)print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv')print '*************************************O****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv')print '*************************************P****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv')print '*************************************Q****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv')print '*************************************R****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv')print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv')"""Training Model"""get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticdef train_model(infile,outfile,fig1,fig2):dataset_train = pd.read_csv(infile)cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']# init a LogisticRegression modelclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3)print("Computing regularization path ...")start = datetime.now()print startcoefs_ = []ks = []for c in cs:clf_l1_LR.set_params(C=c)clf_l1_LR.fit(X_train, y_train)coefs_.append(clf_l1_LR.coef_.ravel().copy())proba = clf_l1_LR.predict_proba(X_train)[:,1]ks.append(get_ks(proba,y_train))end = datetime.now()print endprint("This took ", end - start)coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns)coef_cv_df['ks'] = kscoef_cv_df['cs'] = cscoef_cv_df.to_csv(outfile)coefs_ = np.array(coefs_)plt.figure()plt.plot(np.log10(cs), coefs_)ymin, ymax = plt.ylim()plt.xlabel('log(C)')plt.ylabel('Coefficients')plt.title('Logistic Regression Path')plt.axis('tight')plt.savefig(fig1)plt.close()plt.figure()plt.plot(np.log10(cs), ks)plt.xlabel('log(C)')plt.ylabel('ks score')plt.title('Logistic Regression Performance')plt.axis('tight')plt.savefig(fig2)plt.close()train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd1.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd1.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd1.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd2.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd2.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd2.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd3.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd3.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd3.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd4.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd4.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd4.png') """Model Performance"""def predict_model(train_file,test_file,dataset_validation_path,c):print '*********************************************************'cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced')dataset_train = pd.read_csv(train_file)dataset_test = pd.read_csv(test_file)dataset_validation = pd.read_csv(dataset_validation_path)# fill nullfor var in candidate_var_list:if dataset_validation[var].isnull().sum()&gt;0:dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean()if dataset_test[var].isnull().sum()&gt;0:dataset_test.loc[dataset_test[var].isnull(), (var)] = dataset_test[var].mean()X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']X_test = dataset_test[candidate_var_list]y_test = dataset_test['target']clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]print get_ks(proba,y_train)proba = clf_l1_LR.predict_proba(X_test)[:,1]print get_ks(proba,y_test)# predictionproba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1]print get_ks(proba,dataset_validation['target']) predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv' ,c=0.00948742173925)'''agr_dd1:0.3913372974170.4017604347210.41978303981'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv' ,c=0.00949207179031)'''agr_dd2:0.428815152590.4253306159710.423628758103'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv' ,c=0.0063130606165)'''agr_dd3:0.4448530527120.4429655932780.429972957194'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv' ,c=0.00479980706543)'''agr_dd4:0.456283776350.4451460672930.464371772806''' 结论由切分成4个数据集分别的ks表现来看，分割后的模型表现差异比较明显，并且agr_dd1，agr_dd2，agr_dd3，agr_dd4的ks逐渐提升，这也不难寻求业务理解。可见，在这个字段上分割模型是有意义。 逻辑回归是线性的且单一的分类器，即使我们把这4个数据集增加一个‘A’,’B’,’C’,’D’的字段标识，然后合并为一个数据集，也不能够达到分割的效果。分割模型相对于只是分割数据集会进一步增强模型的表现能力。 分割数据集 分割模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python本地模块名与系统安装的包名重名冲突问题]]></title>
    <url>%2F2017%2F10%2F04%2F%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9D%97%E5%90%8D%E4%B8%8E%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%E5%90%8D%E9%87%8D%E5%90%8D%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本地修改 woe包中的eval模块时，发现 系统老是import&quot;D:\ProgramData\Anaconda2\lib\site-packages\woe\eval.py&quot;路径下的eval文件。那是因为我已经安装了woe这个包，然后需要调用我修改后的woe源码的本地模块文件，系统自动优先import的是之前安装的woe版本。 解决方法如下： 1234567891011#-*-coding:utf-8-*-__author__='boredbird'# importwoe.configasconfig# importwoe.feature_processasfp# importwoe.evalasevalimport impconfig = imp.load_source('config',r'E:\Code\woe\woe\config.py')fp = imp.load_source('feature_process',r'E:\Code\woe\woe\feature_process.py')eval = imp.load_source('eval',r'E:\Code\woe\woe\eval.py') For Python 3.5+ use: 123456import importlib.utilspec = importlib.util.spec_from_file_location("module.name", "/path/to/file.py")foo = importlib.util.module_from_spec(spec)spec.loader.exec_module(foo)foo.MyClass() For Python 3.3 and 3.4 use: 12345from importlib.machinery import SourceFileLoaderfoo = SourceFileLoader("module.name", "/path/to/file.py").load_module()foo.MyClass()# (Although this has been deprecated in Python 3.4.) For Python 2 use: 1234import impfoo = imp.load_source('module.name', '/path/to/file.py')foo.MyClass() There are equivalent convenience functions for compiled Python files and DLLs.See it here. 另外一种方式，将要导入的模块路径添加到系统路径，并且放在&quot;D:\ProgramData\Anaconda2\lib\site-packages\这个路径前面。比如， 1234import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler']) 然后再讲你刚添加的路径元素调整在sys.path这个list中的位置。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（三）之剔除噪声]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E5%89%94%E9%99%A4%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到正则化带来了模型效果的提升。此时，在对建模宽表的加工过程检查发现，有一部分用户在特征加工时间窗口observe_date之前没有表现期，这部分的用户放在样本中其实是噪声，我们将之剔除，重新woe转换，然后再入模查看效果。 首先，搜索最优正则化参数c： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp import woe.config as config import woe.feature_process as fp import woe.eval as eval get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show() print ks print cs``` ![](https://i.imgur.com/812KAjt.png)![](https://i.imgur.com/dAO4rdb.png)然后，将最优正则化参数c代入，训练模型与预测：``` python c = 0.00211473526246312 clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01) ''' 预测并查看模型效果 ''' clf_l1_LR.fit(X_train, y_train) proba = clf_l1_LR.predict_proba(X_train)[:,1] get_ks(proba,y_train) # 0.42519514636341194 proba = clf_l1_LR.predict_proba(X_test)[:,1] get_ks(proba,y_test) # 0.42014866529356498 dataset_validation = pd.read_csv('E:\\ScoreCard\\pos_validation_20160806_agr_woe_trans.csv') # fill null for var in candidate_var_list: if dataset_validation[var].isnull().sum()&gt;0: dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean() # prediction proba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1] get_ks(proba,dataset_validation['target']) # 0.42007842928689282 可以看出，剔除一些噪声样本之后模型的效果有很大的提升，而且此时，训练集的效果与同时期验证，跨时期验证的效果都很接近，可以认为没有发生过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（二）之正则化]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[在上回的基础上，可以看出特征之间存在多重共线性，自然的联想到L1正则化，这次主要尝试用sklearn.linear_model中正则化的LogisticRegression去提升模型效果。 12345678910111213141516171819X = dataset_train[candidate_var_list]y = dataset_train['target']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)from sklearn.linear_model import LogisticRegressionclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]get_ks(proba,y_train)# 0.40141223793092612proba = clf_l1_LR.predict_proba(X_test)[:,1]get_ks(proba,y_test)# 0.39708735770404702 可以看出此时采用C=0.1正则化的模型已经有了很大的提升。 123456C : float, default: 1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. 可是，c是个超参数，如何设置才能达到最优的效果呢？ 还是以结果为导向，一个一个试。虽然不是全局最优，那也大差不差。 import matplotlib.pyplot as plt from datetime import datetime from sklearn import linear_model from sklearn.svm import l1_min_c cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start clf_l1_LR = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6) coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（一）之数据集初探]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[本系列记录我在工作中针对同一个数据集，运用逻辑回归（评分卡）模型不断尝试去提升模型ks值表现的过程。并不是范例，更谈不上指导意义。主要是为了记录自己的思考与验证过程，怕随便零散的写在代码注释里过后就忘了。 评分卡模型在数据处理过程中常用到woe转换，这部分的工作我采用了Python的包woe来完成。 Woe is a python package containing useful tools for WoE Transformation mostly used in ScoreCard Model for Credit Rating. woe包中树节点的分裂方法，是一种Local近似算法。对于每个特征，只考察分位点，减少计算复杂度；每次分裂前，重新提出候选切分点。 关于树节点分裂方法，这里有简单的介绍。 #数据初步处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170 # -*- coding:utf-8 -*- __author__ = 'boredbird' import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np import copy ''' 导入数据集与配置文件 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) ''' 填充空值 ''' for var in cfg.bin_var_list: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] columns = ['cus_id'] columns.extend(cfg.candidate_var_list) columns.append('target') dataset_train = cfg.dataset_train[columns] ''' 删除不用的变量，导出数据集，减少内存占用 ''' dataset_train.to_csv('E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv') ''' 重新导入数据集 并指定变量类型 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] # process woe transformation of continuous variables for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05)) ''' 保存rst至文件 ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'wb') pickle.dump(rst,output) output.close() ''' 格式化输出feature_detail，查看每个变量的iV值 ''' feature_detail = eval.eval_feature_detail(rst,'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail.csv')``` #Woe 转换 ``` python ''' Woe Transformation ''' ''' 加载用于存储分割点信息的列表rst ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'rb') rst = pickle.load(output) output.close() ''' FILL NULL 因为在生成rst之前做了空值填充处理， 所以在利用rst进行woe转换之前，也要分别对连续变量和离散变量做同样的空值填充。 ''' for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing' #Training dataset Woe Transformation for r in rst: cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name],r) cfg.dataset_train.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206.csv') ''' Validation Dataset Transformation ''' #load dataset and eliminate variables dataset_validation = pd.read_csv('E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_validation_20160806.csv') var_exists_list = [rst[i].var_name for i in range(len(rst))] var_exists_list.extend(['cus_id','target',]) columns_to_drop = [var for var in dataset_validation.columns if var not in var_exists_list] dataset_validation = dataset_validation.drop(columns_to_drop,axis=1) #change variable dtypes before woe transformation for var in [tmp for tmp in cfg.bin_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = 'missing' #Validation dataset Woe Transformation for r in rst: if r.var_name in dataset_validation.columns: dataset_validation[r.var_name] = woe_trans(dataset_validation[r.var_name],r) dataset_validation.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_validation_20160806.csv')``` #跑Logit Regression 基于woe替换后的变量，删除iV值比较低的（删除了iv小于0.01的变量），调用statsmodels跑逻辑回归。``` python import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np dataset_train = pd.read_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206_new.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] import numpy as np from sklearn.model_selection import train_test_split X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) import statsmodels.api as sm logit_mod = sm.Logit(y_train,X_train) logit_res = logit_mod.fit(disp=0) # print('Parameters: ', logit_res.params) print logit_res.summary() 由上表可以看出当前这个模型存在两个问题： 1、存在变量系数显著性检验p值不通过的变量； 2、存在coef系数为负的情况，本身经过了woe转换后的变量的woe值与LR的概率值是正相关关系，系数应该为正，这种情况应该是变量之间的多重共线性导致的。 #模型评价，K-S 1234567891011121314from scipy.stats import ks_2sampget_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticproba = logit_res.predict(X_train)get_ks(proba,y_train)# 0.35121344450518605proba = logit_res.predict(X_test)get_ks(proba,y_test)# 0.34393694879059489proba = logit_res.predict(dataset_validation[features_list])get_ks(proba,dataset_validation['target'])# 0.35956755211806057]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM与XGBoost对比]]></title>
    <url>%2F2017%2F10%2F02%2FLightGBM%E4%B8%8EXGBoost%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[XGBoost的系统设计 更高效的工具包LightGBM LightGBM的改进直方图算法 直方图加速 建树过程的两种方法：Level-wise和Leaf-wise 并行优化（Optimization in parallel learning）特征并行 数据并行 GOSS EFB 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT算法原理与系统设计简介-来自wepon的分享]]></title>
    <url>%2F2017%2F10%2F02%2FGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%AE%80%E4%BB%8B-%E6%9D%A5%E8%87%AAwepon%E7%9A%84%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[一、泰勒公式 定义：泰勒公式是一个用函数在某点的信息描述其附近取值的公式。局部有效性二、最优化方法2.1 梯度下降法（Gradient descend method）2.2 牛顿法（Newton’s method）三、从参数空间到函数空间3.1 从Gradient descend 到 Gradient Boosting3.2 从Newton’s method 到 Newton Boosting四、Gradient Boosting Tree算法原理五、Newton Boosting Tree算法原理：详解XGBoost5.1 模型函数形式5.2 目标函数5.2.1 正则项5.2.2 误差函数的二阶泰勒展开5.3 回归树的学习策略5.3.1 打分函数5.3.2 树节点分裂方法5.3.3 缺失值的处理5.4 其它特性]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池老司机天音的经验分享]]></title>
    <url>%2F2017%2F10%2F02%2F%E5%A4%A9%E6%B1%A0%E8%80%81%E5%8F%B8%E6%9C%BA%E5%A4%A9%E9%9F%B3%E7%9A%84%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[化繁为简，各个突破 一、数据处理 25%1.1 缺失值填充某些场景中缺失值也是一个重要特征，如金融信贷，资料完善度不高的用户是潜在的违约对象 数值型：均值，中位数 类别型：众数 通过关联信息补全1.2 数据清洗 剔除缺失值比率大的字段 离群点剔除1.3 数据处理1.3.1 异常数据的处理 log平滑 局部加权1.3.2 数据划分，样本构造 增：增加样本，滑窗采样 减：去掉异常样本，可以借助模型判断 采样：效率和效果的平衡 根据需求构建可靠的验证集 注：线上线下Label窗口大小要一致；滑窗是为了增加样本量，一般把多个窗口的数据集合在一起训练；各窗口构成的数据集Label Window无重叠，特征窗口可以重叠； 1.3.3 噪声样本剔除用一个树模型去训练你的数据，然后用得到的模型去预测数据，同一个叶子节点下偏离叶子节点均值最大的认为它是一个噪声，然后对它的差值进行排序取出前top多少个，把它去掉，然后重新训练一个比较干净的模型 二、特征工程 50%特征没做好，参数调到老 2.1 特征提取结合具体的业务场景思考问题 特征统计：key拆分，组合（按照提供的数据集粒度，不同的维度去交叉组合出更多的指标） 时间窗口统计：3/5/7/15/30 2.2 特征处理 归一化（加快训练速度并且更可能达到最优解，线性模型，对数值大小敏感的模型） onehot编码（增加哑变量） 排序 2.3 特征选择 计算特征与标签相关度（按照相关性大小排序筛选特征） LR+L1正则 树模型对特征打分（用的最多的是XGBoost，RF去对特征打分，然后筛选重要的变量再放到线性模型里面）2.4 特征组合 任意两两组合：维度大 特征选择再组合 GBDT+LR（GBDT每棵树从根节点到叶子节点的路径是一种最优组合） 三、模型设计 20%3.1 分类-回归问题 Xgboost：速度快，效果好 GBDT：拟合能力强，有过拟合风险 RF：不容易过拟合 LR：对稀疏特征效果较好；常用来做stacker3.2 Factorization Machine3.3 基于统计的方法（规则）3.4 时序问题 回归类模型 规则函数3.5 规则函数 四、模型融合 5%]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>FM</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python连接hive]]></title>
    <url>%2F2017%2F09%2F19%2F%E4%BD%BF%E7%94%A8python%E8%BF%9E%E6%8E%A5hive%2F</url>
    <content type="text"><![CDATA[Python连接hive的包有很多，我没有一一尝试，只是演示了工作当中用到的方法。本文主要采用PyHive来进行对hive的增删改查。PyHive is a collection of Python DB-API and SQLAlchemy interfaces for Presto and Hive. Requirementsinstall using1pip install SQLALchemy 1pip install pyhive[presto] 1pip install pyhive[hive] Presto engineCreate engine123```### Presto Select```df = pd.read_sql(sql=(r&apos;select * from &apos;+ &apos;schema.tablename&apos;) , con=engine) Presto Insertcon1234```### Presto UpdateHive does not support update.### Presto Delete engine.execute(“drop table schema.tablename”)123456## Hive engine### Create engine``` engine = create_engine('hive://url:port/hive/schema') Hive Select123```### Hive Insert```df.to_sql(tablename, con=engine, flavor=None, if_exists=&apos;append&apos;, index=False, chunksize=2000000) Hive UpdateHive does not support update. Hive Delete1engine.execute("drop table schema.tablename")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightgbm安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Flightgbm%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 Visual Studio 2017 1、下载源代码git clone –recursive https://github.com/Microsoft/LightGBM 2、用VS编译进入LightGBM目录，用VS打开windows/LightGBM.sln，选择DLL和x64，按Ctrl+Shift+B进行编译，dll文件就会在windows/x64/DLL/目录里 编译成功后对应目录E:\Code\LightGBM\windows\x64\DLL下会生成一些文件，如果存在DLL这个文件夹就说明安装成功了。 3、安装Python包123456chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM (master)$ cd python-package/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ python setup.py install 4、测试12345678910111213141516171819202122232425262728chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ cd ../examples/python-guide/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/examples/python-guide (master )$ python ./simple_example.pyLoad data...Start training...[1] valid_0's auc: 0.764496 valid_0's l2: 0.24288Training until validation scores don't improve for 5 rounds.[2] valid_0's auc: 0.766173 valid_0's l2: 0.239307[3] valid_0's auc: 0.785547 valid_0's l2: 0.235559[4] valid_0's auc: 0.797786 valid_0's l2: 0.230771[5] valid_0's auc: 0.805155 valid_0's l2: 0.226297[6] valid_0's auc: 0.800979 valid_0's l2: 0.223692[7] valid_0's auc: 0.806566 valid_0's l2: 0.220941[8] valid_0's auc: 0.808566 valid_0's l2: 0.217982[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351[10] valid_0's auc: 0.805953 valid_0's l2: 0.213064[11] valid_0's auc: 0.804631 valid_0's l2: 0.211053[12] valid_0's auc: 0.802922 valid_0's l2: 0.209336[13] valid_0's auc: 0.802011 valid_0's l2: 0.207492[14] valid_0's auc: 0.80193 valid_0's l2: 0.206016Early stopping, best iteration is:[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351Save model...Start predicting...('The rmse of prediction is:', 0.4640593794679212) 或者直接通过pip安装。。。 via 在Windows下安装LightGBM的Python包 微软开源分布式高性能GB框架LightGBM安装使用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Fxgboost%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 首先，打开Git Shell，依次执行如下命令： 123456789101112131415161718git clone --recursive https://github.com/dmlc/xgboostcd xgboostgit checkout 9a48a40//新版本这一步可以省略git submodule initgit submodule updatecp make/mingw64.mk config.mkcp make/mingw64.mk dmlc-core/config.mkcd rabitmake lib/librabit_empty.a -j4cd ../dmlc-coremake -j4cd..make -j4 然后，安装到Python包中 123cd python-packagepython setup.py install 最后，导入xgboost包，测试demo 12345cd ..//或者直接进入xgboost目录cd democd guide-pythonpython basic_walkthrough.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172--1--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ git clone --recursive https://github.com/dmlc/xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ cd xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule initchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule updatechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk dmlc-core/config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd rabit/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ make lib/librabit_empty.a -j4chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ cd ../dmlc-corechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/dmlc-core ((b5bec54...))$ make -j4--2--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd python-packagechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ python setup.py install--3--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ cd ..chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd demochunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo (master)$ cd guide-python/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ python basic_walkthrough.py[09:58:28] 6513x127 matrix with 143286 entries loaded from ../data/agaricus.txt.train[09:58:28] 1611x127 matrix with 35442 entries loaded from ../data/agaricus.txt.test[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263[09:58:28] 1611x127 matrix with 35442 entries loaded from dtest.buffererror=0.021726start running example of build DMatrix from scipy.sparse CSR Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from scipy.sparse CSC Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from numpy array[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ via 64位Windows下安装xgboost详细参考指南（支持Python2.x和3.x）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
