<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Note-making]]></title>
    <url>%2F2018%2F07%2F16%2FNote-making%2F</url>
    <content type="text"><![CDATA[Note-making: a fundamental academic activityNote-making is right at the heart of academic study. As a student, you will make notes when you: attend lectures or seminars read to support your writing of essays, reports, dissertations, and thesesNote-making is fundamental to these activities. Note-making: a significant challenge!There are many situations, however, in which note-making can be a real challenge, for example if: the content of the lecture is predominantly factual and you want to try to record all of the facts but the lecturer is goingthrough it really quickly you make notes from masses of background reading, but are then stuck with how to use it all you make lots of notes for a piece of writing, but then worry about how to avoid accidentally plagiarising, as you can’tremember which ideas were your own and which were from existing sources you find reading academic papers and books quite slow, and feel that you miss out on the overall sense of an articlebecause you spend too much time writing detailed notes as you read through it Differences between note-making in lectures and when readingYou may have a set of lecture slides in front of you, but you will still need to make your own notes in lectures. Extra slidesmay be added; and the lecturer will invariably offer more explanation and examples than appear on the slides. You will alsowant to record any ideas or queries of your own that you have during the lecture. The big difference between note-making in lectures and note-making from reading is the lack of control that the student hasover the process, because lectures happen in real time. This means that: …when note-making in lectures: …but while note-making from reading: you can’t pause the lecturer; rewind; then replay; to go over something you haven’t understood you can easily stop and read something again if you need to if you are reminded of some information you want to look up, you have to make a note and remember to follow it up later if you are reminded of some information you want to look up, you may be able to do it straightaway you can’t slow down the lecturer if you fall behind with you can read and make notes at the best pace for you, to make sure your notes are complete A good way of appreciating the importance of good note-making is to spend five minutes trying to answer the followingquestion: If you tried to write an assignment or dissertation without doing any background reading and associated notemaking, what might it be missing? Try to build up a list of at least ten elements that could be missing from your writing if you did no background reading andnote-making at all! Some suggested answers are given on page 7 of this Study Guide. Note-making from readingRisk-takingYou cannot avoid taking risks when you take notes! The risks tend to relate to note-making that is too detailed, and to notemaking that is too brief. You need to decide where to place yourself along this continuum of risk. Being too briefTo avoid making masses of notes that you may not actually use, you decide to write down the minimum The riskYou fail to record crucial material, and have to go back to the source and read it again Being too comprehensiveTo make sure you don’t miss anything important,you decide to write down every piece of information that may possibly be relevant The riskYour reading takes far too long; you end up with masses of notes; you still can’t decide which are the most relevant; and you run out of time to do your writing Managing the risk by being selectiveBeing selective is the key to successful note-making. There are two main levels at which you need to be selective: deciding what to read and what not to read deciding which specific material to make notes on Deciding what to read and what not to readInformation that may help you decide is: the year of publication: how up to date is the information in relation to your specific topic? the contents page and index: are there specific sections devoted to your topic of interest? the abstract, introduction, or preface: they should help you to decide whether to read more beginnings and ends of promising sections: do they suggest that the content is worth reading in more detail? Deciding which specific material to make notes onSome useful questions are posed by Stella Cottrell (2003): Do you really need this information? If so, which bits? Will you really use it? When, and how? Have you noted similar information already? What questions do you want to answer with this information?” Don’t be pushed along by the literature: approach it with a plan!Take the analogy of visiting a supermarket to buy food for a party. If you simply wander in to see what there is, and buy anything that looks nice, you will probably end up back home wondering: why you bought far too many puddings and cakes how you will be able to make use of that huge quantity of fruit and vegetables, before they go bad why you bought masses of drinks of all kinds (although there are just 20 people coming), but you forgot to buy any extra coffee how much money you have just wasted on stuff you don’t need when you’ll be able to fit in another visit to the shops to buy the stuff you’ve forgotten how much better it would have been if you had started off with a list Translating this image onto the academic practice of note-making, you will see that wandering into a text and simply writing down everything that looks interesting is neither an efficient nor an effective approach. A plan is needed. This table gives some suggestions for how to begin your planning. It uses the idea of going food shopping for a party as an analogy for reading for an assignment. Note-making templatesUsing a note-making template can help you to: make notes in a clear and readable format remember the kind of information you want to record from each source standardise your notes so you can find particular elements again more easily when you come to use them. When you have decided that a source is going to be useful and you are going to make notes on it, you need to record the full referencing details. After that there are various headings under which you may want to make notes. Here are some ideas of the kinds of headings you might choose to use. main purpose of text suggested future research problem(s) encountered study population method(s) used useful case study useful example(s) main argument(s) useful material to quote idea(s) you can use supporting evidence for your argument particular relevance to my assignment limitation(s) main finding(s) geographical / political setting writing style + examples context theory useful statistics justification for the research You can also devise your own template, using the kind of headings listed in the table above that are the most appropriate toyour own discipline and topic, so that: instead of wandering into the literature and feeling overwhelmed by it, you take control before you engage with the literature, and go in with your ‘shopping list’ already prepared Note-making in lecturesThe particular challenges presented by trying to make notes in lectures are: you have no control over the speed at which the lecturer talks, so there may be some time pressure on your notemaking you cannot pause and go over some information again, like you can when you are reading you may not be able to identify until later, which elements of the lecture were the most important to make notes about You can keep more control over the situation if you devise ways of streamlining your note-making practice. A lot of the guidance on note-making in lectures seems to hold an idealistic view of what students can routinely manage toattain. The following suggestions are from the usually realistic and helpful Stella Cottrell (2003 p138). “Before the lecture: Get a feel for the subject. Read (or just flick through) a book on the subject of the lecture. Look for themes,issues, topics and headings. Look up any technical words you don’t understand. Write down questions you want answered. Leave space to write the answers under each question either duringor after the lecture. Jot down your own opinion. Notice if it changes during the lecture. Glance through your notes for the previous lecture, and look for links with the next lecture.” She also advises that, after the lecture, you: “Label and file your lecture notes and any handouts. Read through your notes. Fill in details from your reading or research.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Note-making</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using+Word2Vec+for+Better+Embeddings+of+Categorical+Features]]></title>
    <url>%2F2018%2F07%2F16%2FUsing%2BWord2Vec%2Bfor%2BBetter%2BEmbeddings%2Bof%2BCategorical%2BFeatures%2F</url>
    <content type="text"><![CDATA[titile &amp; author &amp; year &amp; source key info titile Using Word2Vec for Better Embeddings of Categorical Features author year 2014 Apr 25 source keyword category2vec links problems encounteredhow do I represent my categorical features as vectors that my network can work with? main argumentsThe most popular approach is embedding layers—you add an extralayer to your network, which assigns a vector to each value of thecategorical feature. During training the network learns the weights forthe diierent layers, including those embeddings. In this post I will show examples of when this approach will fail,introduce category2vec, an alternative method for learning embeddingusing a second network and will present diierent ways of using thoseembeddings in your primary network. 在任务中训练embedding主要会带来几个问题： 这种训练出来的embedding是在拟合一个具体的任务或者训练集的过程中产出的，很难适用于其他问题。 即使是在相同或者相似的训练集中产出的embedding，由于任务不同，可解释变量的也会不同。但独立于任务的预训练词向量可以复用。 embedding训练作为神经网络训练任务的一部分相当于引入了更多的参数会增加模型的复杂度，意味着需要更多的标记数据。 123456789101112131415161718Embedding layers are trained to bt a specibc task—the one the networkwas trained on. Sometimes that’s exactly what you want. But in othercases you might want your embeddings to capture some intuition aboutthe domain of the problem, thus reducing the risk of overbtting. Youcan think of it as adding prior knowledge to your model, which helps itto generalize.Moreover, if you have diierent tasks on similar data, you can use theembeddings from one task in order to improve your results on another.This is one of the major tricks in the Deep Learning toolbox. It’s calledtransfer learning, pretraining or multi-task learning, depending on thecontext. The underlying assumption is that many of the unobservedrandom variables explaining the data are shared across tasks. Since theembeddings try to isolate these variables, they can be reused.Most importantly, learning the embeddings as part of the networkincreases the model’s complexity by adding many weights to the model,which means you’ll need much more labeled data in order to learn. methods used作者举了个例子：广告点击，训练广告商的词向量。把用户点击广告商的序列看成一个sentence，不同的是这些广告商是没有顺序的，可以直接忽略这些顺序（用户量数据比较大的情况下，这些顺序可以忽略）或者随机打乱广告商的点击前后顺序。 category2vec中的category应该指的是广告商，用户点击的是广告sku，广告商属于类别特征，而且类别数量很大。 123456789From Word2Vec to Category2VecThe idea is simple: we train word2vec on users’ click history. Each“sentence” is now a set of advertisers that a user clicked on, and we tryto predict a specibc advertiser (“word”) based on other advertisers theuser liked (“context”). The only diierence is that, unlike sentences, theorder is not necessarily important. We can ignore this fact, or enhancethe data set with permutations of each history. You can apply thismethodto any kinds of categorical features with high modality, e.g,countries, cities, user ids, etc.. critical challengesuseful ideas for later usecategory2vec在实际工作中的应用有很大的想象空间，比如作者举例可以根据用户的点击历史，训练出广告商的embedding，然后将这些embedding平均，用于表征用户的兴趣。 eg. 比如类目embedding的训练可以借鉴这个思路：用户点击sku的历史记录（sentence），sku的类目（words)；可以训练出类目embedding，然后类目embedding求平均可以表征用户的偏好。 123456789101112131415161718192021222324There are three diierent ways to use the new embeddings in ourmodel:1. Use the new embeddings as features for the new network. Forexample, we can average the embeddings of all the advertisers theuser clicked on, to represent the user history and use it to learn theuser’s tastes. The focus of this post is Neural Networks but you canactually do it with any ML algorithm.2. Initialized the weights of the embedding layer in your network,with the embeddings we just learned. It’s like telling the network—this is what I know from other tasks, now adjust it for thecurrent task.3. Multitask Learning—take two networks and train them togetherwith parameter sharing. You can either force them to share theembeddings (hard sharing), or allow them to have slightlydiierent weights by “punishing” the networks when their weightsare too diierent (soft sharing). This is usually done byregularizing the distance between the weights.You may think about it as passing knowledge from one task toanother, but also as a regularization technique: the word2vecnetwork acts as a regularizer to the other network, and forces it tolearn embeddings with characteristics that we are interested in.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Word2Vec</tag>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRF和LSTM模型在序列标注上的优劣]]></title>
    <url>%2F2018%2F07%2F16%2FCRF%E5%92%8CLSTM%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%B8%8A%E7%9A%84%E4%BC%98%E5%8A%A3%2F</url>
    <content type="text"><![CDATA[https://www.zhihu.com/question/46688107 LSTM 优点 像RNN、LSTM、BILSTM这些模型，它们在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方 当数据规模较大时，BILSTM的效果应该会超过CRF LSTM理论上是能拟合任意函数的，对问题的假设明显放宽了很多。不过深度学习类模型的理论原理和可解释性一般。 LSTM想stack起来、改双向、换激活函数等，只不过左手右手一个慢动作的事儿。 LSTM可以当做对序列的一种『中间状态』的建模，建模结果还可以当做特征，扔给其他模型继续用。 LSTM（RNN）的召回率高一些，因为长程依赖强一些，而且可以处理的数据量大。 lstm主要用在搜索空间非常大的场景，比如机器翻译，语言模型，语音识别等场景，由于每一步都求一个最优解，每一步的预测类别一般就是整个词汇表了大概几十万吧，所以全局来说求的就不是最优解了。 LSTM 缺点 对于t时刻来说，输出层y_t受到隐层h_t（包含上下文信息）和输入层x_t（当前的输入）的影响，但是y_t和其他时刻的y_t是相互独立的，感觉像是 一种point wise，对当前t时刻来说，我们希望找到一个概率最大的y_t，但其他时刻的y_t对当前y_t没有影响，如果y_t之间存在较强的依赖关系的话（例如，形容词后面一般接名词，存在一定的约束），LSTM无法对这些约束进行建模，LSTM模型的性能将受到限制。 如果需要识别的任务不需要太依赖长久的信息，此时RNN等模型只会增加额外的复杂度 LSTM有各种GPU加速，多机异步SGD等标准大数据训练套路。但同样的问题，训练数据不够的话过拟合会很严重，效果堪忧。 LSTM虽然部分解决了rnn梯度消失问题，但是信息在过远的距离传播中损失很厉害 其实仔细分析LSTM网络结构 他只不过是CRF的简化版而已 CRF 优点 CRF：它不像LSTM等模型，能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的y_t，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种联合概率，优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。 CRF不管是在实践还是理论上都要优于HMM，HMM模型的参数主要是“初始的状态分布”，“状态之间的概率转移矩阵”，“状态到观测的概率转移矩阵”，这些信息在CRF中都可以有，例如：在特征模版中考虑h(y1), f(y_i-1, y_i), g(y_i, x_i)等特征。 在数据规模较小时，CRF的试验效果要略优于BILSTM CRF在理论上更完美一些，一步一步都有比较坚实的理论基础。不过CRF的假设也比较明确，然而问题不总是能match其假设的。 CRF的准确率高一些，我认为人为构造的特征，特别是字典特征，是非常有用的。 crf在搜索空间较小场景基本无敌了，比如分词，词性标注，命名实体识别等，每一步预测的类别一般不会太多，比如词性标注在每步或者说每个字的预测类别也就几十个，crf求的是全局最优，也就是让整个序列最优，由于每步预测类别不多，所以全局的搜索空间也有限了。 从理论上crf明显优于lstm，但是，在搜索空间大的场景下还是得用lstm。 crf在训练是 考虑的是全局序列最优 可以防止出现标注偏置问题。 crf对远距离的建模能力 其实很强的 业内流行的“图模型加圈嘛” 而且组合自由随意 针对badcase 可以定位到是哪个特征造成的 这在商业交付和版本迭代中都非常重要 crf 相对好解释点,这些权重都是统计概率好解释，也好理解 CRF 缺点 CRF比较难扩展，想在图模型上加边加圈，得重新推导公式和写代码。 CRF针对大数据不好做。 crf的问题在于 需要人工特征工程，这个过程比较需要经验 crf 对数据集的语料要求更高点，标注越好，当然结果也越好；不过crf对有歧义的处理不是非常友好，受语料库的影响还蛮大的，lstm稍微好点 其他 CNN＋BILSTM＋CRF：这是目前学术界比较流行的做法，BILSTM＋CRF是为了结合以上两个模型的优点，CNN主要是处理英文的情况，英文单词是由更细粒度的字母组成，这些字母潜藏着一些特征（例如：前缀后缀特征），通过CNN的卷积操作提取这些特征，在中文中可能并不适用（中文单字无法分解，除非是基于分词后），这里简单举一个例子，例如词性标注场景，单词football与basketball被标为名词的概率较高， 这里后缀ball就是类似这种特征。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CRF</tag>
        <tag>LSTM</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列标注问题中将分类标签引入RNN网络结构的尝试]]></title>
    <url>%2F2018%2F07%2F16%2F%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98%E4%B8%AD%E5%B0%86%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E5%BC%95%E5%85%A5RNN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E5%B0%9D%E8%AF%95%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/malefactor/article/details/52444022#0-tsina-1-63702-397232819ff9a47a7b7e80a40613cfe1 RNN解决序列标注问题 目前使用RNN做序列标注问题，总体而言，跟传统方法比效果相当，并没有表现出特殊明显的优势，我觉得可能跟大多数序列标注问题所需要参考的上下文比较短，所以神经网络模型并不太能发挥出其优势有关。 RNN典型网络结构做序列标注任务的问题是什么呢？很明显RNN结构做序列标注问题，不论你套上多少层RNN或者LSTM，其在预测某个单词的标签时，只是考虑了单词本身以及其所在上下文里包含单词的信息。难道这还不够吗？其实还不够。有一类可以利用的信息在这个结构里没有考虑进来，就是单词的分类标签之间的相互影响。 biLSTM+CRF解决标注偏置问题 如何将标签之间的相互影响这个因素体现到神经网络结构下的序列标注问题里呢？这个问题其实是值得探索的一个方向。之前有类方法这么做：RNN在每个时间步t对每个单词输出若干候选标签(这么讲只是为了方便理解，真实在做的时候并没有输出候选标签，而是把隐层节点的状态作为CRF的输入)，对于一句话来说，每个单词输出若干候选标签，但是这时候还没定下来到底哪个是最终要输出的标签，在这些候选标签之上再套上一层CRF模型来寻找全局最优的标签组合路径（上图）。 RNN考虑的是根据单词和单词所处上下文给出候选标签，CRF融合全局标签信息得出最优解。 标签作为输入特征的RNN模型 和图1的标准RNN解决序列标注问题的网络结构相比，RNN的每个时间步t的输入由原先的单独的t时刻输入Xt，现在改造成每一时刻输入有两个：一个是t时刻的输入Xt，另外一个是t-1时刻输出的标签Y(t-1)。这样，就将前一时刻的标签输出作为当前时刻的输入，也就是将标签作为特征引入到神经网络模型中来。引入这个改进后，貌似好像只是把前一时刻的标签对当前时刻的标签影响建立到模型里，但是其实不仅仅如此，在t时刻的RNN网络隐层对前面时刻1到时刻t-1的标签都建立到模型里去了，只不过是体现在隐层状态上，也就是说这个结构应该可以对标签之间的远程影响也能建立到看上去很简单的模型中去。 在序列标注问题中，从道理上讲也是有一定合理性的。如何训练整个模型呢？在训练阶段，因为训练数据的每个输入单词都是带标准答案标签的，所以在训练时刻，t时刻的输入标签可以直接用t-1时刻的输出标签。这样就能够训练神经网络模型，而训练出的神经网络模型是融合了单词上下文信息以及标签序列之间的相互影响因素的。在推理阶段，t时刻的输入标签则直接用t-1时刻预测输出的标签。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>序列标注</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip源设置]]></title>
    <url>%2F2018%2F07%2F16%2Fpip%E6%BA%90%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[国内pypi镜像 V2EX：pypi.v2ex.com/simple 豆瓣：http://pypi.douban.com/simple 中国科学技术大学：http://pypi.mirrors.ustc.edu.cn/simple/ 指定单次安装源pip install &lt;包名&gt; -i http://pypi.v2ex.com/simple 举例：pip install gensim -i http://pypi.v2ex.com/simple 1234567D:\Program Files (x86)\PowerCmd&gt;pip install gensim -i http://pypi.v2ex.com/simpleCollecting gensim The repository located at pypi.v2ex.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS it is recommended to use HTTPS instead, otherwise you may silence this warning and allow it anyways with '--trusted-host pypi.v2ex.com'. Could not find a version that satisfies the requirement gensim (from versions: )No matching distribution found for gensimYou are using pip version 9.0.1, however version 9.0.3 is available.You should consider upgrading via the 'python -m pip install --upgrade pip' command. pip install gensim –isolated http://pypi.douban.com/simple – trusted-host pypi.douban.com pip install gensim –isolated http://mirrors.aliyun.com/pypi/simple/ – trusted-host mirrors.aliyun.com 尝试了换源仍然没能解决，最后还是乖乖的使用了默认源，还好速度也不是很慢。 指定全局安装源 在unix和macos，配置文件为：$HOME/.pip/pip.conf 在windows上，配置文件为：%HOME%\pip\pip.ini123[global]timeout = 6000 index-url = http://pypi.douban.com/simple]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python：argparse包基本使用]]></title>
    <url>%2F2018%2F07%2F16%2Fpython%EF%BC%9Aargparse%E5%8C%85%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[step 1、导入argparse模块import argparse step 2、创建解析器对象ArgumentParser，可以添加参数。description：描述程序parser=argparse.ArgumentParser(description=”This is a example program “)add_help：默认是True，可以设置False禁用 step 3、add_argument()方法，用来指定程序需要接受的命令参数定位参数：parser.add_argument(“echo”,help=”echo the string”) 可选参数：parser.add_argument(“–verbosity”, help=”increase output verbosity”)在执行程序的时候，定位参数必选，可选参数可选。 add_argument()常用的参数： dest：如果提供dest，例如dest=”a”，那么可以通过args.a访问该参数 default：设置参数的默认值 action：参数触发的动作 store：保存参数，默认 store_const：保存一个被定义为参数规格一部分的值（常量），而不是一个来自参数解析而来的值。 store_ture/store_false：保存相应的布尔值 append：将值保存在一个列表中。 append_const：将一个定义在参数规格中的值（常量）保存在一个列表中。 count：参数出现的次数 parser.add_argument(“-v”, “–verbosity”, action=”count”, default=0, help=”increase output verbosity”) version：打印程序版本信息 type：把从命令行输入的结果转成设置的类型 choice：允许的参数值 parser.add_argument(“-v”, “–verbosity”, type=int, choices=[0, 1, 2], help=”increase output verbosity”) help：参数命令的介绍 example: argparse_example.py example: argparse_example.py12345678import argparseparser = argparse.ArgumentParser()parser.add_argument("--echo", type=str，help="echo the string you use here")parser.add_argument("--square", type=int, help="display a square of a given number")args = parser.parse_args()print(args.echo)print(args.square**2) 这里第一个参数调用了系统的echo工具，将函数名称后的字符串打印在控制台显示。第二个参数做了平方运算。运行：python argparse_example.py --echo ‘hello!’ --square 4返回12hello!16]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>argparse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python __getattr__ 和 __setattr__]]></title>
    <url>%2F2018%2F07%2F16%2Fpython%20__getattr__%20%E5%92%8C%20__setattr__%2F</url>
    <content type="text"><![CDATA[__getattr__方法拦截点号运算。当对未定义的属性名称和实例进行点号运算时，就会用属性名作为字符串调用这个方法。如果继承树可以找到该属性，则不调用此方法。12345678910class empty: def__getattr__(self, attrname): ifattrname =="age": return40 else: raiseAttributeError, attrnamex =empty()print(x.age) #40print(x.name) #error text omitted.....AttributeError, name 这里empty类和实例x并没有属性age，所以执行x.age时，就会调用getattr方法，对于name也是同样。 __setattr__方法12345会拦截所有属性的的赋值语句。如果定义了这个方法，self.arrt = value 就会变成self,__setattr__("attr", value).这个需要注意。当在__setattr__方法内对属性进行赋值是，不可使用self.attr = value,因为他会再次调用self,__setattr__("attr", value),则会形成无穷递归循环，最后导致堆栈溢出异常。应该通过对属性字典做索引运算来赋值任何实例属性，也就是使用self.__dict__['name'] = value. 实现属性的私有化123456789101112131415161718192021222324class PrivateExc(Exception): passclass Privacy: def__setattr__(self, attrname, value): ifattrname inself.privates: raisePrivateExc(attrname, self) else: self.__dict__[attrname]= valueclassTest1(Privacy): privates= ["age"]classTest2(Privacy): privates= ["name","age"] def__init__(self): self.__dict__["name"]= "sun"x =Test1()y =Test2()x.name = "Tom" #执行成功x.age =20 #执行失败，产生异常，该属性在privates列表内y.name = "Mike" #执行失败，产生异常，该属性在privates列表内y.age =20 #执行成功]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive和presto语句对比]]></title>
    <url>%2F2018%2F07%2F16%2Fhive%E5%92%8Cpresto%E8%AF%AD%E5%8F%A5%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[presto常用语句 语句 说明 show catalogs 显示catalog show session 查看当前session的主要属性 show tables like ‘app_idata%’ 查看有哪些表 hive presto 说明 select my_array1 select my_arrayCARDINALITY(my_array) SQL中的下标运算符支持完整的表达式与Hive 数组从0开始 数组从1开始 count(distinct a,b) show partitions orders SHOW PARTITIONS FROM orders 查看分区 show partitions from app_idata_bdl_all_order_app_log where dt&gt;=’2016-10-01’ and biz_type=’rec’order by dt desc; 根据限制条件查看分区 SHOW SCHEMAS [ FROM catalog ] SHOW TABLES [ FROM schema ] [ LIKE pattern ] count(distinct order_id,sku_id) count(distinct concat(order_id,sku_id)) 计算订单行 substring(str,start,len) substr(str,start,len) a rlike(‘^T’) regexp_like(a, ‘^T’) 处理正则表达式的函数不一致,且字段所处位置也不一样https://prestodb.io/docs/0.132/functions/regexp.html lateral view explode() SELECT student, score FROM tests CROSS JOIN UNNEST(scores) AS t (score); presto暂时不支持此行转列函数.http://yugouai.iteye.com/blog/2017528 pmod(99,10) mod(99,10) 取模]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>presto</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter notebook默认启动路径配置]]></title>
    <url>%2F2018%2F07%2F16%2FJupyter%20notebook%E9%BB%98%E8%AE%A4%E5%90%AF%E5%8A%A8%E8%B7%AF%E5%BE%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置 Jupyter notebookjupyter notebook --generate-config D:\Program Files (x86)\PowerCmd&gt;jupyter notebook --generate-config Writing default config to: C:\Users\zhangchunhui\.jupyter\jupyter_notebook_config.py 修改jupyter_notebook_config.py ## The directory to use for notebooks and kernels. c.NotebookApp.notebook_dir = &apos;E:\Code&apos;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word embedding materials]]></title>
    <url>%2F2018%2F07%2F16%2Fword%20embedding%20materials%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s/BLGlefsHTy44hZyXj06pbQ Hinton 1986 《Learning distributed representations of concepts》《Linguistic Regularities in Continuous Space Word Representations》网易有道最初出品的一篇干货《Deep Learning实战之word2vec》（1）第一篇必须是由 gensim 作者 Radim Řehůřek 本人写的 word2vec 使用教程：《Word2vec Tutorial》，在其本人的博客上。博文最后还给出了他是如何将原始 Google 的C版本代码优化到3倍速的视频讲解噢。真是一手资料呢！（2）第二篇是数据挖掘竞赛平台 Kaggle 在其比赛“When bag of words meets bags of popcorn”中配套给出的教程，step by step。结合比赛的实际数据，从清洗数据到调参数。实战味道更浓，排版还更清楚！（3）第三篇是最新的，由微博@52nlp 发表的最新文章《中英文维基百科语料上的Word2Vec实验》，中文语料，代码和结果都贴在了博文里面，完全没有废话。 通过可视化的方法来研究 word embeddings。一篇是 Chris Moody 的博文《A Word is Worth a Thousand Vectors》。如果你也对之前的 vector(‘king’) - vector(‘man’) + vector(‘woman’) ≈ vector(‘queen’) 的结果记忆深刻，还想了解更多，不妨去看这篇博文。在文中作者将这些结果进行了动态展示，分析了其背后的“语义”都究竟是什么，更加脑洞大开地把这种加减法运算运用在服装类商品数据上，结果直观地找出了相似的服饰！第二篇类似的文章是 Hendrik Heuer 的 《word2vec: From theory to practice》的 Slides。他把 word2vec 的语义表示与矩阵分解联系在一起，这类等价矩阵分解的想法可以有很多应用。第三篇也是和矩阵分解有关，发表在 NIPS’14 的论文，《Neural Word Embedding as Implicit Matrix Factorization》，文中认为 word2vec 中的 word embedding 其实就是 PMI 矩阵的分解，很有启发性。 《word analogy task详解及python版本的测试脚本》 word2vec 中考虑的是 linear 的 bag-of-words contexts。这里面，我们分三点来看：linear、bag-of-words、contexts。linear 强调的就是 plain，没有额外 word-level 以外的信息的，比如 syntax，比如 relation，比如 anything you can imagine；bag-of-words 强调的是 non-ordered，也就是说，词袋模型，这个 word2vec 是没有考虑语序的——丢失了很多信息对不对？第三个是 contexts，一方面是说，word2vec 只能考虑上下文信息（没有考虑词汇内部或者更高的 global/document-level 信息），另一方面是说，这个 contexts 也是被 window size 框住大小的，导致太大的 size computational cost 太大；太小的 size 又不能 handle 足够的信息，同时也会容易出现很多不想出现的词和想出现但没出现的词。 linear打破 linear 的方法，对于 NLPer 来说，首先就会想到加入 dependency relation，使用已经比较成熟的 dependency parsing（比如 Stanford 的工具），就可以很快引入各种语法树结构，既多了non-linear 的 relation 关系，又有了更多的 tag 作为额外信息，而且还有很顺手的工具包。这方面的工作主要有： Omer Levy and Yoav Goldberg. 2014. Dependency based word embeddings. In Proceedings of ACL.Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proc. of NAACL.Mohit Bansal, Kevin Gimpel, Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. ACL. 总结他们的思想，主要是将抽取好的 dependency relation 或者其他 relation 和 信息，表达成 modifier-relation/tag 的 pair，这个 pair 作为新的 context，替代原先 word2vec 中的 context 信息。 bag-of-wordsbag-of-words 可以说不仅有 non-order 的问题，还有 words 的问题。也就是说，现在的工作是基于 word/token level 去做的，但实际上已经有不少人质疑这点，认为 character-level 的信息足够可以当做 input 输入。对于 non-order 方面，有人提出了用多个 embedding matrix 来增强 word2vec 中的 同一个 embedding matrix，从而使得原始的 context 位置信息可以保留；而其实刚才上面 linear 增强部分提到的 dependency pair 的方法也可以看做是保留了 order。而，关于 word-level 的问题，很多人虽然考虑了 morpheme 层面的信息，但不能算是直接替代 word-level input；相反，MSR 的 Deep Learning 组，由 Jianfeng Gao 等人 lead 的 DSSM （Deep Structured Semantic Model or Deep Semantic Similarity Model）则提倡从 Sub-Word Unit （SWU）的层面进行输入。相关工作可见： Mikolov Tomáš, Sutskever Ilya, Deoras Anoop, Le Hai-Son, Kombrink Stefan, Černocký Jan: Subword Language Modeling with Neural Networks. Not published (rejected from ICASSP 2012).Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of ACL.MSR DSSM 系列：[Huang, He, Gao, Deng, Acero, Heck, CIKM2013][Shen, He, Gao, Deng, Mesnil, WWW2014][Gao, He, Yih, Deng, ACL2014][Yih, He, Meek, ACL2014][Song, He, Gao, Deng, Shen, MSR-TR 2014][Gao, Pantel, Gamon, He, Deng, Shen, EMNLP2014][Shen, He, Gao, Deng, Mesnil, CIKM2014][He, Gao, Deng, ICASSP2014 Tutorial][Liu, Gao, He, NAACL2015] [Gao, He, Deng, MSR-TR-2015] Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu, Co-learning of Word Representations and Morpheme Representations, COLING 2014. contexts既然单纯的 context 可以认为是有用的，也可以被认为是不足的。除了固定 window size 内的上下文 contexts words 信息，还有什么可以用的信息呢？这样的考虑下，又带来了两种不同的改进。一种是直接替换掉 contexts words，用其他 enriched information words，比如 modifier-dependency-label pairs，比如 related word pairs（比如 WordNet 里那些 synonymy words）。另一种就是直接在 local contexts（即 window size 内的 context）的基础上，结合进 global contexts 或者其他 additional information。这种结合多数是线性 combine 进另一个神经网络，首创的就是 Huang et al., 2012 年的非常重要的论文。其他相关改变 contexts 的文章有： Eric H. Huang, Richard Socher, Christopher D. Manning, Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. ACL.Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasupat, and Ruhi Sarikaya. 2015. Enriching Word Embeddings Using Knowledge Graph for Semantic Tagging in Conversational Dialog Systems. AAAIMo Yu and Dredze. 2014. Improving Lexical Embeddings with Semantic Knowledge. ACL CSE 599 - Advanced in NLP SurveyBengio et al’s survey on representation learningYoshua Bengio, Aaron Courville and Pascal Vincent. “Representation Learning: A Review and New Perspectives.” pdf TPAMI 35:8(1798-1828) 这篇没得说，开山之作，比较长，小S 打印下来读过许多遍，可以让你对这个领域找到一些感觉。 Bengio, LeCun Yann, Yoshua Bengio and Geoffrey Hinton’s survey on NatureYann LeCun, Yoshua Bengio and Geoffrey Hinton. “Deep Learning” pdf Nature 521, 436–444 三位深度学习的鼻祖和大佬联合出品的最新 Nature 文章，简要介绍了几种模型及其分别的优势和适用领域。小S 推荐其中的 CNN 部分，卷积神经网络，写得尤其清楚。另外，现在 CSDN 上有中文翻译的版本。 Embeddings &amp; Language ModelsSkip-gram embeddingsTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.” pdf ICLR, 2013. 这篇没啥好介绍的啦，Mikolov 发力的首篇，word2vec 的前身。 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. “Distributed Representations of Words and Phrases and their Compositionality.” [pdf] NIPS, 2013. [king-man+woman=queen] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. “Linguistic Regularities in Continuous Space Word Representations.” pdf NAACL, 2013. 这篇就是第一次出现 word2vec 的神话的论文，里面指出，我们经过词向量的表示，可以使用加减乘除这种简单的线性计算，得出 king-man = queen-woman 这样的度量！是不是很神奇很炫酷！可以说，word2vec 的火爆，有一定程度来自于这个 magic 公式。而且后来，也演变成了 word analogy 的 evaluation task。 [technical note] Yoav Goldberg and Omer Levy “word2vec explained: deriving Mikolov et al.’s negative-sampling word-embedding method” pdf Tech-report 2013 这是后来 Levy 等人对于 word2vec 的一些推导解释，因为 Mikolov 的论文及其最初放出的 Google 版本的 word2vec 代码，都比较简洁（混乱）。 [buzz-busting] Omer Levy and Yoav Goldberg “Linguistic Regularities in Sparse and Explicit Word Representations” pdf CoNLL-2014 Best Paper Award 如果说 word embedding 这边，除了 Mikolov，就应该是 Levy 了。 [lessons learned] Omer Levy, Yoav Goldberg, Ido Dagan “Improving Distributional Similarity with Lessons Learned from Word Embeddings” pdf, TACL 2015 这篇也是我认为的 DL in NLP 中带来的新风潮，就是说，因为这东西充满了 magic，不可解释性，所以相关的讨论类，分析类的论文就变得多起来（不是简单的应用）。这篇论文很值得一看，里面的评测还是很多的。 [syntax-word order] Wang Liang, Chris Dyer, Alan Black, Isabel Trancoso. “Two/Too Simple Adaptations of Word2Vec for Syntax Problems” pdf NAACL 2015 (Short) 这篇是刚刚结束的 NAACL-HIT 2015 大会的论文，虽然只是 Short。但提出了基于 word2vec 的原始 CBOW 和 SkipGram 的两种改进模型，都用来提升模型对于 syntax-based word order 的建模能力。简单有效有代码。 Embedding enhancement: Syntax, Retrofitting, etc[dependency embeddings] Omer Levy and Yoav Goldberg “Dependency Based Word Embeddings” pdf ACL 2014 (Short) 这篇应该也是很有代表的改进 word2vec 的一篇，改进的主要是 SkipGram 模型，融入 NLP 人最容易考虑到的 dependency relation 信息。 [dependency embeddings] Mohit Bansal, Kevin Gimpel and Karen Livescu. “Tailoring Continuous Word Representations for Dependency Parsing” pdf ACL 2014 (Short) 这篇和楼上（汗，楼上）那篇一样，也是加入 dependency relation 信息，但是对于信息的 representation 不太一样。细微不同。 [retrofitting with lexical knowledge] Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy and Noah A. Smith. “Retrofitting Word Vectors to Semantic Lexicons” pdf, NAACL 2015 [contrastive estimation] Mnih and Kavukcuoglu, “Learning Word Embeddings Efficiently with Noise-Contrastive Estimation.” pdf NIPS 2013 [embedding documents] Quoc V Le, Tomas Mikolov. “Distributed representations of sentences and documents” pdf ICML 2014 Quoc 的经典文章，必看。不过比较有争议……尽管如此，后来还是被作为一个 baseline。 [synonymy relations] Mo Yu, Mark Dredze. “Improving Lexical Embeddings with Semantic Knowledge” pdf ACL 2014 (Short) 这篇使用了 knowledge base 中的一些思想，把一些同义词的关系作为 additional information 考虑进去，但是模型比较复杂。看看就好。 [embedding relations] Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasupat, Ruhi Sarikaya. “Enriching Word Embeddings Using Knowledge Graph for Semantic Tagging in Conversational Dialog Systems” pdf AAAI 2015 (Short) [multimodal] Angeliki Lazaridou, Nghia The Pham and Marco Baroni. “Combining Language and Vision with a Multimodal Skip-gram Model” pdf NAACL 2015 multimodal 的代表作。很多童鞋包括小S 可能对 multimodal 不太熟悉，可以了解一下，这个方向现在也被一些大佬看好。 [syntax-word order] Wang Liang, Chris Dyer, Alan Black, Isabel Trancoso. “Two/Too Simple Adaptations of Word2Vec for Syntax Problems” pdf NAACL 2015 (Short) 刚才上面已经点评过了。这篇是刚刚结束的 NAACL-HIT 2015 大会的论文，虽然只是 Short。但提出了基于 word2vec 的原始 CBOW 和 SkipGram 的两种改进模型，都用来提升模型对于 syntax-based word order 的建模能力。简单有效有代码。 Embedding enhancement: Word order, Morphological, etc[syntax-word order] Wang Liang, Chris Dyer, Alan Black, Isabel Trancoso. “Two/Too Simple Adaptations of Word2Vec for Syntax Problems” pdf NAACL 2015 (Short) 刚才上面已经点评过了。这篇是刚刚结束的 NAACL-HIT 2015 大会的论文，虽然只是 Short。但提出了基于 word2vec 的原始 CBOW 和 SkipGram 的两种改进模型，都用来提升模型对于 syntax-based word order 的建模能力。简单有效有代码。 [word order] Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural networks. pdf NAACL 2015 这篇文章……被小S 一度乌龙的记成了 best paper，到处找人请教其精华所在。现在十分汗颜。Tong Zhang 二作的文章。应该是比较 generic 的把 CNN 运用到 text 上的成功作。有代码，有很多实验结果。分别用在了两个任务上，一个 sentiment classification，一个 text categorisation，分别讨论了不同参数对于这两个任务结果的影响。 [word order] Radu Soricut and Franz Och. “Unsupervised Morphology Induction Using Word Embeddings” pdf NAACL 2015 Best Paper Awards 这才是真正的 best paper。是很直观的 Google 出品。个人感觉是 Google 真的遇到了这种需求，很好的做了出来。简单说就是可以把 beautiful → beautifully 这种类似的 relation 都很好的抽出来，并且不会误判 on → only 这种。他们的算法也很直观，改造了一下 word analogy 的公式，全篇基于 rank 的思想来做 morphological relation 的 extraction 和 apply。具有一定通用性。 [morphology] Minh-Thang Luong Richard Socher Christopher D. Manning. “Better Word Representations with Recursive Neural Networks for Morphology” pdf CoNLL 2013 [morpheme] Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, Tie-Yan Liu. “Co-learning of Word Representations and Morpheme Representations” pdf COLING 2014 MSRA 出品的，可以把 word embedding 和 morpheme embedding 一起学出，个人感觉只是 concatenate 这种思想比较有用，其他内容比较一般。 [morphological] Ryan Cotterell and Hinrich Schütze. “Morphological Word-Embeddings” pdf NAACL 2015 (Short) [regularization] Dani Yogatama, Manaal Faruqui, Chris Dyer, Noah Smith. “Learning Word Representations with Hierarchical Sparse Coding” pdf ICML 2015 小S 重点关注的一篇论文，几度和作者交涉暂时还没要到代码（据说会发布），用 group LASSO 的方法去 regularise 以前的 word embedding。学出来的结果很 promising。另外这个组的人主要就是做 Bayesian Structured Regularization。 Embeddings as matrix factorization[approximate interpretation] Levy and Goldberg, “Neural Word Embedding as Implicit Matrix Factorization.” pdf NIPS 2014 Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. “Do Supervised Distributional Methods Really Learn Lexical Inference Relations?” pdf NAACL 2015 (Short) Tim Rocktaschel, Sameer Singh and Sebastian Riedel. “Injecting Logical Background Knowledge into Embeddings for Relation Extraction” pdf NAACL 2015 [exact interpretation] Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong and Enhong Chen. “Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective” pdf IJCAI 2015 相比 Levy 2014 NIPS 上的那篇解释性文章，这篇论文【直接】【显式】地【证明】了 word embedding 就是矩阵分解。 个人认为未来可能有四个小方向，分别是：interpretable relations, lexical resources, beyond words, beyond English。在分别展开这四个小方向之前，大家可能会问，那 word embedding models 的改进已经画上句号了么？ 对于这个问题，小S 的答案几乎是肯定的。就好像曾经的 topic model 小浪潮，几乎可以认为是印度美女 S. Moghaddam 发表了两篇“集大成”的 topic model 变形大汇总（1. On the design of LDA models for aspect-based opinion mining; 2. The FLDA model for aspect-based opinion mining: addressing the cold start problem ），把圈圈框框画到了极致，抵达了“艺术之巅峰”——从此 topic model variants 鲜有新作。那么最近 pre-print 了一篇 word embedding 的论文，小S 认为也初有这样的风范。这篇论文就是《How to generate a good word embedding》，来自中科院自动化所，Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao，pre-print 在 arXiv 上，有源码，还有博客自动导读。一作就是因一篇《Deep Learning in NLP(一)》博文备受瞩目的@licstar。篇幅有限，建议大家直接搜索作者本人的论文导读。他的这篇论文：先把从 NNLM，到 GloVe 的各种常见 word embedding model / NN 按两大方面分了类：aspect (i) 是如何 model relation between target word and contexts words 的；aspect (ii) 是如何 represent contexts 的。具体可以见 Table 1 和 Table 2. 同时作者指出，aspect (ii) 严重关系到了是否保留了 word order 这一重要信息。保留与否几乎损失 20% information （ref [12]）。为了更公平的比较各大 model，他除了比较了 2.1 中的常见的几大模型，还自创了一个“Virtual Order”模型，去完成单一变量下的比较。同时有一个问题，既然是在不同的任务下比较，那么performance的 metric 最好统一。他们就搞了个 Performance Gain Ratio,PGR 的东西。意思有点像 negative sampling。把你的 embedding 随机替换一个 random embedding，分别测试，然后看你比人家好出多少。Conclusion 其实粗看没什么意义，但是细看 Section 4的 每一部分还是有点东西的。比如 C&amp;W 虽然原始论文中展现的结果是可以刻画 Paradigmatic relation between words，但其实是因为他们 fine-tune for extrinsic NLP tasks 的结果。具体几个 conclusions 可以见 Introduction 最后。 也就是说，曾经小S 总结过的 word embedding 的一些变形，几乎都能归为他论文中讨论到的两类。具体可以回复代码：GH006。那么回到最初的问题，如果 word embedding model 的变形已经鲜有新意，未来的 word embedding 研究热点在哪里？上文所说的 interpretable relations, lexical resources, beyond words, beyond English，都分别是什么意思？ interpretable relations 众所周知，word embedding / distributed embeddings 虽然效果显著，甚至被人称为“magic”，但其随之而来的就是“不可解释性”，或者说太笼统。把语料扔进去，跑出来的 dense vectors，虽然是基于 statistics，也就是 distributional hypothesis 的，但离 lexical/linguistic 的解释还差得很远。还记得之前我写过的 ACL 主席 Christopher Manning 在 ACL 2015 开幕式上的致辞么（回复代码：GH011），我们 NLPer 不能抛弃 linguistics！于是乎，许多研究者已经开始了这一方向的研究。 最熟悉的可能还是 morphology 的研究，这个主要在上次的 word embedding 变形中提了一些（回复代码：GH006），又比如 NAACL 2015 Best Paper 之一，《Retrofitting Word Vectors to Semantic Lexicons》。Motivation 是将 lexicon relations （比如 synset 等等同义词啊这些的 relation 作为 additional information）去“supervised” word vectors。注意，supervised 这个词，通篇都没有出现，是我自己加的。而且这个 idea 不是他们首创的，他们自己在论文里也说了，是 follow 14年的很多 short 工作（比如 Mo Yu 等人的工作），他们只是把它们变得通用了一点。这个工作有 Two Approaches: 一种是如题目一样，retrofit，就是任何已经 pre-trained 的 word vectors，别管你咋 train 出来的，我都可以再给你增强一下。另一种是从头开始，用另一种 learning method，边增强 lexicon relation constraints 边学 word vectors。使用的 Method: 可以直接看 Figure 1，就是用 relation dictionary（knowledge base）中的 counterpart words，作为一种镜像的映射。用 dictionary/knowledge base 中的词和词之间的 connection 关系，让需要 inferred 的 word vectors 中的对应的词更 closed——从而认为提升了语义表达。关于 Two Algorithms: 对应于 retrofit，因为是 convex 的，所以直接求解就好；对应于边 learn 边 retrofit，则比较麻烦，他们叫 lazy method of MAP + periodic method + NCE + AdaGrad。 比如这次 ACL 2015 上的：《SensEmbed: Learning Sense Embeddings for Word and Relational Similarity》来自 Ignacio Iacobacci, Mohammad Taher Pilehvar and Roberto Navigli。这篇主要解决两个问题， However, word embeddings inherit two important limitations from their antecedent corpus- based distributional models: (1) they are unable to model distinct meanings of a word as they conflate the contextual evidence of different meanings of a word into a single vector; and (2) they base their representations solely on the distributional statis- tics obtained from corpora, ignoring the wealth of information provided by existing semantic resources. 所以针对第一个，他们基于 BabelNet 这个 lexical resources，建立了初步的 multi senses，然后又基于别人的算法把 corpora 给自动标记成多 senses 的。后面又改造了不同种的 similarity metrics，为了增强 similarity performances。都是“修修补补”的工作。比较有意思的就是我发现越来越多的人都在用 BabelNet 了。 再比如依然是 ACL 2015 上的：《Revisiting Word Embedding for Contrasting Meaning》来自 Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen, Si Wei, Hui Jiang and Xiaodan Zhu。这篇论文 小S 个人非常喜欢，因为感到文章中的闪光点很多，能看到作者认认真真思考过这个工作，而不是随便想到什么赶紧做点差不多的实验就发出来。文章主要是针对基于 distributional hypothesis 假设的 word embedding models 无法解决的 contrasting meaning word pairs 提出自己的一个 embedding 框架。这个框架乍看有点复杂，但其是完全使用 lexical resources，而没有 distributional hypothesis/corpora statistics 的： Unlike what was suggested in previous work, where relatedness statistics learned from corpora is often claimed to yield extra gains over lexicon-based models, we obtained this new state-of-the-art result relying solely on lexical re- sources (Roget’s and WordNet), and corpus statis- tics does not seem to bring further improvement. To provide a comprehensive understanding, we constructed our study in a framework that exam- ines a number of basic concerns in modeling con- trasting meaning. We hope our efforts would help shed some light on future directions for this basic semantic modeling problem 主要建模思想依然和 word2vec 无异，就是用更符合 main goal 的 pair 应该更”近“，其他更不近就好—— The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. 但是这也不是他们的唯一思想，他们参考了许多相关建模工作，对于被 word embedding 刷屏的各位是一股清流，看看以前的人都是怎么做 relation embedding 的。也就是文章 Section 3.2-3.5 的不不分。比如他们就考虑到，虽然一对 constrasting word，比如 good 和 bad，意思是完全相反的，但是他俩很可能出现在相同的语境中。也就是说，他俩都可能和另一个词语比如说 C semantically closed。所以这时候我们就要考虑 constrasting + semantic 两种相关距离。与此同时，nonlinearity 也非常重要。最后作者也验证了一下到底 distributional hypothesis 下的 model 能做到什么程度，又为什么不能超过 lexical based。结论和 DAN（Deep Averaging Networks） 那篇还蛮像的。 还有 ACL 2015 的《Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning》来自 Angeliki Lazaridou, Georgiana Dinu and Marco Baroni。这篇论文很有意思，讨论的有点像上面提到的 constrasting meaning 那篇相反的问题——在 space 中，离的很近的点，往往很难区分，也就很难准确得出它们的表达（它们之间的区别）——这种点一般叫 hubness，是一个从至少 10年（ref 中有两篇）就被人提出过的问题。大概就是一些非常 general 的 items。这种 hubness 的 cause 和初步解决方法都在 Section 3 中提到，大概是我们常用的 LS 的 objective function 是会加剧这种效应的（十分好理解，毕竟公式就放在那里，ignore low variance），solution 也已经被“不经意地”发现了，那就是 Socher 等人用 max-margin loss “误打误撞”解决了这种效应。本篇论文中用实验证明了是 max-margin helps。 lexical resources 说完第一个方向，第二个方向则更直观一点。lexical relation 的很多过往研究都是基于 high quality 的 human annonatation 的，那么我们有没有办法把基于 statistical/distributional 的 word embedding model 的方法和已有的人工标注的 lexical resources 结合在一起呢？用 lexical resources 的高质量信息/知识，来完善、提高 word embedding 质量呢？ 这方面其实在 NAACL 2015 上已有相关的工作，比如 NAACL 2015 Best Student Paper Awards 得主之一，《Unsupervised Morphology Induction Using Word Embeddings》。Google 出品的，个人感觉是他们在工作中真的有这个需求。整个文章做法虽然宏观上很直观，很简洁，但是细节处，各种 threshold 也比较多，总的来说是一篇偏 linguistic 的论文。Model: 基于 SkipGram 改造的。Motivation: a) 基于 Mikolov 等人的 semantic relation （king-man+woman=queen），作者认为 word embeddings space 里也可以 induce 出 morphological relation；b) 传统的 morphological induction 或者 generation 的方法，基本基于 linguistic rule，而 linguistic rule 的不符合的案例太多，不太 robust，且费时费力；他们这个方法可以说依然是基于统计的（frequency）。Algorithm: a) frequency-ranked equation，挑选出 candidates。这一步用了两种 metric，rank+cosine。个人理解这是基于 word analogy 的一种改造，比如 king-man+woman=queen这种 word analogy，那么 king-man的这个距离，就可以认为是一种已知的衡量标杆，把它们分别认为是 w 和 w’。我们就可以通过 w 和 w’ 的距离来衡量 w1（queen）和 w2（woman）的距离。由此他们自己定义了一个公式（Equation 1）；b) 第二步，有了 rank 和 cosine 两个 metric，才能保证这些候选的 morphological rule candidates 是既常见的（lexicalized）和符合当前带评测 w1, w2 的真实关系的（meaning preservation）；c) 这个多种的 candidates，在他们论文里叫 multiple directions，有点像 Huang’12 年的一词多义问题。Task: a) 很自然的，有了 candidates，就去 induce 1-1 的map；b) 之后就更可以运用到 unseen/rare words 和 OOV 上。Evaluation: 用 word similarity 的 datasets 做评测，涉及到了6种语言，9个 datasets。这里也可以一定程度证明这个方法的简单有效，因为多种语言的话，去 handmade morphological rule 就很繁琐耗时。整体来看，文章思路直观，而且我感觉应用很广。毕竟 relation 这个东西，不仅存在于 semantic 和 morphological 上，还存在于各种问题上。这也是为什么现在 relation embedding 也是一个比较热的方向的原因。PS. 文章中说他们是使用自己实现的（slightly different）版本的 SkipGram，不是原始 word2vec 里面的。 再比如这次的 ACL 2015 Best Student Paper Awards 得主，《AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes》。论文想探究的是三种 data type，word, synset, lexeme，这三种 data type 都常见于 Lexical Resources，比如 WordNet，Freebase, Wiktionary 等等。作者想通过他们在 这种 resources 中的关系，来作为 constraints，去把 word embedding，synset embedding, lexeme embedding 一起学在同一个空间里。同时，论文基于我们任何已有的 word embedding，和任何已有的 resources，不需要额外的 training corpus，就可以得到 synset, lexeme embedding。先来说三种 data type：word，不用说了。synset，一组同义词，由多个与不同 word 有关的 lexeme 组成；lexeme，不知道中文叫啥，反正既有一词多义的意思，也有一词多种形态的意思（syntactic）。具体举例可以见 Section 2 的第二段。基于三种 data type，作者给出了两个 motivation 和 两个 observation 和两个 assumption（都是一个东西），然后这个东西就可以用来做 constraints 了，就是公式（1）（2），也是 Figure 1 架构的主要顺序。word-&gt;lexeme-&gt;synset-&gt;lexeme-&gt;word。除了这俩 motivation 和 这俩 constraints，作者还有第三个 motivation 和 第三个 constraints，而 constraints 第三个则是基于 resources 的性质，在 Section 2.4，用于解决的是当 word 没有 synset 时的问题。所以重新整理一下就是： Motivation: 1) 公式（1），一个 word 由多个 lexeme 组成；2) 公式（2），一个 synset 也由多个 lexeme 组成；3) 一个 word embedding 也可以认为是它不同 sense 的 embedding 的加和。 Constraints: 1) 公式（1），一个 word 由多个 lexeme 组成；2) 公式（2），一个 synset 也由多个 lexeme 组成；3) 公式（25），相关联的 synset 应该有相近的 embedding，这个 constraints 用于解决 word 没有 synset 或者只有一个 synset word 时的问题。 Model: 基于这几种 constraints，就可以设计一个 encoding - decoding autoencoder 的 NN，从 input 到 output，把 synsets 当 words 的 encoding。然后公式（10）到公式（17）就是整个 autoencoder 的 NN 的每一步骤的公式。可以看出，encoding - decoding 的两种表达之间的差值就是我们的 objective 函数了，即公式 （17）。 Implementation: 这个论文的另一个卖点除了这种很优美的 relation constraints 外，是他充分利用了 sparseness 去加速求解。他们把 word synset 之间，用 autoencoder 的 encoding 和 decoding part 中的 lexeme 为中间体，分别组成了一个 rank 4 的 tensor，即 E 和 D。E 和 D 也是 autoencoder 中要 learned 的 parameter。但是他们假设了 E 和 D 中的每一个维度之间是 no interaction 的，且又由于很多 lexeme 并不存在，所以更增加了 E 和 D 的 sparseness——最后的结果就是 E 和 D 的实际有效的维度大大降低——计算大大提速。但是我现在没想清楚这个 assumption 是否合理。另外还有一个实现细节是 Section 2.6。 Problem: 1) 就是上面说到的 no interaction 假设；2) 另一个不太优雅的地方是他们把 三种 constraints 用线性加权组合起来，即 Section 2.5 的地方，这个东西在他们这个框架下是比其他人的线性加权要合理的，因为他们假设 word, synset, lexeme 三者都是在同一个 embedding space。而且他们在实验中也讨论了这三个的权重，还算 OK 了。 还比如上面第一个方向中提到的 ACL 2015 的《Revisiting Word Embedding for Contrasting Meaning》来自 Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen, Si Wei, Hui Jiang and Xiaodan Zhu，内容简介见上方。 beyond words 这个方向相信大家已经不陌生了，从 Mikolov 2013 年放出 word2vec toolkit 源码并同时发表两篇 word2vec 连环弹鼻祖 paper 后，研究者们就开始群策群力地纷纷发表了 paragraph2vector, sentence2vector, document2vector。众所周知，这些工作和原始的 word2vec 差异不大，几乎是完全 follow 的 idea，变换一下 apply 的 unit。这样“粗略”的改进并不能达到真正的好效果。所以可以说，beyond words 这个方向依然有待研究。 最近一篇名字很 fancy 也很“囧”（被 Gordberg 说是个烂名字）的 paper 就是相关的工作，叫 thought vector，“思想向量”——来自 arXiv pre-print 论文《Skip-Thought Vectors》，Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov 等人的工作。Motivation: 很简单，想像 skip-gram 用中心词预测周围词一样，用中心句子预测周围句子。Model: 直接看 Figure 1，具体使用是 RNN-RNN 的 encoder-decoder 模型；其中两个 RNN 都用了 GRU 去“模拟” LSTM 的更优表现。在 encoder 阶段，只是一个 RNN；在 decoder 阶段，则是两个（分别对应前一个句子和后一个句子——也就是说不能预测多个前面的句子和后面的句子）。这样的模型可以保留一个 encoding for each sentence，这个 encoding 会很有用，就被称为 skip-thoughts vector，用来作为特征提取器，进行后续 task。注意是 Figure 1 中所谓的 unattached arrows，对应在 decoder 阶段，是有一个 words conditioned on previous word + previous hidden state 的 probability 束缚。同时，因为 decoder 也是 RNN，所以可用于 generation（在论文结尾处也给出了一些例子）。Mapping: 本文的另一个贡献是 vocabulary mapping。因为 RNN 的复杂性，但作者又不希望不能同时 learn word embedding，所以只好取舍一下——我们 learn 一部分 word embedding（words in training vocabulary）；对于没出现的部分，我们做一个 mapping from word embedding pre-trained from word2vec。这里的思想就是 Mikolov’13 年那篇 word similarity for MT 的，用一个没有正则的 L2 学好 mapping。在实验中，他们用此方法将 20K 的 vocabulary 扩充到了 930K。Experiments: 实验做的很多，我觉得做得还挺 solid 的，没什么 unfair 的（但我对实验一向不太了解，请大家了解的多看看）。 In our experiments we consider 8 tasks: semantic-relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we extract skip-thought vectors and train linear models to evaluate the representations directly, without any additional fine-tuning. As it turns out, skip-thoughts yield generic representations that perform robustly across all tasks considered. 首先是他们有三种 feature vectors，uni-skip/bi-skip/combine-skip。分别对应 encoder 是 unbidirectional，bidirectional，和 combine 的。分别都是 4800 dimensions。对于不同的 task，可能用不同的 feature indicator，比如把两个 skip-thoughts-vectors u 和 v，点乘或者相减，作为两个 feature，再用 linear classifier(logistic)。 beyond English 最后我们来说第四个方向，这个方向也是 ACL 2015 workshop 上，Yoav Goldberg 在其 talk 《word embeddings: what, how and whither》 上重点提及的，那就是现有的 word embedding 工作，放出来的已经 train 好的 vectors，绝绝绝绝绝大多数都是 English 的。可是即便是按照大的语系划分，世界上的语言也有太多不共通的性质。所以如果仅仅去研究 English 上的 word embedding 的改进，比如 compositional, morphological, 是远远不够的。 其实，beyond English 有两种分支，一种是完全抛弃 English，用另一种语言的语料，做一个新问题或者老问题。第二种分支就是 cross-lingual，都知道 English 是 rich-resource 的语言，也就是所谓的富语料，有充足的资源，cross-lingual 的一大优点就是可以用 rich-resource 的性质去弥补 low-resource 这边的不足。这也是为什么，cross-lingual 从来都会占据各大会议的一席之地的原因。 关于 beyond English 这个方向的研究除了 Yoav Goldberg 给的 talk 里提到的一些 Hebrew 的性质外，还有比如这次 ACL 2015 上的《A Generalisation of Lexical Functions for Composition in Distributional Semantics》，来自 Antoine Bride, Tim Van de Cruys and Nicholas Asher 等人的工作。一作是 relation embedding 的“鼻祖”，贡献了一堆 relation embedding 的 paper。这篇论文的一大贡献是他们给出了两组 datasets，一个是 adj-noun 的，一个是 noun-noun 的。分别是英文和法语的。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tf.app.flags定义命令行参数]]></title>
    <url>%2F2018%2F07%2F16%2F%E4%BD%BF%E7%94%A8tf.app.flags%E5%AE%9A%E4%B9%89%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[tf定义了tf.app.flags，用于支持接受命令行传递参数，相当于接受argv。 新建test_tf_flags.py文件内容如下：1234567891011121314151617import tensorflow as tf#第一个是参数名称，第二个参数是默认值，第三个是参数描述tf.app.flags.DEFINE_string('str_name', 'def_v_1',"descrip1")tf.app.flags.DEFINE_integer('int_name', 10,"descript2")tf.app.flags.DEFINE_boolean('bool_name', False, "descript3")FLAGS = tf.app.flags.FLAGS#必须带参数，否则：'TypeError: main() takes no arguments (1 given)'; main的参数名随意定义，无要求def main(_): print(FLAGS.str_name) print(FLAGS.int_name) print(FLAGS.bool_name)if __name__ == '__main__': tf.app.run() #执行main函数 执行：123456789(python3) E:\Code\NN_NER_tensorFlow-master&gt;python test_tf_flags.pydef_v_110False(python3) E:\Code\NN_NER_tensorFlow-master&gt;python test_tf_flags.py --str_name test_str --int_name 99 --bool_name Truetest_str99True]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self Attention原理]]></title>
    <url>%2F2018%2F07%2F16%2FSelf%20Attention%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/robert-dlut/p/8638283.html 简介视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。 这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。 图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。 深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。 注意力模型可以看作一种通用的思想，本身并不依赖于特定框架。 Encoder-Decoder框架要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。 Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。 图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。 文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成： Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息来生成i时刻要生成的单词： 每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。 Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。 Attention模型本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。 Soft Attention模型图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下： 其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。 而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。 如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。 在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。 没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。 上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值： （Tom,0.3）(Chase,0.2) (Jerry,0.5) 每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。 同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的。 增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。 即生成目标句子单词的过程成了下面的形式 而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下： 其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式: 其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，hj而则是Source输入句子中第j个单词的语义编码。 假设下标i就是上面例子所说的“ 汤姆” ，那么就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示的形成过程类似图4。 这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？ 为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。 那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。 对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成之前的时刻i-1时，隐层节点i-1时刻的输出值的，而我们的目的是要计算生成时输入句子中的单词“Tom”、“Chase”、“Jerry”对来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(,)来获得目标单词和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。 绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。 上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。 目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。 图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。 Attention机制的本质思想如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。 我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式： 其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。 当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。 从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。 至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。 在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式： 第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算： 第二阶段的计算结果即为对应的权重系数，然后进行加权求和即可得到Attention数值： 通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。 Self Attention模型通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。 在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。 如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。 从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。 很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Self Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow图变量tf.Variable的用法解析]]></title>
    <url>%2F2018%2F07%2F16%2FTensorFlow%E5%9B%BE%E5%8F%98%E9%87%8Ftf.Variable%E7%9A%84%E7%94%A8%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[图变量的初始化方法对于一般的Python代码，变量的初始化就是变量的定义，向下面这样：1234In [1]: x = 3In [2]: y = 3 * 5In [3]: yOut[3]: 15 如果我们模仿上面的写法来进行TensorFlow编程，就会出现下面的”怪现象”：12345In [1]: import tensorflow as tfIn [2]: x = tf.Variable(3, name='x')In [3]: y = x * 5In [4]: print(y)Tensor("mul:0", shape=(), dtype=int32) y的值并不是我们预想中的15，而是一个莫名其妙的输出。1234567In [1]: import tensorflow as tfIn [2]: x = tf.Variable(3, name='x')In [3]: y = x * 5In [4]: sess = tf.InteractiveSession()In [5]: sess.run(tf.global_variables_initializer())In [6]: sess.run(y)Out[6]: 15 在TensorFlow的世界里，变量的定义和初始化是分开的，所有关于图变量的赋值和计算都要通过tf.Session的run来进行。想要将所有图变量进行集体初始化时应该使用tf.global_variables_initializer。 两种定义图变量的方法tf.Variabletf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None) 参数名称 参数类型 含义 initial_value 所有可以转换为Tensor的类型 变量的初始值 trainable bool 如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer collections list 指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES] validate_shape bool 如果为False，则不进行类型和维度检查 name string 变量的名称，如果没有指定则系统会自动分配一个唯一的值 虽然有一堆参数，但只有第一个参数initial_value是必需的，用法如下（assign函数用于给图变量赋值）：123456789In [1]: import tensorflow as tfIn [2]: v = tf.Variable(3, name='v')In [3]: v2 = v.assign(5)In [4]: sess = tf.InteractiveSession()In [5]: sess.run(v.initializer)In [6]: sess.run(v)Out[6]: 3In [7]: sess.run(v2)Out[7]: 5 Args: initial_value: A Tensor, or Python object convertible to a Tensor, which is the initial value for the Variable. The initial value must have a shape specified unless validate_shape is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, dtype must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.) trainable: If True, the default, also adds the variable to the graph collection GraphKeys.TRAINABLE_VARIABLES. This collection is used as the default list of variables to use by the Optimizer classes. collections: List of graph collections keys. The new variable is added to these collections. Defaults to [GraphKeys.GLOBAL_VARIABLES]. validate_shape: If False, allows the variable to be initialized with a value of unknown shape. If True, the default, the shape of initial_value must be known. caching_device: Optional device string describing where the Variable should be cached for reading. Defaults to the Variable’s device. If not None, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through Switch and other conditional statements. name: Optional name for the variable. Defaults to ‘Variable’ and gets uniquified automatically.variable_def: VariableDef protocol buffer. If not None, recreates the Variable object with its contents, referencing the variable’s nodes in the graph, which must already exist. The graph is not changed. variable_def and the other arguments are mutually exclusive. dtype: If set, initial_value will be converted to the given type. If None, either the datatype will be kept (if initial_value is a Tensor), or convert_to_tensor will decide. expected_shape: A TensorShape. If set, initial_value is expected to have this shape.import_scope: Optional string. Name scope to add to the Variable. Only used when initializing from protocol buffer. constraint: An optional projection function to be applied to the variable after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected Tensor representing the value of the variable and return the Tensor for the projected value (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. tf.get_variabletf.get_variable跟tf.Variable都可以用来定义图变量，但是前者的必需参数（即第一个参数）并不是图变量的初始值，而是图变量的名称。 tf.Variable的用法要更丰富一点，当指定名称的图变量已经存在时表示获取它，当指定名称的图变量不存在时表示定义它，用法如下：1234567In [1]: import tensorflow as tfIn [2]: init = tf.constant_initializer([5])In [3]: x = tf.get_variable('x', shape=[1], initializer=init)In [4]: sess = tf.InteractiveSession()In [5]: sess.run(x.initializer)In [6]: sess.run(x)Out[6]: array([ 5.], dtype=float32) 123456789101112131415tf.get_variable( name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None, constraint=None) scope如何划分命名空间一个深度学习模型的参数变量往往是成千上万的，不加上命名空间加以分组整理，将会成为可怕的灾难。TensorFlow的命名空间分为两种，tf.variable_scope和tf.name_scope。 下面示范使用tf.variable_scope把图变量划分为4组：1234for i in range(4): with tf.variable_scope('scope-&#123;&#125;'.format(i)): for j in range(25): v = tf.Variable(1, name=str(j)) 可视化输出的结果如下： 下面让我们来分析tf.variable_scope和tf.name_scope的区别： tf.variable_scope当使用tf.get_variable定义变量时，如果出现同名的情况将会引起报错12345In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('scope'): ...: v1 = tf.get_variable('var', [1]) ...: v2 = tf.get_variable('var', [1])ValueError: Variable scope/var already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at: 而对于tf.Variable来说，却可以定义“同名”变量1234567In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('scope'): ...: v1 = tf.Variable(1, name='var') ...: v2 = tf.Variable(2, name='var') ...:In [3]: v1.name, v2.nameOut[3]: ('scope/var:0', 'scope/var_1:0') 但是把这些图变量的name属性打印出来，就可以发现它们的名称并不是一样的。 如果想使用tf.get_variable来定义另一个同名图变量，可以考虑加入新一层scope，比如：12345678In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('scope1'): ...: v1 = tf.get_variable('var', shape=[1]) ...: with tf.variable_scope('scope2'): ...: v2 = tf.get_variable('var', shape=[1]) ...:In [3]: v1.name, v2.nameOut[3]: ('scope1/var:0', 'scope1/scope2/var:0') tf.name_scope当tf.get_variable遇上tf.name_scope，它定义的变量的最终完整名称将不受这个tf.name_scope的影响，如下：123456789In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('v_scope'): ...: with tf.name_scope('n_scope'): ...: x = tf.Variable([1], name='x') ...: y = tf.get_variable('x', shape=[1], dtype=tf.int32) ...: z = x + y ...:In [3]: x.name, y.name, z.nameOut[3]: ('v_scope/n_scope/x:0', 'v_scope/x:0', 'v_scope/n_scope/add:0') 图变量的复用想象一下，如果我们正在定义一个循环神经网络RNN，想复用上一层的参数以提高模型最终的表现效果，应该怎么做呢？ 做法一：12345678In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('scope'): ...: v1 = tf.get_variable('var', [1]) ...: tf.get_variable_scope().reuse_variables() ...: v2 = tf.get_variable('var', [1]) ...:In [3]: v1.name, v2.nameOut[3]: ('scope/var:0', 'scope/var:0') 做法二：123456789In [1]: import tensorflow as tfIn [2]: with tf.variable_scope('scope'): ...: v1 = tf.get_variable('x', [1]) ...:In [3]: with tf.variable_scope('scope', reuse=True): ...: v2 = tf.get_variable('x', [1]) ...:In [4]: v1.name, v2.nameOut[4]: ('scope/x:0', 'scope/x:0') 图变量的种类TensorFlow的图变量分为两类：local_variables和global_variables。 如果我们想定义一个不需要长期保存的临时图变量，可以向下面这样定义它：123with tf.name_scope("increment"): zero64 = tf.constant(0, dtype=tf.int64) current = tf.Variable(zero64, name="incr", trainable=False, collections=[ops.GraphKeys.LOCAL_VARIABLES]) tf.trainable_variables方法12345678910111213141516171819202122232425262728import tensorflow as tfv1 = tf.get_variable('v1', shape=[1])v2 = tf.get_variable('v2', shape=[1], trainable=False)with tf.variable_scope('scope1'): s1 = tf.get_variable('s1', shape=[1], initializer=tf.random_normal_initializer())g1=tf.Graph()g2=tf.Graph()with g1.as_default(): g1v1 = tf.get_variable('g1v1', shape=[1]) g1v2 = tf.get_variable('g1v2', shape=[1], trainable=False) g1vs = tf.trainable_variables() # [&lt;tf.Variable 'g1v1:0' shape=(1,) dtype=float32_ref&gt;] print(g1vs)with g2.as_default(): g2v1 = tf.get_variable('g2v1', shape=[1]) g2v2 = tf.get_variable('g2v2', shape=[1], trainable=False) g2vs = tf.trainable_variables() # [&lt;tf.Variable 'g2v1:0' shape=(1,) dtype=float32_ref&gt;] print(g2vs)with tf.Session() as sess: vs = tf.trainable_variables() # [&lt;tf.Variable 'v1:0' shape=(1,) dtype=float32_ref&gt;, &lt;tf.Variable 'scope1/s1:0' shape=(1,) dtype=float32_ref&gt;] print(vs) tf.trainable_variables 返回所有 当前计算图中 在获取变量时未标记 trainable=False 的变量集合 从1.4版本开始可以支持传入scope，来获取指定scope中的变量集合]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gensim word2vec参数说明]]></title>
    <url>%2F2018%2F07%2F16%2Fgensim%20word2vec%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[class gensim.models.word2vec.Word2Vec(sentences=None,size=100,alpha=0.025,window=5, min_count=5,max_vocab_size=None, sample=0.001,seed=1, workers=3,min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1,hashfxn=,iter=5,null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000) 参数： 1.sentences：可以是一个List，对于大语料集，建议使用BrownCorpus,Text8Corpus或·ineSentence构建。2.sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。3.size：是指输出的词的向量维数，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。4.window：为训练的窗口大小，8表示每个词考虑前8个词与后8个词（实际代码中还有一个随机选窗口的过程，窗口大小&lt;=5)，默认值为5。5.alpha: 是学习速率6.seed：用于随机数发生器。与初始化词向量有关。7.min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5。8.max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。9.sample: 表示 采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样。默认为1e-3，范围是(0,1e-5)10.workers:参数控制训练的并行数。11.hs: 是否使用HS方法，0表示不使用，1表示使用 。默认为012.negative: 如果&gt;0,则会采用negativesamp·ing，用于设置多少个noise words13.cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（default）则采用均值。只有使用CBOW的时候才起作用。14.hashfxn： hash函数来初始化权重。默认使用python的hash函数15.iter： 迭代次数，默认为5。16.trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。17.sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。18.batch_words：每一批的传递给线程的单词的数量，默认为10000]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Word2vec</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.summary]]></title>
    <url>%2F2018%2F07%2F16%2Ftf.summary%2F</url>
    <content type="text"><![CDATA[TensorBoard可以将训练过程中的各种绘制数据展示出来，包括标量（scalars），图片（images），音频（Audio）,计算图（graph）,数据分布，直方图（histograms）和嵌入式向量。 使用TensorBoard展示数据，需要在执行Tensorflow就算图的过程中，将各种类型的数据汇总并记录到日志文件中。然后使用TensorBoard读取这些日志文件，解析数据并生产数据可视化的Web页面，让我们可以在浏览器中观察各种汇总数据。 summary_op包括了summary.scalar、summary.histogram、summary.image等操作，这些操作输出的是各种summary protobuf，最后通过summary.writer写入到event文件中。Tensorflow API中包含系列生成summary数据的API接口，这些函数将汇总信息存放在protobuf中，以字符串形式表达。 tf.summary.FileWriter将汇总的protobuf写入到event文件中去的相关的类： SummaryWriter是一个类，它可以调用以下成员函数来往event文件中添加相关的数据 addsummary(), add sessionlog(), add_event(), or add_graph()1234567Writes `Summary` protocol buffers to event files.The `FileWriter` class provides a mechanism to create an event file in agiven directory and add summaries and events to it. The class updates thefile contents asynchronously. This allows a training program to call methodsto add data to the file directly from the training loop, without slowing downtraining. 12345...create a graph...# Launch the graph in a session.sess = tf.Session()# Create a summary writer, add the 'graph' to the event file.writer = tf.summary.FileWriter(&lt;some-directory&gt;, sess.graph) exampleummary_waiter = tf.summary.FileWriter(“log”,tf.get_default_graph()) log是事件文件所在的目录，这里是工程目录下的log目录。第二个参数是事件文件要记录的图，也就是tensorflow默认的图。 tf.summary.scalar对标量数据汇总和记录使用tf.summary.scalar，函数格式如下：tf.summary.scalar(tags, values, collections=None, name=None)123456789101112131415161718Outputs a `Summary` protocol buffer containing a single scalar value.The generated Summary has a Tensor.proto containing the input Tensor.Args: name: A name for the generated node. Will also serve as the series name in TensorBoard. tensor: A real numeric Tensor containing a single value. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. family: Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard.Returns: A scalar `Tensor` of type `string`. Which contains a `Summary` protobuf.Raises: ValueError: If tensor has the wrong shape or type. 一般在画loss,accuary时会用到这个函数。 tf.summary.histogram使用tf.summary.histogram直接记录变量var的直方图，输出带直方图的汇总的protobuf，函数格式如下：tf.summary.histogram(tag, values, collections=None, name=None)1234567891011121314151617181920212223242526Outputs a `Summary` protocol buffer with a histogram.Adding a histogram summary makes it possible to visualize your data'sdistribution in TensorBoard. You can see a detailed explanation of theTensorBoard histogram dashboard[here](https://www.tensorflow.org/get_started/tensorboard_histograms).The generated[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)has one summary value containing a histogram for `values`.This op reports an `InvalidArgument` error if any value is not finite.Args: name: A name for the generated node. Will also serve as a series name in TensorBoard. values: A real numeric `Tensor`. Any shape. Values to use to build the histogram. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. family: Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard.Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer. tf.summary.merge将上面几种类型的汇总再进行一次合并，具体合并哪些由inputs指定，格式如下：tf.summary.merge(inputs, collections=None, name=None) tf.summaries.merge_all合并默认图形中的所有汇总：tf.summaries.merge_all(key=&#39;summaries&#39;) add_summary12345678910111213141516Adds a `Summary` protocol buffer to the event file.This method wraps the provided summary in an `Event` protocol bufferand adds it to the event file.You can pass the result of evaluating any summary op, using@&#123;tf.Session.run&#125; or@&#123;tf.Tensor.eval&#125;, to thisfunction. Alternatively, you can pass a `tf.Summary` protocolbuffer that you populate with your own data. The latter iscommonly done to report evaluation results in event files.Args: summary: A `Summary` protocol buffer, optionally serialized as a string. global_step: Number. Optional global step value to record with the summary. 一个完整的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from __future__ import print_functionimport tensorflow as tfimport numpy as npdef add_layer(inputs, in_size, out_size, n_layer, activation_function=None): # add one more layer and return the output of this layer layer_name = 'layer%s' % n_layer with tf.name_scope(layer_name): with tf.name_scope('weights'): Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W') tf.summary.histogram(layer_name + '/weights', Weights) with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b') tf.summary.histogram(layer_name + '/biases', biases) with tf.name_scope('Wx_plus_b'): Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) tf.summary.histogram(layer_name + '/outputs', outputs) return outputs# Make up some real datax_data = np.linspace(-1, 1, 300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkwith tf.name_scope('inputs'): xs = tf.placeholder(tf.float32, [None, 1], name='x_input') ys = tf.placeholder(tf.float32, [None, 1], name='y_input')# add hidden layerl1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)# the error between prediciton and real datawith tf.name_scope('loss'): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) tf.summary.scalar('loss', loss)with tf.name_scope('train'): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)sess = tf.Session()merged = tf.summary.merge_all()writer = tf.summary.FileWriter("logs/", sess.graph)init = tf.global_variables_initializer()sess.run(init)for i in range(1000): sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: result = sess.run(merged, feed_dict=&#123;xs: x_data, ys: y_data&#125;) writer.add_summary(result, i) tf_graph tf_loss tf_distributions tf_histograms]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Critical Reading]]></title>
    <url>%2F2018%2F07%2F16%2FCritical%20Reading%2F</url>
    <content type="text"><![CDATA[What is critical reading?The most characteristic features of critical reading are that you will: examine the evidence or arguments presented; check out any influences on the evidence or arguments; check out the limitations of study design or focus; examine the interpretations made; and decide to what extent you are prepared to accept the authors’ arguments, opinions, or conclusions. Why do we need to take a critical approach to reading?Regardless of how objective, technical, or scientific the subject matter, the author(s) will have made many decisions duringthe research and writing process, and each of these decisions is a potential topic for examination and debate, rather than forblind acceptance. You need to be prepared to step into the academic debate and to make your own evaluation of how much you are willing toaccept what you read. A practical starting point therefore, is to consider anything you read not as fact, but as the argument of the writer. Taking thisstarting point you will be ready to engage in critical reading. Critical reading does not have to be all negativeThe aim of critical reading is not to find fault, but to assess the strength of the evidence and the argument. It is just as usefulto conclude that a study, or an article, presents very strong evidence and a well-reasoned argument, as it is to identify thestudies or articles that are weak. EvidenceDepending on the kind of writing it is, and the discipline in which it sits, different kinds of evidence will be presented for youto examine. At the technical and scientific end of the spectrum, relevant evidence may include information on: measurements, timing,equipment, control of extraneous factors, and careful following of standard procedures. Specific guidance will be availablewithin specialties on what to look for. At the other end of the spectrum is writing where there is clearer scope for personal interpretation, for example: analysis of individuals’ experiences of healthcare; the translation of a text from a foreign language; or the identification and analysis of a range of themes in a novel. In these cases the evidence may include items such as quotes from interviews, extracts of text, and diagrams showing howthemes might connect. The nature of the evidence presented at these two extremes is different, but in both cases you need to look for the rationalefor the selection and interpretation of the evidence presented, and the rationale for the construction of the argument. Broadening the definition of evidenceThis Study Guide takes a broad view of evidence: it maintains that all that you read can be considered as evidence, not purelythe actual data collected/presented. This encompasses: the report of the context within which the data were collected or created; the choice of the method for data collection or selection; the audit trail for the analysis of the data i.e.: the decisions made and the steps in the analysis process; the rationale for the interpretations made and the conclusions drawn; the relevance of, and the use made of the theoretical perspective, ideology, or philosophy that is underpinning theargument. Linking evidence to argumentOn its own, evidence cannot contribute to academic debate. The interpretation and presentation of that evidence within anargument allows the evidence to make a contribution. The term ‘argument’ in this context means the carefully constructed rationale for the enquiry, and for the place of its resultswithin the academic arena. It will explain for example: why the authors considered that what they did was worth doing; why it was worth doing in that particular way; why the data collected, or the material selected, were the most appropriate; how the conclusions drawn link to the wider context of their enquiry. Even in the most technical and scientific disciplines, the presentation of argument will always involve elements that can beexamined and questioned. For example, you could ask: Why did the writer select that particular topic of enquiry in the first place? Why did the writer decide to use that particular methodology, choose that specific method, and conduct the work inthat way? Why did the writer select that particular process of analysis? Note takingAs you read, it can be helpful to use a table to record the information that you know you will need later. In addition to theusual bibliographical details, you can devise your own list of extra information you want to collect at the initial reading stage.Some suggestions are given below. Two important points about using such tables are: it is essential that you devise your own list of information to collect from each source, based on what you know youwill need to comment upon; and realistically, it is probably best not to try to collect this information from every single source you use, only fromthose you decide to refer to in your report or assignment. Otherwise it could really slow down your backgroundreading, and result in the collection of a mass of material that you never use. Descriptive details you may want to record about sources details details Setting Type of data Sample size Use of theory Sample profile Equipment Follow up Style of writing Statistics used Measurements Methods Sources of bias Questions raised Limitations Main arguments Intended audience Some interpretative questions you may need to ask about sourcesThese are questions that need more input from you as the critical reader. You will need to make judgements about youranswers, and will need to record the reasons for your answers. This list is a mix of arts and science-based questions, as thereare several areas of common interest. Q How well-developed are the themes or arguments? Q Did the theoretical perspective used introduce any potential bias? Q Are you convinced by the interpretations presented? Q Are the conclusions supported firmly by the preceding argument? Q How appropriate are the comparisons that are used? Q Did the response options, or measurement categories or techniques used affect the data that were collected? Q Have any ethical considerations been adequately addressed? If you take a critical approach right from the start of your reading and note taking, it can save a lot of time later on. When youcome to write your assignment or thesis, you will need to comment on the validity of the writing that you refer to. So, if you have kept a systematic record of the results of your critical reading, you will be able to refer to it easily. If you have not, youwill find yourself wasting a lot of time re-reading material, and re-reviewing the evidence presented. Helpful guidance from other sources Does the writing assume a causal connection when there may not be one? Are general conclusions drawn based on only a few examples? Are inappropriate comparisons being made? Might there be other explanations apart from the one proposed? Are there any hidden assumptions that need to be questioned? Is enough evidence presented to allow readers to draw their own conclusions? Does the line of reasoning make sense?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>critical reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux建文件夹软链]]></title>
    <url>%2F2018%2F07%2F16%2Flinux%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9%E8%BD%AF%E9%93%BE%2F</url>
    <content type="text"><![CDATA[当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的 目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。例如：ln -s /bin/less /usr/local/bin/less-s 是代号（symbolic）的意思。这 里有两点要注意： 第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二，ln的链接又软链接 和硬链接两种，软链接就是ln -s ,它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接ln ,没有参数-s, 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 具体用法：ln -s 源文件 目标文件1234567891011-f : 链结时先将与 dist 同档名的档案删除 -d : 允许系统管理者硬链结自己的目录 -i : 在删除与 dist 同档名的档案时先进行询问 -n : 在进行软连结时，将 dist 视为一般的档案 -s : 进行软链结(symbolic link) -v : 在连结之前显示其档名 -b : 将在链结时会被覆写或删除的档案进行备份 -S SUFFIX : 将备份的档案都加上 SUFFIX 的字尾 -V METHOD : 指定备份的方式 --help : 显示辅助说明 --version : 显示版本 实例：ln -s /home/gamestat /gamestat linux下的软链接类似于windows下的快捷方式 ln -s a b 中的 a 就是源文件，b是链接文件名,其作用是当进入b目录，实际上是链接进入了a目录 如上面的示例，当我们执行命令 cd /gamestat/的时候 实际上是进入了 /home/gamestat/删除软链接： rm -rf b 注意不是rm -rf b/ 值得注意的是执行命令的时候,应该是a目录已经建立，目录b没有建立。我最开始操作的是也把b目录给建立了，结果就不对了ln -s /data0/zch10 /home/szdata/zch10]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[from __future__ import print_function]]></title>
    <url>%2F2018%2F07%2F16%2Ffrom%20__future__%20import%20print_function%2F</url>
    <content type="text"><![CDATA[Python提供了future模块,把下一个新版本的特性导入到当前版本,于是我们就可以在当前版本中测试一些新版本的特性。 下面从future引入对于python2来说比较先进的模块print_function接口参数:12345678910print(*values, sep=' ', end='/n', file=sys.stdout)print(value1, value2, value3, sep=' ', end='/n', file=sys.stdout)这里,输出的变量可以是一个序列或者多个变量,可以像上面一样用逗号分开每个变量。 参数sep,end,file是三个可选参数。sep:指每个输出变量之间的分隔符,默认是一个空格end:指的是输出结束后的内容,默认是换行file:指的是输出流要输出的目的文件,默认sys.stdout(标准输出) 效果如下所示:1234567891011121314151617181920Python 2.7.14 |Anaconda, Inc.| (default, Nov 8 2017, 13:40:45) [MSC v.1500 64 bit (AMD64)] on win32Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; value1 = 1&gt;&gt;&gt; value2 = 2&gt;&gt;&gt; value3 = 3&gt;&gt;&gt; print(value1, value2, value3, sep=' ', end='/n', file=sys.stdout) File "&lt;stdin&gt;", line 1 print(value1, value2, value3, sep=' ', end='/n', file=sys.stdout) ^SyntaxError: invalid syntax&gt;&gt;&gt; from __future__ import print_function&gt;&gt;&gt; print(value1, value2, value3, sep=' ', end='/n', file=sys.stdout)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;NameError: name 'sys' is not defined&gt;&gt;&gt; import sys&gt;&gt;&gt; print(value1, value2, value3, sep=' ', end='/n', file=sys.stdout)1 2 3/n&gt;&gt;&gt;&gt;&gt;&gt;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime查看设置文件编码格式]]></title>
    <url>%2F2018%2F07%2F16%2FSublime%E6%9F%A5%E7%9C%8B%E8%AE%BE%E7%BD%AE%E6%96%87%E4%BB%B6%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Sublime Text的默认设置是不开启显示编码的，如果想开启，可通过菜单Perference → Settings – User，在打开的配置文件里 ，在大括号后面，增加以下内容：1234// Display file encoding in the status bar"show_encoding": true,// Display line endings in the status bar"show_line_endings": true ,此时保存该配置文件，就能够看到sublime最底下一行会显示文件编码格式了。以上的配置内容在Perference → Setting─Default都是false的。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vmware虚拟环境搭建]]></title>
    <url>%2F2018%2F07%2F16%2Fvmware%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[虚拟机环境安装： 1. VMware workstation 12 pro 2. ubuntu-16.04-desktop-amd64.iso 3. Boredbird,zch,zch0302 系统菜单》设置》软件和更新》切换成清华的源sudo apt-get updatesudo apt-get upgradesudo apt-get install ssh xshell连接： 判断是否unbuntu安装了SSH服务： #ps -e |grep ssh, 需要能看到sshd，否则需要安装SSH 安装SSH： # sudo apt-get install openssh-server 查看是否启动ssh服务 ps -e |grep ssh 启动SSH服务： #/etc/init/ssh start 查看Ubuntu本机IP：ifconfig -a 在虚拟机上测试是否能成功登陆： #ssh -l 用户名 本机IP root@ubuntu:/# ssh -l zch 192.168.171.128 xshell连接配置 Ubuntu虚拟机python环境： 在宿主机中将Anaconda2-4.3.1-Linux-x86_64.sh放在VMware的共享文件夹 Ubuntu中进入目录 cd /mnt/hgfs/ Anaconda2安装： sudo sh Anaconda2-4.3.1-Linux-x86_64.sh bash Anaconda2-4.3.1-Linux-x86_64.sh 12345678910111213141516zch@ubuntu:/mnt/hgfs/shareVM$ sudo sh Anaconda2-4.3.1-Linux-x86_64.sh[sudo] password for zch:Anaconda2-4.3.1-Linux-x86_64.sh: 16: Anaconda2-4.3.1-Linux-x86_64.sh: 0: not foundAnaconda2-4.3.1-Linux-x86_64.sh: 61: Anaconda2-4.3.1-Linux-x86_64.sh: 0: not foundAnaconda2-4.3.1-Linux-x86_64.sh: 75: Anaconda2-4.3.1-Linux-x86_64.sh: Syntax error: word unexpected (expecting ")")此时共享文件夹中的可执行文件sh有个后缀*星号zch@ubuntu:/mnt/hgfs/shareVM$ lltotal 473123drwxrwxrwx 1 root root 0 Mar 24 02:09 ./dr-xr-xr-x 1 root root 4192 Mar 24 02:16 ../-rwxrwxrwx 1 root root 484472684 Jun 13 2017 Anaconda2-4.3.1-Linux-x86_64.sh*zch@ubuntu:/mnt/hgfs/shareVM$ ls -FAnaconda2-4.3.1-Linux-x86_64.sh*chmod -x Anaconda2-4.3.1-Linux-x86_64.sh Linux下系统自带python和Anaconda切换 1234567891011121314root@ubuntu:~# vim .bashrc添加：export PATH="/root/anaconda2/bin:$PATH"root@ubuntu:~# source ~/.bashrc效果如下：root@ubuntu:~# vim .bashrcroot@ubuntu:~# source ~/.bashrcroot@ubuntu:~# pythonPython 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15)[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2Type "help", "copyright", "credits" or "license" for more information.Anaconda is brought to you by Continuum Analytics.Please check out: http://continuum.io/thanks and https://anaconda.org&gt;&gt;&gt; 设置Ubuntu允许root用户ssh登陆 12345678910111213141516171819201. 修改root密码root@ubuntu:~/anaconda2/bin# sudo passwd rootEnter new UNIX password:Retype new UNIX password:passwd: password updated successfully2. 修改配置文件root@ubuntu:~/anaconda2/bin# vim /etc/ssh/sshd_config将原来：# Authentication: LoginGraceTime 120 PermitRootLogin prohibit-password StrictModes yes 替换为：# Authentication: LoginGraceTime 120 #PermitRootLogin prohibit-password PermitRootLogin yes StrictModes yes3.重启ssh服务root@ubuntu:~/anaconda2/bin# sudo service ssh restart pycharm中连接Ubuntu的python环境 pip 安装： root@ubuntu:~# sudo apt-get install python-pip conda设置国内镜像: # 添加Anaconda的TUNA镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # TUNA的help中镜像地址加有引号，需要去掉 # 设置搜索时显示通道地址 conda config --set show_channel_urls yes pytorch安装与测试： root@ubuntu:~# conda install pytorch torchvision cuda80 -c soumith 问题: conda: command not found 解决办法：立即生效环境变量 root@ubuntu:~/anaconda2/bin# source ~/.bashrc root@ubuntu:~/anaconda2/bin# conda list 问题： 在终端下输入官网给出的命令，安装pytorch、torchvision 由于被墙的原因无法进行选择，本文采用shadowsocks代理 解决办法：[采用shadowsocks代理](https://blog.csdn.net/h406395933/article/details/75200540) 1. 添加ppa源： $ sudo add-apt-repository ppa:hzwhuang/ss-qt5 $ sudo apt-get update $ sudo apt-get install shadowsocks-qt5 2. 启动shadowsocks 点击连接-添加-手动进行配置 torch-0.3.0.post4-cp27-cp27mu-linux_x86_64.whl下载 tensorflow安装： sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl tensorflow中文教程 crf++安装与测试： crfpp downloads crf++ 0.58 下载 crf++ 安装 import CRFPP报错： 解决办法： ln -s /usr/local/lib/libcrfpp.so.0 /usr/lib/]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crf++ 官方tutorial]]></title>
    <url>%2F2018%2F07%2F16%2Fcrf%2B%2B%20%E5%AE%98%E6%96%B9tutorial%2F</url>
    <content type="text"><![CDATA[Training and Test file formatsBoth the training file and the test file need to be in a particular format for CRF++ to work properly. Generally speaking, training and test file must consist of multiple tokens. In addition, a token consists of multiple (but fixed-numbers) columns. The definition of tokens depends on tasks, however, in most of typical cases, they simply correspond to words. Each token must be represented in one line, with the columns separated by white space (spaces or tabular characters). A sequence of token becomes a sentence. To identify the boundary between sentences, an empty line is put. You can give as many columns as you like, however the number of columns must be fixed through all tokens. Furthermore, there are some kinds of “semantics” among the columns. For example, 1st column is ‘word’, second column is ‘POS tag’ third column is ‘sub-category of POS’ and so on. The last column represents a true answer tag which is going to be trained by CRF. Here’s an example of such a file: (data for CoNLL shared task) 1234567891011121314151617181920He PRP B-NPreckons VBZ B-VPthe DT B-NPcurrent JJ I-NPaccount NN I-NPdeficit NN I-NPwill MD B-VPnarrow VB I-VPto TO B-PPonly RB B-NP# # I-NP1.8 CD I-NPbillion CD I-NPin IN B-PPSeptember NNP B-NP. . OHe PRP B-NPreckons VBZ B-VP.. There are 3 columns for each token. The word itself (e.g. reckons); part-of-speech associated with the word (e.g. VBZ); Chunk(answer) tag represented in IOB2 format; The following data is invalid, since the number of columns of second and third are 2. (They have no POS column.) The number of columns should be fixed. 123456He PRP B-NPreckons B-VPthe B-NPcurrent JJ I-NPaccount NN I-NP.. Preparing feature templatesAs CRF++ is designed as a general purpose tool, you have to specify the feature templates in advance. This file describes which features are used in training and testing. Template basic and macroEach line in the template file denotes one template. In each template, special macro %x[row,col] will be used to specify a token in the input data. row specfies the relative position from the current focusing token and col specifies the absolute position of the column. Here you can find some examples for the replacements123456Input: DataHe PRP B-NPreckons VBZ B-VPthe DT B-NP &lt;&lt; CURRENT TOKENcurrent JJ I-NPaccount NN I-NP template expanded feature %x[0,0] the %x[0,1] DT %x[-1,0] reckons %x[-2,1] PRP %x[0,0]/%x[0,1] the/DT ABC%x[0,1]123 ABCDT123 Template typeNote also that there are two types of templates. The types are specified with the first character of templates. Unigram template: first character, ‘U’ This is a template to describe unigram features. When you give a template “U01:%x[0,1]”, CRF++ automatically generates a set of feature functions (func1 … funcN) like: 1234567func1 = if (output = B-NP and feature="U01:DT") return 1 else return 0func2 = if (output = I-NP and feature="U01:DT") return 1 else return 0func3 = if (output = O and feature="U01:DT") return 1 else return 0....funcXX = if (output = B-NP and feature="U01:NN") return 1 else return 0funcXY = if (output = O and feature="U01:NN") return 1 else return 0... The number of feature functions generated by a template amounts to (L * N), where L is the number of output classes and N is the number of unique string expanded from the given template. Bigram template: first character, ‘B’ This is a template to describe bigram features. With this template, a combination of the current output token and previous output token (bigram) is automatically generated. Note that this type of template generates a total of (L L N) distinct features, where L is the number of output classes and N is the number of unique features generated by the templates. When the number of classes is large, this type of templates would produce a tons of distinct features that would cause inefficiency both in training/testing. What is the diffrence between unigram and bigram features? The words unigram/bigram are confusing, since a macro for unigram-features does allow you to write word-level bigram like %x[-1,0]%x[0,0]. Here, unigram and bigram features mean uni/bigrams of output tags. unigram: |output tag| x |all possible strings expanded with a macro| bigram: |output tag| x |output tag| x |all possible strings expanded with a macro| Identifiers for distinguishing relative positions You also need to put an identifier in templates when relative positions of tokens must be distinguished. In the following case, the macro “%x[-2,1]” and “%x[1,1]” will be replaced into “DT”. But they indicates different “DT”. 1234The DT B-NPpen NN I-NPis VB B-VP &lt;&lt; CURRENT TOKENa DT B-NP To distinguish both two, put an unique identifier (U01: or U02:) in the template: 12U01:%x[-2,1]U02:%x[1,1] In this case both two templates are regarded as different ones, as they are expanded into different features, “U01:DT” and “U02:DT”. You can use any identifier whatever you like, but it is useful to use numerical numbers to manage them, because they simply correspond to feature IDs. If you want to use “bag-of-words” feature, in other words, not to care the relative position of features, You don’t need to put such identifiers. ExampleHere is the template example for CoNLL 2000 shared task and Base-NP chunking task. Only one bigram template (‘B’) is used. This means that only combinations of previous output token and current token are used as bigram features. The lines starting from # or empty lines are discarded as comments 12345678910111213141516171819202122232425# UnigramU00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-1,0]/%x[0,0]U06:%x[0,0]/%x[1,0]U10:%x[-2,1]U11:%x[-1,1]U12:%x[0,1]U13:%x[1,1]U14:%x[2,1]U15:%x[-2,1]/%x[-1,1]U16:%x[-1,1]/%x[0,1]U17:%x[0,1]/%x[1,1]U18:%x[1,1]/%x[2,1]U20:%x[-2,1]/%x[-1,1]/%x[0,1]U21:%x[-1,1]/%x[0,1]/%x[1,1]U22:%x[0,1]/%x[1,1]/%x[2,1]# BigramB Training (encoding) Use crf_learn command: % crf_learn template_file train_file model_file, where template_file and train_file are the files you need to prepare in advance. crf_learn generates the trained model file in model_file. crf_learn outputs the following information. 123456789101112131415161718192021222324CRF++: Yet Another CRF Tool KitCopyright(C) 2005 Taku Kudo, All rights reserved.reading training data: 100.. 200.. 300.. 400.. 500.. 600.. 700.. 800..Done! 1.94 sNumber of sentences: 823Number of features: 1075862Number of thread(s): 1Freq: 1eta: 0.00010C: 1.00000shrinking size: 20Algorithm: CRFiter=0 terr=0.99103 serr=1.00000 obj=54318.36623 diff=1.00000iter=1 terr=0.35260 serr=0.98177 obj=44996.53537 diff=0.17161iter=2 terr=0.35260 serr=0.98177 obj=21032.70195 diff=0.53257iter=3 terr=0.23879 serr=0.94532 obj=13642.32067 diff=0.35138iter=4 terr=0.15324 serr=0.88700 obj=8985.70071 diff=0.34134iter=5 terr=0.11605 serr=0.80680 obj=7118.89846 diff=0.20775iter=6 terr=0.09305 serr=0.72175 obj=5531.31015 diff=0.22301iter=7 terr=0.08132 serr=0.68408 obj=4618.24644 diff=0.16507iter=8 terr=0.06228 serr=0.59174 obj=3742.93171 diff=0.18953 iter: number of iterations processed terr: error rate with respect to tags. (# of error tags/# of all tag) serr: error rate with respect to sentences. (# of error sentences/# of all sentences) obj: current object value. When this value converges to a fixed point, CRF++ stops the iteration. diff: relative difference from the previous object value. There are 4 major parameters to control the training condition -a CRF-L2 or CRF-L1:Changing the regularization algorithm. Default setting is L2. Generally speaking, L2 performs slightly better than L1, while the number of non-zero features in L1 is drastically smaller than that in L2. -c float:With this option, you can change the hyper-parameter for the CRFs. With larger C value, CRF tends to overfit to the give training corpus. This parameter trades the balance between overfitting and underfitting. The results will significantly be influenced by this parameter. You can find an optimal value by using held-out data or more general model selection method such as cross validation. -f NUM:This parameter sets the cut-off threshold for the features. CRF++ uses the features that occurs no less than NUM times in the given training data. The default value is 1. When you apply CRF++ to large data, the number of unique features would amount to several millions. This option is useful in such cases. -p NUM:If the PC has multiple CPUs, you can make the training faster by using multi-threading. NUM is the number of threads. Here is the example where these two parameters are used. % crf_learn -f 3 -c 1.5 template_file train_file model_file Since version 0.45, CRF++ supports single-best MIRA training. MIRA training is used when -a MIRA option is set. 123456789101112131415161718192021222324% crf_learn -a MIRA template train.data modelCRF++: Yet Another CRF Tool KitCopyright(C) 2005 Taku Kudo, All rights reserved.reading training data: 100.. 200.. 300.. 400.. 500.. 600.. 700.. 800..Done! 1.92 sNumber of sentences: 823Number of features: 1075862Number of thread(s): 1Freq: 1eta: 0.00010C: 1.00000shrinking size: 20Algorithm: MIRAiter=0 terr=0.11381 serr=0.74605 act=823 uact=0 obj=24.13498 kkt=28.00000iter=1 terr=0.04710 serr=0.49818 act=823 uact=0 obj=35.42289 kkt=7.60929iter=2 terr=0.02352 serr=0.30741 act=823 uact=0 obj=41.86775 kkt=5.74464iter=3 terr=0.01836 serr=0.25881 act=823 uact=0 obj=47.29565 kkt=6.64895iter=4 terr=0.01106 serr=0.17011 act=823 uact=0 obj=50.68792 kkt=3.81902iter=5 terr=0.00610 serr=0.10085 act=823 uact=0 obj=52.58096 kkt=3.98915iter=0 terr=0.11381 serr=0.74605 act=823 uact=0 obj=24.13498 kkt=28.00000... iter, terr, serror: same as CRF training act: number of active examples in working set uact: number of examples whose dual parameters reach soft margin upper-bound C. 0 uact suggests that given training data was linear separable obj: current object value, ||w||^2 kkt: max kkt violation value. When it gets 0.0, MIRA training finishes There are some parameters to control the MIRA training condition -c float: Changes soft margin parameter, which is an analogue to the soft margin parameter C in Support Vector Machines. The definition is basically the same as -c option in CRF training. With larger C value, MIRA tends to overfit to the give training corpus. -f NUM: Same as CRF -H NUM: Changes shrinking size. When a training sentence is not used in updating parameter vector NUM times, we can consider that the instance doesn’t contribute training any more. MIRA tries to remove such instances. The process is called “shrinking”. When setting smaller NUM, shrinking occurs in early stage, which drastically reduces training time. However, too small NUM is not recommended. When training finishes, MIRA tries to go through all training examples again to know whether or not all KKT conditions are really satisfied. Too small NUM would increase the chances of recheck. Testing (decoding)Use crf_test command:% crf_test -m model_file test_files ...,where model_file is the file crf_learncreates. In the testing, you don’t need to specify the template file, because the model file has the same information for the template. test_file is the test data you want to assign sequential tags. This file has to be written in the same format as training file. Here is an output of crf_test:12345678% crf_test -m model test.dataRockwell NNP B BInternational NNP I ICorp. NNP I I's POS B BTulsa NNP I Iunit NN I I.. The last column is given (estimated) tag. If the 3rd column is true answer tag , you can evaluate the accuracy by simply seeing the difference between the 3rd and 4th columns. verbose levelThe -v option sets verbose level. default value is 0. By increasing the level, you can have an extra information from CRF++ level 1 You can also have marginal probabilities for each tag (a kind of confidece measure for each output tag) and a conditional probably for the output (confidence measure for the entire output). 12345678% crf_test -v1 -m model test.data| head# 0.478113Rockwell NNP B B/0.992465International NNP I I/0.979089Corp. NNP I I/0.954883's POS B B/0.986396Tulsa NNP I I/0.991966... The first line “# 0.478113” shows the conditional probably for the output. Also, each output tag has a probability represented like “B/0.992465”. level 2You can also have marginal probabilities for all other candidates.123456789% crf_test -v2 -m model test.data# 0.478113Rockwell NNP B B/0.992465 B/0.992465 I/0.00144946 O/0.00608594International NNP I I/0.979089 B/0.0105273 I/0.979089 O/0.0103833Corp. NNP I I/0.954883 B/0.00477976 I/0.954883 O/0.040337's POS B B/0.986396 B/0.986396 I/0.00655976 O/0.00704426Tulsa NNP I I/0.991966 B/0.00787494 I/0.991966 O/0.00015949unit NN I I/0.996169 B/0.00283111 I/0.996169 O/0.000999975.. N-best outputs With the -n option, you can obtain N-best results sorted by the conditional probability of CRF. With n-best output mode, CRF++ first gives one additional line like “# N prob”, where N means that rank of the output starting from 0 and prob denotes the conditional probability for the output. Note that CRF++ sometimes discards enumerating N-best results if it cannot find candidates any more. This is the case when you give CRF++ a short sentence. CRF++ uses a combination of forward Viterbi and backward A* search. This combination yields the exact list of n-best results. Here is the example of the N-best results. 1234567891011% crf_test -n 20 -m model test.data# 0 0.478113Rockwell NNP B BInternational NNP I ICorp. NNP I I's POS B B...# 1 0.194335Rockwell NNP B BInternational NNP I I Case studiesIn the example directories, you can find three case studies, baseNP chunking, Text Chunking, and Japanese named entity recognition, to use CRF++. In each directory, please try the following commands12% crf_learn template train model% crf_test -m model test]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>crf++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crf]]></title>
    <url>%2F2018%2F07%2F16%2Fcrf%2F</url>
    <content type="text"><![CDATA[CRF++ 0.58 在Windows中 运行与安装 命令行格式 4.1 训练程序 命令行： % crf_learn template_file train_file model_file 这个训练过程的时间、迭代次数等信息会输出到控制台上（感觉上是crf_learn程序的输出信息到标准输出流上了），如果想保存这些信息，我们可以将这些标准输出流到文件上，命令格式如下： % crf_learn template_file train_file model_file &gt;&gt; train_info_file 有四个主要的参数可以调整： -a CRF-L2 or CRF-L1 规范化算法选择。默认是CRF-L2。一般来说L2算法效果要比L1算法稍微好一点，虽然L1算法中非零特征的数值要比L2中大幅度的小。 -c float 这个参数设置CRF的hyper-parameter。c的数值越大，CRF拟合训练数据的程度越高。这个参数可以调整过度拟合和不拟合之间的平衡度。这个参数可以通过交叉验证等方法寻找较优的参数。 -f NUM 这个参数设置特征的cut-off threshold。CRF++使用训练数据中至少NUM次出现的特征。默认值为1。当使用CRF++到大规模数据时，只出现一次的特征可能会有几百万，这个选项就会在这样的情况下起到作用。 -p NUM 如果电脑有多个CPU，那么那么可以通过多线程提升训练速度。NUM是线程数量。 带两个参数的命令行例子： % crf_learn -f 3 -c 1.5 template_file train_file model_file 4.2 测试程序 命令行： % crf_test -m model_file test_files 有两个参数-v和-n都是显示一些信息的，-v可以显示预测标签的概率值，-n可以显示不同可能序列的概率值，对于准确率，召回率，运行效率，没有影响，这里不说明了。 与crf_learn类似，输出的结果放到了标准输出流上，而这个输出结果是最重要的预测结果信息（测试文件的内容+预测标注），同样可以使用重定向，将结果保存下来，命令行如下。 % crf_test -m model_file test_files &gt;&gt; result_file]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crf++ tutorial]]></title>
    <url>%2F2018%2F07%2F16%2Fcrf%2B%2B%20tutorial%2F</url>
    <content type="text"><![CDATA[Training and Test file formatsBoth the training file and the test file need to be in a particular format for CRF++ to work properly. Generally speaking, training and test file must consist of multiple tokens. In addition, a token consists of multiple (but fixed-numbers) columns. The definition of tokens depends on tasks, however, in most of typical cases, they simply correspond to words. Each token must be represented in one line, with the columns separated by white space (spaces or tabular characters). A sequence of token becomes a sentence. To identify the boundary between sentences, an empty line is put. You can give as many columns as you like, however the number of columns must be fixed through all tokens. Furthermore, there are some kinds of “semantics” among the columns. For example, 1st column is ‘word’, second column is ‘POS tag’ third column is ‘sub-category of POS’ and so on. The last column represents a true answer tag which is going to be trained by CRF. Here’s an example of such a file: (data for CoNLL shared task) 1234567891011121314151617181920He PRP B-NPreckons VBZ B-VPthe DT B-NPcurrent JJ I-NPaccount NN I-NPdeficit NN I-NPwill MD B-VPnarrow VB I-VPto TO B-PPonly RB B-NP# # I-NP1.8 CD I-NPbillion CD I-NPin IN B-PPSeptember NNP B-NP. . OHe PRP B-NPreckons VBZ B-VP.. There are 3 columns for each token. The word itself (e.g. reckons); part-of-speech associated with the word (e.g. VBZ); Chunk(answer) tag represented in IOB2 format; The following data is invalid, since the number of columns of second and third are 2. (They have no POS column.) The number of columns should be fixed. 123456He PRP B-NPreckons B-VPthe B-NPcurrent JJ I-NPaccount NN I-NP.. Preparing feature templatesAs CRF++ is designed as a general purpose tool, you have to specify the feature templates in advance. This file describes which features are used in training and testing. Template basic and macroEach line in the template file denotes one template. In each template, special macro %x[row,col] will be used to specify a token in the input data. row specfies the relative position from the current focusing token and col specifies the absolute position of the column. Here you can find some examples for the replacements123456Input: DataHe PRP B-NPreckons VBZ B-VPthe DT B-NP &lt;&lt; CURRENT TOKENcurrent JJ I-NPaccount NN I-NP template expanded feature %x[0,0] the %x[0,1] DT %x[-1,0] reckons %x[-2,1] PRP %x[0,0]/%x[0,1] the/DT ABC%x[0,1]123 ABCDT123 Template typeNote also that there are two types of templates. The types are specified with the first character of templates. Unigram template: first character, ‘U’ This is a template to describe unigram features. When you give a template “U01:%x[0,1]”, CRF++ automatically generates a set of feature functions (func1 … funcN) like: 1234567func1 = if (output = B-NP and feature="U01:DT") return 1 else return 0func2 = if (output = I-NP and feature="U01:DT") return 1 else return 0func3 = if (output = O and feature="U01:DT") return 1 else return 0....funcXX = if (output = B-NP and feature="U01:NN") return 1 else return 0funcXY = if (output = O and feature="U01:NN") return 1 else return 0... The number of feature functions generated by a template amounts to (L * N), where L is the number of output classes and N is the number of unique string expanded from the given template. Bigram template: first character, ‘B’ This is a template to describe bigram features. With this template, a combination of the current output token and previous output token (bigram) is automatically generated. Note that this type of template generates a total of (L L N) distinct features, where L is the number of output classes and N is the number of unique features generated by the templates. When the number of classes is large, this type of templates would produce a tons of distinct features that would cause inefficiency both in training/testing. What is the diffrence between unigram and bigram features? The words unigram/bigram are confusing, since a macro for unigram-features does allow you to write word-level bigram like %x[-1,0]%x[0,0]. Here, unigram and bigram features mean uni/bigrams of output tags. unigram: |output tag| x |all possible strings expanded with a macro| bigram: |output tag| x |output tag| x |all possible strings expanded with a macro| Identifiers for distinguishing relative positions You also need to put an identifier in templates when relative positions of tokens must be distinguished. In the following case, the macro “%x[-2,1]” and “%x[1,1]” will be replaced into “DT”. But they indicates different “DT”. 1234The DT B-NPpen NN I-NPis VB B-VP &lt;&lt; CURRENT TOKENa DT B-NP To distinguish both two, put an unique identifier (U01: or U02:) in the template: 12U01:%x[-2,1]U02:%x[1,1] In this case both two templates are regarded as different ones, as they are expanded into different features, “U01:DT” and “U02:DT”. You can use any identifier whatever you like, but it is useful to use numerical numbers to manage them, because they simply correspond to feature IDs. If you want to use “bag-of-words” feature, in other words, not to care the relative position of features, You don’t need to put such identifiers. ExampleHere is the template example for CoNLL 2000 shared task and Base-NP chunking task. Only one bigram template (‘B’) is used. This means that only combinations of previous output token and current token are used as bigram features. The lines starting from # or empty lines are discarded as comments 12345678910111213141516171819202122232425# UnigramU00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-1,0]/%x[0,0]U06:%x[0,0]/%x[1,0]U10:%x[-2,1]U11:%x[-1,1]U12:%x[0,1]U13:%x[1,1]U14:%x[2,1]U15:%x[-2,1]/%x[-1,1]U16:%x[-1,1]/%x[0,1]U17:%x[0,1]/%x[1,1]U18:%x[1,1]/%x[2,1]U20:%x[-2,1]/%x[-1,1]/%x[0,1]U21:%x[-1,1]/%x[0,1]/%x[1,1]U22:%x[0,1]/%x[1,1]/%x[2,1]# BigramB Training (encoding) Use crf_learn command: % crf_learn template_file train_file model_file, where template_file and train_file are the files you need to prepare in advance. crf_learn generates the trained model file in model_file. crf_learn outputs the following information. 123456789101112131415161718192021222324CRF++: Yet Another CRF Tool KitCopyright(C) 2005 Taku Kudo, All rights reserved.reading training data: 100.. 200.. 300.. 400.. 500.. 600.. 700.. 800..Done! 1.94 sNumber of sentences: 823Number of features: 1075862Number of thread(s): 1Freq: 1eta: 0.00010C: 1.00000shrinking size: 20Algorithm: CRFiter=0 terr=0.99103 serr=1.00000 obj=54318.36623 diff=1.00000iter=1 terr=0.35260 serr=0.98177 obj=44996.53537 diff=0.17161iter=2 terr=0.35260 serr=0.98177 obj=21032.70195 diff=0.53257iter=3 terr=0.23879 serr=0.94532 obj=13642.32067 diff=0.35138iter=4 terr=0.15324 serr=0.88700 obj=8985.70071 diff=0.34134iter=5 terr=0.11605 serr=0.80680 obj=7118.89846 diff=0.20775iter=6 terr=0.09305 serr=0.72175 obj=5531.31015 diff=0.22301iter=7 terr=0.08132 serr=0.68408 obj=4618.24644 diff=0.16507iter=8 terr=0.06228 serr=0.59174 obj=3742.93171 diff=0.18953 iter: number of iterations processed terr: error rate with respect to tags. (# of error tags/# of all tag) serr: error rate with respect to sentences. (# of error sentences/# of all sentences) obj: current object value. When this value converges to a fixed point, CRF++ stops the iteration. diff: relative difference from the previous object value. There are 4 major parameters to control the training condition -a CRF-L2 or CRF-L1:Changing the regularization algorithm. Default setting is L2. Generally speaking, L2 performs slightly better than L1, while the number of non-zero features in L1 is drastically smaller than that in L2. -c float:With this option, you can change the hyper-parameter for the CRFs. With larger C value, CRF tends to overfit to the give training corpus. This parameter trades the balance between overfitting and underfitting. The results will significantly be influenced by this parameter. You can find an optimal value by using held-out data or more general model selection method such as cross validation. -f NUM:This parameter sets the cut-off threshold for the features. CRF++ uses the features that occurs no less than NUM times in the given training data. The default value is 1. When you apply CRF++ to large data, the number of unique features would amount to several millions. This option is useful in such cases. -p NUM:If the PC has multiple CPUs, you can make the training faster by using multi-threading. NUM is the number of threads. Here is the example where these two parameters are used. % crf_learn -f 3 -c 1.5 template_file train_file model_file Since version 0.45, CRF++ supports single-best MIRA training. MIRA training is used when -a MIRA option is set. 123456789101112131415161718192021222324% crf_learn -a MIRA template train.data modelCRF++: Yet Another CRF Tool KitCopyright(C) 2005 Taku Kudo, All rights reserved.reading training data: 100.. 200.. 300.. 400.. 500.. 600.. 700.. 800..Done! 1.92 sNumber of sentences: 823Number of features: 1075862Number of thread(s): 1Freq: 1eta: 0.00010C: 1.00000shrinking size: 20Algorithm: MIRAiter=0 terr=0.11381 serr=0.74605 act=823 uact=0 obj=24.13498 kkt=28.00000iter=1 terr=0.04710 serr=0.49818 act=823 uact=0 obj=35.42289 kkt=7.60929iter=2 terr=0.02352 serr=0.30741 act=823 uact=0 obj=41.86775 kkt=5.74464iter=3 terr=0.01836 serr=0.25881 act=823 uact=0 obj=47.29565 kkt=6.64895iter=4 terr=0.01106 serr=0.17011 act=823 uact=0 obj=50.68792 kkt=3.81902iter=5 terr=0.00610 serr=0.10085 act=823 uact=0 obj=52.58096 kkt=3.98915iter=0 terr=0.11381 serr=0.74605 act=823 uact=0 obj=24.13498 kkt=28.00000... iter, terr, serror: same as CRF training act: number of active examples in working set uact: number of examples whose dual parameters reach soft margin upper-bound C. 0 uact suggests that given training data was linear separable obj: current object value, ||w||^2 kkt: max kkt violation value. When it gets 0.0, MIRA training finishes There are some parameters to control the MIRA training condition -c float: Changes soft margin parameter, which is an analogue to the soft margin parameter C in Support Vector Machines. The definition is basically the same as -c option in CRF training. With larger C value, MIRA tends to overfit to the give training corpus. -f NUM: Same as CRF -H NUM: Changes shrinking size. When a training sentence is not used in updating parameter vector NUM times, we can consider that the instance doesn’t contribute training any more. MIRA tries to remove such instances. The process is called “shrinking”. When setting smaller NUM, shrinking occurs in early stage, which drastically reduces training time. However, too small NUM is not recommended. When training finishes, MIRA tries to go through all training examples again to know whether or not all KKT conditions are really satisfied. Too small NUM would increase the chances of recheck. Testing (decoding)Use crf_test command:% crf_test -m model_file test_files ...,where model_file is the file crf_learncreates. In the testing, you don’t need to specify the template file, because the model file has the same information for the template. test_file is the test data you want to assign sequential tags. This file has to be written in the same format as training file. Here is an output of crf_test:12345678% crf_test -m model test.dataRockwell NNP B BInternational NNP I ICorp. NNP I I's POS B BTulsa NNP I Iunit NN I I.. The last column is given (estimated) tag. If the 3rd column is true answer tag , you can evaluate the accuracy by simply seeing the difference between the 3rd and 4th columns. verbose levelThe -v option sets verbose level. default value is 0. By increasing the level, you can have an extra information from CRF++ level 1 You can also have marginal probabilities for each tag (a kind of confidece measure for each output tag) and a conditional probably for the output (confidence measure for the entire output). 12345678% crf_test -v1 -m model test.data| head# 0.478113Rockwell NNP B B/0.992465International NNP I I/0.979089Corp. NNP I I/0.954883's POS B B/0.986396Tulsa NNP I I/0.991966... The first line “# 0.478113” shows the conditional probably for the output. Also, each output tag has a probability represented like “B/0.992465”. level 2You can also have marginal probabilities for all other candidates.123456789% crf_test -v2 -m model test.data# 0.478113Rockwell NNP B B/0.992465 B/0.992465 I/0.00144946 O/0.00608594International NNP I I/0.979089 B/0.0105273 I/0.979089 O/0.0103833Corp. NNP I I/0.954883 B/0.00477976 I/0.954883 O/0.040337's POS B B/0.986396 B/0.986396 I/0.00655976 O/0.00704426Tulsa NNP I I/0.991966 B/0.00787494 I/0.991966 O/0.00015949unit NN I I/0.996169 B/0.00283111 I/0.996169 O/0.000999975.. N-best outputs With the -n option, you can obtain N-best results sorted by the conditional probability of CRF. With n-best output mode, CRF++ first gives one additional line like “# N prob”, where N means that rank of the output starting from 0 and prob denotes the conditional probability for the output. Note that CRF++ sometimes discards enumerating N-best results if it cannot find candidates any more. This is the case when you give CRF++ a short sentence. CRF++ uses a combination of forward Viterbi and backward A* search. This combination yields the exact list of n-best results. Here is the example of the N-best results. 1234567891011% crf_test -n 20 -m model test.data# 0 0.478113Rockwell NNP B BInternational NNP I ICorp. NNP I I's POS B B...# 1 0.194335Rockwell NNP B BInternational NNP I I Case studiesIn the example directories, you can find three case studies, baseNP chunking, Text Chunking, and Japanese named entity recognition, to use CRF++. In each directory, please try the following commands12% crf_learn template train model% crf_test -m model test]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>crf++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRF++模型格式说明]]></title>
    <url>%2F2018%2F07%2F16%2FCRF%2B%2B%E6%A8%A1%E5%9E%8B%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[CRF++模型格式说明通过追加-t, –textmodel参数可以输出文本格式的CRF模型文件，通过该模型文本，可以加深对条件随机场的理解或为其他应用所利用。 训练语料以BMES标注语料为例：12345678910111213141516171819202122那 S音 B韵 E如 S轻 B柔 E的 S夜 B风 E， S惊 S溅 S起 S不 B可 M言 M传 E的 S天 B籁 E。 S 注意字与标签之间的分隔符为制表符\t，否则会导致feature_index.cpp(86) [max_size == size] inconsistent column size错误。 特征模板1234567891011121314# UnigramU00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-2,0]/%x[-1,0]/%x[0,0]U06:%x[-1,0]/%x[0,0]/%x[1,0]U07:%x[0,0]/%x[1,0]/%x[2,0]U08:%x[-1,0]/%x[0,0]U09:%x[0,0]/%x[1,0]# BigramB T**:%x[#,#]中的T表示模板类型，两个”#”分别表示相对的行偏移与列偏移。 一共有两种模板： 第一种是Unigram template:第一个字符是U，这是用于描述unigram feature的模板。每一行%x[#,#]生成一个CRFs中的点(state)函数: f(s, o), 其中s为t时刻的的标签(output)，o为t时刻的上下文.如CRF++说明文件中的示例函数: func1 = if (output = B and feature=”U02:那”) return 1 else return 0 它是由U02:%x[0,0]在输入文件的第一行生成的点函数.将输入文件的第一行”代入”到函数中,函数返回1,同时,如果输入文件的某一行在第1列也是“那”,并且它的output（第2列）同样也为B,那么这个函数在这一行也返回1。 第二种是Bigram template:第一个字符是B，每一行%x[#,#]生成一个CRFs中的边(Edge)函数:f(s’, s, o), 其中s’为t – 1时刻的标签.也就是说,Bigram类型与Unigram大致机同,只是还要考虑到t – 1时刻的标签.如果只写一个B的话,默认生成f(s’, s)，这意味着前一个output token和current token将组合成bigram features。 命令行使用下列命令可以得到一个model文件和一个model.txt文件，后者是本文的主要研究对象。 ..\..\crf_learn -f 3 -c 4.0 template pku_training.bmes.txt model -t 参数解释如下：123456789101112131415161718192021222324252627可选参数-f, –freq=INT使用属性的出现次数不少于INT(默认为1)-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认10k)-c, –cost=FLOAT 设置FLOAT为代价参数，过大会过度拟合 (默认1.0)-e, –eta=FLOAT设置终止标准FLOAT(默认0.0001)-C, –convert将文本模式转为二进制模式-t, –textmodel为调试建立文本模型文件-a, –algorithm=(CRF|MIRA)选择训练算法，默认为CRF-L2-p, –thread=INT线程数(默认1)，利用多个CPU减少训练时间-H, –shrinking-size=INT设置INT为最适宜的跌代变量次数 (默认20)-v, –version显示版本号并退出-h, –help显示帮助并退出 输出训练过程中会输出一些信息，其意义如下：123456789iter：迭代次数。当迭代次数达到maxiter时，迭代终止terr：标记错误率serr：句子错误率obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成 可见，如果希望训练快速结束，可以在命令行中适当减小maxiter值，增大eta值。 CRF模型格式骨架打开model.txt，其基本内容骨架如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172version: 100cost-factor: 1maxid: 2159868xsize: 1BEMSU00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-2,0]/%x[-1,0]/%x[0,0]U06:%x[-1,0]/%x[0,0]/%x[1,0]U07:%x[0,0]/%x[1,0]/%x[2,0]U08:%x[-1,0]/%x[0,0]U09:%x[0,0]/%x[1,0]B0 B16 U00:-20 U00:024 U00:128 U00:232 U00:336 U00:440 U00:544 U00:648 U00:752 U00:856 U00:960 U00:_B-164 U00:_B-2……17404 U01:厨17408 U01:去17412 U01:县17416 U01:参17420 U01:又17424 U01:叉17428 U01:及17432 U01:友17436 U01:双17440 U01:反17444 U01:发17448 U01:叔17452 U01:取17456 U01:受……77800 U05:_B-1/一/个107540 U05:一/方/面107544 U05:一/无/所107548 U05:一/日/三107552 U05:一/日/为107556 U05:一/日/之……566536 U06:万/吨/_B+1……2159864 U09:ｖ/ｅ-8.53540175259997199.04914538141489017.0388286231971700-7.25455581640930095.2799470769112835-8.5333633546653758-5.35491907356069335.2575182675282477-5.4259109736696054 接下来我会把上述骨架分解说明—— 文件头1234version: 100cost-factor: 1maxid: 2159868xsize: 1 说明了模型的版本，通过-c参数指定的cost-factor，特征函数的最大id，xsize是特征维数，也就是训练语料列数-1。 值得注意的是maxid比我们从文本中观察到的最大id大了4，这是为什么呢？且听下文分解。 标签1234BEMS 也就是最终的输出。 模板1234567891011U00:%x[-2,0]U01:%x[-1,0]U02:%x[0,0]U03:%x[1,0]U04:%x[2,0]U05:%x[-2,0]/%x[-1,0]/%x[0,0]U06:%x[-1,0]/%x[0,0]/%x[1,0]U07:%x[0,0]/%x[1,0]/%x[2,0]U08:%x[-1,0]/%x[0,0]U09:%x[0,0]/%x[1,0]B 训练时用到的模板。 特征函数123456789101112131415161718192021222324252627282930313233343536373839400 B16 U00:-20 U00:024 U00:128 U00:232 U00:336 U00:440 U00:544 U00:648 U00:752 U00:856 U00:960 U00:_B-164 U00:_B-2……17404 U01:厨17408 U01:去17412 U01:县17416 U01:参17420 U01:又17424 U01:叉17428 U01:及17432 U01:友17436 U01:双17440 U01:反17444 U01:发17448 U01:叔17452 U01:取17456 U01:受……77800 U05:_B-1/一/个107540 U05:一/方/面107544 U05:一/无/所107548 U05:一/日/三107552 U05:一/日/为107556 U05:一/日/之……566536 U06:万/吨/_B+1……2159864 U09:ｖ/ｅ 按照[id] [参数o]的格式排列，你可能会奇怪，f(s, o)应该接受两个参数才对。其实s隐藏起来了，注意到id不是连续的，而是隔了四个，这表示这四个标签（s=b|m|e|s）和公共的参数o组合成了四个特征函数。特别的，0-15为BEMS转移到BEMS的转移函数，也就是f(s’, s, o=null)。 值得注意的是，_B-1表示句子第一个单词前面的一个单词，_B+1表示末尾后面的一个单词，你可以在最大熵的模型中找到类似的逻辑处理，依次类推。 特征函数权值后面的小数依id顺序对应每个特征函数的权值。123456789.04914538141489017.0388286231971700-7.25455581640930095.2799470769112835-8.5333633546653758-5.35491907356069335.2575182675282477-5.4259109736696054 关于解码严格来讲，解码并不属于本文的范围，但是不说说解码的话，对特征函数权值的理解就仅仅限于“浮点数”这一表面。所以简要地说说解码，比如说我们有一个句子“商品和服务”，对于每个字都按照上述模板生成一系列U特征函数的参数代入，得到一些类似010101的函数返回值，乘上这些函数的权值求和，就得到了各个标签的分数，由大到小代表输出这些标签的可能性。 至于B特征函数（这里特指简单的f(s’, s)），在Viterbi后向解码的时候，前一个标签确定了后就可以代入当前的B特征函数，计算出每个输出标签的分数，再次求和排序即可。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>crf++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下使用crf++]]></title>
    <url>%2F2018%2F07%2F16%2Fwindows%E4%B8%8B%E4%BD%BF%E7%94%A8crf%2B%2B%2F</url>
    <content type="text"><![CDATA[crf++工具包下载：crf++ 0.58 下载 (后缀为.tar.gz的适合Linux和MacOS用户使用，后缀为.zip的适合Windows用户使用。)选择编译好的zip包，解压之后目录结构如下： 123456789101112131415161718192021D:\CRF++-0.58&gt;dir 驱动器 D 中的卷没有标签。 卷的序列号是 000A-6B8F D:\CRF++-0.58 的目录2018/03/30 11:30 &lt;DIR&gt; .2018/03/30 11:30 &lt;DIR&gt; ..2013/02/13 00:40 28 AUTHORS2013/02/13 00:40 1,494 BSD2013/02/13 00:40 164 COPYING2013/02/13 00:40 50,688 crf_learn.exe2013/02/13 00:40 50,688 crf_test.exe2018/03/30 11:30 &lt;DIR&gt; doc2018/03/30 11:30 &lt;DIR&gt; example2013/02/13 00:40 26,428 LGPL2013/02/13 00:40 337,408 libcrfpp.dll2013/02/13 00:40 20 README2018/03/30 11:30 &lt;DIR&gt; sdk 8 个文件 466,918 字节 5 个目录 384,789,987,328 可用字节 doc文件夹： 就是官方主页的内容。 example文件夹： 有四个任务的训练数据、测试数据和模板文件。 sdk文件夹： CRF++的头文件和静态链接库。 crf_learn.exe：CRF++的训练程序。 crf_test.exe：CRF++的预测程序 libcrfpp.dll： 训练程序和预测程序需要使用的静态链接库。 实际上，需要使用的就是crf_learn.exe，crf_test.exe和libcrfpp.dll，这三个文件。 主要参数：12345678910111213141516D:\CRF++-0.58&gt;crf_learn.exeCRF++: Yet Another CRF Tool KitCopyright (C) 2005-2013 Taku Kudo, All rights reserved.Usage: crf_learn.exe [options] files -f, --freq=INT use features that occuer no less than INT(default 1) -m, --maxiter=INT set INT for max iterations in LBFGS routine(default 10k) -c, --cost=FLOAT set FLOAT for cost parameter(default 1.0) -e, --eta=FLOAT set FLOAT for termination criterion(default 0.0001) -C, --convert convert text model to binary model -t, --textmodel build also text model file for debugging -a, --algorithm=(CRF|MIRA) select training algorithm -p, --thread=INT number of threads (default auto-detect) -H, --shrinking-size=INT set INT for number of iterations variable needs to be optimal before considered for shrinking. (default 20) -v, --version show the version and exit -h, --help show this help and exit 运行测试:先拿example中的某个例子，做一下测试。例如：example中chunking文件夹，其中原有4个文件： exec.sh template：特征模版 test.data：测试数据 train.data：训练数据在cmd中进入CRF++-0.58所在的文件夹，使用crf_learn template train.data model训练数据，crf_test -m model test.data &gt;output.txt测试数据,具体效果如下： 效果如下：目录结构如下：12345678910111213D:\CRF++-0.58\example\chunking 的目录2018/03/30 11:37 &lt;DIR&gt; .2018/03/30 11:37 &lt;DIR&gt; ..2013/02/13 00:40 280 exec.sh2018/03/30 11:36 1,102,576 model2018/03/30 11:37 367,759 output.txt2013/02/13 00:40 359 template2013/02/13 00:40 258,104 test.data2013/02/13 00:40 25,682 train.data 6 个文件 1,754,760 字节 2 个目录 384,789,987,328 可用字节 output.txt如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546Rockwell NNP B-NP B-NPInternational NNP I-NP I-NPCorp. NNP I-NP I-NP's POS B-NP B-NPTulsa NNP I-NP I-NPunit NN I-NP I-NPsaid VBD B-VP B-VPit PRP B-NP B-NPsigned VBD B-VP B-VPa DT B-NP B-NPtentative JJ I-NP I-NPagreement NN I-NP I-NPextending VBG B-VP B-VPits PRP$ B-NP B-NPcontract NN I-NP I-NPwith IN B-PP B-PPBoeing NNP B-NP B-NPCo. NNP I-NP I-NPto TO B-VP B-VPprovide VB I-VP I-VPstructural JJ B-NP B-NPparts NNS I-NP I-NPfor IN B-PP B-PPBoeing NNP B-NP B-NP's POS B-NP I-NP747 CD I-NP I-NPjetliners NNS I-NP I-NP. . O ORockwell NNP B-NP B-NPsaid VBD B-VP B-VPthe DT B-NP B-NPagreement NN I-NP I-NPcalls VBZ B-VP B-VPfor IN B-SBAR B-PPit PRP B-NP B-NPto TO B-VP B-VPsupply VB I-VP I-VP200 CD B-NP B-NPadditional JJ I-NP I-NPso-called JJ I-NP I-NPshipsets NNS I-NP I-NPfor IN B-PP B-PPthe DT B-NP B-NPplanes NNS I-NP I-NP. . O O]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>crf++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.7. Tables for Probability Distributions]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.7.%20Tables%20for%20Probability%20Distributions%2F</url>
    <content type="text"><![CDATA[Several commonly used tables for probability distributions can be referenced below.The values from these tables can also be obtained from most general purpose statistical software programs. Most introductory statistics textbooks (e.g., Snedecor and Cochran) contain more extensive tables than are included here. These tables are included for convenience. Cumulative distribution function for the standard normal distribution Upper critical values of Student’s t-distribution with degrees of freedom Upper critical values of the F-distributionwith ν1 and ν2 degrees of freedom Upper critical values of the chi-square distribution with ν degrees of freedom Critical values of t* distribution for testing the output of a linear calibration line at 3 points Upper critical values of the normal PPCC distribution]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Tables for Probability Distributions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.4. Location and Scale Parameters]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.4.%20Location%20and%20Scale%20Parameters%2F</url>
    <content type="text"><![CDATA[Normal PDFA probability distribution is characterized by location and scale parameters. Location and scale parameters are typically used in modeling applications. For example, the following graph is the probability density function for the standard normal distribution, which has the location parameter equal to zero and scale parameter equal to one. Location ParameterThe next plot shows the probability density function for a normal distribution with a location parameter of 10 and a scale parameter of 1. The effect of the location parameter is to translate the graph, relative to the standard normal distribution, 10 units to the right on the horizontal axis. A location parameter of -10 would have shifted the graph 10 units to the left on the horizontal axis.That is, a location parameter simply shifts the graph left or right on the horizontal axis. Scale ParameterThe next plot has a scale parameter of 3 (and a location parameter of zero). The effect of the scale parameter is to stretch out the graph. The maximum y value is approximately 0.13 as opposed 0.4 in the previous graphs. The y value, i.e., the vertical axis value, approaches zero at about (+/-) 9 as opposed to (+/-) 3 with the first graph. In contrast, the next graph has a scale parameter of 1/3 (=0.333). The effect of this scale parameter is to squeeze the pdf. That is, the maximum y value is approximately 1.2 as opposed to 0.4 and the y value is near zero at (+/-) 1 as opposed to (+/-) 3. The effect of a scale parameter greater than one is to stretch the pdf. The greater the magnitude, the greater the stretching. The effect of a scale parameter less than one is to compress the pdf. The compressing approaches a spike as the scale parameter goes to zero. A scale parameter of 1 leaves the pdf unchanged (if the scale parameter is 1 to begin with) and non-positive scale parameters are not allowed. Location and Scale TogetherThe following graph shows the effect of both a location and a scale parameter. The plot has been shifted right 10 units and stretched by a factor of 3. Standard FormThe standard form of any distribution is the form that has location parameter zero and scale parameter one. It is common in statistical software packages to only compute the standard form of the distribution. There are formulas for converting from the standard form to the form with other location and scale parameters. These formulas are independent of the particular probability distribution. Formulas for Location and Scale Based on the Standard FormThe following are the formulas for computing various probability functions based on the standard form of the distribution. The parametera refers to the location parameter and the parameter b refers to the scale parameter. Shape parameters are not included. Relationship to Mean and Standard DeviationFor the normal distribution, the location and scale parameters correspond to the mean and standard deviation, respectively. However, this is not necessarily true for other distributions. In fact, it is not true for most distributions. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda364.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Location and Scale Parameters</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.5. Estimating the Parameters of a Distribution]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.5.%20Estimating%20the%20Parameters%20of%20a%20Distribution%2F</url>
    <content type="text"><![CDATA[Model a univariate data set with a probability distributionOne common application of probability distributions is modeling univariate data with a specific probability distribution. This involves the following two steps: 1. Determination of the &quot;best-fitting&quot; distribution. 2. Estimation of the parameters (shape, location, and scale parameters) for that distribution. Various MethodsThere are various methods, both numerical and graphical, for estimating the parameters of a probability distribution. Method of moments Maximum likelihood Least squares PPCC and probability plots]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Estimating the Parameters of a Distribution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.3 Families of Distributions]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.3%20Families%20of%20Distributions%2F</url>
    <content type="text"><![CDATA[Shape ParametersMany probability distributions are not a single distribution, but are in fact a family of distributions. This is due to the distribution having one or more shape parameters. Shape parameters allow a distribution to take on a variety of shapes, depending on the value of the shape parameter. These distributions are particularly useful in modeling applications since they are flexible enough to model a variety of data sets. Example: Weibull DistributionThe Weibull distribution is an example of a distribution that has a shape parameter. The following graph plots the Weibull pdf with the following values for the shape parameter: 0.5, 1.0, 2.0, and 5.0. The shapes above include an exponential distribution, a right-skewed distribution, and a relatively symmetric distribution. The Weibull distribution has a relatively simple distributional form. However, the shape parameter allows the Weibull to assume a wide variety of shapes. This combination of simplicity and flexibility in the shape of the Weibull distribution has made it an effective distributional model in reliability applications. This ability to model a wide variety of distributional shapes using a relatively simple distributional form is possible with many other distributional families as well. PPCC PlotsThe PPCC plot is an effective graphical tool for selecting the member of a distributional family with a single shape parameter that best fits a given set of data.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Families of Distributions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.2 Related Distributions]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.2%20%20Related%20Distributions%2F</url>
    <content type="text"><![CDATA[Probability Density FunctionFor a continuous function, the probability density function (pdf) is the probability that the variate has the value x. Since for continuous distributions the probability at a single point is zero, this is often expressed in terms of an integral between two points. For a discrete distribution, the pdf is the probability that the variate takes the value x. The following is the plot of the normal probability density function. Cumulative Distribution FunctionThe cumulative distribution function (cdf) is the probability that the variable takes a value less than or equal to x. That is For a continuous distribution, this can be expressed mathematically as For a discrete distribution, the cdf can be expressed as The following is the plot of the normal cumulative distribution function. The horizontal axis is the allowable domain for the given probability function. Since the vertical axis is a probability, it must fall between zero and one. It increases from zero to one as we go from left to right on the horizontal axis. Percent Point FunctionThe percent point function (ppf) is the inverse of the cumulative distribution function. For this reason, the percent point function is also commonly referred to as the inverse distribution function. That is, for a distribution function we calculate the probability that the variable is less than or equal to x for a given x. For the percent point function, we start with the probability and compute the corresponding x for the cumulative distribution. Mathematically, this can be expressed as The following is the plot of the normal percent point function. Since the horizontal axis is a probability, it goes from zero to one. The vertical axis goes from the smallest to the largest value of the cumulative distribution function. Hazard FunctionThe hazard function is the ratio of the probability density function to the survival function, S(x). The following is the plot of the normal distribution hazard function. Hazard plots are most commonly used in reliability applications. Note that Johnson, Kotz, and Balakrishnan refer to this as the conditional failure density function rather than the hazard function. Cumulative Hazard FunctionThe cumulative hazard function is the integral of the hazard function. This can alternatively be expressed as The following is the plot of the normal cumulative hazard function. Cumulative hazard plots are most commonly used in reliability applications. Note that Johnson, Kotz, and Balakrishnan refer to this as the hazard function rather than the cumulative hazard function. Survival FunctionSurvival functions are most often used in reliability and related fields. The survival function is the probability that the variate takes a value greater than x. The following is the plot of the normal distribution survival function. For a survival function, the y value on the graph starts at 1 and monotonically decreases to zero. The survival function should be compared to the cumulative distribution function. Inverse Survival FunctionJust as the percent point function is the inverse of the cumulative distribution function, the survival function also has an inverse function. The inverse survival function can be defined in terms of the percent point function. The following is the plot of the normal distribution inverse survival function. As with the percent point function, the horizontal axis is a probability. Therefore the horizontal axis goes from 0 to 1 regardless of the particular distribution. The appearance is similar to the percent point function. However, instead of going from the smallest to the largest value on the vertical axis, it goes from the largest to the smallest value. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Related Distributions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6.1 What is a Probability Distribution]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.1%20What%20is%20a%20Probability%20Distribution%2F</url>
    <content type="text"><![CDATA[Discrete DistributionsThe mathematical definition of a discrete probability function, p(x), is a function that satisfies the following properties. 1. The probability that x can take a specific value is p(x). That is P[X=x]=p(x)=px 2. p(x) is non-negative for all real x. 3. The sum of p(x) over all possible values of x is 1, that is ∑jpj=1 where j represents all possible values that x can have and pj is the probability at xj. One consequence of properties 2 and 3 is that 0 &lt;= p(x) &lt;= 1. What does this actually mean? A discrete probability function is a function that can take a discrete number of values (not necessarily finite). This is most often the non-negative integers or some subset of the non-negative integers. There is no mathematical restriction that discrete probability functions only be defined at integers, but in practice this is usually what makes sense. For example, if you toss a coin 6 times, you can get 2 heads or 3 heads but not 2 1/2 heads. Each of the discrete values has a certain probability of occurrence that is between zero and one. That is, a discrete function that allows negative values or values greater than one is not a probability function. The condition that the probabilities sum to one means that at least one of the values has to occur. Continuous DistributionsThe mathematical definition of a continuous probability function, f(x), is a function that satisfies the following properties. 1. The probability that x is between two points a and b is p[a≤x≤b]=∫baf(x)dx 2. It is non-negative for all real x. 3. The integral of the probability function is one, that is ∫∞−∞f(x)dx=1 What does this actually mean? Since continuous probability functions are defined for an infinite number of points over a continuous interval, the probability at a single point is always zero. Probabilities are measured over intervals, not single points. That is, the area under the curve between two distinct points defines the probability for that interval. This means that the height of the probability function can in fact be greater than one. The property that the integral must equal one is equivalent to the property for discrete distributions that the sum of all the probabilities must equal one. Probability Mass Functions Versus Probability Density FunctionsDiscrete probability functions are referred to as probability mass functions and continuous probability functions are referred to as probability density functions. The term probability functions covers both discrete and continuous distributions. There are a few occasions in the e-Handbook when we use the term probability density function in a generic sense where it may apply to either probability density or probability mass functions. It should be clear from the context whether we are referring only to continuous distributions or to either continuous or discrete distributions.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>What is a Probability Distribution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.18. Yates Algorithm]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.18.%20Yates%20Algorithm%2F</url>
    <content type="text"><![CDATA[Purpose: Estimate Factor Effects in a 2-Level Factorial DesignFull factorial and fractional factorial designs are common in designed experiments for engineering and scientific applications. In these designs, each factor is assigned two levels. These are typically called the low and high levels. For computational purposes, the factors are scaled so that the low level is assigned a value of -1 and the high level is assigned a value of +1. These are also commonly referred to as “-“ and “+”. A full factorial design contains all possible combinations of low/high levels for all the factors. A fractional factorial design contains a carefully chosen subset of these combinations. The criterion for choosing the subsets is discussed in detail in the process improvement chapter. The Yates algorithm exploits the special structure of these designs to generate least squares estimates for factor effects for all factors and all relevant interactions. The mathematical details of the Yates algorithm are given in chapter 10 of Box, Hunter, and Hunter (1978). Natrella (1963) also provides a procedure for testing the significance of effect estimates.The effect estimates are typically complemented by a number of graphical techniques such as theDOE mean plot and the DOE contour plot (“DOE” represents “design of experiments”). These are demonstrated in the eddy current case study. Yates OrderBefore performing the Yates algorithm, the data should be arranged in “Yates order”. That is, given k factors, the kth column consists of 2k-1minus signs (i.e., the low level of the factor) followed by 2k-1 plus signs (i.e., the high level of the factor). For example, for a full factorial design with three factors, the design matrix is - - - + - - - + - + + - - - + + - + - + + + + + Determining the Yates order for fractional factorial designs requires knowledge of theconfounding structure of the fractional factorial design. Yates AlgorithmThe Yates algorithm is demonstrated for the eddy current data set. The data set contains eight measurements from a two-level, full factorial design with three factors. The purpose of the experiment is to identify factors that have the most effect on eddy current measurements. In the “Effect” column, we list the main effects and interactions from our factorial experiment in standard order. In the “Response” column, we list the measurement results from our experiment in Yates order. Effect Response Col 1 Col 2 Col 3 Estimate ------ -------- ----- ----- ----- -------- Mean 1.70 6.27 10.21 21.27 2.65875 X1 4.57 3.94 11.06 12.41 1.55125 X2 0.55 6.10 5.71 -3.47 -0.43375 X1*X2 3.39 4.96 6.70 0.51 0.06375 X3 1.51 2.87 -2.33 0.85 0.10625 X1*X3 4.59 2.84 -1.14 0.99 0.12375 X2*X3 0.67 3.08 -0.03 1.19 0.14875 X1*X2*X3 4.29 3.62 0.54 0.57 0.07125 Sum of responses: 21.27 Sum-of-squared responses: 77.7707 Sum-of-squared Col 3: 622.1656 The first four values in Col 1 are obtained by adding adjacent pairs of responses, for example 4.57 + 1.70 = 6.27, and 3.39 + 0.55 = 3.94. The second four values in Col 1 are obtained by subtracting the same adjacent pairs of responses, for example, 4.57 - 1.70 = 2.87, and 3.39 - 0.55 = 2.84. The values in Col 2 are calculated in the same way, except that we are adding and subtracting adjacent values from Col 1. Col 3 is computed using adjacent values from Col 2. Finally, we obtain the “Estimate” column by dividing the values in Col 3 by the total number of responses, 8.We can check our calculations by making sure that the first value in Col 3 (21.27) is the sum of all the responses. In addition, the sum-of-squared responses (77.7707) should equal the sum-of-squared Col 3 values divided by 8 (622.1656/8 = 77.7707). Practical ConsiderationsThe Yates algorithm provides a convenient method for computing effect estimates; however, the same information is easily obtained from statistical software using either an analysis of variance or regression procedure. The methods for analyzing data from a designed experiment are discussed more fully in the chapter on Process Improvement. Graphical PresentationThe following plots may be useful to complement the quantitative information from the Yates algorithm. 1. Ordered data plot 2. Ordered absolute effects plot 3. Cumulative residual standard deviation plot QuestionsThe Yates algorithm can be used to answer the following question. 1. What is the estimated effect of a factor on the response? Related Techniques Multi-factor analysis of variance DOE mean plot Block plot DOE contour plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35i.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Yates Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.17.2. Tietjen-Moore Test for Outliers]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.17.2.%20Tietjen-Moore%20Test%20for%20Outliers%2F</url>
    <content type="text"><![CDATA[Purpose: Detection of OutliersThe Tietjen-Moore test (Tietjen-Moore 1972) is used to detect multiple outliersin a univariate data set that follows an approximately normal distribution. The Tietjen-Moore test is a generalization of the Grubbs’ test to the case of multiple outliers. If testing for a single outlier, the Tietjen-Moore test is equivalent to the Grubbs’ test.It is important to note that the Tietjen-Moore test requires that the suspected number of outliers be specified exactly. If this is not known, it is recommended that the generalized extreme studentized deviate test be used instead (this test only requires an upper bound on the number of suspected outliers). Definition Sample OutputThe Tietjen-Moore paper gives the following 15 observations of vertical semi-diameters of the planet Venus (this example originally appeared in Grubbs’ 1950paper): -1.40 -0.44 -0.30 -0.24 -0.22 -0.13 -0.05 0.06 0.10 0.18 0.20 0.39 0.48 0.63 1.01 As a first step, a normal probability plot was generated. This plot indicates that the normality assumption is reasonable. The minimum value appears to be an outlier. To a lesser extent, the maximum value may also be an outlier. The Tietjen-Moore test of the two most extreme points (-1.40 and 1.01) is shown below. H0: there are no outliers in the data Ha: the two most extreme points are outliers Test statistic: Ek = 0.292 Significance level: α = 0.05 Critical value for lower tail: 0.317 Critical region: Reject H0 if Ek &lt; 0.317 The Tietjen-Moore test is a lower, one-tailed test, so we reject the null hypothesis that there are no outliers when the value of the test statistic is less than the critical value. For our example, the null hypothesis is rejected at the 0.05 level of significance and we conclude that the two most extreme points are outliers. QuestionsThe Tietjen-Moore test can be used to answer the following question: 1. Does the data set contain k outliers? ImportanceMany statistical techniques are sensitive to the presence of outliers. For example, simple calculations of the mean and standard deviation may be distorted by a single grossly inaccurate data point.Checking for outliers should be a routine part of any data analysis. Potential outliers should be examined to see if they are possibly erroneous. If the data point is in error, it should be corrected if possible and deleted if it is not possible. If there is no reason to believe that the outlying point is in error, it should not be deleted without careful consideration. However, the use of more robust techniques may be warranted. Robust techniques will often downweight the effect of outlying points without deleting them. Related TechniquesSeveral graphical techniques can, and should, be used to help detect outliers. A simple normal probability plot, run sequence plot, a box plot, or a histogram should show any obviously outlying points. In addition to showing potential outliers, several of these graphics also help assess whether the data follow an approximately normal distribution. Normal Probability Plot Run Sequence Plot Histogram Box Plot Lag Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h2.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Tietjen-Moore Test for Outliers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.17.3. Generalized ESD Test for Outliers]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.17.3.%20Generalized%20ESD%20Test%20for%20Outliers%2F</url>
    <content type="text"><![CDATA[Purpose: Detection of OutliersThe generalized (extreme Studentized deviate) ESD test (Rosner 1983) is used to detect one or more outliers in a univariate data set that follows an approximately normal distribution.The primary limitation of the Grubbs test and the Tietjen-Moore test is that the suspected number of outliers, k, must be specified exactly. If kis not specified correctly, this can distort the conclusions of these tests. On the other hand, the generalized ESD test (Rosner 1983) only requires that an upper bound for the suspected number of outliers be specified. Definition Note that although the generalized ESD is essentially Grubbs test applied sequentially, there are a few important distinctions: • The generalized ESD test makes approriate adjustments for the critical values based on the number of outliers being tested for that the sequential application of Grubbs test does not. • If there is significant masking, applying Grubbs test sequentially may stop too soon. The example below identifies three outliers at the 5 % level when using the generalized ESD test. However, trying to use Grubbs test sequentially would stop at the first iteration and declare no outliers. Generalized ESD Test ExampleThe Rosner paper gives an example with the following data. -0.25 0.68 0.94 1.15 1.20 1.26 1.26 1.34 1.38 1.43 1.49 1.49 1.55 1.56 1.58 1.65 1.69 1.70 1.76 1.77 1.81 1.91 1.94 1.96 1.99 2.06 2.09 2.10 2.14 2.15 2.23 2.24 2.26 2.35 2.37 2.40 2.47 2.54 2.62 2.64 2.90 2.92 2.92 2.93 3.21 3.26 3.30 3.59 3.68 4.30 4.64 5.34 5.42 6.01 As a first step, a normal probability plot was generated This plot indicates that the normality assumption is questionable.Following the Rosner paper, we test for up to 10 outliers: H0: there are no outliers in the data Ha: there are up to 10 outliers in the data Significance level: α = 0.05 Critical region: Reject H0 if Ri &gt; critical value Summary Table for Two-Tailed Test --------------------------------------- Exact Test Critical Number of Statistic Value, λi Outliers, i Value, Ri 5 % --------------------------------------- 1 3.118 3.158 2 2.942 3.151 3 3.179 3.143 * 4 2.810 3.136 5 2.815 3.128 6 2.848 3.120 7 2.279 3.111 8 2.310 3.103 9 2.101 3.094 10 2.067 3.085 For the generalized ESD test above, there are essentially 10 separate tests being performed. For this example, the largest number of outliers for which the test statistic is greater than the critical value (at the 5 % level) is three. We therefore conclude that there are three outliers in this data set. QuestionsThe generalized ESD test can be used to answer the following question: 1. How many outliers does the data set contain? ImportanceMany statistical techniques are sensitive to the presence of outliers. For example, simple calculations of the mean and standard deviation may be distorted by a single grossly inaccurate data point.Checking for outliers should be a routine part of any data analysis. Potential outliers should be examined to see if they are possibly erroneous. If the data point is in error, it should be corrected if possible and deleted if it is not possible. If there is no reason to believe that the outlying point is in error, it should not be deleted without careful consideration. However, the use of more robust techniques may be warranted. Robust techniques will often downweight the effect of outlying points without deleting them. Related TechniquesSeveral graphical techniques can, and should, be used to help detect outliers. A simple normal probability plot, run sequence plot, a box plot, or a histogram should show any obviously outlying points. In addition to showing potential outliers, several of these graphics also help assess whether the data follow an approximately normal distribution. Run Sequence Plot Histogram Box Plot Normal Probability Plot Lag Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Generalized ESD Test for Outliers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.17.1. Grubbs' Test for Outliers]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.17.1.%20%20Grubbs'%20Test%20for%20Outliers%2F</url>
    <content type="text"><![CDATA[Purpose: Detection of OutliersGrubbs’ test (Grubbs 1969 and Stefansky 1972) is used to detect a single outlierin a univariate data set that follows an approximately normal distribution. If you suspect more than one outlier may be present, it is recommended that you use either the Tietjen-Moore test or the generalized extreme studentized deviate test instead of the Grubbs’ test. Grubbs’ test is also known as the maximum normed residual test. Definition Grubbs’ Test ExampleThe Tietjen and Moore paper gives the following set of 8 mass spectrometer measurements on a uranium isotope: 199.31 199.53 200.19 200.82 201.92 201.95 202.18 245.57 As a first step, a normal probability plot was generated This plot indicates that the normality assumption is reasonable with the exception of the maximum value. We therefore compute Grubbs’ test for the case that the maximum value, 245.57, is an outlier. H0: there are no outliers in the data Ha: the maximum value is an outlier Test statistic: G = 2.4687 Significance level: α = 0.05 Critical value for an upper one-tailed test: 2.032 Critical region: Reject H0 if G &gt; 2.032 For this data set, we reject the null hypothesis and conclude that the maximum value is in fact an outlier at the 0.05 significance level. QuestionsGrubbs’ test can be used to answer the following questions: 1. Is the maximum value an outlier? 2. Is the minimum value an outlier? ImportanceMany statistical techniques are sensitive to the presence of outliers. For example, simple calculations of the mean and standard deviation may be distorted by a single grossly inaccurate data point. Checking for outliers should be a routine part of any data analysis. Potential outliers should be examined to see if they are possibly erroneous. If the data point is in error, it should be corrected if possible and deleted if it is not possible. If there is no reason to believe that the outlying point is in error, it should not be deleted without careful consideration. However, the use of more robust techniques may be warranted. Robust techniques will often downweight the effect of outlying points without deleting them. Related TechniquesSeveral graphical techniques can, and should, be used to help detect outliers. A simple normal probability plot, run sequence plot, a box plot, or a histogram should show any obviously outlying points. In addition to showing potential outliers, several of these graphics also help assess whether the data follow an approximately normal distribution. Normal Probability Plot Run Sequence Plot Histogram Box Plot Lag Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h1.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Grubbs&#39; Test for Outliers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.17. Detection of Outliers]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.17.%20Detection%20of%20Outliers%2F</url>
    <content type="text"><![CDATA[IntroductionAn outlier is an observation that appears to deviate markedly from other observations in the sample. Identification of potential outliers is important for the following reasons. 1. An outlier may indicate bad data. For example, the data may have been coded incorrectly or an experiment may not have been run correctly. If it can be determined that an outlying point is in fact erroneous, then the outlying value should be deleted from the analysis (or corrected if possible). 2. In some cases, it may not be possible to determine if an outlying point is bad data. Outliers may be due to random variation or may indicate something scientifically interesting. In any event, we typically do not want to simply delete the outlying observation. However, if the data contains significant outliers, we may need to consider the use of robust statistical techniques. Labeling, Accomodation, IdentificationIglewicz and Hoaglin distinguish the three following issues with regards to outliers. 1. outlier labeling - flag potential outliers for further investigation (i.e., are the potential outliers erroneous data, indicative of an inappropriate distributional model, and so on). 2. outlier accomodation - use robust statistical techniques that will not be unduly affected by outliers. That is, if we cannot determine that potential outliers are erroneous observations, do we need modify our statistical analysis to more appropriately account for these observations? 3. outlier identification - formally test whether observations are outliers. This section focuses on the labeling and identification issues.Normality AssumptionIdentifying an observation as an outlier depends on the underlying distribution of the data. In this section, we limit the discussion to univariate data sets that are assumed to follow an approximately normal distribution. If the normality assumption for the data being tested is not valid, then a determination that there is an outlier may in fact be due to the non-normality of the data rather than the prescence of an outlier. For this reason, it is recommended that you generate a normal probability plot of the data before applying an outlier test. Although you can also perform formal tests for normality, the prescence of one or more outliers may cause the tests to reject normality when it is in fact a reasonable assumption for applying the outlier test. In addition to checking the normality assumption, the lower and upper tails of the normal probability plot can be a useful graphical technique for identifying potential outliers. In particular, the plot can help determine whether we need to check for a single outlier or whether we need to check for multiple outliers. The box plot and the histogram can also be useful graphical tools in checking the normality assumption and in identifying potential outliers. Single Versus Multiple OutliersSome outlier tests are designed to detect the prescence of a single outlier while other tests are designed to detect the prescence of multiple outliers. It is not appropriate to apply a test for a single outlier sequentially in order to detect multiple outliers. In addition, some tests that detect multiple outliers may require that you specify the number of suspected outliers exactly. Masking and SwampingMasking can occur when we specify too few outliers in the test. For example, if we are testing for a single outlier when there are in fact two (or more) outliers, these additional outliers may influence the value of the test statistic enough so that no points are declared as outliers.On the other hand, swamping can occur when we specify too many outliers in the test. For example, if we are testing for two or more outliers when there is in fact only a single outlier, both points may be declared outliers (many tests will declare either all or none of the tested points as outliers). Due to the possibility of masking and swamping, it is useful to complement formal outlier tests with graphical methods. Graphics can often help identify cases where masking or swamping may be an issue. Swamping and masking are also the reason that many tests require that the exact number of outliers being tested must be specified. Also, masking is one reason that trying to apply a single outlier test sequentially can fail. For example, if there are multiple outliers, masking may cause the outlier test for the first outlier to return a conclusion of no outliers (and so the testing for any additional outliers is not performed). Z-Scores and Modified Z-ScoresThe Z-score of an observation is defined as with Y¯ and s denoting the sample mean and sample standard deviation, respectively. In other words, data is given in units of how many standard deviations it is from the mean. Although it is common practice to use Z-scores to identify possible outliers, this can be misleading (partiucarly for small sample sizes) due to the fact that the maximum Z-score is at most Iglewicz and Hoaglin recommend using the modified Z-score with MAD denoting the median absolute deviation and x~ denoting the median. These authors recommend that modified Z-scores with an absolute value of greater than 3.5 be labeled as potential outliers. Formal Outlier TestsA number of formal outlier tests have proposed in the literature. These can be grouped by the following characteristics: • What is the distributional model for the data? We restrict our discussion to tests that assume the data follow an approximately normal distribution. • Is the test designed for a single outlier or is it designed for multiple outliers? • If the test is designed for multiple outliers, does the number of outliers need to be specified exactly or can we specify an upper bound for the number of outliers? The following are a few of the more commonly used outlier tests for normally distributed data. This list is not exhaustive (a large number of outlier tests have been proposed in the literature). The tests given here are essentially based on the criterion of “distance from the mean”. This is not the only criterion that could be used. For example, the Dixon test, which is not discussed here, is based a value being too large (or small) compared to its nearest neighbor. 1. Grubbs&apos; Test - this is the recommended test when testing for a single outlier. 2. Tietjen-Moore Test - this is a generalization of the Grubbs&apos; test to the case of more than one outlier. It has the limitation that the number of outliers must be specified exactly. 3. Generalized Extreme Studentized Deviate (ESD) Test - this test requires only an upper bound on the suspected number of outliers and is the recommended test when the exact number of outliers is not known. Lognormal DistributionThe tests discussed here are specifically based on the assumption that the data follow an approximately normal disribution. If your data follow an approximately lognormal distribution, you can transform the data to normality by taking the logarithms of the data and then applying the outlier tests discussed here. Further InformationIglewicz and Hoaglin provide an extensive discussion of the outlier tests given above (as well as some not given above) and also give a good tutorial on the subject of outliers. Barnett and Lewis provide a book length treatment of the subject. In addition to discussing additional tests for data that follow an approximately normal distribution, these sources also discuss the case where the data are not normally distributed. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Detection of Outliers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.16. Kolmogorov-Smirnov Goodness-of-Fit Test]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.16.%20Kolmogorov-Smirnov%20Goodness-of-Fit%20Test%2F</url>
    <content type="text"><![CDATA[Purpose: Test for Distributional AdequacyThe Kolmogorov-Smirnov test (Chakravart, Laha, and Roy, 1967) is used to decide if a sample comes from a population with a specific distribution. The Kolmogorov-Smirnov (K-S) test is based on the empirical distribution function (ECDF). Given Nordered data points Y1, Y2, …, YN, the ECDF is defined as EN=n(i)/N where n(i) is the number of points less than Yi and the Yi are ordered from smallest to largest value. This is a step function that increases by 1/N at the value of each ordered data point.The graph below is a plot of the empirical distribution function with a normal cumulative distribution function for 100 normal random numbers. The K-S test is based on the maximum distance between these two curves. Characteristics and Limitations of the K-S TestAn attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations: 1. It only applies to continuous distributions. 2. It tends to be more sensitive near the center of the distribution than at the tails. 3. Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation. Several goodness-of-fit tests, such as the Anderson-Darling test and the Cramer Von-Mises test, are refinements of the K-S test. As these refined tests are generally considered to be more powerful than the original K-S test, many analysts prefer them. Also, the advantage for the K-S test of having the critical values be indpendendent of the underlying distribution is not as much of an advantage as first appears. This is due to limitation 3 above (i.e., the distribution parameters are typically not known and have to be estimated from the data). So in practice, the critical values for the K-S test have to be determined by simulation just as for the Anderson-Darling and Cramer Von-Mises (and related) tests. Note that although the K-S test is typically developed in the context of continuous distributions for uncensored and ungrouped data, the test has in fact been extended to discrete distributions and to censored and grouped data. We do not discuss those cases here. Definition Technical Note Kolmogorov-Smirnov Test ExampleWe generated 1,000 random numbers for normal, double exponential, t with 3 degrees of freedom, and lognormal distributions. In all cases, the Kolmogorov-Smirnov test was applied to test for a normal distribution. The normal random numbers were stored in the variable Y1, the double exponential random numbers were stored in the variable Y2, the t random numbers were stored in the variable Y3, and the lognormal random numbers were stored in the variable Y4. H0: the data are normally distributed Ha: the data are not normally distributed Y1 test statistic: D = 0.0241492 Y2 test statistic: D = 0.0514086 Y3 test statistic: D = 0.0611935 Y4 test statistic: D = 0.5354889 Significance level: α = 0.05 Critical value: 0.04301 Critical region: Reject H0 if D &gt; 0.04301 As expected, the null hypothesis is not rejected for the normally distributed data, but is rejected for the remaining three data sets that are not normally distributed. QuestionsThe Kolmogorov-Smirnov test can be used to answer the following types of questions: • Are the data from a normal distribution? • Are the data from a log-normal distribution? • Are the data from a Weibull distribution? • Are the data from an exponential distribution? • Are the data from a logistic distribution? ImportanceMany statistical tests and procedures are based on specific distributional assumptions. The assumption of normality is particularly common in classical statistical tests. Much reliability modeling is based on the assumption that the data follow a Weibull distribution. There are many non-parametric and robust techniques that are not based on strong distributional assumptions. By non-parametric, we mean a technique, such as the sign test, that is not based on a specific distributional assumption. By robust, we mean a statistical technique that performs well under a wide range of distributional assumptions. However, techniques based on specific distributional assumptions are in general more powerful than these non-parametric and robust techniques. By power, we mean the ability to detect a difference when that difference actually exists. Therefore, if the distributional assumptions can be confirmed, the parametric techniques are generally preferred. If you are using a technique that makes a normality (or some other type of distributional) assumption, it is important to confirm that this assumption is in fact justified. If it is, the more powerful parametric techniques can be used. If the distributional assumption is not justified, using a non-parametric or robust technique may be required. Related Techniques Anderson-Darling goodness-of-fit Test Chi-Square goodness-of-fit Test Shapiro-Wilk Normality Test Probability Plots Probability Plot Correlation Coefficient Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Kolmogorov-Smirnov Goodness-of-Fit Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.15. Chi-Square Goodness-of-Fit Test]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.15.%20Chi-Square%20Goodness-of-Fit%20Test%2F</url>
    <content type="text"><![CDATA[Purpose: Test for distributional adequacyThe chi-square test (Snedecor and Cochran, 1989) is used to test if a sample of data came from a population with a specific distribution. An attractive feature of the chi-square goodness-of-fit test is that it can be applied to any univariate distribution for which you can calculate the cumulative distribution function. The chi-square goodness-of-fit test is applied to binned data (i.e., data put into classes). This is actually not a restriction since for non-binned data you can simply calculate a histogram or frequency table before generating the chi-square test. However, the value of the chi-square test statistic are dependent on how the data is binned. Another disadvantage of the chi-square test is that it requires a sufficient sample size in order for the chi-square approximation to be valid. The chi-square test is an alternative to theAnderson-Darling and Kolmogorov-Smirnovgoodness-of-fit tests. The chi-square goodness-of-fit test can be applied to discrete distributions such as the binomialand the Poisson. The Kolmogorov-Smirnov and Anderson-Darling tests are restricted to continuous distributions. Additional discussion of the chi-square goodness-of-fit test is contained in theproduct and process comparisons chapter (chapter 7). DefinitionThe chi-square test is defined for the hypothesis: Chi-Square Test ExampleWe generated 1,000 random numbers for normal, double exponential, t with 3 degrees of freedom, and lognormal distributions. In all cases, a chi-square test with k = 32 bins was applied to test for normally distributed data. Because the normal distribution has two parameters, c = 2 + 1 = 3 The normal random numbers were stored in the variable Y1, the double exponential random numbers were stored in the variable Y2, the trandom numbers were stored in the variable Y3, and the lognormal random numbers were stored in the variable Y4. H0: the data are normally distributed Ha: the data are not normally distributed Y1 Test statistic: Χ2 = 32.256 Y2 Test statistic: Χ2 = 91.776 Y3 Test statistic: Χ2 = 101.488 Y4 Test statistic: Χ2 = 1085.104 Significance level: α = 0.05 Degrees of freedom: k - c = 32 - 3 = 29 Critical value: Χ21-α,k-c = 42.557 Critical region: Reject H0 if Χ2 &gt; 42.557 As we would hope, the chi-square test fails to reject the null hypothesis for the normally distributed data set and rejects the null hypothesis for the three non-normal data sets. QuestionsThe chi-square test can be used to answer the following types of questions: • Are the data from a normal distribution? • Are the data from a log-normal distribution? • Are the data from a Weibull distribution? • Are the data from an exponential distribution? • Are the data from a logistic distribution? • Are the data from a binomial distribution? ImportanceMany statistical tests and procedures are based on specific distributional assumptions. The assumption of normality is particularly common in classical statistical tests. Much reliability modeling is based on the assumption that the distribution of the data follows a Weibull distribution. There are many non-parametric and robust techniques that are not based on strong distributional assumptions. By non-parametric, we mean a technique, such as the sign test, that is not based on a specific distributional assumption. By robust, we mean a statistical technique that performs well under a wide range of distributional assumptions. However, techniques based on specific distributional assumptions are in general more powerful than these non-parametric and robust techniques. By power, we mean the ability to detect a difference when that difference actually exists. Therefore, if the distributional assumption can be confirmed, the parametric techniques are generally preferred. If you are using a technique that makes a normality (or some other type of distributional) assumption, it is important to confirm that this assumption is in fact justified. If it is, the more powerful parametric techniques can be used. If the distributional assumption is not justified, a non-parametric or robust technique may be required. Related Techniques Anderson-Darling Goodness-of-Fit Test Kolmogorov-Smirnov Test Shapiro-Wilk Normality Test Probability Plots Probability Plot Correlation Coefficient Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Chi-Square Goodness-of-Fit Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.12. Autocorrelation]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.12.%20Autocorrelation%2F</url>
    <content type="text"><![CDATA[Purpose: Detect Non-Randomness, Time Series ModelingThe autocorrelation ( Box and Jenkins, 1976) function can be used for the following two purposes: 1. To detect non-randomness in data. 2. To identify an appropriate time series model if the data are not random. DefinitionGiven measurements, Y1, Y2, …, YN at timeX1, X2, …, XN, the lag k autocorrelation function is defined as rk=∑N−ki=1(Yi−Y¯)(Yi+k−Y¯)∑Ni=1(Yi−Y¯)2 Although the time variable, X, is not used in the formula for autocorrelation, the assumption is that the observations are equi-spaced. Autocorrelation is a correlation coefficient. However, instead of correlation between two different variables, the correlation is between two values of the same variable at times Xi and Xi+k.When the autocorrelation is used to detect non-randomness, it is usually only the first (lag 1) autocorrelation that is of interest. When the autocorrelation is used to identify an appropriate time series model, the autocorrelations are usually plotted for many lags. Autocorrelation ExampleLag-one autocorrelations were computed for the the LEW.DAT data set. lag autocorrelation 0. 1.00 1. -0.31 2. -0.74 3. 0.77 4. 0.21 5. -0.90 6. 0.38 7. 0.63 8. -0.77 9. -0.12 10. 0.82 11. -0.40 12. -0.55 13. 0.73 14. 0.07 15. -0.76 16. 0.40 17. 0.48 18. -0.70 19. -0.03 20. 0.70 21. -0.41 22. -0.43 23. 0.67 24. 0.00 25. -0.66 26. 0.42 27. 0.39 28. -0.65 29. 0.03 30. 0.63 31. -0.42 32. -0.36 33. 0.64 34. -0.05 35. -0.60 36. 0.43 37. 0.32 38. -0.64 39. 0.08 40. 0.58 41. -0.45 42. -0.28 43. 0.62 44. -0.10 45. -0.55 46. 0.45 47. 0.25 48. -0.61 49. 0.14 QuestionsThe autocorrelation function can be used to answer the following questions. 1. Was this sample data set generated from a random process? 2. Would a non-linear or time series model be a more appropriate model for these data than a simple constant plus error model? ImportanceRandomness is one of the key assumptions in determining if a univariate statistical process is in control. If the assumptions of constant location and scale, randomness, and fixed distribution are reasonable, then the univariate process can be modeled as: Yi=A0+Ei where Ei is an error term. If the randomness assumption is not valid, then a different model needs to be used. This will typically be either a time series modelor a non-linear model (with time as the independent variable). Related Techniques Autocorrelation Plot Run Sequence Plot Lag Plot Runs Test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Autocorrelation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.14. Anderson-Darling Test]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.14.%20Anderson-Darling%20Test%2F</url>
    <content type="text"><![CDATA[Purpose: Test for Distributional AdequacyThe Anderson-Darling test (Stephens, 1974) is used to test if a sample of data came from a population with a specific distribution. It is a modification of theKolmogorov-Smirnov (K-S) test and gives more weight to the tails than does the K-S test. The K-S test is distribution free in the sense that the critical values do not depend on the specific distribution being tested (note that this is true only for a fully specified distribution, i.e. the parameters are known). The Anderson-Darling test makes use of the specific distribution in calculating critical values. This has the advantage of allowing a more sensitive test and the disadvantage that critical values must be calculated for each distribution. Currently, tables of critical values are available for the normal, uniform, lognormal,exponential, Weibull, extreme value type I, generalized Pareto, and logistic distributions. We do not provide the tables of critical values in this Handbook (seeStephens 1974, 1976, 1977, and 1979) since this test is usually applied with a statistical software program that will print the relevant critical values.The Anderson-Darling test is an alternative to the chi-square and Kolmogorov-Smirnov goodness-of-fit tests. Definition Sample OutputWe generated 1,000 random numbers for normal, double exponential, Cauchy, and lognormal distributions. In all four cases, the Anderson-Darling test was applied to test for a normal distribution.The normal random numbers were stored in the variable Y1, the double exponential random numbers were stored in the variable Y2, the Cauchy random numbers were stored in the variable Y3, and the lognormal random numbers were stored in the variable Y4. Distribution Mean Standard Deviation ------------ -------- ------------------ Normal (Y1) 0.004360 1.001816 Double Exponential (Y2) 0.020349 1.321627 Cauchy (Y3) 1.503854 35.130590 Lognormal (Y4) 1.518372 1.719969 H0: the data are normally distributed Ha: the data are not normally distributed Y1 adjusted test statistic: A2 = 0.2576 Y2 adjusted test statistic: A2 = 5.8492 Y3 adjusted test statistic: A2 = 288.7863 Y4 adjusted test statistic: A2 = 83.3935 Significance level: α = 0.05 Critical value: 0.752 Critical region: Reject H0 if A2 &gt; 0.752 When the data were generated using a normal distribution, the test statistic was small and the hypothesis of normality was not rejected. When the data were generated using the double exponential, Cauchy, and lognormal distributions, the test statistics were large, and the hypothesis of an underlying normal distribution was rejected at the 0.05 significance level. QuestionsThe Anderson-Darling test can be used to answer the following questions: • Are the data from a normal distribution? • Are the data from a log-normal distribution? • Are the data from a Weibull distribution? • Are the data from an exponential distribution? • Are the data from a logistic distribution? ImportanceMany statistical tests and procedures are based on specific distributional assumptions. The assumption of normality is particularly common in classical statistical tests. Much reliability modeling is based on the assumption that the data follow a Weibull distribution. There are many non-parametric and robust techniques that do not make strong distributional assumptions. However, techniques based on specific distributional assumptions are in general more powerful than non-parametric and robust techniques. Therefore, if the distributional assumptions can be validated, they are generally preferred. Related Techniques Chi-Square goodness-of-fit Test Kolmogorov-Smirnov Test Shapiro-Wilk Normality Test Probability Plot Probability Plot Correlation Coefficient Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Anderson-Darling Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.13. Runs Test for Detecting Non-randomness]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.13.%20Runs%20Test%20for%20Detecting%20Non-randomness%2F</url>
    <content type="text"><![CDATA[Purpose: Detect Non-RandomnessThe runs test (Bradley, 1968) can be used to decide if a data set is from a random process. A run is defined as a series of increasing values or a series of decreasing values. The number of increasing, or decreasing, values is the length of the run. In a random data set, the probability that the (I+1)th value is larger or smaller than the Ith value follows a binomial distribution, which forms the basis of the runs test. Typical Analysis and Test StatisticsThe first step in the runs test is to count the number of runs in the data sequence. There are several ways to define runs in the literature, however, in all cases the formulation must produce a dichotomous sequence of values. For example, a series of 20 coin tosses might produce the following sequence of heads (H) and tails (T). H H T T H T H H H H T H H T T T T T H H The number of runs for this series is nine. There are 11 heads and 9 tails in the sequence. DefinitionWe will code values above the median as positive and values below the median as negative. A run is defined as a series of consecutive positive (or negative) values. The runs test is defined as: Runs Test ExampleA runs test was performed for 200 measurements of beam deflection contained in the LEW.DAT data set. Since the test statistic is greater than the critical value, we conclude that the data are not random at the 0.05 significance level. QuestionThe runs test can be used to answer the following question: • Were these sample data generated from a random process? ImportanceRandomness is one of the key assumptions in determining if a univariate statistical process is in control. If the assumptions of constant location and scale, randomness, and fixed distribution are reasonable, then the univariate process can be modeled as: Yi=A0+Ei where Ei is an error term. If the randomness assumption is not valid, then a different model needs to be used. This will typically be either a times series model or a non-linear model (with time as the independent variable). Related Techniques Autocorrelation Run Sequence Plot Lag Plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35d.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Runs Test for Detecting Non-randomness</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.11. Measures of Skewness and Kurtosis]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.11.%20Measures%20of%20Skewness%20and%20Kurtosis%2F</url>
    <content type="text"><![CDATA[Skewness and KurtosisA fundamental task in many statistical analyses is to characterize the location and variability of a data set. A further characterization of the data includes skewness and kurtosis. Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case. The histogram is an effective graphical technique for showing both the skewness and kurtosis of data set. Definition of SkewnessFor univariate data Y1, Y2, …, YN, the formula for skewness is: This is an adjustment for sample size. The adjustment approaches 1 as N gets large. For reference, the adjustment factor is 1.49 for N = 5, 1.19 for N = 10, 1.08 for N = 20, 1.05 for N = 30, and 1.02 for N = 100. The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail. If the data are multi-modal, then this may affect the sign of the skewness. Some measurements have a lower bound and are skewed right. For example, in reliability studies, failure times cannot be negative. It should be noted that there are alternative definitions of skewness in the literature. For example, the Galton skewness (also known as Bowley’s skewness) is defined as Definition of Kurtosis Alternative Definition of Kurtosis Examples The following example shows histograms for 10,000 random numbers generated from a normal, a double exponential, a Cauchy, and a Weibull distribution. Normal DistributionThe first histogram is a sample from a normal distribution. The normal distribution is a symmetric distribution with well-behaved tails. This is indicated by the skewness of 0.03. The kurtosis of 2.96 is near the expected value of 3. The histogram verifies the symmetry. Double Exponential DistributionThe second histogram is a sample from a double exponential distribution. The double exponential is a symmetric distribution. Compared to the normal, it has a stronger peak, more rapid decay, and heavier tails. That is, we would expect a skewness near zero and a kurtosis higher than 3. The skewness is 0.06 and the kurtosis is 5.9. Cauchy DistributionThe third histogram is a sample from a Cauchy distribution. For better visual comparison with the other data sets, we restricted the histogram of the Cauchy distribution to values between -10 and 10. The full data set for the Cauchy data in fact has a minimum of approximately -29,000 and a maximum of approximately 89,000. The Cauchy distribution is a symmetric distribution with heavy tails and a single peak at the center of the distribution. Since it is symmetric, we would expect a skewness near zero. Due to the heavier tails, we might expect the kurtosis to be larger than for a normal distribution. In fact the skewness is 69.99 and the kurtosis is 6,693. These extremely high values can be explained by the heavy tails. Just as the mean and standard deviation can be distorted by extreme values in the tails, so too can the skewness and kurtosis measures. Weibull DistributionThe fourth histogram is a sample from a Weibull distribution with shape parameter 1.5. The Weibull distribution is a skewed distribution with the amount of skewness depending on the value of the shape parameter. The degree of decay as we move away from the center also depends on the value of the shape parameter. For this data set, the skewness is 1.08 and the kurtosis is 4.46, which indicates moderate skewness and kurtosis. Dealing with Skewness and KurtosisMany classical statistical tests and intervals depend on normality assumptions. Significant skewness and kurtosis clearly indicate that data are not normal. If a data set exhibits significant skewness or kurtosis (as indicated by a histogram or the numerical measures), what can we do about it? One approach is to apply some type of transformation to try to make the data normal, or more nearly normal. The Box-Cox transformation is a useful technique for trying to normalize a data set. In particular, taking the log or square root of a data set is often useful for data that exhibit moderate right skewness. Another approach is to use techniques based on distributions other than the normal. For example, in reliability studies, the exponential, Weibull, and lognormal distributions are typically used as a basis for modeling rather than using the normal distribution. The probability plot correlation coefficient plot and the probability plot are useful tools for determining a good distributional model for the data. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Measures of Skewness and Kurtosis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.10. Levene Test for Equality of Variances]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.10.%20Levene%20Test%20for%20Equality%20of%20Variances%2F</url>
    <content type="text"><![CDATA[Purpose: Test for Homogeneity of VariancesLevene’s test ( Levene 1960) is used to test if ksamples have equal variances. Equal variances across samples is called homogeneity of variance. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Levene test can be used to verify that assumption. Levene’s test is an alternative to the Bartlett test. The Levene test is less sensitive than the Bartlett test to departures from normality. If you have strong evidence that your data do in fact come from a normal, or nearly normal, distribution, then Bartlett’s test has better performance. DefinitionThe Levene test is defined as: The three choices for defining Zijdetermine the robustness and power of Levene’s test. By robustness, we mean the ability of the test to not falsely detect unequal variances when the underlying data are not normally distributed and the variables are in fact equal. By power, we mean the ability of the test to detect unequal variances when the variances are in fact unequal. Levene’s original paper only proposed using the mean. Brown and Forsythe (1974)) extended Levene’s test to use either the median or the trimmed mean in addition to the mean. They performed Monte Carlo studies that indicated that using the trimmed mean performed best when the underlying data followed a Cauchy distribution (i.e., heavy-tailed) and the median performed best when the underlying data followed a χ24 (i.e., skewed) distribution. Using the mean provided the best power for symmetric, moderate-tailed, distributions. Although the optimal choice depends on the underlying distribution, the definition based on the median is recommended as the choice that provides good robustness against many types of non-normal data while retaining good power. If you have knowledge of the underlying distribution of the data, this may indicate using one of the other choices. Significance Level: α Levene’s Test ExampleLevene’s test, based on the median, was performed for the GEAR.DAT data set. The data set includes ten measurements of gear diameter for each of ten batches for a total of 100 measurements. H0: σ12 = ... = σ102 Ha: σ12 ≠ ... ≠ σ102 Test statistic: W = 1.705910 Degrees of freedom: k-1 = 10-1 = 9 N-k = 100-10 = 90 Significance level: α = 0.05 Critical value (upper tail): Fα,k-1,N-k = 1.9855 Critical region: Reject H0 if F &gt; 1.9855 We are testing the hypothesis that the group variances are equal. We fail to reject the null hypothesis at the 0.05 significance level since the value of the Levene test statistic is less than the critical value. We conclude that there is insufficient evidence to claim that the variances are not equal. QuestionLevene’s test can be used to answer the following question: • Is the assumption of equal variances valid? Related Techniques Standard Deviation Plot Box Plot Bartlett Test Chi-Square Test Analysis of Variance via http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Levene Test for Equality of Variances</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.9. F-Test for Equality of Two Variances]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.9.%20F-Test%20for%20Equality%20of%20Two%20Variances%2F</url>
    <content type="text"><![CDATA[Purpose: Test if variances from two populations are equalAn F-test (Snedecor and Cochran, 1983) is used to test if the variances of two populations are equal. This test can be a two-tailed test or a one-tailed test. The two-tailed version tests against the alternative that the variances are not equal. The one-tailed version only tests in one direction, that is the variance from the first population is either greater than or less than (but not both) the second population variance. The choice is determined by the problem. For example, if we are testing a new process, we may only be interested in knowing if the new process is less variable than the old process. DefinitionThe F hypothesis test is defined as: F Test ExampleThe following F-test was generated for theAUTO83B.DAT data set. The data set contains 480 ceramic strength measurements for two batches of material. The summary statistics for each batch are shown below. BATCH 1: NUMBER OF OBSERVATIONS = 240 MEAN = 688.9987 STANDARD DEVIATION = 65.54909 BATCH 2: NUMBER OF OBSERVATIONS = 240 MEAN = 611.1559 STANDARD DEVIATION = 61.85425 We are testing the null hypothesis that the variances for the two batches are equal. H0: σ12 = σ22 Ha: σ12 ≠ σ22 Test statistic: F = 1.123037 Numerator degrees of freedom: N1 - 1 = 239 Denominator degrees of freedom: N2 - 1 = 239 Significance level: α = 0.05 Critical values: F(1-α/2,N1-1,N2-1) = 0.7756 F(α/2,N1-1,N2-1) = 1.2894 Rejection region: Reject H0 if F &lt; 0.7756 or F &gt; 1.2894 The F test indicates that there is not enough evidence to reject the null hypothesis that the two batch variancess are equal at the 0.05 significance level. QuestionsThe F-test can be used to answer the following questions: 1. Do two samples come from populations with equal variancess? 2. Does a new process, treatment, or test reduce the variability of the current process? Related Techniques Quantile-Quantile Plot Bihistogram Chi-Square Test Bartlett’s Test Levene Test via http://www.itl.nist.gov/div898/handbook/eda/section3/eda359.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>F-Test for Equality of Two Variances</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.8. Chi-Square Test for the Variance]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.8.%20Chi-Square%20Test%20for%20the%20Variance%2F</url>
    <content type="text"><![CDATA[Purpose: Test if the variance is equal to a specified valueA chi-square test ( Snedecor and Cochran, 1983) can be used to test if the variance of a population is equal to a specified value. This test can be either a two-sided test or a one-sided test. The two-sided version tests against the alternative that the true variance is either less than or greater than the specified value. The one-sided version only tests in one direction. The choice of a two-sided or one-sided test is determined by the problem. For example, if we are testing a new process, we may only be concerned if its variability is greater than the variability of the current process. DefinitionThe chi-square hypothesis test is defined as: Chi-Square Test ExampleA chi-square test was performed for the GEAR.DAT data set. The observed variance for the 100 measurements of gear diameter is 0.00003969 (the standard deviation is 0.0063). We will test the null hypothesis that the true variance is equal to 0.01. QuestionsThe chi-square test can be used to answer the following questions: Is the variance equal to some pre-determined threshold value? Is the variance greater than some pre-determined threshold value? Is the variance less than some pre-determined threshold value? Related Techniques F Test Bartlett Test Levene Test via http://www.itl.nist.gov/div898/handbook/eda/section3/eda358.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Chi-Square Test for the Variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.7. Bartlett's Test]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.7.%20Bartlett's%20Test%2F</url>
    <content type="text"><![CDATA[Purpose: Test for Homogeneity of VariancesBartlett’s test (Snedecor and Cochran, 1983) is used to test if k samples have equal variances. Equal variances across samples is called homogeneity of variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Bartlett test can be used to verify that assumption. Bartlett’s test is sensitive to departures from normality. That is, if your samples come from non-normal distributions, then Bartlett’s test may simply be testing for non-normality. The Levene test is an alternative to the Bartlett test that is less sensitive to departures from normality. DefinitionThe Bartlett test is defined as: ExampleBartlett’s test was performed for the GEAR.DATdata set. The data set contains 10 measurements of gear diameter for ten different batches for a total of 100 measurements. H0: σ12 = σ22 = ... = σ102 Ha: At least one σi2 is not equal to the others. Test statistic: T = 20.78580 Degrees of freedom: k - 1 = 9 Significance level: α = 0.05 Critical value: Χ21-α,k-1 = 16.919 Critical region: Reject H0 if T &gt; 16.919 We are testing the null hypothesis that the batch variances are all equal. Because the test statistic is larger than the critical value, we reject the null hypotheses at the 0.05 significance level and conclude that at least one batch variance is different from the others. QuestionBartlett’s test can be used to answer the following question: • Is the assumption of equal variances valid? ImportanceBartlett’s test is useful whenever the assumption of equal variances is made. In particular, this assumption is made for the frequently used one-way analysis of variance. In this case, Bartlett’s or Levene’s test should be applied to verify the assumption. Related Techniques Standard Deviation Plot Box Plot Levene Test Chi-Square Test Analysis of Variance via http://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Bartlett&#39;s Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.6. Measures of Scale]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.6.%20Measures%20of%20Scale%2F</url>
    <content type="text"><![CDATA[Scale, Variability, or SpreadA fundamental task in many statistical analyses is to characterize the spread, or variability, of a data set. Measures of scale are simply attempts to estimate this variability.When assessing the variability of a data set, there are two key components: 1. How spread out are the data values near the center? 2. How spread out are the tails? Different numerical summaries will give different weight to these two elements. The choice of scale estimator is often driven by which of these components you want to emphasize. The histogram is an effective graphical technique for showing both of these components of the spread. Definitions of VariabilityFor univariate data, there are several common numerical measures of the spread: 1. variance - the variance is defined as s2=∑Ni=1(Yi−Y¯)2/(N−1) where Y¯ is the mean of the data. The variance is roughly the arithmetic average of the squared distance from the mean. Squaring the distance from the mean has the effect of giving greater weight to values that are further from the mean. For example, a point 2 units from the mean adds 4 to the above sum while a point 10 units from the mean adds 100 to the sum. Although the variance is intended to be an overall measure of spread, it can be greatly affected by the tail behavior. 2. standard deviation - the standard deviation is the square root of the variance. That is, s=∑Ni=1(Yi−Y¯)2/(N−1)−−−−−−−−−−−−−−−−−−−√ The standard deviation restores the units of the spread to the original data units (the variance squares the units). 3. range - the range is the largest value minus the smallest value in a data set. Note that this measure is based only on the lowest and highest extreme values in the sample. The spread near the center of the data is not captured at all. 4. average absolute deviation - the average absolute deviation (AAD) is defined as AAD=∑Ni=1(|Yi−Y¯|)/N where Y¯ is the mean of the data and |Y|is the absolute value of Y. This measure does not square the distance from the mean, so it is less affected by extreme observations than are the variance and standard deviation. 5. median absolute deviation - the median absolute deviation (MAD) is defined as MAD=median(|Yi−Y~|) where Y~ is the median of the data and |Y|is the absolute value of Y. This is a variation of the average absolute deviation that is even less affected by extremes in the tail because the data in the tails have less influence on the calculation of the median than they do on the mean. 6. interquartile range - this is the value of the 75th percentile minus the value of the 25th percentile. This measure of scale attempts to measure the variability of points near the center. In summary, the variance, standard deviation, average absolute deviation, and median absolute deviation measure both aspects of the variability; that is, the variability near the center and the variability in the tails. They differ in that the average absolute deviation and median absolute deviation do not give undue weight to the tail behavior. On the other hand, the range only uses the two most extreme points and the interquartile range only uses the middle portion of the data. Why Different Measures?The following example helps to clarify why these alternative defintions of spread are useful and necessary.This plot shows histograms for 10,000 random numbers generated from a normal, a double exponential, a Cauchy, and a Tukey-Lambda distribution. Normal DistributionThe first histogram is a sample from a normal distribution. The standard deviation is 0.997, the median absolute deviation is 0.681, and the range is 7.87. The normal distribution is a symmetric distribution with well-behaved tails and a single peak at the center of the distribution. By symmetric, we mean that the distribution can be folded about an axis so that the two sides coincide. That is, it behaves the same to the left and right of some center point. In this case, the median absolute deviation is a bit less than the standard deviation due to the downweighting of the tails. The range of a little less than 8 indicates the extreme values fall within about 4 standard deviations of the mean. If a histogram or normal probability plot indicates that your data are approximated well by a normal distribution, then it is reasonable to use the standard deviation as the spread estimator. Double Exponential DistributionThe second histogram is a sample from a double exponential distribution. The standard deviation is 1.417, the median absolute deviation is 0.706, and the range is 17.556. Comparing the double exponential and the normal histograms shows that the double exponential has a stronger peak at the center, decays more rapidly near the center, and has much longer tails. Due to the longer tails, the standard deviation tends to be inflated compared to the normal. On the other hand, the median absolute deviation is only slightly larger than it is for the normal data. The longer tails are clearly reflected in the value of the range, which shows that the extremes fall about 6 standard deviations from the mean compared to about 4 for the normal data. Cauchy DistributionThe third histogram is a sample from a Cauchy distribution. The standard deviation is 998.389, the median absolute deviation is 1.16, and the range is 118,953.6. The Cauchy distribution is a symmetric distribution with heavy tails and a single peak at the center of the distribution. The Cauchy distribution has the interesting property that collecting more data does not provide a more accurate estimate for the mean or standard deviation. That is, the sampling distribution of the means and standard deviation are equivalent to the sampling distribution of the original data. That means that for the Cauchy distribution the standard deviation is useless as a measure of the spread. From the histogram, it is clear that just about all the data are between about -5 and 5. However, a few very extreme values cause both the standard deviation and range to be extremely large. However, the median absolute deviation is only slightly larger than it is for the normal distribution. In this case, the median absolute deviation is clearly the better measure of spread. Although the Cauchy distribution is an extreme case, it does illustrate the importance of heavy tails in measuring the spread. Extreme values in the tails can distort the standard deviation. However, these extreme values do not distort the median absolute deviation since the median absolute deviation is based on ranks. In general, for data with extreme values in the tails, the median absolute deviation or interquartile range can provide a more stable estimate of spread than the standard deviation. Tukey-Lambda DistributionThe fourth histogram is a sample from a Tukey lambda distribution with shape parameter λ = 1.2. The standard deviation is 0.49, the median absolute deviation is 0.427, and the range is 1.666. The Tukey lambda distribution has a range limited to (-1/λ,1/λ). That is, it has truncated tails. In this case the standard deviation and median absolute deviation have closer values than for the other three examples which have significant tails. RobustnessTukey and Mosteller defined two types of robustness where robustness is a lack of susceptibility to the effects of nonnormality. 1. Robustness of validity means that the confidence intervals for a measure of the population spread (e.g., the standard deviation) have a 95 % chance of covering the true value (i.e., the population value) of that measure of spread regardless of the underlying distribution. 2. Robustness of efficiency refers to high effectiveness in the face of non-normal tails. That is, confidence intervals for the measure of spread tend to be almost as narrow as the best that could be done if we knew the true shape of the distribution. The standard deviation is an example of an estimator that is the best we can do if the underlying distribution is normal. However, it lacks robustness of validity. That is, confidence intervals based on the standard deviation tend to lack precision if the underlying distribution is in fact not normal. The median absolute deviation and the interquartile range are estimates of scale that have robustness of validity. However, they are not particularly strong for robustness of efficiency.If histograms and probability plots indicate that your data are in fact reasonably approximated by a normal distribution, then it makes sense to use the standard deviation as the estimate of scale. However, if your data are not normal, and in particular if there are long tails, then using an alternative measure such as the median absolute deviation, average absolute deviation, or interquartile range makes sense. The range is used in some applications, such as quality control, for its simplicity. In addition, comparing the range to the standard deviation gives an indication of the spread of the data in the tails. Since the range is determined by the two most extreme points in the data set, we should be cautious about its use for large values of N. Tukey and Mosteller give a scale estimator that has both robustness of validity and robustness of efficiency. However, it is more complicated and we do not give the formula here. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda356.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Measures of Scale</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.5. Multi-factor Analysis of Variance]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.5.%20Multi-factor%20Analysis%20of%20Variance%2F</url>
    <content type="text"><![CDATA[Purpose: Detect significant factorsThe analysis of variance (ANOVA) (Neter, Wasserman, and Kutner, 1990) is used to detect significant factors in a multi-factor model. In the multi-factor model, there is a response (dependent) variable and one or more factor (independent) variables. This is a common model in designed experiments where the experimenter sets the values for each of the factor variables and then measures the response variable. Each factor can take on a certain number of values. These are referred to as the levels of a factor. The number of levels can vary betweeen factors. For designed experiments, the number of levels for a given factor tends to be small. Each factor and level combination is a cell. Balanced designs are those in which the cells have an equal number of observations and unbalanced designs are those in which the number of observations varies among cells. It is customary to use balanced designs in designed experiments. DefinitionThe Product and Process Comparisons chapter (chapter 7) contains a more extensive discussion of two-factor ANOVA, including the details for the mathematical computations. The model for the analysis of variance can be stated in two mathematically equivalent ways. We explain the model for a two-way ANOVA (the concepts are the same for additional factors). In the following discussion, each combination of factors and levels is called a cell. In the following, the subscript i refers to the level of factor 1, j refers to the level of factor 2, and the subscript k refers to thekth observation within the (i,j)th cell. For example, Y235refers to the fifth observation in the second level of factor 1 and the third level of factor 2. The first model is Yijk=μij+Eijk This model decomposes the response into a mean for each cell and an error term. The analysis of variance provides estimates for each cell mean. These cell means are the predicted values of the model and the differences between the response variable and the estimated cell means are the residuals. That is Y^ijk=μ^ij Rijk=Yijk−μ^ij The second model is Yijk=μ+αi+βj+Eijk This model decomposes the response into an overall (grand) mean, factor effects (α^i and β^j represent the effects of the i-th level of the first factor and the j-th level of the second factor, respectively), and an error term. The analysis of variance provides estimates of the grand mean and the factor effects. The predicted values and the residuals of the model are Y^ijk=μ^+α^i+β^j Rijk=Yijk−μ^−α^i−β^j The distinction between these models is that the second model divides the cell mean into an overall mean and factor effects. This second model makes the factor effect more explicit, so we will emphasize this approach. Model ValidationNote that the ANOVA model assumes that the error term,Eijk, should follow the assumptions for a univariate measurement process. That is, after performing an analysis of variance, the model should be validated by analyzing the residuals. Multi-Factor ANOVA ExampleAn analysis of variance was performed for the JAHANMI2.DATdata set. The data contains four, two-level factors: table speed, down feed rate, wheel grit size, and batch. There are 30 measurements of ceramic strength for each factor combination for a total of 480 measurements. SOURCE DF SUM OF SQUARES MEAN SQUARE F STATISTIC ------------------------------------------------------------------ TABLE SPEED 1 26672.726562 26672.726562 6.7080 DOWN FEED RATE 1 11524.053711 11524.053711 2.8982 WHEEL GRIT SIZE 1 14380.633789 14380.633789 3.6166 BATCH 1 727143.125000 727143.125000 182.8703 RESIDUAL 475 1888731.500000 3976.276855 TOTAL (CORRECTED) 479 2668446.000000 5570.868652 RESIDUAL STANDARD DEVIATION = 63.05772781 FACTOR LEVEL N MEAN SD(MEAN) ------------------------------------------------ TABLE SPEED -1 240 657.53168 2.87818 1 240 642.62286 2.87818 DOWN FEED RATE -1 240 645.17755 2.87818 1 240 654.97723 2.87818 WHEEL GRIT SIZE -1 240 655.55084 2.87818 1 240 644.60376 2.87818 BATCH 1 240 688.99890 2.87818 2 240 611.15594 2.87818 The ANOVA decomposes the variance into the following component sum of squares: • Total sum of squares. The degrees of freedom for this entry is the number of observations minus one. • Sum of squares for each of the factors. The degrees of freedom for these entries are the number of levels for the factor minus one. The mean square is the sum of squares divided by the number of degrees of freedom. • Residual sum of squares. The degrees of freedom is the total degrees of freedom minus the sum of the factor degrees of freedom. The mean square is the sum of squares divided by the number of degrees of freedom. The analysis of variance summarizes how much of the variance in the data (total sum of squares) is accounted for by the factor effects (factor sum of squares) and how much is due to random error (residual sum of squares). Ideally, we would like most of the variance to be explained by the factor effects. The ANOVA table provides a formal Ftest for the factor effects. To test the overall batch effect in our example we use the following hypotheses. H0: All individual batch means are equal. Ha: At least one batch mean is not equal to the others. The F statistic is the mean square for the factor divided by the residual mean square. This statistic follows an Fdistribution with (k-1) and (N-k) degrees of freedom wherek is the number of levels for the given factor. Here, we see that the size of the “direction” effect dominates the size of the other effects. For our example, the critical Fvalue (upper tail) for α = 0.05, (k-1) = 1, and (N-k) = 475 is 3.86111. Thus, “table speed” and “batch” are significant at the 5 % level while “down feed rate” and “wheel grit size” are not significant at the 5 % level.In addition to the quantitative ANOVA output, it is recommended that any analysis of variance be complemented with model validation. At a minimum, this should include 1. A run sequence plot of the residuals. 2. A normal probability plot of the residuals. 3. A scatter plot of the predicted values against the residuals. QuestionsThe analysis of variance can be used to answer the following questions: 1. Do any of the factors have a significant effect? 2. Which is the most important factor? 3. Can we account for most of the variability in the data? Related Techniques One-factor analysis of variance Two-sample t-test Box plot Block plot DOE mean plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda355.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Multi-factor Analysis of Variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.4. One-Factor ANOVA]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.4.%20One-Factor%20ANOVA%2F</url>
    <content type="text"><![CDATA[Purpose: Test for Equal Means Across GroupsOne factor analysis of variance (Snedecor and Cochran, 1989) is a special case of analysis of variance (ANOVA), for one factor of interest, and a generalization of the two-sample t-test. The two-sample t-test is used to decide whether two groups (levels) of a factor have the same mean. One-way analysis of variance generalizes this to levels where k, the number of levels, is greater than or equal to 2. For example, data collected on, say, five instruments have one factor (instruments) at five levels. The ANOVA tests whether instruments have a significant effect on the results. DefinitionThe Product and Process Comparisons chapter (chapter 7) contains a more extensive discussion of one-factor ANOVA, including the details for the mathematical computations of one-way analysis of variance. The model for the analysis of variance can be stated in two mathematically equivalent ways. In the following discussion, each level of each factor is called a cell. For the one-way case, a cell and a level are equivalent since there is only one factor. In the following, the subscripti refers to the level and the subscript j refers to the observation within a level. For example,Y23 refers to the third observation in the second level. The first model is Yij=μi+Eij This model decomposes the response into a mean for each cell and an error term. The analysis of variance provides estimates for each cell mean. These estimated cell means are the predicted values of the model and the differences between the response variable and the estimated cell means are the residuals. That is Yij^=μ^i Rij=Yij−μ^i The second model is Yij=μ+αi+Eij This model decomposes the response into an overall (grand) mean, the effect of the ith factor level, and an error term. The analysis of variance provides estimates of the grand mean and the effect of the ith factor level. The predicted values and the residuals of the model are Y^ij=μ^+α^i Rij=Yij−μ^−α^i The distinction between these models is that the second model divides the cell mean into an overall mean and the effect of the i-th factor level. This second model makes the factor effect more explicit, so we will emphasize this approach. Model ValidationNote that the ANOVA model assumes that the error term, Eij, should follow the assumptions for a univariate measurement process. That is, after performing an analysis of variance, the model should be validated by analyzing the residuals. One-Way ANOVA ExampleA one-way analysis of variance was generated for theGEAR.DAT data set. The data set contains 10 measurements of gear diameter for ten different batches for a total of 100 measurements. DEGREES OF SUM OF MEAN SOURCE FREEDOM SQUARES SQUARE F STATISTIC ---------------- ---------- -------- -------- ----------- BATCH 9 0.000729 0.000081 2.2969 RESIDUAL 90 0.003174 0.000035 TOTAL (CORRECTED) 99 0.003903 0.000039 RESIDUAL STANDARD DEVIATION = 0.00594 BATCH N MEAN SD(MEAN) --------------------------------- 1 10 0.99800 0.00188 2 10 0.99910 0.00188 3 10 0.99540 0.00188 4 10 0.99820 0.00188 5 10 0.99190 0.00188 6 10 0.99880 0.00188 7 10 1.00150 0.00188 8 10 1.00040 0.00188 9 10 0.99830 0.00188 10 10 0.99480 0.00188 The ANOVA table decomposes the variance into the following component sum of squares: • Total sum of squares. The degrees of freedom for this entry is the number of observations minus one. • Sum of squares for the factor. The degrees of freedom for this entry is the number of levels minus one. The mean square is the sum of squares divided by the number of degrees of freedom. • Residual sum of squares. The degrees of freedom is the total degrees of freedom minus the factor degrees of freedom. The mean square is the sum of squares divided by the number of degrees of freedom. The sums of squares summarize how much of the variance in the data (total sum of squares) is accounted for by the factor effect (batch sum of squares) and how much is random error (residual sum of squares). Ideally, we would like most of the variance to be explained by the factor effect. The ANOVA table provides a formal F test for the factor effect. For our example, we are testing the following hypothesis. H0: All individual batch means are equal. Ha: At least one batch mean is not equal to the others. The F statistic is the batch mean square divided by the residual mean square. This statistic follows an Fdistribution with (k-1) and (N-k) degrees of freedom. For our example, the critical F value (upper tail) for α = 0.05, (k-1) = 9, and (N-k) = 90 is 1.9856. Since the Fstatistic, 2.2969, is greater than the critical value, we conclude that there is a significant batch effect at the 0.05 level of significance. Once we have determined that there is a significant batch effect, we might be interested in comparing individual batch means. The batch means and the standard errors of the batch means provide some information about the individual batches. However, we may want to employ multiple comparison methods for a more formal analysis. (See Box, Hunter, and Hunter for more information.) In addition to the quantitative ANOVA output, it is recommended that any analysis of variance be complemented with model validation. At a minimum, this should include: 1. a run sequence plot of the residuals, 2. a normal probability plot of the residuals, and 3. a scatter plot of the predicted values against the residuals. QuestionThe analysis of variance can be used to answer the following question • Are means the same across groups in the data? ImportanceThe analysis of uncertainty depends on whether the factor significantly affects the outcome. Related Techniques Two-sample t-test Multi-factor analysis of variance Regression Box plot via http://www.itl.nist.gov/div898/handbook/eda/section3/eda354.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>One-Factor ANOVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.3. Two-Sample t-Test for Equal Means]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.3.%20Two-Sample%C2%A0t-Test%20for%20Equal%20Means%2F</url>
    <content type="text"><![CDATA[Purpose: Test if two population means are equalThe two-sample t-test (Snedecor and Cochran, 1989) is used to determine if two population means are equal. A common application is to test if a new process or treatment is superior to a current process or treatment. There are several variations on this test. 1. The data may either be paired or not paired. By paired, we mean that there is a one-to-one correspondence between the values in the two samples. That is, if X1, X2, ..., Xn and Y1, Y2, ... , Yn are the two samples, then Xi corresponds to Yi. For paired samples, the differenceXi - Yi is usually calculated. For unpaired samples, the sample sizes for the two samples may or may not be equal. The formulas for paired data are somewhat simpler than the formulas for unpaired data. 2. The variances of the two samples may be assumed to be equal or unequal. Equal variances yields somewhat simpler formulas, although with computers this is no longer a significant issue. 3. In some applications, you may want to adopt a new process or treatment only if it exceeds the current treatment by some threshold. In this case, we can state the null hypothesis in the form that the difference between the two populations means is equal to some constant μ1−μ2=d0where the constant is the desired threshold. DefinitionThe two-sample t-test for unpaired data is defined as: H0: μ1=μ2 Ha: μ1≠μ2 Test Statistic: T=Y1¯−Y2¯s21/N1+s22/N2√ where N1 and N2 are the sample sizes, Y1¯ and Y2¯ are the sample means, and s21 and s22 are the sample variances. If equal variances are assumed, then the formula reduces to: T=Y1¯−Y2¯sp1/N1+1/N2√ where s2p=(N1−1)s21+(N2−1)s22N1+N2−2 Significance Level: α. Critical Region: Reject the null hypothesis that the two means are equal if |T| &gt; t1-α/2,ν where t1-α/2,ν is the critical value of the t distributionwith ν degrees of freedom where υ=(s21/N1+s22/N2)2(s21/N1)2/(N1−1)+(s22/N2)2/(N2−1) If equal variances are assumed, then ν = N1 + N2 - 2 Two-Samplet-Test ExampleThe following two-sample t-test was generated for the AUTO83B.DAT data set. The data set contains miles per gallon for U.S. cars (sample 1) and for Japanese cars (sample 2); the summary statistics for each sample are shown below. SAMPLE 1: NUMBER OF OBSERVATIONS = 249 MEAN = 20.14458 STANDARD DEVIATION = 6.41470 STANDARD ERROR OF THE MEAN = 0.40652 SAMPLE 2: NUMBER OF OBSERVATIONS = 79 MEAN = 30.48101 STANDARD DEVIATION = 6.10771 STANDARD ERROR OF THE MEAN = 0.68717 We are testing the hypothesis that the population means are equal for the two samples. We assume that the variances for the two samples are equal. H0: μ1 = μ2 Ha: μ1 ≠ μ2 Test statistic: T = -12.62059 Pooled standard deviation: sp = 6.34260 Degrees of freedom: ν = 326 Significance level: α = 0.05 Critical value (upper tail): t1-α/2,ν = 1.9673 Critical region: Reject H0 if |T| &gt; 1.9673 The absolute value of the test statistic for our example, 12.62059, is greater than the critical value of 1.9673, so we reject the null hypothesis and conclude that the two population means are different at the 0.05 significance level. In general, there are three possible alternative hypotheses and rejection regions for the one-sample t-test: Alternative Hypothesis Rejection RegionHa: μ1 ≠ μ2 |T| &gt; t1-α/2,ν Ha: μ1 &gt; μ2 T &gt; t1-α,ν Ha: μ1 &lt; μ2 T &lt; tα,ν For our two-tailed t-test, the critical value is t1-α/2,ν = 1.9673, whereα = 0.05 and ν = 326. If we were to perform an upper, one-tailed test, the critical value would be t1-α,ν = 1.6495. The rejection regions for three posssible alternative hypotheses using our example data are shown below. QuestionsTwo-sample t-tests can be used to answer the following questions: 1. Is process 1 equivalent to process 2? 2. Is the new process better than the current process? 3. Is the new process better than the current process by at least some pre-determined threshold amount? Related Techniques Confidence Limits for the Mean Analysis of Variance via http://www.itl.nist.gov/div898/handbook/eda/section3/eda353.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Two-Sample t-Test for Equal Means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.2 Confidence Limits for the Mean]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.2%20Confidence%20Limits%20for%20the%20Mean%2F</url>
    <content type="text"><![CDATA[Purpose: Interval Estimate for MeanConfidence limits for the mean (Snedecor and Cochran, 1989) are an interval estimate for the mean. Interval estimates are often desirable because the estimate of the mean varies from sample to sample. Instead of a single estimate for the mean, a confidence interval generates a lower and upper limit for the mean. The interval estimate gives an indication of how much uncertainty there is in our estimate of the true mean. The narrower the interval, the more precise is our estimate. Confidence limits are expressed in terms of a confidence coefficient. Although the choice of confidence coefficient is somewhat arbitrary, in practice 90 %, 95 %, and 99 % intervals are often used, with 95 % being the most commonly used. As a technical note, a 95 % confidence interval does not mean that there is a 95 % probability that the interval contains the true mean. The interval computed from a given sample either contains the true mean or it does not. Instead, the level of confidence is associated with the method of calculating the interval. The confidence coefficient is simply the proportion of samples of a given size that may be expected to contain the true mean. That is, for a 95 % confidence interval, if many samples are collected and the confidence interval computed, in the long run about 95 % of these intervals would contain the true mean. Definition: Confidence IntervalConfidence limits are defined as: Y¯±t1−α/2,N−1sN−−√ where Y¯ is the sample mean, s is the sample standard deviation, N is the sample size, α is the desired significance level, and t1-α/2,N-1 is the 100(1-α/2) percentile of the t distribution with N - 1 degrees of freedom. Note that the confidence coefficient is 1 - α.From the formula, it is clear that the width of the interval is controlled by two factors: 1. As N increases, the interval gets narrower from the N−−√ term. That is, one way to obtain more precise estimates for the mean is to increase the sample size. 2. The larger the sample standard deviation, the larger the confidence interval. This simply means that noisy data, i.e., data with a large standard deviation, are going to generate wider intervals than data with a smaller standard deviation. Definition: Hypothesis TestTo test whether the population mean has a specific value, μ0, against the two-sided alternative that it does not have a value μ0, the confidence interval is converted to hypothesis-test form. The test is a one-sample t-test, and it is defined as: H0: μ=μ0 Ha: μ≠μ0 Test Statistic: T=(Y¯−μ0)/(s/N−−√) where Y¯, N, and s are defined as above. Significance Level: α. The most commonly used value for α is 0.05. Critical Region: Reject the null hypothesis that the mean is a specified value, μ0, if T&lt;tα/2,N−1 or T&gt;t1−α/2,N−1 Confidence Interval Example We generated a 95 %, two-sided confidence interval for the ZARR13.DAT data set based on the following information. N = 195 MEAN = 9.261460 STANDARD DEVIATION = 0.022789 t1-0.025,N-1 = 1.9723 LOWER LIMIT = 9.261460 - 1.9723*0.022789/√195 UPPER LIMIT = 9.261460 + 1.9723*0.022789/√195 Thus, a 95 % confidence interval for the mean is (9.258242, 9.264679). t-Test Example We performed a two-sided, one-sample t-test using the ZARR13.DAT data set to test the null hypothesis that the population mean is equal to 5. H0: μ = 5 Ha: μ ≠ 5 Test statistic: T = 2611.284 Degrees of freedom: ν = 194 Significance level: α = 0.05 Critical value: t1-α/2,ν = 1.9723 Critical region: Reject H0 if |T| &gt; 1.9723 We reject the null hypotheses for our two-tailed t-test because the absolute value of the test statistic is greater than the critical value. If we were to perform an upper, one-tailed test, the critical value would be t1-α,ν = 1.6527, and we would still reject the null hypothesis.The confidence interval provides an alternative to the hypothesis test. If the confidence interval contains 5, then H0 cannot be rejected. In our example, the confidence interval (9.258242, 9.264679) does not contain 5, indicating that the population mean does not equal 5 at the 0.05 level of significance. In general, there are three possible alternative hypotheses and rejection regions for the one-sample t-test: Alternative Hypothesis Rejection RegionHa: μ ≠ μ0 |T| &gt; t1-α/2,ν Ha: μ &gt; μ0 T &gt; t1-α,ν Ha: μ &lt; μ0 T &lt; tα,ν The rejection regions for three posssible alternative hypotheses using our example data are shown in the following graphs. QuestionsConfidence limits for the mean can be used to answer the following questions: 1. What is a reasonable estimate for the mean? 2. How much variability is there in the estimate of the mean? 3. Does a given target value fall within the confidence limits? Related Techniques Two-Sample t-Test Confidence intervals for other location estimators such as the median or mid-mean tend to be mathematically difficult or intractable. For these cases, confidence intervals can be obtained using the bootstrap. via http://www.itl.nist.gov/div898/handbook/eda/section3/eda352.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Confidence Limits for the Mean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5.1. Measures of Location]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.1.%20Measures%20of%20Location%2F</url>
    <content type="text"><![CDATA[LocationA fundamental task in many statistical analyses is to estimate a location parameter for the distribution; i.e., to find a typical or central value that best describes the data. Definition of LocationThe first step is to define what we mean by a typical value. For univariate data, there are three common definitions: 1. mean - the mean is the sum of the data points divided by the number of data points. That is, Y¯=∑i=1NYi/N The mean is that value that is most commonly referred to as the average. We will use the term average as a synonym for the mean and the term typical value to refer generically to measures of location. 2. median - the median is the value of the point which has half the data smaller than that point and half the data larger than that point. That is, if X1, X2, ... ,XN is a random sample sorted from smallest value to largest value, then the median is defined as: Y~=Y(N+1)/2if N is odd Y~=(YN/2+Y(N/2)+1)/2if N is even 3. mode - the mode is the value of the random sample that occurs with the greatest frequency. It is not necessarily unique. The mode is typically used in a qualitative fashion. For example, there may be a single dominant hump in the data perhaps two or more smaller humps in the data. This is usually evident from a histogram of the data. When taking samples from continuous populations, we need to be somewhat careful in how we define the mode. That is, any specific value may not occur more than once if the data are continuous. What may be a more meaningful, if less exact measure, is the midpoint of the class interval of the histogram with the highest peak. Why Different MeasuresA natural question is why we have more than one measure of the typical value. The following example helps to explain why these alternative definitions are useful and necessary.This plot shows histograms for 10,000 random numbers generated from a normal, an exponential, a Cauchy, and a lognormal distribution. Normal DistributionThe first histogram is a sample from a normal distribution. The mean is 0.005, the median is -0.010, and the mode is -0.144 (the mode is computed as the midpoint of the histogram interval with the highest peak). The normal distribution is a symmetric distribution with well-behaved tails and a single peak at the center of the distribution. By symmetric, we mean that the distribution can be folded about an axis so that the 2 sides coincide. That is, it behaves the same to the left and right of some center point. For a normal distribution, the mean, median, and mode are actually equivalent. The histogram above generates similar estimates for the mean, median, and mode. Therefore, if a histogram or normal probability plot indicates that your data are approximated well by a normal distribution, then it is reasonable to use the mean as the location estimator. Exponential DistributionThe second histogram is a sample from anexponential distribution. The mean is 1.001, the median is 0.684, and the mode is 0.254 (the mode is computed as the midpoint of the histogram interval with the highest peak). The exponential distribution is a skewed, i. e., not symmetric, distribution. For skewed distributions, the mean and median are not the same. The mean will be pulled in the direction of the skewness. That is, if the right tail is heavier than the left tail, the mean will be greater than the median. Likewise, if the left tail is heavier than the right tail, the mean will be less than the median. For skewed distributions, it is not at all obvious whether the mean, the median, or the mode is the more meaningful measure of the typical value. In this case, all three measures are useful. Cauchy DistributionThe third histogram is a sample from a Cauchy distribution. The mean is 3.70, the median is -0.016, and the mode is -0.362 (the mode is computed as the midpoint of the histogram interval with the highest peak). For better visual comparison with the other data sets, we restricted the histogram of the Cauchy distribution to values between -10 and 10. The full Cauchy data set in fact has a minimum of approximately -29,000 and a maximum of approximately 89,000. The Cauchy distribution is a symmetric distribution with heavy tails and a single peak at the center of the distribution. The Cauchy distribution has the interesting property that collecting more data does not provide a more accurate estimate of the mean. That is, the sampling distribution of the mean is equivalent to the sampling distribution of the original data. This means that for the Cauchy distribution the mean is useless as a measure of the typical value. For this histogram, the mean of 3.7 is well above the vast majority of the data. This is caused by a few very extreme values in the tail. However, the median does provide a useful measure for the typical value. Although the Cauchy distribution is an extreme case, it does illustrate the importance of heavy tails in measuring the mean. Extreme values in the tails distort the mean. However, these extreme values do not distort the median since the median is based on ranks. In general, for data with extreme values in the tails, the median provides a better estimate of location than does the mean. Lognormal DistributionThe fourth histogram is a sample from alognormal distribution. The mean is 1.677, the median is 0.989, and the mode is 0.680 (the mode is computed as the midpoint of the histogram interval with the highest peak). The lognormal is also a skewed distribution. Therefore the mean and median do not provide similar estimates for the location. As with the exponential distribution, there is no obvious answer to the question of which is the more meaningful measure of location. RobustnessThere are various alternatives to the mean and median for measuring location. These alternatives were developed to address non-normal data since the mean is an optimal estimator if in fact your data are normal. Tukey and Mosteller defined two types of robustness where robustness is a lack of susceptibility to the effects of nonnormality. 1. Robustness of validity means that the confidence intervals for the population location have a 95% chance of covering the population location regardless of what the underlying distribution is. 2. Robustness of efficiency refers to high effectiveness in the face of non-normal tails. That is, confidence intervals for the population location tend to be almost as narrow as the best that could be done if we knew the true shape of the distributuion. The mean is an example of an estimator that is the best we can do if the underlying distribution is normal. However, it lacks robustness of validity. That is, confidence intervals based on the mean tend not to be precise if the underlying distribution is in fact not normal.The median is an example of a an estimator that tends to have robustness of validity but not robustness of efficiency. The alternative measures of location try to balance these two concepts of robustness. That is, the confidence intervals for the case when the data are normal should be almost as narrow as the confidence intervals based on the mean. However, they should maintain their validity even if the underlying data are not normal. In particular, these alternatives address the problem of heavy-tailed distributions. Alternative Measures of LocationA few of the more common alternative location measures are: 1. Mid-Mean - computes a mean using the data between the 25th and 75th percentiles. 2. Trimmed Mean - similar to the mid-mean except different percentile values are used. A common choice is to trim 5% of the points in both the lower and upper tails, i.e., calculate the mean for data between the 5th and 95th percentiles. 3. Winsorized Mean - similar to the trimmed mean. However, instead of trimming the points, they are set to the lowest (or highest) value. For example, all data below the 5th percentile are set equal to the value of the 5th percentile and all data greater than the 95th percentile are set equal to the 95th percentile. 4. Mid-range = (smallest + largest)/2. The first three alternative location estimators defined above have the advantage of the median in the sense that they are not unduly affected by extremes in the tails. However, they generate estimates that are closer to the mean for data that are normal (or nearly so). The mid-range, since it is based on the two most extreme points, is not robust. Its use is typically restricted to situations in which the behavior at the extreme points is relevant.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Measures of Location</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.6. Probability Distributions]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.6.%20Probability%20Distributions%2F</url>
    <content type="text"><![CDATA[Probability DistributionsProbability distributions are a fundamental concept in statistics. They are used both on a theoretical level and a practical level. Some practical uses of probability distributions are: • To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests. • For univariate data, it is often useful to determine a reasonable distributional model for the data. • Statistical intervals and hypothesis tests are often based on specific distributional assumptions. Before computing an interval or test based on a distributional assumption, we need to verify that the assumption is justified for the given data set. In this case, the distribution does not need to be the best-fitting distribution for the data, but an adequate enough model so that the statistical technique yields valid conclusions. • Simulation studies with random numbers generated from using a specific probability distribution are often needed. Table of Contents 1. What is a probability distribution? 2. Related probability functions 3. Families of distributions 4. Location and scale parameters 5. Estimating the parameters of a distribution 6. A gallery of common distributions 7. Tables for probability distributions via http://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Probability Distributions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.5. Quantitative Techniques]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.5.%20Quantitative%20Techniques%2F</url>
    <content type="text"><![CDATA[Confirmatory StatisticsThe techniques discussed in this section are classical statistical methods as opposed to EDA techniques. EDA and classical techniques are not mutually exclusive and can be used in a complementary fashion. For example, the analysis can start with some simple graphical techniques such as the 4-plot followed by the classical confirmatory methods discussed herein to provide more rigorous statements about the conclusions. If the classical methods yield different conclusions than the graphical analysis, then some effort should be invested to explain why. Often this is an indication that some of the assumptions of the classical techniques are violated. Many of the quantitative techniques fall into two broad categories: 1. Interval estimation 2. Hypothesis tests Interval EstimatesIt is common in statistics to estimate a parameter from a sample of data. The value of the parameter using all of the possible data, not just the sample data, is called the population parameter or true value of the parameter. An estimate of the true parameter value is made using the sample data. This is called a point estimate or a sample estimate. For example, the most commonly used measure of location is the mean. The population, or true, mean is the sum of all the members of the given population divided by the number of members in the population. As it is typically impractical to measure every member of the population, a random sample is drawn from the population. The sample mean is calculated by summing the values in the sample and dividing by the number of values in the sample. This sample mean is then used as the point estimate of the population mean. Interval estimates expand on point estimates by incorporating the uncertainty of the point estimate. In the example for the mean above, different samples from the same population will generate different values for the sample mean. An interval estimate quantifies this uncertainty in the sample estimate by computing lower and upper values of an interval which will, with a given level of confidence (i.e., probability), contain the population parameter. Hypothesis TestsHypothesis tests also address the uncertainty of the sample estimate. However, instead of providing an interval, a hypothesis test attempts to refute a specific claim about a population parameter based on the sample data. For example, the hypothesis might be one of the following: • the population mean is equal to 10 • the population standard deviation is equal to 5 • the means from two populations are equal • the standard deviations from 5 populations are equal To reject a hypothesis is to conclude that it is false. However, to accept a hypothesis does not mean that it is true, only that we do not have evidence to believe otherwise. Thus hypothesis tests are usually stated in terms of both a condition that is doubted (null hypothesis) and a condition that is believed (alternative hypothesis). A common format for a hypothesis test is: H0: A statement of the null hypothesis, e.g., two population means are equal. Ha: A statement of the alternative hypothesis, e.g., two population means are not equal. Test Statistic: The test statistic is based on the specific hypothesis test. Significance Level: The significance level, α,defines the sensitivity of the test. A value of α = 0.05 means that we inadvertently reject the null hypothesis 5% of the time when it is in fact true. This is also called the type I error. The choice of α is somewhat arbitrary, although in practice values of 0.1, 0.05, and 0.01 are commonly used. The probability of rejecting the null hypothesis when it is in fact false is called the power of the test and is denoted by 1 - β. Its complement, the probability of accepting the null hypothesis when the alternative hypothesis is, in fact, true (type II error), is called β and can only be computed for a specific alternative hypothesis. Critical Region: The critical region encompasses those values of the test statistic that lead to a rejection of the null hypothesis. Based on the distribution of the test statistic and the significance level, a cut-off value for the test statistic is computed. Values either above or below or both (depending on the direction of the test) this cut-off define the critical region. Practical Versus Statistical SignificanceIt is important to distinguish between statistical significance and practical significance. Statistical significance simply means that we reject the null hypothesis. The ability of the test to detect differences that lead to rejection of the null hypothesis depends on the sample size. For example, for a particularly large sample, the test may reject the null hypothesis that two process means are equivalent. However, in practice the difference between the two means may be relatively small to the point of having no real engineering significance. Similarly, if the sample size is small, a difference that is large in engineering terms may not lead to rejection of the null hypothesis. The analyst should not just blindly apply the tests, but should combine engineering judgement with statistical analysis. Bootstrap Uncertainty EstimatesIn some cases, it is possible to mathematically derive appropriate uncertainty intervals. This is particularly true for intervals based on the assumption of a normal distribution. However, there are many cases in which it is not possible to mathematically derive the uncertainty. In these cases, the bootstrap provides a method for empirically determining an appropriate interval. Table of ContentsSome of the more common classical quantitative techniques are listed below. This list of quantitative techniques is by no means meant to be exhaustive. Additional discussions of classical statistical techniques are contained in the product comparisons chapter. • Location 1. Measures of Location 2. Confidence Limits for the Mean and One Sample t-Test 3. Two Sample t-Test for Equal Means 4. One Factor Analysis of Variance 5. Multi-Factor Analysis of Variance • Scale (or variability or spread) 1. Measures of Scale 2. Bartlett&apos;s Test 3. Chi-Square Test 4. F-Test 5. Levene Test • Skewness and Kurtosis 1. Measures of Skewness and Kurtosis • Randomness 1. Autocorrelation 2. Runs Test • Distributional Measures 1. Anderson-Darling Test 2. Chi-Square Goodness-of-Fit Test 3. Kolmogorov-Smirnov Test • Outliers 1. Detection of Outliers 2. Grubbs Test 3. Tietjen-Moore Test 4. Generalized Extreme Deviate Test • 2-Level Factorial Designs 1. Yates Algorithm]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Quantitative Techniques</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.33. 6-Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.33.%206-Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Graphical Model ValidationThe 6-plot is a collection of 6 specific graphical techniques whose purpose is to assess the validity of a Y versus X fit. The fit can be a linear fit, a non-linear fit, a LOWESS (locally weighted least squares) fit, a spline fit, or any other fit utilizing a single independent variable. The 6 plots are: 1. Scatter plot of the response and predicted values versus the independent variable; 2. Scatter plot of the residuals versus the independent variable; 3. Scatter plot of the residuals versus the predicted values; 4. Lag plot of the residuals; 5. Histogram of the residuals; 6. Normal probability plot of the residuals. Sample Plot This 6-plot, which followed a linear fit, shows that the linear model is not adequate. It suggests that a quadratic model would be a better model. Definition:6 Component Plots The 6-plot consists of the following: 1. Response and predicted values ○ Vertical axis: Response variable, predicted values ○ Horizontal axis: Independent variable 2. Residuals versus independent variable ○ Vertical axis: Residuals ○ Horizontal axis: Independent variable 3. Residuals versus predicted values ○ Vertical axis: Residuals ○ Horizontal axis: Predicted values 4. Lag plot of residuals ○ Vertical axis: RES(I) ○ Horizontal axis: RES(I-1) 5. Histogram of residuals ○ Vertical axis: Counts ○ Horizontal axis: Residual values 6. Normal probability plot of residuals ○ Vertical axis: Ordered residuals ○ Horizontal axis: Theoretical values from a normal N(0,1) distribution for ordered residuals QuestionsThe 6-plot can be used to answer the following questions: 1. Are the residuals approximately normally distributed with a fixed location and scale? 2. Are there outliers? 3. Is the fit adequate? 4. Do the residuals suggest a better fit? Importance:Validating Model A model involving a response variable and a single independent variable has the form: Yi=f(Xi)+Ei where Y is the response variable, X is the independent variable, f is the linear or non-linear fit function, and E is the random component. For a good model, the error component should behave like: 1. random drawings (i.e., independent); 2. from a fixed distribution; 3. with fixed location; and 4. with fixed variation. In addition, for fitting models it is usually further assumed that the fixed distribution is normal and the fixed location is zero. For a good model the fixed variation should be as small as possible. A necessary component of fitting models is to verify these assumptions for the error component and to assess whether the variation for the error component is sufficiently small. The histogram, lag plot, and normal probability plot are used to verify the fixed distribution, location, and variation assumptions on the error component. The plot of the response variable and the predicted values versus the independent variable is used to assess whether the variation is sufficiently small. The plots of the residuals versus the independent variable and the predicted values is used to assess the independence assumption. Assessing the validity and quality of the fit in terms of the above assumptions is an absolutely vital part of the model-fitting process. No fit should be considered complete without an adequate model validation step. Related Techniques Linear Least Squares Non-Linear Least Squares Scatter Plot Run Sequence Plot Lag Plot Normal Probability Plot Histogram]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>6-Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.31. Youden Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.31.%20Youden%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Interlab ComparisonsYouden plots are a graphical technique for analyzing interlab data when each lab has made two runs on the same product or one run on two different products.The Youden plot is a simple but effective method for comparing both the within-laboratory variability and the between-laboratory variability. Sample Plot This plot shows: 1. Not all labs are equivalent. 2. Lab 4 is biased low. 3. Lab 3 has within-lab variability problems. 4. Lab 5 has an outlying run. Definition:Response 1 Versus Response 2 Coded by Lab Youden plots are formed by: 1. Vertical axis: Response variable 1 (i.e., run 1 or product 1 response value) 2. Horizontal axis: Response variable 2 (i.e., run 2 or product 2 response value) In addition, the plot symbol is the lab id (typically an integer from 1 to k where k is the number of labs). Sometimes a 45-degree reference line is drawn. Ideally, a lab generating two runs of the same product should produce reasonably similar results. Departures from this reference line indicate inconsistency from the lab. If two different products are being tested, then a 45-degree line may not be appropriate. However, if the labs are consistent, the points should lie near some fitted straight line. QuestionsThe Youden plot can be used to answer the following questions: 1. Are all labs equivalent? 2. What labs have between-lab problems (reproducibility)? 3. What labs have within-lab problems (repeatability)? 4. What labs are outliers? ImportanceIn interlaboratory studies or in comparing two runs from the same lab, it is useful to know if consistent results are generated. Youden plots should be a routine plot for analyzing this type of data. DOE Youden Plot The DOE Youden plot is a specialized Youden plot used in the design of experiments. In particular, it is useful for full and fractional designs. Related TechniquesScatter Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Youden Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.30. Weibull Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.30.%20Weibull%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Graphical Check To See If Data Come From a Population That Would Be Fit by a Weibull DistributionThe Weibull plot (Nelson 1982) is a graphical technique for determining if a data set comes from a population that would logically be fit by a 2-parameter Weibull distribution (the location is assumed to be zero). The Weibull plot has special scales that are designed so that if the data do in fact follow a Weibull distribution, the points will be linear (or nearly linear). The least squares fit of this line yields estimates for the shape and scale parameters of the Weibull distribution (the location is assumed to be zero). Specifically, the shape parameter is the reciprocal of the slope of the fitted line and the scale parameter is the exponent of the intercept of the fitted line.The Weibull distribution also has the property that the scale parameter falls at the 63.2% point irrespective of the value of the shape parameter. The plot shows a horizontal line at this 63.2% point and a vertical line where the horizontal line intersects the least squares fitted line. This vertical line shows the value of scale parameter. Sample Plot This Weibull plot shows that: 1. the assumption of a Weibull distribution is reasonable; 2. the scale parameter estimate is computed to be 33.32; 3. the shape parameter estimate is computed to be 5.28; and 4. there are no outliers. Note that the values on the x-axis (“0”, “1”, and “2”) are the exponents. These actually denote the value 100 = 1, 101 = 10, and 102 = 100. Definition:Weibull Cumulative Probability Versus LN(Ordered Response) The Weibull plot is formed by: • Vertical axis: Weibull cumulative probability expressed as a percentage • Horizontal axis: ordered failure times (in a LOG10 scale) The vertical scale is ln(-ln(1-p)) where p=(i-0.3)/(n+0.4) and i is the rank of the observation. This scale is chosen in order to linearize the resulting plot for Weibull data. QuestionsThe Weibull plot can be used to answer the following questions: 1. Do the data follow a 2-parameter Weibull distribution? 2. What is the best estimate of the shape parameter for the 2-parameter Weibull distribution? 3. What is the best estimate of the scale (= variation) parameter for the 2-parameter Weibull distribution? Importance:Check Distributional Assumptions Many statistical analyses, particularly in the field of reliability, are based on the assumption that the data follow a Weibull distribution. If the analysis assumes the data follow a Weibull distribution, it is important to verify this assumption and, if verified, find good estimates of the Weibull parameters. Related Techniques Weibull Probability Plot Weibull PPCC Plot Weibull Hazard Plot The Weibull probability plot (in conjunction with the Weibull PPCC plot), the Weibull hazard plot, and the Weibull plot are all similar techniques that can be used for assessing the adequacy of the Weibull distribution as a model for the data, and additionally providing estimation for the shape, scale, or location parameters.The Weibull hazard plot and Weibull plot are designed to handle censored data (which the Weibull probability plot does not).]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Weibull Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.32. 4-Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.32.%204-Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check Underlying Statistical AssumptionsThe 4-plot is a collection of 4 specific EDA graphical techniques whose purpose is to test the assumptions that underlie most measurement processes. A 4-plot consists of a 1. run sequence plot; 2. lag plot; 3. histogram; 4. normal probability plot. If the 4 underlying assumptions of a typical measurement process hold, then the above 4 plots will have a characteristic appearance (see the normal random numbers case study below); if any of the underlying assumptions fail to hold, then it will be revealed by an anomalous appearance in one or more of the plots. Several commonly encountered situations are demonstrated in the case studies below.Although the 4-plot has an obvious use for univariate and time series data, its usefulness extends far beyond that. Many statisticalmodels of the form Yi=f(X1,...,Xk)+Ei have the same underlying assumptions for the error term. That is, no matter how complicated the functional fit, the assumptions on the underlying error term are still the same. The 4-plot can and should be routinely applied to the residuals when fitting models regardless of whether the model is simple or complicated. Sample Plot: Process Has Fixed Location, Fixed Variation, Non-Random (Oscillatory), Non-Normal U-Shaped Distribution, and Has 3 Outliers. This 4-plot reveals the following: 1. the fixed location assumption is justified as shown by the run sequence plot in the upper left corner. 2. the fixed variation assumption is justified as shown by the run sequence plot in the upper left corner. 3. the randomness assumption is violated as shown by the non-random (oscillatory) lag plot in the upper right corner. 4. the assumption of a common, normal distribution is violated as shown by the histogram in the lower left corner and the normal probability plot in the lower right corner. The distribution is non-normal and is a U-shaped distribution. 5. there are several outliers apparent in the lag plot in the upper right corner. Definition: Run Sequence Plot; Lag Plot; Histogram; Normal Probability Plot The 4-plot consists of the following: 1. Run sequence plot to test fixed location and variation. ○ Vertically: Yi ○ Horizontally: i 2. Lag Plot to test randomness. ○ Vertically: Yi ○ Horizontally: Yi-1 3. Histogram to test (normal) distribution. ○ Vertically: Counts ○ Horizontally: Y 4. Normal probability plot to test normal distribution. ○ Vertically: Ordered Yi ○ Horizontally: Theoretical values from a normal N(0,1) distribution for ordered Yi Questions4-plots can provide answers to many questions: 1. Is the process in-control, stable, and predictable? 2. Is the process drifting with respect to location? 3. Is the process drifting with respect to variation? 4. Are the data random? 5. Is an observation related to an adjacent observation? 6. If the data are a time series, is is white noise? 7. If the data are a time series and not white noise, is it sinusoidal, autoregressive, etc.? 8. If the data are non-random, what is a better model? 9. Does the process follow a normal distribution? 10. If non-normal, what distribution does the process follow? 11. Is the model Yi=A0+Ei valid and sufficient? 12. If the default model is insufficient, what is a better model? 13. Is the formula sY¯=s/N−−√ valid? 14. Is the sample mean a good estimator of the process location? 15. If not, what would be a better estimator? 16. Are there any outliers? Importance: Testing Underlying Assumptions Helps Ensure the Validity of the Final Scientific and Engineering ConclusionsThere are 4 assumptions that typically underlie all measurement processes; namely, that the data from the process at hand “behave like”: 1. random drawings; 2. from a fixed distribution; 3. with that distribution having a fixed location; and 4. with that distribution having fixed variation. Predictability is an all-important goal in science and engineering. If the above 4 assumptions hold, then we have achieved probabilistic predictability–the ability to make probability statements not only about the process in the past, but also about the process in the future. In short, such processes are said to be “statistically in control”. If the 4 assumptions do not hold, then we have a process that is drifting (with respect to location, variation, or distribution), is unpredictable, and is out of control. A simple characterization of such processes by a location estimate, a variation estimate, or a distribution “estimate” inevitably leads to optimistic and grossly invalid engineering conclusions. Inasmuch as the validity of the final scientific and engineering conclusions is inextricably linked to the validity of these same 4 underlying assumptions, it naturally follows that there is a real necessity for all 4 assumptions to be routinely tested. The 4-plot (run sequence plot, lag plot, histogram, and normal probability plot) is seen as a simple, efficient, and powerful way of carrying out this routine checking. Interpretation:Flat, Equi-Banded, Random, Bell-Shaped, and LinearOf the 4 underlying assumptions: 1. If the fixed location assumption holds, then the run sequence plot will be flat and non-drifting. 2. If the fixed variation assumption holds, then the vertical spread in the run sequence plot will be approximately the same over the entire horizontal axis. 3. If the randomness assumption holds, then the lag plot will be structureless and random. 4. If the fixed distribution assumption holds (in particular, if the fixed normal distribution assumption holds), then the histogram will be bell-shaped and the normal probability plot will be approximatelylinear. If all 4 of the assumptions hold, then the process is “statistically in control”. In practice, many processes fall short of achieving this ideal. Related Techniques Run Sequence Plot Lag Plot Histogram Normal Probability Plot Autocorrelation Plot Spectral Plot PPCC Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>4-Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.29. Star Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.29.%20Star%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Display Multivariate DataThe star plot (Chambers 1983) is a method of displaying multivariate data. Each star represents a single observation. Typically, star plots are generated in a multi-plot format with many stars on each page and each star representing one observation. Star plots are used to examine the relative values for a single data point (e.g., point 3 is large for variables 2 and 4, small for variables 1, 3, 5, and 6) and to locate similar points or dissimilar points. Sample Plot The plot below contains the star plots of 16 cars. The data file actually contains 74 cars, but we restrict the plot to what can reasonably be shown on one page. The variable list for the sample star plot is 1. Price 2. Mileage (MPG) 3. 1978 Repair Record (1 = Worst, 5 = Best) 4. 1977 Repair Record (1 = Worst, 5 = Best) 5. Headroom 6. Rear Seat Room 7. Trunk Space 8. Weight 9. 9 10. Length We can look at these plots individually or we can use them to identify clusters of cars with similar features. For example, we can look at the star plot of the Cadillac Seville and see that it is one of the most expensive cars, gets below average (but not among the worst) gas mileage, has an average repair record, and has average-to-above-average roominess and size. We can then compare the Cadillac models (the last three plots) with the AMC models (the first three plots). This comparison shows distinct patterns. The AMC models tend to be inexpensive, have below average gas mileage, and are small in both height and weight and in roominess. The Cadillac models are expensive, have poor gas mileage, and are large in both size and roominess. DefinitionThe star plot consists of a sequence of equi-angular spokes, called radii, with each spoke representing one of the variables. The data length of a spoke is proportional to the magnitude of the variable for the data point relative to the maximum magnitude of the variable across all data points. A line is drawn connecting the data values for each spoke. This gives the plot a star-like appearance and the origin of the name of this plot. QuestionsThe star plot can be used to answer the following questions: 1. What variables are dominant for a given observation? 2. Which observations are most similar, i.e., are there clusters of observations? 3. Are there outliers? Weakness in TechniqueStar plots are helpful for small-to-moderate-sized multivariate data sets. Their primary weakness is that their effectiveness is limited to data sets with less than a few hundred points. After that, they tend to be overwhelming. Graphical techniques suited for large data sets are discussed by Scott. Related TechniquesAlternative ways to plot multivariate data are discussed in Chambers, du Toit, and Everitt.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Star Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.28. Standard Deviation Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.28.%20Standard%20Deviation%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Detect Changes in Scale Between GroupsStandard deviation plots are used to see if the standard deviation varies between different groups of the data. The grouping is determined by the analyst. In most cases, the data provide a specific grouping variable. For example, the groups may be the levels of a factor variable. In the sample plot below, the months of the year provide the grouping. Standard deviation plots can be used with ungrouped data to determine if the standard deviation is changing over time. In this case, the data are broken into an arbitrary number of equal-sized groups. For example, a data series with 400 points can be divided into 10 groups of 40 points each. A standard deviation plot can then be generated with these groups to see if the standard deviation is increasing or decreasing over time. Although the standard deviation is the most commonly used measure of scale, the same concept applies to other measures of scale. For example, instead of plotting the standard deviation of each group, the median absolute deviation or theaverage absolute deviation might be plotted instead. This might be done if there were significant outliers in the data and a more robust measure of scale than the standard deviation was desired. Standard deviation plots are typically used in conjunction with mean plots. The mean plot would be used to check for shifts in location while the standard deviation plot would be used to check for shifts in scale. Sample Plot This sample standard deviation plot shows 1. there is a shift in variation; 2. greatest variation is during the summer months. Definition:Group Standard Deviations Versus Group ID Standard deviation plots are formed by: • Vertical axis: Group standard deviations • Horizontal axis: Group identifier A reference line is plotted at the overall standard deviation. QuestionsThe standard deviation plot can be used to answer the following questions. 1. Are there any shifts in variation? 2. What is the magnitude of the shifts in variation? 3. Is there a distinct pattern in the shifts in variation? Importance: Checking AssumptionsA common assumption in 1-factor analyses is that of equal variances. That is, the variance is the same for different levels of the factor variable. The standard deviation plot provides a graphical check for that assumption. A common assumption for univariate data is that the variance is constant. By grouping the data into equi-sized intervals, the standard deviation plot can provide a graphical test of this assumption. Related Techniques Mean Plot DOE Standard Deviation Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Standard Deviation Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.25 Run-Sequence Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.25%20Run-Sequence%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check for Shifts in Location and Scale and OutliersRun sequence plots (Chambers 1983) are an easy way to graphically summarize a univariate data set. A common assumption of univariate data sets is that they behave like: 1. random drawings; 2. from a fixed distribution; 3. with a common location; and 4. with a common scale. With run sequence plots, shifts in location and scale are typically quite evident. Also, outliers can easily be detected. Sample Plot: Last Third of Data Shows a Shift of Location This sample run sequence plot shows that the location shifts up for the last third of the data. Definition:y(i) Versus i Run sequence plots are formed by: • Vertical axis: Response variable Yi • Horizontal axis: Index i (i = 1, 2, 3, ... ) QuestionsThe run sequence plot can be used to answer the following questions 1. Are there any shifts in location? 2. Are there any shifts in variation? 3. Are there any outliers? The run sequence plot can also give the analyst an excellent feel for the data. Importance: Check Univariate AssumptionsFor univariate data, the default model is Y = constant + error where the error is assumed to be random, from a fixed distribution, and with constant location and scale. The validity of this model depends on the validity of these assumptions. The run sequence plot is useful for checking for constant location and scale. Even for more complex models, the assumptions on the error term are still often the same. That is, a run sequence plot of the residuals (even from very complex models) is still vital for checking for outliers and for detecting shifts in location and scale. Related Techniques Scatter Plot Histogram Autocorrelation Plot Lag Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Run-Sequence Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.27. Spectral Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.27.%20Spectral%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Examine Cyclic StructureA spectral plot ( Jenkins and Watts 1968 orBloomfield 1976) is a graphical technique for examining cyclic structure in the frequency domain. It is a smoothed Fourier transform of the autocovariance function. The frequency is measured in cycles per unit time where unit time is defined to be the distance between 2 points. A frequency of 0 corresponds to an infinite cycle while a frequency of 0.5 corresponds to a cycle of 2 data points. Equi-spaced time series are inherently limited to detecting frequencies between 0 and 0.5. Trends should typically be removed from the time series before applying the spectral plot. Trends can be detected from a run sequence plot. Trends are typically removed by differencing the series or by fitting a straight line (or some other polynomial curve) and applying the spectral analysis to the residuals. Spectral plots are often used to find a starting value for the frequency, ω, in the sinusoidal model Yi=C+αsin(2πωti+ϕ)+Ei See the beam deflection case study for an example of this. Sample Plot This spectral plot shows one dominant frequency of approximately 0.3 cycles per observation. Definition:Variance Versus Frequency The spectral plot is formed by: • Vertical axis: Smoothed variance (power) • Horizontal axis: Frequency (cycles per observation) The computations for generating the smoothed variances can be involved and are not discussed further here. The details can be found in the Jenkins and Bloomfield references and in most texts that discuss the frequency analysis of time series. QuestionsThe spectral plot can be used to answer the following questions: 1. How many cyclic components are there? 2. Is there a dominant cyclic frequency? 3. If there is a dominant cyclic frequency, what is it? Importance: Check Cyclic Behavior of Time SeriesThe spectral plot is the primary technique for assessing the cyclic nature of univariate time series in the frequency domain. It is almost always the second plot (after a run sequence plot) generated in a frequency domain analysis of a time series. Examples1. Random (= White Noise) 2. Strong autocorrelation and autoregressive model 3. Sinusoidal model Related Techniques Autocorrelation Plot Complex Demodulation Amplitude Plot Complex Demodulation Phase Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Spectral Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.26. Scatter Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.26.%20Scatter%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check for RelationshipA scatter plot (Chambers 1983) reveals relationships or association between two variables. Such relationships manifest themselves by any non-random structure in the plot. Various common types of patterns are demonstrated in the examples. Sample Plot:Linear Relationship Between Variables Y and X This sample plot reveals a linear relationship between the two variables indicating that alinear regression model might be appropriate. Definition:Y Versus X A scatter plot is a plot of the values of Yversus the corresponding values of X: • Vertical axis: variable Y--usually the response variable • Horizontal axis: variable X--usually some variable we suspect may ber related to the response QuestionsScatter plots can provide answers to the following questions: 1. Are variables X and Y related? 2. Are variables X and Y linearly related? 3. Are variables X and Y non-linearly related? 4. Does the variation in Y change depending onX? 5. Are there outliers? Examples1. No relationship 2. Strong linear (positive correlation) 3. Strong linear (negative correlation) 4. Exact linear (positive correlation) 5. Quadratic relationship 6. Exponential relationship 7. Sinusoidal relationship (damped) 8. Variation of Y doesn&apos;t depend on X(homoscedastic) 9. Variation of Y does depend on X(heteroscedastic) 10. Outlier Combining Scatter PlotsScatter plots can also be combined in multiple plots per page to help understand higher-level structure in data sets with more than two variables. The scatterplot matrix generates all pairwise scatter plots on a single page. The conditioning plot, also called a co-plot or subset plot, generates scatter plots of Y versus X dependent on the value of a third variable. Causality Is Not Proved By AssociationThe scatter plot uncovers relationships in data. “Relationships” means that there is some structured association (linear, quadratic, etc.) between X and Y. Note, however, that even though causality implies association association does NOT imply causality. Scatter plots are a useful diagnostic tool for determining association, but if such association exists, the plot may or may not suggest an underlying cause-and-effect mechanism. A scatter plot can never “prove” cause and effect–it is ultimately only the researcher (relying on the underlying science/engineering) who can conclude that causality actually exists. AppearanceThe most popular rendition of a scatter plot is 1. some plot character (e.g., X) at the data points, and 2. no line connecting data points. Other scatter plot format variants include 1. an optional plot character (e.g, X) at the data points, but 2. a solid line connecting data points. In both cases, the resulting plot is referred to as a scatter plot, although the former (discrete and disconnected) is the author’s personal preference since nothing makes it onto the screen except the data–there are no interpolative artifacts to bias the interpretation. Related Techniques Run Sequence Plot Box Plot Block Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Scatter Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.23. Probability Plot Correlation Coefficient Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.23.%20Probability%20Plot%20Correlation%20Coefficient%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Graphical Technique for Finding the Shape Parameter of a Distributional Family that Best Fits aData Set The probability plot correlation coefficient (PPCC) plot (Filliben 1975) is a graphical technique for identifying the shape parameterfor a distributional family that best describes the data set. This technique is appropriate for families, such as the Weibull, that are defined by a single shape parameter and location and scale parameters, and it is not appropriate for distributions, such as the normal, that are defined only by location and scale parameters. The PPCC plot is generated as follows. For a series of values for the shape parameter, the correlation coefficient is computed for theprobability plot associated with a given value of the shape parameter. These correlation coefficients are plotted against their corresponding shape parameters. The maximum correlation coefficient corresponds to the optimal value of the shape parameter. For better precision, two iterations of the PPCC plot can be generated; the first is for finding the right neighborhood and the second is for fine tuning the estimate. The PPCC plot is used first to find a good value of the shape parameter. The probability plot is then generated to find estimates of the location and scale parameters and in addition to provide a graphical assessment of the adequacy of the distributional fit. Compare DistributionsIn addition to finding a good choice for estimating the shape parameter of a given distribution, the PPCC plot can be useful in deciding which distributional family is most appropriate. For example, given a set of reliabilty data, you might generate PPCC plots for a Weibull, lognormal, gamma, and inverse Gaussian distributions, and possibly others, on a single page. This one page would show the best value for the shape parameter for several distributions and would additionally indicate which of these distributional families provides the best fit (as measured by the maximum probability plot correlation coefficient). That is, if the maximum PPCC value for the Weibull is 0.99 and only 0.94 for the lognormal, then we could reasonably conclude that the Weibull family is the better choice. Tukey-Lambda PPCC Plot for Symmetric DistributionsThe Tukey Lambda PPCC plot, with shape parameter λ, is particularly useful for symmetric distributions. It indicates whether a distribution is short or long tailed and it can further indicate several common distributions. Specifically, 1. λ = -1: distribution is approximately Cauchy 2. λ = 0: distribution is exactly logistic 3. λ = 0.14: distribution is approximately normal 4. λ = 0.5: distribution is U-shaped 5. λ = 1: distribution is exactly uniform If the Tukey Lambda PPCC plot gives a maximum value near 0.14, we can reasonably conclude that the normal distribution is a good model for the data. If the maximum value is less than 0.14, a long-tailed distribution such as the double exponential or logistic would be a better choice. If the maximum value is near -1, this implies the selection of very long-tailed distribution, such as the Cauchy. If the maximum value is greater than 0.14, this implies a short-tailed distribution such as the Beta or uniform. The Tukey-Lambda PPCC plot is used to suggest an appropriate distribution. You should follow-up with PPCC and probability plots of the appropriate alternatives. Use Judgement When Selecting An Appropriate Distributional FamilyWhen comparing distributional models, do not simply choose the one with the maximum PPCC value. In many cases, several distributional fits provide comparable PPCC values. For example, a lognormal and Weibull may both fit a given set of reliability data quite well. Typically, we would consider the complexity of the distribution. That is, a simpler distribution with a marginally smaller PPCC value may be preferred over a more complex distribution. Likewise, there may be theoretical justification in terms of the underlying scientific model for preferring a distribution with a marginally smaller PPCC value in some cases. In other cases, we may not need to know if the distributional model is optimal, only that it is adequate for our purposes. That is, we may be able to use techniques designed for normally distributed data even if other distributions fit the data somewhat better. Sample Plot The following is a PPCC plot of 100 normal random numbers. The maximum value of the correlation coefficient = 0.997 at λ = 0.099. This PPCC plot shows that: 1. the best-fit symmetric distribution is nearly normal; 2. the data are not long tailed; 3. the sample mean would be an appropriate estimator of location. We can follow-up this PPCC plot with a normal probability plot to verify the normality model for the data. Definition:The PPCC plot is formed by: • Vertical axis: Probability plot correlation coefficient; • Horizontal axis: Value of shape parameter. QuestionsThe PPCC plot answers the following questions: 1. What is the best-fit member within a distributional family? 2. Does the best-fit member provide a good fit (in terms of generating a probability plot with a high correlation coefficient)? 3. Does this distributional family provide a good fit compared to other distributions? 4. How sensitive is the choice of the shape parameter? ImportanceMany statistical analyses are based on distributional assumptions about the population from which the data have been obtained. However, distributional families can have radically different shapes depending on the value of the shape parameter. Therefore, finding a reasonable choice for the shape parameter is a necessary step in the analysis. In many analyses, finding a good distributional model for the data is the primary focus of the analysis. In both of these cases, the PPCC plot is a valuable tool. Related Techniques Probability Plot Maximum Likelihood Estimation Least Squares Estimation Method of Moments Estimation]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Probability Plot Correlation Coefficient Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.21. Normal Probability Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.21.%20Normal%20Probability%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check If Data Are Approximately Normally DistributedThe normal probability plot (Chambers et al., 1983) is a graphical technique for assessing whether or not a data set is approximatelynormally distributed. The data are plotted against a theoretical normal distribution in such a way that the points should form an approximate straight line. Departures from this straight line indicate departures from normality. The normal probability plot is a special case of the probability plot. We cover the normal probability plot separately due to its importance in many applications. Sample Plot The points on this plot form a nearly linear pattern, which indicates that the normal distribution is a good model for this data set. Definition:Ordered Response Values Versus Normal Order Statistic MediansThe normal probability plot is formed by: • Vertical axis: Ordered response values • Horizontal axis: Normal order statistic medians The observations are plotted as a function of the corresponding normal order statistic medians which are defined as: Ni = G(Ui) where Ui are the uniform order statistic medians (defined below) and G is the percent point function of the normal distribution. The percent point function is the inverse of thecumulative distribution function (probability that x is less than or equal to some value). That is, given a probability, we want the corresponding x of the cumulative distribution function. The uniform order statistic medians (seeFilliben 1975) can be approximated by: Ui = 1 - Un for i = 1 Ui = (i - 0.3175)/(n + 0.365) for i = 2, 3, ..., n-1 Ui = 0.5(1/n) for i = n In addition, a straight line can be fit to the points and added as a reference line. The further the points vary from this line, the greater the indication of departures from normality. Probability plots for distributions other than the normal are computed in exactly the same way. The normal percent point function (the G) is simply replaced by the percent point function of the desired distribution. That is, a probability plot can easily be generated for any distribution for which you have the percent point function. One advantage of this method of computing probability plots is that the intercept and slope estimates of the fitted line are in fact estimates for the location and scale parameters of the distribution. Although this is not too important for the normal distribution since the location and scale are estimated by the mean and standard deviation, respectively, it can be useful for many other distributions. The correlation coefficient of the points on the normal probability plot can be compared to a table of critical values to provide a formal test of the hypothesis that the data come from a normal distribution. QuestionsThe normal probability plot is used to answer the following questions. 1. Are the data normally distributed? 2. What is the nature of the departure from normality (data skewed, shorter than expected tails, longer than expected tails)? Importance: Check Normality AssumptionThe underlying assumptions for a measurement process are that the data should behave like: 1. random drawings; 2. from a fixed distribution; 3. with fixed location; 4. with fixed scale. Probability plots are used to assess the assumption of a fixed distribution. In particular, most statistical models are of the form: response = deterministic + random where the deterministic part is the fit and the random part is error. This error component in most common statistical models is specifically assumed to be normally distributed with fixed location and scale. This is the most frequent application of normal probability plots. That is, a model is fit and a normal probability plot is generated for the residuals from the fitted model. If the residuals from the fitted model are not normally distributed, then one of the major assumptions of the model has been violated. Examples1. Data are normally distributed 2. Data have short tails 3. Data have fat tails 4. Data are skewed right Related Techniques Histogram Probability plots for other distributions (e.g., Weibull) Probability plot correlation coefficient plot (PPCC plot) Anderson-Darling Goodness-of-Fit Test Chi-Square Goodness-of-Fit Test Kolmogorov-Smirnov Goodness-of-Fit Test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Normal Probability Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.24. Quantile-Quantile Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.24.%20Quantile-Quantile%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check If Two Data Sets Can Be Fit With the Same DistributionThe quantile-quantile (q-q) plot is a graphical technique for determining if two data sets come from populations with a common distribution. A q-q plot is a plot of the quantiles of the first data set against the quantiles of the second data set. By a quantile, we mean the fraction (or percent) of points below the given value. That is, the 0.3 (or 30%) quantile is the point at which 30% percent of the data fall below and 70% fall above that value. A 45-degree reference line is also plotted. If the two sets come from a population with the same distribution, the points should fall approximately along this reference line. The greater the departure from this reference line, the greater the evidence for the conclusion that the two data sets have come from populations with different distributions. The advantages of the q-q plot are: 1. The sample sizes do not need to be equal. 2. Many distributional aspects can be simultaneously tested. For example, shifts in location, shifts in scale, changes in symmetry, and the presence of outliers can all be detected from this plot. For example, if the two data sets come from populations whose distributions differ only by a shift in location, the points should lie along a straight line that is displaced either up or down from the 45-degree reference line. The q-q plot is similar to a probability plot. For a probability plot, the quantiles for one of the data samples are replaced with the quantiles of a theoretical distribution. Sample Plot This q-q plot shows that 1. These 2 batches do not appear to have come from populations with a common distribution. 2. The batch 1 values are significantly higher than the corresponding batch 2 values. 3. The differences are increasing from values 525 to 625. Then the values for the 2 batches get closer again. Definition:Quantiles for Data Set 1 Versus Quantiles of Data Set 2 The q-q plot is formed by: • Vertical axis: Estimated quantiles from data set 1 • Horizontal axis: Estimated quantiles from data set 2 Both axes are in units of their respective data sets. That is, the actual quantile level is not plotted. For a given point on the q-q plot, we know that the quantile level is the same for both points, but not what that quantile level actually is. If the data sets have the same size, the q-q plot is essentially a plot of sorted data set 1 against sorted data set 2. If the data sets are not of equal size, the quantiles are usually picked to correspond to the sorted values from the smaller data set and then the quantiles for the larger data set are interpolated. QuestionsThe q-q plot is used to answer the following questions: • Do two data sets come from populations with a common distribution? • Do two data sets have common location and scale? • Do two data sets have similar distributional shapes? • Do two data sets have similar tail behavior? Importance: Check for Common DistributionWhen there are two data samples, it is often desirable to know if the assumption of a common distribution is justified. If so, then location and scale estimators can pool both data sets to obtain estimates of the common location and scale. If two samples do differ, it is also useful to gain some understanding of the differences. The q-q plot can provide more insight into the nature of the difference than analytical methods such as the chi-square and Kolmogorov-Smirnov 2-sample tests. Related Techniques Bihistogram T Test F Test 2-Sample Chi-Square Test 2-Sample Kolmogorov-Smirnov Test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Quantile-Quantile Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.22. Probability Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.22.%20Probability%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check If Data Follow a Given DistributionThe probability plot (Chambers et al., 1983) is a graphical technique for assessing whether or not a data set follows a given distribution such as the normal or Weibull. The data are plotted against a theoretical distribution in such a way that the points should form approximately a straight line. Departures from this straight line indicate departures from the specified distribution. The correlation coefficient associated with the linear fit to the data in the probability plot is a measure of the goodness of the fit. Estimates of the location and scale parametersof the distribution are given by the intercept and slope. Probability plots can be generated for several competing distributions to see which provides the best fit, and the probability plot generating the highest correlation coefficient is the best choice since it generates the straightest probability plot. For distributions with shape parameters (not counting location and scale parameters), the shape parameters must be known in order to generate the probability plot. For distributions with a single shape parameter, the probability plot correlation coefficient(PPCC) plot provides an excellent method for estimating the shape parameter. We cover the special case of the normal probability plot separately due to its importance in many statistical applications. Sample Plot This data is a set of 500 Weibull random numbers with a shape parameter = 2, location parameter = 0, and scale parameter = 1. The Weibull probability plot indicates that the Weibull distribution does in fact fit these data well. Definition:Ordered Response Values Versus Order Statistic Medians for the Given DistributionThe probability plot is formed by: • Vertical axis: Ordered response values • Horizontal axis: Order statistic medians for the given distribution The order statistic medians (see Filliben 1975)can be approximated by: Ni = G(Ui) where Ui are the uniform order statistic medians (defined below) and G is the percent point function for the desired distribution. The percent point function is the inverse of the cumulative distribution function(probability that x is less than or equal to some value). That is, given a probability, we want the corresponding x of the cumulative distribution function. The uniform order statistic medians are defined as: mi = 1 - mn for i = 1 mi = (i - 0.3175)/(n + 0.365) for i = 2, 3, ..., n-1 mi = 0.5(1/n) for i = n In addition, a straight line can be fit to the points and added as a reference line. The further the points vary from this line, the greater the indication of a departure from the specified distribution. This definition implies that a probability plot can be easily generated for any distribution for which the percent point function can be computed. One advantage of this method of computing proability plots is that the intercept and slope estimates of the fitted line are in fact estimates for the location and scale parameters of the distribution. Although this is not too important for the normal distribution (the location and scale are estimated by the mean and standard deviation, respectively), it can be useful for many other distributions. QuestionsThe probability plot is used to answer the following questions: • Does a given distribution, such as the Weibull, provide a good fit to my data? • What distribution best fits my data? • What are good estimates for the location and scale parameters of the chosen distribution? Importance: Check distributional assumptionThe discussion for the normal probability plotcovers the use of probability plots for checking the fixed distribution assumption. Some statistical models assume data have come from a population with a specific type of distribution. For example, in reliability applications, the Weibull, lognormal, and exponential are commonly used distributional models. Probability plots can be useful for checking this distributional assumption. Related Techniques Histogram Probability Plot Correlation Coefficient (PPCC) Plot Hazard Plot Quantile-Quantile Plot Anderson-Darling Goodness of Fit Chi-Square Goodness of Fit Kolmogorov-Smirnov Goodness of Fit]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Probability Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.20. Mean Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.20.%20Mean%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Detect changes in location between groupsMean plots are used to see if the mean varies between different groups of the data. The grouping is determined by the analyst. In most cases, the data set contains a specific grouping variable. For example, the groups may be the levels of a factor variable. In the sample plot below, the months of the year provide the grouping. Mean plots can be used with ungrouped data to determine if the mean is changing over time. In this case, the data are split into an arbitrary number of equal-sized groups. For example, a data series with 400 points can be divided into 10 groups of 40 points each. A mean plot can then be generated with these groups to see if the mean is increasing or decreasing over time. Although the mean is the most commonly used measure of location, the same concept applies to other measures of location. For example, instead of plotting the mean of each group, the median or the trimmed mean might be plotted instead. This might be done if there were significant outliers in the data and a more robust measure of location than the mean was desired. Mean plots are typically used in conjunction with standard deviation plots. The mean plot checks for shifts in location while the standard deviation plot checks for shifts in scale. Sample Plot This sample mean plot shows a shift of location after the 6th month. Definition:Group Means Versus Group ID Mean plots are formed by: • Vertical axis: Group mean • Horizontal axis: Group identifier A reference line is plotted at the overall mean. QuestionsThe mean plot can be used to answer the following questions. 1. Are there any shifts in location? 2. What is the magnitude of the shifts in location? 3. Is there a distinct pattern in the shifts in location? Importance: Checking AssumptionsA common assumption in 1-factor analyses is that of constant location. That is, the location is the same for different levels of the factor variable. The mean plot provides a graphical check for that assumption. A common assumption for univariate data is that the location is constant. By grouping the data into equal intervals, the mean plot can provide a graphical test of this assumption. Related Techniques Standard Deviation Plot DOE Mean Plot Box Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Mean Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.19. Linear Residual Standard Deviation Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.19.%20Linear%20Residual%20Standard%20Deviation%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Detect Changes in Linear Residual Standard Deviation Between GroupsLinear residual standard deviation (RESSD) plots are used to graphically assess whether or not linear fits are consistent across groups. That is, if your data have groups, you may want to know if a single fit can be used across all the groups or whether separate fits are required for each group. The residual standard deviation is a goodness-of-fit measure. That is, the smaller the residual standard deviation, the closer is the fit to the data. Linear RESSD plots are typically used in conjunction with linear intercept and linear slope plots. The linear intercept and slope plots convey whether or not the fits are consistent across groups while the linear RESSD plot conveys whether the adequacy of the fit is consistent across groups. In some cases you might not have groups. Instead, you have different data sets and you want to know if the same fit can be adequately applied to each of the data sets. In this case, simply think of each distinct data set as a group and apply the linear RESSD plot as for groups. Sample Plot This linear RESSD plot shows that the residual standard deviations from a linear fit are about 0.0025 for all the groups. Definition:Group Residual Standard Deviation Versus Group ID Linear RESSD plots are formed by: • Vertical axis: Group residual standard deviations from linear fits • Horizontal axis: Group identifier A reference line is plotted at the residual standard deviation from a linear fit using all the data. This reference line will typically be much greater than any of the individual residual standard deviations. QuestionsThe linear RESSD plot can be used to answer the following questions. 1. Is the residual standard deviation from a linear fit constant across groups? 2. If the residual standard deviations vary, is there a discernible pattern across the groups? Importance: Checking Group HomogeneityFor grouped data, it may be important to know whether the different groups are homogeneous (i.e., similar) or heterogeneous (i.e., different). Linear RESSD plots help answer this question in the context of linear fitting. Related Techniques Linear Intercept Plot Linear Slope Plot Linear Correlation Plot Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Linear Residual Standard Deviation Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.18. Linear Slope Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.18.%20Linear%20Slope%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Detect changes in linear slopes between groupsLinear slope plots are used to graphically assess whether or not linear fits are consistent across groups. That is, if your data have groups, you may want to know if a single fit can be used across all the groups or whether separate fits are required for each group. Linear slope plots are typically used in conjunction with linear intercept and linear residual standard deviation plots. In some cases you might not have groups. Instead, you have different data sets and you want to know if the same fit can be adequately applied to each of the data sets. In this case, simply think of each distinct data set as a group and apply the linear slope plot as for groups. Sample Plot This linear slope plot shows that the slopes are about 0.174 (plus or minus 0.002) for all groups. There does not appear to be a pattern in the variation of the slopes. This implies that a single fit may be adequate. Definition:Group Slopes Versus Group ID Linear slope plots are formed by: • Vertical axis: Group slopes from linear fits • Horizontal axis: Group identifier A reference line is plotted at the slope from a linear fit using all the data. QuestionsThe linear slope plot can be used to answer the following questions. 1. Do you get the same slope across groups for linear fits? 2. If the slopes differ, is there a discernible pattern in the slopes? Importance: Checking Group HomogeneityFor grouped data, it may be important to know whether the different groups are homogeneous (i.e., similar) or heterogeneous (i.e., different). Linear slope plots help answer this question in the context of linear fitting. Related Techniques Linear Intercept Plot Linear Correlation Plot Linear Residual Standard Deviation Plot Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Linear Slope Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.17. Linear Intercept Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.17.%20Linear%20Intercept%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Detect changes in linear intercepts between groupsLinear intercept plots are used to graphically assess whether or not linear fits are consistent across groups. That is, if your data have groups, you may want to know if a single fit can be used across all the groups or whether separate fits are required for each group. Linear intercept plots are typically used in conjunction with linear slope and linear residual standard deviation plots. In some cases you might not have groups. Instead, you have different data sets and you want to know if the same fit can be adequately applied to each of the data sets. In this case, simply think of each distinct data set as a group and apply the linear intercept plot as for groups. Sample Plot This linear intercept plot shows that there is a shift in intercepts. Specifically, the first three intercepts are lower than the intercepts for the other groups. Note that these are small differences in the intercepts. Definition:Group Intercepts Versus Group ID Linear intercept plots are formed by: • Vertical axis: Group intercepts from linear fits • Horizontal axis: Group identifier A reference line is plotted at the intercept from a linear fit using all the data. QuestionsThe linear intercept plot can be used to answer the following questions. 1. Is the intercept from linear fits relatively constant across groups? 2. If the intercepts vary across groups, is there a discernible pattern? Importance:Checking Group HomogeneityFor grouped data, it may be important to know whether the different groups are homogeneous (i.e., similar) or heterogeneous (i.e., different). Linear intercept plots help answer this question in the context of linear fitting. Related Techniques Linear Correlation Plot Linear Slope Plot Linear Residual Standard Deviation Plot Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Linear Intercept Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.16. Linear Correlation Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.16.%20Linear%20Correlation%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Detect changes in correlation between groupsLinear correlation plots are used to assess whether or not correlations are consistent across groups. That is, if your data is in groups, you may want to know if a single correlation can be used across all the groups or whether separate correlations are required for each group. Linear correlation plots are often used in conjunction with linear slope, linear intercept, and linear residual standard deviation plots. A linear correlation plot could be generated intially to see if linear fitting would be a fruitful direction. If the correlations are high, this implies it is worthwhile to continue with the linear slope, intercept, and residual standard deviation plots. If the correlations are weak, a different model needs to be pursued. In some cases, you might not have groups. Instead you may have different data sets and you want to know if the same correlation can be adequately applied to each of the data sets. In this case, simply think of each distinct data set as a group and apply the linear slope plot as for groups. Sample Plot This linear correlation plot shows that the correlations are high for all groups. This implies that linear fits could provide a good model for each of these groups. Definition:Group Correlations Versus Group ID Linear correlation plots are formed by: • Vertical axis: Group correlations • Horizontal axis: Group identifier A reference line is plotted at the correlation between the full data sets. QuestionsThe linear correlation plot can be used to answer the following questions. 1. Are there linear relationships across groups? 2. Are the strength of the linear relationships relatively constant across the groups? Importance:Checking Group HomogeneityFor grouped data, it may be important to know whether the different groups are homogeneous (i.e., similar) or heterogeneous (i.e., different). Linear correlation plots help answer this question in the context of linear fitting. Related Techniques Linear Intercept Plot Linear Slope Plot Linear Residual Standard Deviation Plot Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Linear Correlation Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.15. Lag Plot]]></title>
    <url>%2F2018%2F03%2F12%2FEDA_1.3.3.15.%20Lag%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose: Check for randomnessA lag plot checks whether a data set or time series is random or not. Random data should not exhibit any identifiable structure in the lag plot. Non-random structure in the lag plot indicates that the underlying data are not random. Several common patterns for lag plots are shown in the examples below. Sample Plot This sample lag plot exhibits a linear pattern. This shows that the data are strongly non-random and further suggests that an autoregressive model might be appropriate. DefinitionA lag is a fixed time displacement. For example, given a data set Y1, Y2 …, Yn, Y2 and Y7 have lag 5 since 7 - 2 = 5. Lag plots can be generated for any arbitrary lag, although the most commonly used lag is 1.A plot of lag 1 is a plot of the values of Yiversus Yi-1 • Vertical axis: Yi for all i • Horizontal axis: Yi-1 for all i QuestionsLag plots can provide answers to the following questions: 1. Are the data random? 2. Is there serial correlation in the data? 3. What is a suitable model for the data? 4. Are there outliers in the data? ImportanceInasmuch as randomness is an underlying assumption for most statistical estimation and testing techniques, the lag plot should be a routine tool for researchers. Examples• Random (White Noise) • Weak autocorrelation • Strong autocorrelation and autoregressive model • Sinusoidal model and outliers Related Techniques Autocorrelation Plot Spectrum Runs Test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Lag Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip和pip3的共存问题]]></title>
    <url>%2F2018%2F03%2F05%2Fpip%E5%92%8Cpip3%E7%9A%84%E5%85%B1%E5%AD%98%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题在win电脑同时装了python2和python3，为了同时共存，将python3环境下的python.exe改为了python3.exe。但是导致对应的pip3无法使用。 12PS C:\Users\maomaochong&gt; pip3Fatal error in launcher: Unable to create process using '"' 解决方案 官方的解法是什么？事实上这个问题几年以前Python社区就给出了官方解决方案，只不过国内一直没有注意到罢了。我们在安装Python3（&gt;=3.3）时，Python的安装包实际上在系统中安装了一个启动器py.exe，默认放置在文件夹C:\Windows\下面。这个启动器允许我们指定使用Python2还是Python3来运行代码（当然前提是你已经成功安装了Python2和Python3）。 使用pip当Python2和Python3同时存在于windows上时，它们对应的pip都叫pip.exe，所以不能够直接使用 pip install 命令来安装软件包。而是要使用启动器py.exe来指定pip的版本。命令如下： py -2 -m pip install XXXX-2 还是表示使用 Python2，-m pip 表示运行 pip 模块，也就是运行pip命令了。 如果是为Python3安装软件，那么命令类似的变成 py -3 -m pip install XXXX #! python2 和 # coding: utf-8 哪个写在前面？对于Python2用户还有另外一个困惑，Python2要在代码文件顶部增加一行说明，才能够在代码中使用中文。如果指明使用的Python版本也需要在文件顶部增加一行，那哪一行应该放在第一行呢？ #! python2 需要放在第一行，编码说明可以放在第二行。所以文件开头应该类似于： 12#! python2# coding: utf-8 例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586Windows PowerShell版权所有 (C) Microsoft Corporation。保留所有权利。PS C:\Users\maomaochong&gt; pip3Fatal error in launcher: Unable to create process using '"'PS C:\Users\maomaochong&gt; pipUsage: pip &lt;command&gt; [options]Commands: install Install packages. download Download packages. uninstall Uninstall packages. freeze Output installed packages in requirements format. list List installed packages. show Show information about installed packages. check Verify installed packages have compatible dependencies. search Search PyPI for packages. wheel Build wheels from your requirements. hash Compute hashes of package archives. completion A helper command used for command completion. help Show help for commands.General Options: -h, --help Show help. --isolated Run pip in an isolated mode, ignoring environment variables and user configuration. -v, --verbose Give more output. Option is additive, and can be used up to 3 times. -V, --version Show version and exit. -q, --quiet Give less output. Option is additive, and can be used up to 3 times (corresponding to WARNING, ERROR, and CRITICAL logging levels). --log &lt;path&gt; Path to a verbose appending log. --proxy &lt;proxy&gt; Specify a proxy in the form [user:passwd@]proxy.server:port. --retries &lt;retries&gt; Maximum number of retries each connection should attempt (default 5 times). --timeout &lt;sec&gt; Set the socket timeout (default 15 seconds). --exists-action &lt;action&gt; Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort. --trusted-host &lt;hostname&gt; Mark this host as trusted, even though it does not have valid or any HTTPS. --cert &lt;path&gt; Path to alternate CA bundle. --client-cert &lt;path&gt; Path to SSL client certificate, a single file containing the private key and the certificate in PEM format. --cache-dir &lt;dir&gt; Store the cache data in &lt;dir&gt;. --no-cache-dir Disable the cache. --disable-pip-version-check Don't periodically check PyPI to determine whether a new version of pip is available for download. Implied with --no-index.PS C:\Users\maomaochong&gt; py -3 -m pip install woeCollecting woe Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f5/32/ba4d592dfef45338ee04ca90c37b4f3aa345bdeafcad4dcbf654ad0b14c2/woe-0.1.4-py3-none-any.whlCollecting matplotlib&gt;=2.0.0 (from woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/9c/6ce11b82f9f7276c24711646c3ee43d40f78974ae8e0227a1d2200a44736/matplotlib-2.1.2-cp35-cp35m-win32.whl (8.5MB) 100% |████████████████████████████████| 8.5MB 81kB/sCollecting scipy&gt;=0.18.1 (from woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ab/4d/6468b15538132ffc31112945aa6672a851888b4aad03f0c4710121aff707/scipy-1.0.0-cp35-none-win32.whl (26.0MB) 100% |████████████████████████████████| 26.0MB 34kB/sCollecting pandas&gt;=0.19.2 (from woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/ae/3eacbdfaf2c47ba4a7eff5ce4e1a7d5f79d87be67d1cb186c238f3118245/pandas-0.22.0-cp35-cp35m-win32.whl (8.2MB) 100% |████████████████████████████████| 8.2MB 85kB/sCollecting numpy&gt;=1.11.3 (from woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/12/22cded1311ac12946c4ac51257000427269c115fbf544446548022d154a3/numpy-1.14.1-cp35-none-win32.whl (9.8MB) 100% |████████████████████████████████| 9.8MB 73kB/sCollecting six&gt;=1.10 (from matplotlib&gt;=2.0.0-&gt;woe) Using cached https://pypi.tuna.tsinghua.edu.cn/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whlCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 (from matplotlib&gt;=2.0.0-&gt;woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/8a/718fd7d3458f9fab8e67186b00abdd345b639976bc7fb3ae722e1b026a50/pyparsing-2.2.0-py2.py3-none-any.whl (56kB) 100% |████████████████████████████████| 61kB 273kB/sCollecting pytz (from matplotlib&gt;=2.0.0-&gt;woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/80/32e98784a8647880dedf1f6bf8e2c91b195fe18fdecc6767dcf5104598d6/pytz-2018.3-py2.py3-none-any.whl (509kB) 100% |████████████████████████████████| 512kB 203kB/sCollecting python-dateutil&gt;=2.1 (from matplotlib&gt;=2.0.0-&gt;woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/0d/7ed381ab4fe80b8ebf34411d14f253e1cf3e56e2820ffa1d8844b23859a2/python_dateutil-2.6.1-py2.py3-none-any.whl (194kB) 100% |████████████████████████████████| 194kB 194kB/sCollecting cycler&gt;=0.10 (from matplotlib&gt;=2.0.0-&gt;woe) Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whlInstalling collected packages: numpy, six, pyparsing, pytz, python-dateutil, cycler, matplotlib, scipy, pandas, woeSuccessfully installed cycler-0.10.0 matplotlib-2.1.2 numpy-1.14.1 pandas-0.22.0 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2018.3 scipy-1.0.0 six-1.11.0 woe-0.1.4You are using pip version 8.1.1, however version 9.0.1 is available.You should consider upgrading via the 'python -m pip install --upgrade pip' command. via https://www.zhihu.com/question/21653286/answer/95532074]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.14. Histogram]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.14.%20Histogram%2F</url>
    <content type="text"><![CDATA[Purpose: Summarize a Univariate Data SetThe purpose of a histogram (Chambers) is to graphically summarize the distribution of a univariate data set.The histogram graphically shows the following: 1. center (i.e., the location) of the data; 2. spread (i.e., the scale) of the data; 3. skewness of the data; 4. presence of outliers; and 5. presence of multiple modes in the data. These features provide strong indications of the proper distributional model for the data. Theprobability plot or a goodness-of-fit test can be used to verify the distributional model. The examples section shows the appearance of a number of common features revealed by histograms. Sample Plot DefinitionThe most common form of the histogram is obtained by splitting the range of the data into equal-sized bins (called classes). Then for each bin, the number of points from the data set that fall into each bin are counted. That is Vertical axis: Frequency (i.e., counts for each bin) Horizontal axis: Response variable The classes can either be defined arbitrarily by the user or via some systematic rule. A number of theoretically derived rules have been proposed by Scott (Scott 1992).The cumulative histogram is a variation of the histogram in which the vertical axis gives not just the counts for a single bin, but rather gives the counts for that bin plus all bins for smaller values of the response variable.Both the histogram and cumulative histogram have an additional variant whereby the counts are replaced by the normalized counts. The names for these variants are the relative histogram and the relative cumulative histogram.There are two common ways to normalize the counts. 1. The normalized count is the count in a class divided by the total number of observations. In this case the relative counts are normalized to sum to one (or 100 if a percentage scale is used). This is the intuitive case where the height of the histogram bar represents the proportion of the data in each class. 2. The normalized count is the count in the class divided by the number of observations times the class width. For this normalization, the area (or integral) under the histogram is equal to one. From a probabilistic point of view, this normalization results in a relative histogram that is most akin to the probability density function and a relative cumulative histogram that is most akin to the cumulative distribution function. If you want to overlay a probability density or cumulative distribution function on top of the histogram, use this normalization. Although this normalization is less intuitive (relative frequencies greater than 1 are quite permissible), it is the appropriate normalization if you are using the histogram to model a probability density function.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Histogram</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.13. DOE Standard Deviation Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.13.%20DOE%20Standard%20Deviation%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Detect Important Factors With Respect to Scale The DOE standard deviation plot is appropriate for analyzing data from a designed experiment, with respect to important factors, where the factors are at two or more levels and there are repeated values at each level. The plot shows standard deviation values for the two or more levels of each factor plotted by factor. The standard deviations for a single factor are connected by a straight line. The DOE standard deviation plot is a complement to the traditional analysis of varianceof designed experiments. This plot is typically generated for the standard deviation. However, it can also be generated for other scale statistics such as the range, the median absolute deviation, or the average absolute deviation. Sample Plot This sample DOE standard deviation plot shows that: 1. factor 1 has the greatest difference in standard deviations between factor levels; 2. factor 4 has a significantly lower average standard deviation than the average standard deviations of other factors (but the level 1 standard deviation for factor 1 is about the same as the level 1 standard deviation for factor 4); 3. for all factors, the level 1 standard deviation is smaller than the level 2 standard deviation. Definition:Response Standard Deviations Versus Factor Variables DOE standard deviation plots are formed by: Vertical axis: Standard deviation of the response variable for each level of the factor Horizontal axis: Factor variable QuestionsThe DOE standard deviation plot can be used to answer the following questions: 1. How do the standard deviations vary across factors? 2. How do the standard deviations vary within a factor? 3. Which are the most important factors with respect to scale? 4. What is the ranked list of the important factors with respect to scale? Importance:Assess Variability The goal with many designed experiments is to determine which factors are significant. This is usually determined from the means of the factor levels (which can be conveniently shown with a DOE mean plot). A secondary goal is to assess the variability of the responses both within a factor and between factors. The DOE standard deviation plot is a convenient way to do this. Related Techniques DOE scatter plot DOE mean plot Block plot Box plot Analysis of variance]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>DOE Standard Deviation Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.12. DOE Mean Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.12.%20DOE%20Mean%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Detect Important Factors With Respect to Location The DOE mean plot is appropriate for analyzing data from a designed experiment, with respect to important factors, where the factors are at two or more levels. The plot shows mean values for the two or more levels of each factor plotted by factor. The means for a single factor are connected by a straight line. The DOE mean plot is a complement to the traditional analysis of variance of designed experiments. This plot is typically generated for the mean. However, it can be generated for other location statistics such as the median. Sample Plot:Factors 4, 2, and 1 Are the Most Important Factors This sample DOE mean plot shows that: 1. factor 4 is the most important; 2. factor 2 is the second most important; 3. factor 1 is the third most important; 4. factor 7 is the fourth most important; 5. factor 6 is the fifth most important; 6. factors 3 and 5 are relatively unimportant. In summary, factors 4, 2, and 1 seem to be clearly important, factors 3 and 5 seem to be clearly unimportant, and factors 6 and 7 are borderline factors whose inclusion in any subsequent models will be determined by further analyses. Definition:Mean Response Versus Factor Variables DOE mean plots are formed by: • Vertical axis: Mean of the response variable for each level of the factor • Horizontal axis: Factor variable QuestionsThe DOE mean plot can be used to answer the following questions: 1. Which factors are important? The DOE mean plot does not provide a definitive answer to this question, but it does help categorize factors as &quot;clearly important&quot;, &quot;clearly not important&quot;, and &quot;borderline importance&quot;. 2. What is the ranking list of the important factors? Importance:Determine Significant Factors The goal of many designed experiments is to determine which factors are significant. A ranked order listing of the important factors is also often of interest. The DOE mean plot is ideally suited for answering these types of questions and we recommend its routine use in analyzing designed experiments. Extension for Interaction EffectsUsing the concept of the scatter plot matrix, the DOE mean plot can be extended to display first-order interaction effects. Specifically, if there are k factors, we create a matrix of plots with k rows and k columns. On the diagonal, the plot is simply a DOE mean plot with a single factor. For the off-diagonal plots, measurements at each level of the interaction are plotted versus level, where level is Xi times Xj and Xi is the code for the ith main effect level and Xj is the code for the jth main effect. For the common 2-level designs (i.e., each factor has two levels) the values are typically coded as -1 and 1, so the multiplied values are also -1 and 1. We then generate a DOE mean plot for this interaction variable. This plot is called a DOE interaction effects plot and an example is shown below. DOE Interaction Effects Plot This plot shows that the most significant factor is X1 and the most significant interaction is between X1 and X3. Related Techniques DOE scatter plot DOE standard deviation plot Block plot Box plot Analysis of variance]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>DOE Scatter Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.10. Contour Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.10.%20Contour%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Display 3-d surface on 2-d plot A contour plot is a graphical technique for representing a 3-dimensional surface by plotting constant z slices, called contours, on a 2-dimensional format. That is, given a value for z, lines are drawn for connecting the (x,y) coordinates where that z value occurs. The contour plot is an alternative to a 3-D surface plot. Sample Plot: This contour plot shows that the surface is symmetric and peaks in the center. DefinitionThe contour plot is formed by: Vertical axis: Independent variable 2 Horizontal axis: Independent variable 1 Lines: iso-response values The independent variables are usually restricted to a regular grid. The actual techniques for determining the correct iso-response values are rather complex and are almost always computer generated. An additional variable may be required to specify the Z values for drawing the iso-lines. Some software packages require explicit values. Other software packages will determine them automatically. If the data (or function) do not form a regular grid, you typically need to perform a 2-D interpolation to form a regular grid. QuestionsThe contour plot is used to answer the question How does Z change as a function of X and Y? Importance:Visualizing 3-dimensional data For univariate data, a run sequence plot and ahistogram are considered necessary first steps in understanding the data. For 2-dimensional data, ascatter plot is a necessary first step in understanding the data. In a similar manner, 3-dimensional data should be plotted. Small data sets, such as result from designed experiments, can typically be represented by block plots, DOE mean plots, and the like (“DOE” stands for “Design of Experiments”). For large data sets, a contour plot or a 3-D surface plot should be considered a necessary first step in understanding the data. DOE Contour PlotThe DOE contour plot is a specialized contour plot used in the design of experiments. In particular, it is useful for full and fractionaldesigns. Related Techniques3-D Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Contour Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.11. DOE Scatter Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.11.%20DOE%20Scatter%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Determine Important Factors with Respect to Location and Scale The DOE scatter plot shows the response values for each level of each factor (i.e., independent) variable. This graphically shows how the location and scale vary for both within a factor variable and between different factor variables. This graphically shows which are the important factors and can help provide a ranked list of important factors from a designed experiment. The DOE scatter plot is a complement to the traditional analyis of variance of designed experiments. DOE scatter plots are typically used in conjunction with the DOE mean plotand the DOE standard deviation plot. The DOE mean plot replaces the raw response values with mean response values while the DOE standard deviation plot replaces the raw response values with the standard deviation of the response values. There is value in generating all 3 of these plots. The DOE mean and standard deviation plots are useful in that the summary measures of location and spread stand out (they can sometimes get lost with the raw plot). However, the raw data points can reveal subtleties, such as the presence of outliers, that might get lost with the summary statistics. Sample Plot:Factors 4, 2, 3, and 7 are the Important Factors. Description of the PlotFor this sample plot, there are seven factors and each factor has two levels. For each factor, we define a distinct x coordinate for each level of the factor. For example, for factor 1, level 1 is coded as 0.8 and level 2 is coded as 1.2. The y coordinate is simply the value of the response variable. The solid horizontal line is drawn at the overall mean of the response variable. The vertical dotted lines are added for clarity. Although the plot can be drawn with an arbitrary number of levels for a factor, it is really only useful when there are two or three levels for a factor. ConclusionsThis sample DOE scatter plot shows that: 1. there does not appear to be any outliers; 2. the levels of factors 2 and 4 show distinct location differences; and 3. the levels of factor 1 show distinct scale differences. Definition:Response Values Versus Factor VariablesDOE scatter plots are formed by: • Vertical axis: Value of the response variable • Horizontal axis: Factor variable (with each level of the factor coded with a slightly offset x coordinate) QuestionsThe DOE scatter plot can be used to answer the following questions: 1. Which factors are important with respect to location and scale? 2. Are there outliers? Importance:Identify Important Factors with Respect to Location and Scale The goal of many designed experiments is to determine which factors are important with respect to location and scale. A ranked list of the important factors is also often of interest. DOE scatter, mean, and standard deviation plots show this graphically. The DOE scatter plot additionally shows if outliers may potentially be distorting the results. DOE scatter plots were designed primarily for analyzing designed experiments. However, they are useful for any type of multi-factor data (i.e., a response variable with two or more factor variables having a small number of distinct levels) whether or not the data were generated from a designed experiment. Extension for Interaction EffectsUsing the concept of the scatterplot matrix, the DOE scatter plot can be extended to display first order interaction effects. Specifically, if there are k factors, we create a matrix of plots with krows and k columns. On the diagonal, the plot is simply a DOE scatter plot with a single factor. For the off-diagonal plots, we multiply the values ofXi and Xj. For the common 2-level designs (i.e., each factor has two levels) the values are typically coded as -1 and 1, so the multiplied values are also -1 and 1. We then generate a DOE scatter plot for this interaction variable. This plot is called a DOE interaction effects plot and an example is shown below. Interpretation of the DOE Interaction Effects PlotWe can first examine the diagonal elements for the main effects. These diagonal plots show a great deal of overlap between the levels for all three factors. This indicates that location and scale effects will be relatively small. We can then examine the off-diagonal plots for the first order interaction effects. For example, the plot in the first row and second column is the interaction between factors X1 and X2. As with the main effect plots, no clear patterns are evident. Related Techniques DOE mean plot DOE standard deviation plot Block plot Box plot Analysis of variance]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>DOE Scatter Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.9. Complex Demodulation Phase Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.9.%20Complex%20Demodulation%20Phase%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Improve the estimate of frequency in sinusoidal time series modelsAs stated previously, in the frequency analysis of time series models, a common model is the sinusoidal model: Yi=C+αsin(2πωti+ϕ)+Ei In this equation, α is the amplitude, φ is the phase shift, and ω is the dominant frequency. In the above model, α and φ are constant, that is they do not vary with time ti.The complex demodulation phase plot (Granger, 1964) is used to improve the estimate of the frequency (i.e., ω) in this model. If the complex demodulation phase plot shows lines sloping from left to right, then the estimate of the frequency should be increased. If it shows lines sloping right to left, then the frequency should be decreased. If there is essentially zero slope, then the frequency estimate does not need to be modified. Sample Plot: This complex demodulation phase plot shows that: the specified demodulation frequency is incorrect; the demodulation frequency should be increased. DefinitionThe complex demodulation phase plot is formed by: Vertical axis: Phase Horizontal axis: Time The mathematical computations for the phase plot are beyond the scope of the Handbook. Consult Granger (Granger, 1964) for details. QuestionsThe complex demodulation phase plot answers the following question: Is the specified demodulation frequency correct? Importance of a Good Initial Estimate for the FrequencyThe non-linear fitting for the sinusoidal model: Yi=C+αsin(2πωti+ϕ)+Ei is usually quite sensitive to the choice of good starting values. The initial estimate of the frequency, ω, is obtained from a spectral plot. The complex demodulation phase plot is used to assess whether this estimate is adequate, and if it is not, whether it should be increased or decreased. Using the complex demodulation phase plot with the spectral plot can significantly improve the quality of the non-linear fits obtained. Related Techniques Spectral Plot Complex Demodulation Phase Plot Non-Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Complex Demodulation Phase Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.7. Box Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.7.%20Box%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Check location and variation shifts Box plots (Chambers 1983) are an excellent tool for conveying location and variation information in data sets, particularly for detecting and illustrating location and variation changes between different groups of data. Sample Plot:This box plot reveals that machine has a significant effect on energy with respect to location and possibly variation This box plot, comparing four machines for energy output, shows that machine has a significant effect on energy with respect to both location and variation. Machine 3 has the highest energy response (about 72.5); machine 4 has the least variable energy response with about 50% of its readings being within 1 energy unit. DefinitionBox plots are formed by Vertical axis: Response variable Horizontal axis: The factor of interest More specifically, we 1. Calculate the median and the quartiles (the lower quartile is the 25th percentile and the upper quartile is the 75th percentile). 2. Plot a symbol at the median (or draw a line) and draw a box (hence the name--box plot) between the lower and upper quartiles; this box represents the middle 50% of the data--the &quot;body&quot; of the data. 3. Draw a line from the lower quartile to the minimum point and another line from the upper quartile to the maximum point. Typically a symbol is drawn at these minimum and maximum points, although this is optional. Thus the box plot identifies the middle 50% of the data, the median, and the extreme points. Single or multiple box plots can be drawnA single box plot can be drawn for one batch of data with no distinct groups. Alternatively, multiple box plots can be drawn together to compare multiple data sets or to compare groups in a single data set. For a single box plot, the width of the box is arbitrary. For multiple box plots, the width of the box plot can be set proportional to the number of points in the given group or sample (some software implementations of the box plot simply set all the boxes to the same width). Box plots with fencesThere is a useful variation of the box plot that more specifically identifies outliers. To create this variation: 1. Calculate the median and the lower and upper quartiles. 2. Plot a symbol at the median and draw a box between the lower and upper quartiles. 3. Calculate the interquartile range (the difference between the upper and lower quartile) and call it IQ. 4. Calculate the following points: L1 = lower quartile - 1.5*IQ L2 = lower quartile - 3.0*IQ U1 = upper quartile + 1.5*IQ U2 = upper quartile + 3.0*IQ 5. The line from the lower quartile to the minimum is now drawn from the lower quartile to the smallest point that is greater than L1. Likewise, the line from the upper quartile to the maximum is now drawn to the largest point smaller than U1. 6. Points between L1 and L2 or between U1 and U2 are drawn as small circles. Points less than L2 or greater than U2 are drawn as large circles. QuestionsThe box plot can provide answers to the following questions: 1. Is a factor significant? 2. Does the location differ between subgroups? 3. Does the variation differ between subgroups? 4. Are there any outliers? Importance:Check the significance of a factorThe box plot is an important EDA tool for determining if a factor has a significant effect on the response with respect to either location or variation.The box plot is also an effective tool for summarizing large quantities of information. Related Techniques Mean Plot Analysis of Variance]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Box Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.8. Complex Demodulation Amplitude Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.8.%20Complex%20Demodulation%20Amplitude%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Detect Changing Amplitude in Sinusoidal Models In the frequency analysis of time series models, a common model is the sinusoidal model: Yi=C+αsin(2πωti+ϕ)+Ei In this equation, α is the amplitude, φ is the phase shift, and ω is the dominant frequency. In the above model, α and φ are constant, that is they do not vary with time, ti. The complex demodulation amplitude plot (Granger, 1964) is used to determine if the assumption of constant amplitude is justifiable. If the slope of the complex demodulation amplitude plot is not zero, then the above model is typically replaced with the model: Yi=C+αisin(2πωti+ϕ)+Ei where α^i is some type of linear model fit with standard least squares. The most common case is a linear fit, that is the model becomes Yi=C+(B0+B1∗ti)sin(2πωti+ϕ)+Ei Quadratic models are sometimes used. Higher order models are relatively rare. Sample Plot: This complex demodulation amplitude plot shows that: the amplitude is fixed at approximately 390; there is a start-up effect; and there is a change in amplitude at around x = 160 that should be investigated for an outlier. Definition:The complex demodulation amplitude plot is formed by: Vertical axis: Amplitude Horizontal axis: Time The mathematical computations for determining the amplitude are beyond the scope of the Handbook. Consult Granger (Granger, 1964) for details. QuestionsThe complex demodulation amplitude plot answers the following questions: 1. Does the amplitude change over time? 2. Are there any outliers that need to be investigated? 3. Is the amplitude different at the beginning of the series (i.e., is there a start-up effect)? Importance:Assumption Checking As stated previously, in the frequency analysis of time series models, a common model is the sinusoidal model: Yi=C+αsin(2πωti+ϕ)+EiIn this equation, α is assumed to be constant, that is it does not vary with time. It is important to check whether or not this assumption is reasonable. The complex demodulation amplitude plot can be used to verify this assumption. If the slope of this plot is essentially zero, then the assumption of constant amplitude is justified. If it is not, α should be replaced with some type of time-varying model. The most common cases are linear (B0 + B1t) and quadratic (B0 + B1t +B2*t2). Related Techniques Spectral Plot Complex Demodulation Phase Plot Non-Linear Fitting]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Complex Demodulation Amplitude Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.6. Box-Cox Normality Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.6.%20Box-Cox%20Normality%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Find transformation to normalize data Many statistical tests and intervals are based on the assumption of normality. The assumption of normality often leads to tests that are simple, mathematically tractable, and powerful compared to tests that do not make the normality assumption. Unfortunately, many real data sets are in fact not normal. However, an appropriate transformation of a data set can often yield a data set that does follow a normal distribution. This increases the applicability and usefulness of statistical techniques based on the normality assumption.The Box-Cox transformation is a particulary useful family of transformations. It is defined as: where X is the response variable and lamda is the transformation parameter. Forlamda = 0, the log of the data is taken instead of using the above formula.Given a particular transformation, it is helpful to define a measure of the normality of the resulting transformation. One measure is to compute the correlation coefficient of a normal probability plot. The correlation is computed between the vertical and horizontal axis variables of the probability plot and is a convenient measure of the linearity of the probability plot (the more linear the probability plot, the better a normal distribution fits the data).The Box-Cox normality plot is a plot of these correlation coefficients for various values of the lambda parameter. The value of lambda corresponding to the maximum correlation on the plot is then the optimal choice for Sample Plot The histogram in the upper left-hand shows a data set that has significant right skewness (and so does not follow a normal distribution). The Box-Cox normality plot shows that the maximum value of the correlation coefficient is atlamda = -0.3.The histogram of the data after applying the Box-Cox transformation with lamda = -0.3. shows a data set for which the normality assumption is reasonable. This is verified with a normal probability plot of the transformed data. DefinitionBox-Cox normality plots are formed by: Vertical axis: Correlation coefficient from the normal probability plot after applying Box-Cox transformation Horizontal axis: Value for QuestionsThe Box-Cox normality plot can provide answers to the following questions: 1. Is there a transformation that will normalize my data? 2. What is the optimal value of the transformation parameter? Importance:Normalization improves validity of tests Normality assumptions are critical for many univariate intervals and tests. It is important to test this normality assumption. If the data are in fact not normal, the Box-Cox normality plot can often be used to find a transformation that will normalize the data. Related Techniques Normal Probability Plot Box-Cox Linearity Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Box-Cox Normality Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.4. Bootstrap Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.4.%20Bootstrap%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Estimate uncertainty The bootstrap (Efron and Gong) plot is used to estimate the uncertainty of a statistic. Generate subsamples with replacementTo generate a bootstrap uncertainty estimate for a given statistic from a set of data, a subsample of a size less than or equal to the size of the data set is generated from the data, and the statistic is calculated. This subsample is generated with replacement so that any data point can be sampled multiple times or not sampled at all. This process is repeated for many subsamples, typically between 500 and 1000. The computed values for the statistic form an estimate of the sampling distribution of the statistic. For example, to estimate the uncertainty of the median from a dataset with 50 elements, we generate a subsample of 50 elements and calculate the median. This is repeated at least 500 times so that we have at least 500 values for the median. Although the number of bootstrap samples to use is somewhat arbitrary, 500 subsamples is usually sufficient. To calculate a 90% confidence interval for the median, the sample medians are sorted into ascending order and the value of the 25th median (assuming exactly 500 subsamples were taken) is the lower confidence limit while the value of the 475th median (assuming exactly 500 subsamples were taken) is the upper confidence limit. Sample Plot: This bootstrap plot was generated from 500 uniform random numbers. Bootstrap plots and corresponding histograms were generated for the mean, median, and mid-range. The histograms for the corresponding statistics clearly show that for uniform random numbers the mid-range has the smallest variance and is, therefore, a superior location estimator to the mean or the median. DefinitionThe bootstrap plot is formed by: Vertical axis: Computed value of the desired statistic for a given subsample. Horizontal axis: Subsample number. The bootstrap plot is simply the computed value of the statistic versus the subsample number. That is, the bootstrap plot generates the values for the desired statistic. This is usually immediately followed by a histogram or some other distributional plot to show the location and variation of the sampling distribution of the statistic. QuestionsThe bootstrap plot is used to answer the following questions: What does the sampling distribution for the statistic look like? What is a 95% confidence interval for the statistic? Which statistic has a sampling distribution with the smallest variance? That is, which statistic generates the narrowest confidence interval? ImportanceThe most common uncertainty calculation is generating a confidence interval for the mean. In this case, the uncertainty formula can be derived mathematically. However, there are many situations in which the uncertainty formulas are mathematically intractable. The bootstrap provides a method for calculating the uncertainty in these cases. Cautuion on use of the bootstrapThe bootstrap is not appropriate for all distributions and statistics (Efron and Tibrashani). For example, because of the shape of the uniform distribution, the bootstrap is not appropriate for estimating the distribution of statistics that are heavily dependent on the tails, such as the range. Related Techniques Histogram Jackknife The jacknife is a technique that is closely related to the bootstrap. The jackknife is beyond the scope of this handbook. See the Efron and Gong article for a discussion of the jackknife.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Bootstrap Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.5. Box-Cox Linearity Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.5.%20Box-Cox%20Linearity%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Find the transformation of the X variable that maximizes the correlation between a Y and an X variable When performing a linear fit of Y against X, an appropriate transformation of X can often significantly improve the fit. The Box-Cox transformation (Box and Cox, 1964) is a particularly useful family of transformations. It is defined as: T(X)=(Xλ−1)/λ where X is the variable being transformed and λis the transformation parameter. For λ = 0, the natural log of the data is taken instead of using the above formula. The Box-Cox linearity plot is a plot of the correlation between Y and the transformed X for given values of λ. That is, λ is the coordinate for the horizontal axis variable and the value of the correlation between Y and the transformed X is the coordinate for the vertical axis of the plot. The value of λcorresponding to the maximum correlation (or minimum for negative correlation) on the plot is then the optimal choice for λ. Transforming X is used to improve the fit. The Box-Cox transformation applied to Y can be used as the basis for meeting the error assumptions. That case is not covered here. See page 225 of (Draper and Smith, 1981) or page 77 of (Ryan, 1997) for a discussion of this case. Sample Plot The plot of the original data with the predicted values from a linear fit indicate that a quadratic fit might be preferable. The Box-Cox linearity plot shows a value of λ = 2.0. The plot of the transformed data with the predicted values from a linear fit with the transformed data shows a better fit (verified by the significant reduction in the residual standard deviation). DefinitionBox-Cox linearity plots are formed by Vertical axis: Correlation coefficient from the transformed X and Y Horizontal axis: Value for λ QuestionsThe Box-Cox linearity plot can provide answers to the following questions: 1. Would a suitable transformation improve my fit? 2. What is the optimal value of the transformation parameter? Importance:Find a suitable transformation Transformations can often significantly improve a fit. The Box-Cox linearity plot provides a convenient way to find a suitable transformation without engaging in a lot of trial and error fitting. Related Techniques Linear Regression Box-Cox Normality Plot]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Box-Cox Linearity Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.3. Block Plot]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.3.%20Block%20Plot%2F</url>
    <content type="text"><![CDATA[Purpose:Check to determine if a factor of interest has an effect robust over all other factors The block plot (Filliben 1993) is an EDA tool for assessing whether the factor of interest (the primary factor) has a statistically significant effect on the response, and whether that conclusion about the primary factor effect is valid robustly over all other nuisance or secondary factors in the experiment. It replaces the analysis of variance test with a less assumption-dependent binomial test and should be routinely used whenever we are trying to robustly decide whether a primary factor has an effect. Sample Plot:Weld method 2 is lower (better) than weld method 1 in 10 of 12 cases This block plot reveals that in 10 of the 12 cases (bars), weld method 2 is lower (better) than weld method 1. From a binomial point of view, weld method is statistically significant. DefinitionBlock Plots are formed as follows: Vertical axis: Response variable Y Horizontal axis: All combinations of all levels of all nuisance (secondary) factors X1, X2, … Plot Character: Levels of the primary factor XP Discussion:Primary factor is denoted by plot character: within-bar plot character.Average number of defective lead wires per hour from a study with four factors, 1. weld strength (2 levels) 2. plant (2 levels) 3. speed (2 levels) 4. shift (3 levels) are shown in the plot above. Weld strength is the primary factor and the other three factors are nuisance factors. The 12 distinct positions along the horizontal axis correspond to all possible combinations of the three nuisance factors, i.e., 12 = 2 plants x 2 speeds x 3 shifts. These 12 conditions provide the framework for assessing whether any conclusions about the 2 levels of the primary factor (weld method) can truly be called “general conclusions”. If we find that one weld method setting does better (smaller average defects per hour) than the other weld method setting for all or most of these 12 nuisance factor combinations, then the conclusion is in fact general and robust. Ordering along the horizontal axisIn the above chart, the ordering along the horizontal axis is as follows: The left 6 bars are from plant 1 and the right 6 bars are from plant 2. The first 3 bars are from speed 1, the next 3 bars are from speed 2, the next 3 bars are from speed 1, and the last 3 bars are from speed 2. Bars 1, 4, 7, and 10 are from the first shift, bars 2, 5, 8, and 11 are from the second shift, and bars 3, 6, 9, and 12 are from the third shift. Setting 2 is better than setting 1 in 10 out of 12 casesIn the block plot for the first bar (plant 1, speed 1, shift 1), weld method 1 yields about 28 defects per hour while weld method 2 yields about 22 defects per hour–hence the difference for this combination is about 6 defects per hour and weld method 2 is seen to be better (smaller number of defects per hour). Is “weld method 2 is better than weld method 1” a general conclusion? For the second bar (plant 1, speed 1, shift 2), weld method 1 is about 37 while weld method 2 is only about 18. Thus weld method 2 is again seen to be better than weld method 1. Similarly for bar 3 (plant 1, speed 1, shift 3), we see weld method 2 is smaller than weld method 1. Scanning over all of the 12 bars, we see that weld method 2 is smaller than weld method 1 in 10 of the 12 cases, which is highly suggestive of a robust weld method effect. An event with chance probability of only 2%What is the chance of 10 out of 12 happening by chance? This is probabilistically equivalent to testing whether a coin is fair by flipping it and getting 10 heads in 12 tosses. The chance (from the binomial distribution) of getting 10 (or more extreme: 11, 12) heads in 12 flips of a fair coin is about 2%. Such low-probability events are usually rejected as untenable and in practice we would conclude that there is a difference in weld methods. Advantage:Graphical and binomialThe advantages of the block plot are as follows: • A quantitative procedure (analysis of variance) is replaced by a graphical procedure. • An F-test (analysis of variance) is replaced with a binomial test, which requires fewer assumptions. QuestionsThe block plot can provide answers to the following questions: 1. Is the factor of interest significant? 2. Does the factor of interest have an effect? 3. Does the location change between levels of the primary factor? 4. Has the process improved? 5. What is the best setting (= level) of the primary factor? 6. How much of an average improvement can we expect with this best setting of the primary factor? 7. Is there an interaction between the primary factor and one or more nuisance factors? 8. Does the effect of the primary factor change depending on the setting of some nuisance factor? 9. Are there any outliers? Importance:Robustly checks the significance of the factor of interest The block plot is a graphical technique that pointedly focuses on whether or not the primary factor conclusions are in fact robustly general. This question is fundamentally different from the generic multi-factor experiment question where the analyst asks, “What factors are important and what factors are not” (a screening problem)? Global data analysis techniques, such as analysis of variance, can potentially be improved by local, focused data analysis techniques that take advantage of this difference. Related Techniques t test (for shift in location for exactly 2 levels) ANOVA (for shift in location for 2 or more levels) Bihistogram (for shift in location, variation, and distribution for exactly 2 levels).]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Block Plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EDA_1.3.3.2. Bihistogram]]></title>
    <url>%2F2018%2F03%2F02%2FEDA_1.3.3.2.%20Bihistogram%2F</url>
    <content type="text"><![CDATA[Purpose:Check for a change in location, variation, or distribution The bihistogram is an EDA tool for assessing whether a before-versus-after engineering modification has caused a change in location; variation; or distribution. It is a graphical alternative to the two-sample t-test. The bihistogram can be more powerful than the t-test in that all of the distributional features (location, scale, skewness, outliers) are evident on a single plot. It is also based on the common and well-understood histogram. Sample Plot:This bihistogram reveals that there is a significant difference in ceramic breaking strength between batch 1 (above) and batch 2 (below) From the above bihistogram, we can see that batch 1 is centered at a ceramic strength value of approximately 725 while batch 2 is centered at a ceramic strength value of approximately 625. That indicates that these batches are displaced by about 100 strength units. Thus the batch factor has a significant effect on the location (typical value) for strength and hence batch is said to be “significant” or to “have an effect”. We thus see graphically and convincingly what a t-test or analysis of variance would indicate quantitatively. With respect to variation, note that the spread (variation) of the above-axis batch 1 histogram does not appear to be that much different from the below-axis batch 2 histogram. With respect to distributional shape, note that the batch 1 histogram is skewed left while the batch 2 histogram is more symmetric with even a hint of a slight skewness to the right. Thus the bihistogram reveals that there is a clear difference between the batches with respect to location and distribution, but not in regard to variation. Comparing batch 1 and batch 2, we also note that batch 1 is the “better batch” due to its 100-unit higher average strength (around 725). Definition:Two adjoined histogramsBihistograms are formed by vertically juxtaposing two histograms: Above the axis: Histogram of the response variable for condition 1 Below the axis: Histogram of the response variable for condition 2 QuestionsThe bihistogram can provide answers to the following questions: 1. Is a (2-level) factor significant? 2. Does a (2-level) factor have an effect? 3. Does the location change between the 2 subgroups? 4. Does the variation change between the 2 subgroups? 5. Does the distributional shape change between subgroups? 6. Are there any outliers? Importance:Checks 3 out of the 4 underlying assumptions of a measurement process The bihistogram is an important EDA tool for determining if a factor “has an effect”. Since the bihistogram provides insight into the validity of three (location, variation, and distribution) out of the four (missing only randomness) underlying assumptions in a measurement process, it is an especially valuable tool. Because of the dual (above/below) nature of the plot, the bihistogram is restricted to assessing factors that have only two levels. However, this is very common in the before-versus-after character of many scientific and engineering experiments. Related Techniques t test (for shift in location) F test (for shift in variation) Kolmogorov-Smirnov test (for shift in distribution) Quantile-quantile plot (for shift in location and distribution)]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Bihistogram</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exploratory Data Analysis(三)]]></title>
    <url>%2F2018%2F03%2F02%2FExploratory%20Data%20Analysis(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[1.3. EDA TechniquesAfter you have collected a set of data, how do you do an exploratory data analysis? What techniques do you employ? What do the various techniques focus on? What conclusions can you expect to reach?This section provides answers to these kinds of questions via a gallery of EDA techniques and a detailed description of each technique. The techniques are divided into graphical and quantitative techniques. For exploratory data analysis, the emphasis is primarily on the graphical techniques. 1.3.1.IntroductionGraphical and Quantitative TechniquesThis section describes many techniques that are commonly used in exploratory and classical data analysis. This list is by no means meant to be exhaustive. Additional techniques (both graphical and quantitative) are discussed in the other chapters. Specifically, the product comparisons chapter has a much more detailed description of many classical statistical techniques.EDA emphasizes graphical techniques while classical techniques emphasize quantitative techniques. In practice, an analyst typically uses a mixture of graphical and quantitative techniques. In this section, we have divided the descriptions into graphical and quantitative techniques. This is for organizational clarity and is not meant to discourage the use of both graphical and quantitiative techniques when analyzing data. Use of Techniques Shown in Case StudiesThis section emphasizes the techniques themselves; how the graph or test is defined, published references, and sample output. The use of the techniques to answer engineering questions is demonstrated in the case studiessection. The case studies do not demonstrate all of the techniques. Availability in SoftwareThe sample plots and output in this section were generated with the Dataplot software program. Other general purpose statistical data analysis programs can generate most of the plots, intervals, and tests discussed here, or macros can be written to acheive the same result. 1.3.2.Analysis QuestionsEDA QuestionsSome common questions that exploratory data analysis is used to answer are: 1. What is a typical value? 2. What is the uncertainty for a typical value? 3. What is a good distributional fit for a set of numbers? 4. What is a percentile? 5. Does an engineering modification have an effect? 6. Does a factor have an effect? 7. What are the most important factors? 8. Are measurements coming from different laboratories equivalent? 9. What is the best function for relating a response variable to a set of factor variables? 10. What are the best settings for factors? 11. Can we separate signal from noise in time dependent data? 12. Can we extract any structure from multivariate data? 13. Does the data have outliers? Analyst Should Identify Relevant Questions for his Engineering ProblemA critical early step in any analysis is to identify (for the engineering problem at hand) which of the above questions are relevant. That is, we need to identify which questions we want answered and which questions have no bearing on the problem at hand. After collecting such a set of questions, an equally important step, which is invaluable for maintaining focus, is to prioritize those questions in decreasing order of importance. EDA techniques are tied in with each of the questions. There are some EDA techniques (e.g., the scatter plot) that are broad-brushed and apply almost universally. On the other hand, there are a large number of EDA techniques that are specific and whose specificity is tied in with one of the above questions. Clearly if one chooses not to explicitly identify relevant questions, then one cannot take advantage of these question-specific EDA technqiues. EDA Approach Emphasizes GraphicsMost of these questions can be addressed by techniques discussed in this chapter. The process modeling and process improvement chapters also address many of the questions above. These questions are also relevant for the classical approach to statistics. What distinguishes the EDA approach is an emphasis on graphical techniques to gain insight as opposed to the classical approach of quantitative tests. Most data analysts will use a mix of graphical and classical quantitative techniques to address these problems. 1.3.3.Graphical Techniques: Alphabetic1.3.3.1. Autocorrelation PlotPurpose: Check RandomnessAutocorrelation plots (Box and Jenkins, pp. 28-32) are a commonly-used tool for checking randomness in a data set. This randomness is ascertained by computing autocorrelations for data values at varying time lags. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero.In addition, autocorrelation plots are used in the model identification stage for Box-Jenkinsautoregressive, moving average time series models. Autocorrelation is Only One Measure of RandomnessNote that uncorrelated does not necessarily mean random. Data that has significant autocorrelation is not random. However, data that does not show significant autocorrelation can still exhibit non-randomness in other ways. Autocorrelation is just one measure of randomness. In the context of model validation (which is the primary type of randomness we dicuss in the Handbook), checking for autocorrelation is typically a sufficient test of randomness since the residuals from a poor fitting models tend to display non-subtle randomness. However, some applications require a more rigorous determination of randomness. In these cases, a battery of tests, which might include checking for autocorrelation, are applied since data can be non-random in many different and often subtle ways.An example of where a more rigorous check for randomness is needed would be in testing random number generators. Sample Plot:Autocorrelations should be near-zero for randomness. Such is not the case in this example and thus the randomness assumption fails This sample autocorrelation plot shows that the time series is not random, but rather has a high degree of autocorrelation between adjacent and near-adjacent observations. Definition: r(h) versus h12345678910111213141516171819Autocorrelation plots are formed by • Vertical axis: Autocorrelation coefficient Rh=Ch/C0 where Ch is the autocovariance function Ch=1N∑t=1N−h(Yt−Y¯)(Yt+h−Y¯) and C0 is the variance function C0=∑Nt=1(Yt−Y¯)2N Note that Rh is between -1 and +1. Note that some sources may use the following formula for the autocovariance function Ch=1N−h∑t=1N−h(Yt−Y¯)(Yt+h−Y¯) Although this definition has less bias, the (1/N) formulation has some desirable statistical properties and is the form most commonly used in the statistics literature. See pages 20 and 49-50 in Chatfield for details. • Horizontal axis: Time lag h (h = 1, 2, 3, ...) • The above line also contains several horizontal reference lines. The middle line is at zero. The other four lines are 95 % and 99 % confidence bands. Note that there are two distinct formulas for generating the confidence bands. 1. If the autocorrelation plot is being used to test for randomness (i.e., there is no time dependence in the data), the following formula is recommended: ±z1−α/2N−−√ where N is the sample size, z is the cumulative distribution function of the standard normal distribution and αis the significance level. In this case, the confidence bands have fixed width that depends on the sample size. This is the formula that was used to generate the confidence bands in the above plot. 2. Autocorrelation plots are also used in the model identification stage for fitting ARIMA models. In this case, a moving average model is assumed for the data and the following confidence bands should be generated: ±z1−α/21N(1+2∑i=1ky2i)−−−−−−−−−−−−−⎷ where k is the lag, N is the sample size, z is the cumulative distribution function of the standard normal distribution and α is the significance level. In this case, the confidence bands increase as the lag increases. QuestionsThe autocorrelation plot can provide answers to the following questions: 1. Are the data random? 2. Is an observation related to an adjacent observation? 3. Is an observation related to an observation twice-removed? (etc.) 4. Is the observed time series white noise? 5. Is the observed time series sinusoidal? 6. Is the observed time series autoregressive? 7. What is an appropriate model for the observed time series? 8. Is the model Y = constant + error valid and sufficient? 9. Is the formula sY¯=s/N−−√ valid? Importance:Ensure validity of engineering conclusions Randomness (along with fixed model, fixed variation, and fixed distribution) is one of the four assumptions that typically underlie all measurement processes. The randomness assumption is critically important for the following three reasons: 1. Most standard statistical tests depend on randomness. The validity of the test conclusions is directly linked to the validity of the randomness assumption. 2. Many commonly-used statistical formulae depend on the randomness assumption, the most common formula being the formula for determining the standard deviation of the sample mean: sY¯=s/N−−√ where s is the standard deviation of the data. Although heavily used, the results from using this formula are of no value unless the randomness assumption holds. 3. For univariate data, the default model is Y = constant + error If the data are not random, this model is incorrect and invalid, and the estimates for the parameters (such as the constant) become nonsensical and invalid. In short, if the analyst does not check for randomness, then the validity of many of the statistical conclusions becomes suspect. The autocorrelation plot is an excellent way of checking for such randomness.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Comparative</tag>
        <tag>Screening</tag>
        <tag>Optimization</tag>
        <tag>Regression</tag>
        <tag>Time Series</tag>
        <tag>Multivariate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MathJax basic tutorial and quick reference]]></title>
    <url>%2F2018%2F03%2F02%2FMathJax%20basic%20tutorial%20and%20quick%20reference%2F</url>
    <content type="text"><![CDATA[For inline formulas, enclose the formula in $…$. For displayed formulas, use $$…$$.These render differently. For example, type$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$to show ∑ni=0i2=(n2+n)(2n+1)6 (which is inline mode) or type$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$to show $$ x \href{why-equal.html}{=} y^2 + 1 $$ $ x = {-b \pm \sqrt{b^2-4ac} \over 2a}. $]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exploratory Data Analysis(二)]]></title>
    <url>%2F2018%2F03%2F02%2FExploratory%20Data%20Analysis(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[1.2. EDA AssumptionsThe gamut of scientific and engineering experimentation is virtually limitless. In this sea of diversity is there any common basis that allows the analyst to systematically and validly arrive at supportable, repeatable research conclusions? Fortunately, there is such a basis and it is rooted in the fact that every measurement process, however complicated, has certain underlying assumptions. This section deals with what those assumptions are, why they are important, how to go about testing them, and what the consequences are if the assumptions do not hold. 1.2.1. Underlying AssumptionsAssumptions Underlying a Measurement ProcessThere are four assumptions that typically underlie all measurement processes; namely, that the data from the process at hand “behave like”: 1. random drawings; 2. from a fixed distribution; 3. with the distribution having fixed location; and 4. with the distribution having fixed variation. Univariate or Single Response VariableThe “fixed location” referred to in item 3 above differs for different problem types. The simplest problem type is univariate; that is, a single variable. For the univariate problem, the general model response = deterministic component + random component becomes response = constant + error Assumptions for Univariate ModelFor this case, the “fixed location” is simply the unknown constant. We can thus imagine the process at hand to be operating under constant conditions that produce a single column of data with the properties that • the data are uncorrelated with one another; • the random component has a fixed distribution; • the deterministic component consists of only a constant; and • the random component has fixed variation. Extrapolation to a Function of Many VariablesThe universal power and importance of the univariate model is that it can easily be extended to the more general case where the deterministic component is not just a constant, but is in fact a function of many variables, and the engineering objective is tocharacterize and model the function. Residuals Will Behave According to Univariate AssumptionsThe key point is that regardless of how many factors there are, and regardless of how complicated the function is, if the engineer succeeds in choosing a good model, then the differences (residuals) between the raw response data and the predicted values from the fitted model should themselves behave like a univariate process. Furthermore, the residuals from this univariate process fit will behave like: • random drawings; • from a fixed distribution; • with fixed location (namely, 0 in this case); and • with fixed variation. Validation of ModelThus if the residuals from the fitted model do in fact behave like the ideal, then testing of underlying assumptions becomes a tool for the validation and quality of fit of the chosen model. On the other hand, if the residuals from the chosen fitted model violate one or more of the above univariate assumptions, then the chosen fitted model is inadequate and an opportunity exists for arriving at an improved model. 1.2.2.ImportancePredictability and Statistical ControlPredictability is an all-important goal in science and engineering. If the four underlying assumptions hold, then we have achieved probabilistic predictability–the ability to make probability statements not only about the process in the past, but also about the process in the future. In short, such processes are said to be “in statistical control”. Validity of Engineering ConclusionsMoreover, if the four assumptions are valid, then the process is amenable to the generation of valid scientific and engineering conclusions. If the four assumptions are not valid, then the process is drifting (with respect to location, variation, or distribution), unpredictable, and out of control. A simple characterization of such processes by a location estimate, a variation estimate, or a distribution “estimate” inevitably leads to engineering conclusions that are not valid, are not supportable (scientifically or legally), and which are not repeatable in the laboratory. 1.2.3. Techniques for Testing AssumptionsTesting Underlying Assumptions Helps Assure the Validity of Scientific and Engineering ConclusionsBecause the validity of the final scientific/engineering conclusions is inextricably linked to the validity of the underlying univariate assumptions, it naturally follows that there is a real necessity that each and every one of the above four assumptions be routinely tested. Four Techniques to Test Underlying AssumptionsThe following EDA techniques are simple, efficient, and powerful for the routine testing of underlying assumptions: 1. run sequence plot (Yi versus i) 2. lag plot (Yi versus Yi-1) 3. histogram (counts versus subgroups of Y) 4. normal probability plot (ordered Y versus theoretical ordered Y) Plot on a Single Page for a Quick Characterization of the DataThe four EDA plots can be juxtaposed for a quick look at the characteristics of the data. The plots below are ordered as follows: 1. Run sequence plot - upper left 2. Lag plot - upper right 3. Histogram - lower left 4. Normal probability plot - lower right Sample Plot: Assumptions Hold This 4-plot reveals a process that has fixed location, fixed variation, is random, apparently has a fixed approximately normal distribution, and has no outliers. Sample Plot: Assumptions Do Not HoldIf one or more of the four underlying assumptions do not hold, then it will show up in the various plots as demonstrated in the following example. This 4-plot reveals a process that has fixed location, fixed variation, is non-random (oscillatory), has a non-normal, U-shaped distribution, and has several outliers. 1.2.4. Interpretation of 4-PlotInterpretation of EDA Plots:Flat and Equi-Banded, Random, Bell-Shaped, and Linear The four EDA plots discussed on the previous page are used to test the underlying assumptions: 1. Fixed Location: If the fixed location assumption holds, then the run sequence plot will be flat and non-drifting. 2. Fixed Variation: If the fixed variation assumption holds, then the vertical spread in the run sequence plot will be the approximately the same over the entire horizontal axis. 3. Randomness: If the randomness assumption holds, then the lag plot will be structureless and random. 4. Fixed Distribution: If the fixed distribution assumption holds, in particular if the fixed normal distribution holds, then 1. the histogram will be bell-shaped, and 2. the normal probability plot will be linear. Plots Utilized to Test the AssumptionsConversely, the underlying assumptions are tested using the EDA plots: Run Sequence Plot: If the run sequence plot is flat and non-drifting, the fixed-location assumption holds. If the run sequence plot has a vertical spread that is about the same over the entire plot, then the fixed-variation assumption holds. Lag Plot: If the lag plot is structureless, then the randomness assumption holds. Histogram: If the histogram is bell-shaped, the underlying distribution is symmetric and perhaps approximately normal. Normal Probability Plot: If the normal probability plot is linear, the underlying distribution is approximately normal. If all four of the assumptions hold, then the process is said definitionally to be “in statistical control”. 1.2.5 ConsequencesWhat If Assumptions Do Not Hold?If some of the underlying assumptions do not hold, what can be done about it? What corrective actions can be taken? The positive way of approaching this is to view the testing of underlying assumptions as a framework for learning about the process. Assumption-testing promotes insight into important aspects of the process that may not have surfaced otherwise. Primary Goal is Correct and Valid Scientific ConclusionsThe primary goal is to have correct, validated, and complete scientific/engineering conclusions flowing from the analysis. This usually includes intermediate goals such as the derivation of a good-fitting model and the computation of realistic parameter estimates. It should always include the ultimate goal of an understanding and a “feel” for “what makes the process tick”. There is no more powerful catalyst for discovery than the bringing together of an experienced/expert scientist/engineer and a data set ripe with intriguing “anomalies” and characteristics. Consequences of Invalid AssumptionsThe following sections discuss in more detail the consequences of invalid assumptions: 1. Consequences of non-randomness 2. Consequences of non-fixed location parameter 3. Consequences of non-fixed variation 4. Consequences related to distributional assumptions 1.2.5.1. Consequences of Non-RandomnessRandomness AssumptionThere are four underlying assumptions: 1. randomness; 2. fixed location; 3. fixed variation; and 4. fixed distribution. The randomness assumption is the most critical but the least tested. Consequeces of Non-RandomnessIf the randomness assumption does not hold, then 1. All of the usual statistical tests are invalid. 2. The calculated uncertainties for commonly used statistics become meaningless. 3. The calculated minimal sample size required for a pre-specified tolerance becomes meaningless. 4. The simple model: y = constant + error becomes invalid. 5. The parameter estimates become suspect and non-supportable. Non-Randomness Due to AutocorrelationOne specific and common type of non-randomness is autocorrelation. Autocorrelation is the correlation between Ytand Yt-k, where k is an integer that defines the lag for the autocorrelation. That is, autocorrelation is a time dependent non-randomness. This means that the value of the current point is highly dependent on the previous point if k = 1 (or k points ago if kis not 1). Autocorrelation is typically detected via an autocorrelation plot or a lag plot.If the data are not random due to autocorrelation, then 1. Adjacent data values may be related. 2. There may not be n independent snapshots of the phenomenon under study. 3. There may be undetected &quot;junk&quot;-outliers. 4. There may be undetected &quot;information-rich&quot;-outliers. 1.2.5.2. Consequences of Non-Fixed Location ParameterLocation EstimateThe usual estimate of location is the mean Y¯=1N∑i=1NYifrom N measurements Y1, Y2, … , YN. Consequences of Non-Fixed LocationIf the run sequence plot does not support the assumption of fixed location, then 1. The location may be drifting. 2. The single location estimate may be meaningless (if the process is drifting). 3. The choice of location estimator (e.g., the sample mean) may be sub-optimal. 4. The usual formula for the uncertainty of the mean: s(Y¯)=1N(N−1)−−−−−−−−√∑i=1N(Yi−Y¯)2−−−−−−−−−−−⎷may be invalid and the numerical value optimistically small. 5. The location estimate may be poor. 6. The location estimate may be biased. 1.2.5.3. Consequences of Non-Fixed Variation ParameterVariation EstimateThe usual estimate of variation is the standard deviation sY=1(N−1)−−−−−−−√∑i=1N(Yi−Y¯)2−−−−−−−−−−−⎷from N measurements Y1, Y2, … , YN. Consequences of Non-Fixed VariationIf the run sequence plot does not support the assumption of fixed variation, then 1. The variation may be drifting. 2. The single variation estimate may be meaningless (if the process variation is drifting). 3. The variation estimate may be poor. 4. The variation estimate may be biased. 1.2.5.4. Consequences Related to Distributional AssumptionsDistributional AnalysisScientists and engineers routinely use the mean (average) to estimate the “middle” of a distribution. It is not so well known that the variability and the noisiness of the mean as a location estimator are intrinsically linked with the underlying distribution of the data. For certain distributions, the mean is a poor choice. For any given distribution, there exists an optimal choice– that is, the estimator with minimum variability/noisiness. This optimal choice may be, for example, the median, the midrange, the midmean, the mean, or something else. The implication of this is to “estimate” the distribution first, and then–based on the distribution–choose the optimal estimator. The resulting engineering parameter estimators will have less variability than if this approach is not followed. Case StudiesThe airplane glass failure case study gives an example of determining an appropriate distribution and estimating the parameters of that distribution. The uniform random numberscase study gives an example of determining a more appropriate centrality parameter for a non-normal distribution. Other consequences that flow from problems with distributional assumptions are: Distribution1. The distribution may be changing. 2. The single distribution estimate may be meaningless (if the process distribution is changing). 3. The distribution may be markedly non-normal. 4. The distribution may be unknown. 5. The true probability distribution for the error may remain unknown. Model1. The model may be changing. 2. The single model estimate may be meaningless. 3. The default model Y = constant + error may be invalid. 4. If the default model is insufficient, information about a better model may remain undetected. 5. A poor deterministic model may be fit. 6. Information about an improved model may go undetected. Process1. The process may be out-of-control. 2. The process may be unpredictable. 3. The process may be un-modelable.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Comparative</tag>
        <tag>Screening</tag>
        <tag>Optimization</tag>
        <tag>Regression</tag>
        <tag>Time Series</tag>
        <tag>Multivariate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exploratory Data Analysis(一)]]></title>
    <url>%2F2018%2F03%2F02%2FExploratory%20Data%20Analysis(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[1. Exploratory Data AnalysisThis chapter presents the assumptions, principles, and techniques necessary to gain insight into data via EDA–exploratory data analysis. 1.1. EDA IntroductionWhat is exploratory data analysis? How did it begin? How and where did it originate? How is it differentiated from other data analysis approaches, such as classical and Bayesian? Is EDA the same as statistical graphics? What role does statistical graphics play in EDA? Is statistical graphics identical to EDA?These questions and related questions are dealt with in this section. This section answers these questions and provides the necessary frame of reference for EDA assumptions, principles, and techniques. 1.1.1. What is EDA?ApproachExploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to maximize insight into a data set; uncover underlying structure; extract important variables; detect outliers and anomalies; test underlying assumptions; develop parsimonious models; and determine optimal factor settings. FocusThe EDA approach is precisely that–an approach–not a set of techniques, but an attitude/philosophy about how a data analysis should be carried out. PhilosophyEDA is not identical to statistical graphics although the two terms are used almost interchangeably. Statistical graphics is a collection of techniques–all graphically based and all focusing on one data characterization aspect. EDA encompasses a larger venue; EDA is an approach to data analysis that postpones the usual assumptions about what kind of model the data follow with the more direct approach of allowing the data itself to reveal its underlying structure and model. EDA is not a mere collection of techniques; EDA is a philosophy as to how we dissect a data set; what we look for; how we look; and how we interpret. It is true that EDA heavily uses the collection of techniques that we call “statistical graphics”, but it is not identical to statistical graphics per se. HistoryThe seminal work in EDA is Exploratory Data Analysis, Tukey, (1977). Over the years it has benefitted from other noteworthy publications such as Data Analysis and Regression, Mosteller and Tukey (1977), Interactive Data Analysis, Hoaglin (1977), The ABC’s of EDA, Velleman and Hoaglin (1981) and has gained a large following as “the” way to analyze a data set. TechniquesMost EDA techniques are graphical in nature with a few quantitative techniques. The reason for the heavy reliance on graphics is that by its very nature the main role of EDA is to open-mindedly explore, and graphics gives the analysts unparalleled power to do so, enticing the data to reveal its structural secrets, and being always ready to gain some new, often unsuspected, insight into the data. In combination with the natural pattern-recognition capabilities that we all possess, graphics provides, of course, unparalleled power to carry this out.The particular graphical techniques employed in EDA are often quite simple, consisting of various techniques of: Plotting the raw data (such as data traces,histograms, bihistograms, probability plots,lag plots, block plots, and Youden plots. Plotting simple statistics such as mean plots, standard deviation plots, box plots, and main effects plots of the raw data. Positioning such plots so as to maximize our natural pattern-recognition abilities, such as using multiple plots per page. 1.1.2. How Does Exploratory Data Analysis differ from Classical Data Analysis?Data Analysis ApproachesEDA is a data analysis approach. What other data analysis approaches exist and how does EDA differ from these other approaches? Three popular data analysis approaches are: Classical Exploratory (EDA) Bayesian Paradigms for Analysis TechniquesThese three approaches are similar in that they all start with a general science/engineering problem and all yield science/engineering conclusions. The difference is the sequence and focus of the intermediate steps. For classical analysis, the sequence isProblem =&gt; Data =&gt; Model =&gt; Analysis =&gt; Conclusions For EDA, the sequence isProblem =&gt; Data =&gt; Analysis =&gt; Model =&gt; Conclusions For Bayesian, the sequence isProblem =&gt; Data =&gt; Model =&gt; Prior Distribution =&gt; Analysis =&gt; Conclusions Method of dealing with underlying model for the data distinguishes the 3 approachesThus for classical analysis, the data collection is followed by the imposition of a model (normality, linearity, etc.) and the analysis, estimation, and testing that follows are focused on the parameters of that model. For EDA, the data collection is not followed by a model imposition; rather it is followed immediately by analysis with a goal of inferring what model would be appropriate. Finally, for a Bayesian analysis, the analyst attempts to incorporate scientific/engineering knowledge/expertise into the analysis by imposing a data-independent distribution on the parameters of the selected model; the analysis thus consists of formally combining both the prior distribution on the parameters and the collected data to jointly make inferences and/or test assumptions about the model parameters. In the real world, data analysts freely mix elements of all of the above three approaches (and other approaches). The above distinctions were made to emphasize the major differences among the three approaches. Further discussion of the distinction between the classical and EDA approaches Focusing on EDA versus classical, these two approaches differ as follows: Models Focus Techniques Rigor Data Treatment Assumptions 1.1.3 How Does Exploratory Data Analysis Differ from Summary Analysis?SummaryA summary analysis is simply a numeric reduction of a historical data set. It is quite passive. Its focus is in the past. Quite commonly, its purpose is to simply arrive at a few key statistics (for example, mean and standard deviation) which may then either replace the data set or be added to the data set in the form of a summary table. ExploratoryIn contrast, EDA has as its broadest goal the desire to gain insight into the engineering/scientific process behind the data. Whereas summary statistics are passive and historical, EDA is active and futuristic. In an attempt to “understand” the process and improve it in the future, EDA uses the data as a “window” to peer into the heart of the process that generated the data. There is an archival role in the research and manufacturing world for summary statistics, but there is an enormously larger role for the EDA approach. 1.1.4. What are the EDA Goals?Primary and Secondary GoalsThe primary goal of EDA is to maximize the analyst’s insight into a data set and into the underlying structure of a data set, while providing all of the specific items that an analyst would want to extract from a data set, such as: 1. a good-fitting, parsimonious model 2. a list of outliers 3. a sense of robustness of conclusions 4. estimates for parameters 5. uncertainties for those estimates 6. a ranked list of important factors 7. conclusions as to whether individual factors are statistically significant 8. optimal settings Insight into the DataInsight implies detecting and uncovering underlying structure in the data. Such underlying structure may not be encapsulated in the list of items above; such items serve as the specific targets of an analysis, but the real insight and “feel” for a data set comes as the analyst judiciously probes and explores the various subtleties of the data. The “feel” for the data comes almost exclusively from the application of various graphical techniques, the collection of which serves as the window into the essence of the data. Graphics are irreplaceable–there are no quantitative analogues that will give the same insight as well-chosen graphics. To get a “feel” for the data, it is not enough for the analyst to know what is in the data; the analyst also must know what is not in the data, and the only way to do that is to draw on our own human pattern-recognition and comparative abilities in the context of a series of judicious graphical techniques applied to the data. 1.1.5.The Role of GraphicsGraphicalStatistics and data analysis procedures can broadly be split into two parts: quantitative graphical QuantitativeQuantitative techniques are the set of statistical procedures that yield numeric or tabular output. Examples of quantitative techniques include: hypothesis testing analysis of variance point estimates and confidence intervals least squares regression These and similar techniques are all valuable and are mainstream in terms of classical analysis. GraphicalOn the other hand, there is a large collection of statistical tools that we generally refer to as graphical techniques. These include: scatter plots histograms probability plots residual plots box plots block plots EDA Approach Relies Heavily on Graphical TechniquesThe EDA approach relies heavily on these and similar graphical techniques. Graphical procedures are not just tools that we could use in an EDA context, they are tools that we must use. Such graphical tools are the shortest path to gaining insight into a data set in terms of testing assumptions model selection model validation estimator selection relationship identification factor effect determination outlier detection If one is not using statistical graphics, then one is forfeiting insight into one or more aspects of the underlying structure of the data. 1.1.6. An EDA/Graphics ExampleAnscombe ExampleA simple, classic (Anscombe) example of the central role that graphics play in terms of providing insight into a data set starts with the following data set: Data X Y 10.00 8.04 8.00 6.95 13.00 7.58 9.00 8.81 11.00 8.33 14.00 9.96 6.00 7.24 4.00 4.26 12.00 10.84 7.00 4.82 5.00 5.68 Summary StatisticsIf the goal of the analysis is to compute summary statistics plus determine the best linear fit for Y as a function of X, the results might be given as: N = 11 Mean of X = 9.0 Mean of Y = 7.5 Intercept = 3 Slope = 0.5 Residual standard deviation = 1.237 Correlation = 0.816 The above quantitative analysis, although valuable, gives us only limited insight into the data. Scatter PlotIn contrast, the following simple scatter plotof the data suggests the following: The data set “behaves like” a linear curve with some scatter; there is no justification for a more complicated model (e.g., quadratic); there are no outliers; the vertical spread of the data appears to be of equal height irrespective of the X-value; this indicates that the data are equally-precise throughout and so a “regular” (that is, equi-weighted) fit is appropriate. Three Additional Data SetsThis kind of characterization for the data serves as the core for getting insight/feel for the data. Such insight/feel does not come from the quantitative statistics; on the contrary, calculations of quantitative statistics such as intercept and slope should be subsequent to the characterization and will make sense only if the characterization is true. To illustrate the loss of information that results when the graphics insight step is skipped, consider the following three data sets [Anscombe data sets 2, 3, and 4]: X2 Y2 X3 Y3 X4 Y4 10.00 9.14 10.00 7.46 8.00 6.58 8.00 8.14 8.00 6.77 8.00 5.76 13.00 8.74 13.00 12.74 8.00 7.71 9.00 8.77 9.00 7.11 8.00 8.84 11.00 9.26 11.00 7.81 8.00 8.47 14.00 8.10 14.00 8.84 8.00 7.04 6.00 6.13 6.00 6.08 8.00 5.25 4.00 3.10 4.00 5.39 19.00 12.50 12.00 9.13 12.00 8.15 8.00 5.56 7.00 7.26 7.00 6.42 8.00 7.91 5.00 4.74 5.00 5.73 8.00 6.89 Quantitative Statistics for Data Set 2 A quantitative analysis on data set 2 yields N = 11 Mean of X = 9.0 Mean of Y = 7.5 Intercept = 3 Slope = 0.5 Residual standard deviation = 1.237 Correlation = 0.816 which is identical to the analysis for data set 1. One might naively assume that the two data sets are “equivalent” since that is what the statistics tell us; but what do the statistics not tell us? Quantitative Statistics for Data Sets 3 and 4 Remarkably, a quantitative analysis on data sets 3 and 4 also yields N = 11 Mean of X = 9.0 Mean of Y = 7.5 Intercept = 3 Slope = 0.5 Residual standard deviation = 1.236 Correlation = 0.816 (0.817 for data set 4) which implies that in some quantitative sense, all four of the data sets are “equivalent”. In fact, the four data sets are far from “equivalent” and a scatter plot of each data set, which would be step 1 of any EDA approach, would tell us that immediately. Scatter Plots Interpretation of Scatter PlotsConclusions from the scatter plots are: data set 1 is clearly linear with some scatter. data set 2 is clearly quadratic. data set 3 clearly has an outlier. data set 4 is obviously the victim of a poor experimental design with a single point far removed from the bulk of the data “wagging the dog”. Importance of Exploratory AnalysisThese points are exactly the substance that provide and define “insight” and “feel” for a data set. They are the goals and the fruits of an open exploratory data analysis (EDA) approach to the data. Quantitative statistics are not wrong per se, but they are incomplete. They are incomplete because they are numericsummaries which in the summarization operation do a good job of focusing on a particular aspect of the data (e.g., location, intercept, slope, degree of relatedness, etc.) by judiciously reducing the data to a few numbers. Doing so also filters the data, necessarily omitting and screening out other sometimes crucial information in the focusing operation. Quantitative statistics focus but also filter; and filtering is exactly what makes the quantitative approach incomplete at best and misleading at worst. The estimated intercepts (= 3) and slopes (= 0.5) for data sets 2, 3, and 4 are misleading because the estimation is done in the context of an assumed linear model and that linearity assumption is the fatal flaw in this analysis. The EDA approach of deliberately postponing the model selection until further along in the analysis has many rewards, not the least of which is the ultimate convergence to a much-improved model and the formulation of valid and supportable scientific and engineering conclusions. 1.1.7.General Problem CategoriesProblem ClassificationThe following table is a convenient way to classify EDA problems. Univariate and ControlUNIVARIATEData: A single column of numbers, Y. Model: y = constant + error Output: 1. A number (the estimated constant in the model). 2. An estimate of uncertainty for the constant. 3. An estimate of the distribution for the error. Techniques: • 4-Plot • Probability Plot • PPCC Plot CONTROLData: A single column of numbers, Y. Model: y = constant + error Output: A &quot;yes&quot; or &quot;no&quot; to the question &quot;Is the system out of control?&quot;. Techniques: • Control Charts Comparative and ScreeningCOMPARATIVEData: A single response variable and k independent variables (Y, X1, X2, ... , Xk), primary focus is onone (the primary factor) of these independent variables. Model: y = f(x1, x2, ..., xk) + error Output: A &quot;yes&quot; or &quot;no&quot; to the question &quot;Is the primary factor significant?&quot;. Techniques: • Block Plot • Scatter Plot • Box Plot SCREENINGData: A single response variable and k independent variables (Y, X1, X2, ... , Xk). Model: y = f(x1, x2, ..., xk) + error Output: 1. A ranked list (from most important to least important) of factors. 2. Best settings for the factors. 3. A good model/prediction equation relating Y to the factors. Techniques: • Block Plot • Probability Plot • Bihistogram Optimization and RegressionOPTIMIZATIONData: A single response variable and k independent variables (Y, X1, X2, ... , Xk). Model: y = f(x1, x2, ..., xk) + error Output: Best settings for the factor variables. Techniques: • Block Plot • Least Squares Fitting • Contour Plot REGRESSIONData: A single response variable and k independent variables (Y, X1, X2, ... , Xk). The independent variables can be continuous. Model: y = f(x1, x2, ..., xk) + error Output: A good model/prediction equation relating Y to the factors. Techniques: • Least Squares Fitting • Scatter Plot • 6-Plot Time Series and MultivariateTIME SERIESData: A column of time dependent numbers, Y. In addition, time is an indpendent variable. The time variable can be either explicit or implied. If the data are not equi-spaced, the time variable should be explicitly provided. Model: yt = f(t) + error The model can be either a time domain based or frequency domain based. Output: A good model/prediction equation relating Y to previous values of Y. Techniques: • Autocorrelation Plot • Spectrum • Complex Demodulation Amplitude Plot • Complex Demodulation Phase Plot • ARIMA Models MULTIVARIATEData: k factor variables (X1, X2, ... , Xk). Model: The model is not explicit. Output: Identify underlying correlation structure in the data. Techniques: • Star Plot • Scatter Plot Matrix • Conditioning Plot • Profile Plot • Principal Components • Clustering • Discrimination/Classification Note that multivarate analysis is only covered lightly in this Handbook.]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Comparative</tag>
        <tag>Screening</tag>
        <tag>Optimization</tag>
        <tag>Regression</tag>
        <tag>Time Series</tag>
        <tag>Multivariate</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F01%2Fwin10%E7%9A%84%E5%BC%80%E5%A7%8B%E8%8F%9C%E5%8D%95%E7%82%B9%E5%87%BB%E6%B2%A1%E6%9C%89%E5%8F%8D%E5%BA%94%2F</url>
    <content type="text"><![CDATA[1、在键盘上按下win+R键，或在开始菜单图标上点击右键选择运行;2、输入powershell，按下“确定”运行;3、在窗口里输入或复制粘贴以下命令，注意只有一行：Get-AppxPackage | % { Add-AppxPackage -DisableDevelopmentMode -Register “$($_.InstallLocation)\AppxManifest.xml” -verbose }4、点击enter键，等待修复命令运行完成，重启电脑，完成之后BUG就被修复了 via]]></content>
  </entry>
  <entry>
    <title><![CDATA[Received "500 Internal Server Error"]]></title>
    <url>%2F2018%2F03%2F01%2FReceived%20500%20Internal%20Server%20Error%2F</url>
    <content type="text"><![CDATA[问题使用twine在whl文件打包上传到pypi时，出现如下错误： 换种方式上传：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133PS E:\Code\woe&gt; python setup.py sdist uploadrunning sdistrunning egg_infowriting requirements to woe.egg-info\requires.txtwriting woe.egg-info\PKG-INFOwriting top-level names to woe.egg-info\top_level.txtwriting dependency_links to woe.egg-info\dependency_links.txtreading manifest file 'woe.egg-info\SOURCES.txt'reading manifest template 'MANIFEST.in'writing manifest file 'woe.egg-info\SOURCES.txt'running checkcreating woe-0.1.4creating woe-0.1.4\examplescreating woe-0.1.4\woecreating woe-0.1.4\woe.egg-infocopying files to woe-0.1.4...copying LICENSE.txt -&gt; woe-0.1.4copying MANIFEST.in -&gt; woe-0.1.4copying README.rst -&gt; woe-0.1.4copying setup.py -&gt; woe-0.1.4copying examples\HereWeGo.py -&gt; woe-0.1.4\examplescopying examples\README.rst -&gt; woe-0.1.4\examplescopying examples\UCI_Credit_Card.csv -&gt; woe-0.1.4\examplescopying examples\config.csv -&gt; woe-0.1.4\examplescopying woe\GridSearch.py -&gt; woe-0.1.4\woecopying woe\__init__.py -&gt; woe-0.1.4\woecopying woe\config.py -&gt; woe-0.1.4\woecopying woe\eval.py -&gt; woe-0.1.4\woecopying woe\feature_process.py -&gt; woe-0.1.4\woecopying woe\ftrl.py -&gt; woe-0.1.4\woecopying woe.egg-info\PKG-INFO -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\SOURCES.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\dependency_links.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\requires.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\top_level.txt -&gt; woe-0.1.4\woe.egg-infoWriting woe-0.1.4\setup.cfgCreating tar archiveremoving 'woe-0.1.4' (and everything under it)running uploadPassword:Submitting dist\woe-0.1.4.tar.gz to https://upload.pypi.org/legacy/Upload failed (403): Invalid or non-existent authentication information.error: Upload failed (403): Invalid or non-existent authentication information.PS E:\Code\woe&gt;PS E:\Code\woe&gt; python setup.py sdist uploadrunning sdistrunning egg_infowriting requirements to woe.egg-info\requires.txtwriting woe.egg-info\PKG-INFOwriting top-level names to woe.egg-info\top_level.txtwriting dependency_links to woe.egg-info\dependency_links.txtreading manifest file 'woe.egg-info\SOURCES.txt'reading manifest template 'MANIFEST.in'writing manifest file 'woe.egg-info\SOURCES.txt'running checkcreating woe-0.1.4creating woe-0.1.4\examplescreating woe-0.1.4\woecreating woe-0.1.4\woe.egg-infocopying files to woe-0.1.4...copying LICENSE.txt -&gt; woe-0.1.4copying MANIFEST.in -&gt; woe-0.1.4copying README.rst -&gt; woe-0.1.4copying setup.py -&gt; woe-0.1.4copying examples\HereWeGo.py -&gt; woe-0.1.4\examplescopying examples\README.rst -&gt; woe-0.1.4\examplescopying examples\UCI_Credit_Card.csv -&gt; woe-0.1.4\examplescopying examples\config.csv -&gt; woe-0.1.4\examplescopying woe\GridSearch.py -&gt; woe-0.1.4\woecopying woe\__init__.py -&gt; woe-0.1.4\woecopying woe\config.py -&gt; woe-0.1.4\woecopying woe\eval.py -&gt; woe-0.1.4\woecopying woe\feature_process.py -&gt; woe-0.1.4\woecopying woe\ftrl.py -&gt; woe-0.1.4\woecopying woe.egg-info\PKG-INFO -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\SOURCES.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\dependency_links.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\requires.txt -&gt; woe-0.1.4\woe.egg-infocopying woe.egg-info\top_level.txt -&gt; woe-0.1.4\woe.egg-infoWriting woe-0.1.4\setup.cfgCreating tar archiveremoving 'woe-0.1.4' (and everything under it)running uploadSubmitting dist\woe-0.1.4.tar.gz to https://upload.pypi.org/legacy/Server response (200): OKPS E:\Code\woe&gt; twine upload dist/*Uploading distributions to https://upload.pypi.org/legacy/Uploading woe-0.1.4-py2-none-any.whlReceived "500: Internal Server Error" Package upload appears to have failed. Retry 1 of 5Uploading woe-0.1.4-py2-none-any.whlReceived "500: Internal Server Error" Package upload appears to have failed. Retry 2 of 5Uploading woe-0.1.4-py2-none-any.whlReceived "500: Internal Server Error" Package upload appears to have failed. Retry 3 of 5Uploading woe-0.1.4-py2-none-any.whlReceived "500: Internal Server Error" Package upload appears to have failed. Retry 4 of 5Uploading woe-0.1.4-py2-none-any.whlReceived "500: Internal Server Error" Package upload appears to have failed. Retry 5 of 5HTTPError: 500 Server Error: Internal Server Error for url: https://upload.pypi.org/legacy/PS E:\Code\woe&gt; python setup.py bdist_wheel uploadrunning bdist_wheelrunning buildrunning build_pyinstalling to build\bdist.win-amd64\wheelrunning installrunning install_libcreating build\bdist.win-amd64\wheelcreating build\bdist.win-amd64\wheel\woecopying build\lib\woe\config.py -&gt; build\bdist.win-amd64\wheel\.\woecopying build\lib\woe\eval.py -&gt; build\bdist.win-amd64\wheel\.\woecopying build\lib\woe\feature_process.py -&gt; build\bdist.win-amd64\wheel\.\woecopying build\lib\woe\ftrl.py -&gt; build\bdist.win-amd64\wheel\.\woecopying build\lib\woe\GridSearch.py -&gt; build\bdist.win-amd64\wheel\.\woecopying build\lib\woe\__init__.py -&gt; build\bdist.win-amd64\wheel\.\woerunning install_egg_inforunning egg_infowriting requirements to woe.egg-info\requires.txtwriting woe.egg-info\PKG-INFOwriting top-level names to woe.egg-info\top_level.txtwriting dependency_links to woe.egg-info\dependency_links.txtreading manifest file 'woe.egg-info\SOURCES.txt'reading manifest template 'MANIFEST.in'writing manifest file 'woe.egg-info\SOURCES.txt'Copying woe.egg-info to build\bdist.win-amd64\wheel\.\woe-0.1.4-py2.7.egg-inforunning install_scriptscreating build\bdist.win-amd64\wheel\woe-0.1.4.dist-info\WHEELrunning uploadSubmitting E:\Code\woe\dist\woe-0.1.4-py2-none-any.whl to https://upload.pypi.org/legacy/Server response (200): OKPS E:\Code\woe&gt; twine upload dist/*Uploading distributions to https://upload.pypi.org/legacy/Uploading woe-0.1.4-py2-none-any.whlUploading woe-0.1.4-py3-none-any.whlUploading woe-0.1.4-py2-none-any.tar.gz 如上，最后原来是没有.pypirc文件，在自己用户根路径下新建，内容如下：1234567[distutils]index-servers = pypi[pypi]repository: https://upload.pypi.org/legacy/username: boredbirdpassword: &lt;密码&gt; 最后，如上才上传成功（一开始用python setup.py 上传成功了一个文件，等了一会twine也可以用了，就能够批量上传到pypi了）。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[py2转py3遇到的问题]]></title>
    <url>%2F2018%2F03%2F01%2Fpy2%E8%BD%ACpy3%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[python2与python3在处理异常的区别： 1.所以异常都从 BaseException继承，并删除了StardardError 2.去除了异常类的序列行为和.message属性 3.用 raise Exception(args)代替 raise Exception, args语法 4.捕获异常的语法改变，引入了as关键字来标识异常实例，在Py2中：1234567&gt;&gt;&gt; try:... raise NotImplementedError('Error')... except NotImplementedError, error:... print error.message...Error 在Py3中：12345&gt;&gt;&gt; try: raise NotImplementedError('Error') except NotImplementedError as error: #注意这个 as print(str(error))Error 5.异常链，因为__context__在3.0a1版本中没有实现 tuple parameter unpacking is not supported in Pyhton 3As tuple parameters are used by lambdas because of the single expression limitation, they must also be supported. This is done by having the expected sequence argument bound to a single parameter and then indexing on that parameter:1lambda (x, y): x + y will be translated into:1lambda x_y: x_y[0] + x_y[1] py2与py3在map返回类型上的区别py2：123456789Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32In[2]: list_a = [2,3,4,5]In[3]: list_b = [1,2,3,4]In[4]: map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b))Out[4]:[1.9999999979999998, 1.49999999925, 1.3333333328888888, 1.2499999996875]In[5]: type(map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b)))Out[5]:list py3:123456789Python 3.5.2 |Anaconda custom (64-bit)| (default, Jul 5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]In[2]: list_a = [2,3,4,5]In[3]: list_b = [1,2,3,4]In[4]: map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b))Out[4]:&lt;map at 0x24dc066f160&gt;In[5]: type(map(lambda a: a[0] / (a[1]+0.000000001), zip(list_a, list_b)))Out[5]:map 在py3中需要用list()转换一下。 windows下py2与py3共存切换问题配置环境变量 修改文件名称 效果如下]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git切换分支报错Permission denied]]></title>
    <url>%2F2018%2F03%2F01%2Fgit%E5%88%87%E6%8D%A2%E5%88%86%E6%94%AF%E6%8A%A5%E9%94%99Permission%20denied%2F</url>
    <content type="text"><![CDATA[git切换分支报错，如下：123456789101112131415161718192021222324maomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (master)$ git checkout sourceerror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniederror: cannot stat 'source/_posts': Permission deniedmaomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (master)$ git checkout sourcewarning: unable to rmdir .deploy_git: Directory not emptyChecking out files: 100% (10345/10345), done.M themes/nextSwitched to branch 'source'Your branch is up-to-date with 'origin/source'.maomaochong@Boredbird MINGW64 /d/Program Files/nodejs/blog (source)$ 最后发现是文件占用问题，关掉后台的编辑器，顺利切换分支。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ERROR Local hexo not found]]></title>
    <url>%2F2018%2F03%2F01%2FERROR%20Local%20hexo%20not%20found%2F</url>
    <content type="text"><![CDATA[via]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用atom插件打造一套python的IDE]]></title>
    <url>%2F2018%2F02%2F12%2F%E5%88%A9%E7%94%A8atom%E6%8F%92%E4%BB%B6%E6%89%93%E9%80%A0%E4%B8%80%E5%A5%97python%E7%9A%84IDE%2F</url>
    <content type="text"><![CDATA[利用atom插件打造一套python的IDE快捷键 快捷键 说明 ctrl-shift-P 打开Command Palette Shift + Ctrl + M 快速预览markdown ctrl+shift+F 通过关键字段全项目检索目标代码文件 ctrl+F 通过关键字段在目标文件中定位目标代码 ctrl+T 直接通过文件名全局搜索目标文件 alt+ctrl+shift+{ 代码折叠让代码看起来更加简洁（注意要依次按这四个键，不能同时按） cmd+shift+t 开启新控制台 ctrl+` 打开或关闭控制台 cmd+shift+j/k 切换控制台 shift+atl+e 在多个文件中切换 alt+shift+p Project Manager: List Projects (Projects can be filtered by title, group and template by typing group: atom which would give all projects with the atom group.) Project Manager: Edit Project 在Command Palette输入这个命令，用于编辑配置文件 插件 插件 说明 Atom Runner 运行python脚本 autocomplete-python python代码自动补全 hydrogen 在写脚本的同时运行ipython notebook linter+linter-pylama 代码错误提示 platformio-ide-terminal Atom内置终端 project-manager 快速切换你的多个项目文件夹 python-autopep8 自动规范化python代码(使用pep8准则) isort+python-isort 自动规范化代码中的import python-tools+hyperclick ctrl+单击转到源代码 todo-show 管理你代码中的TODO(在你没有完成的代码最后加上TODO即可) python-debugger 调试python代码 atom-python-test 测试python代码 atom-python-virtualenv python的虚拟环境 minimap 代码缩略图 highlight-selected 选择某段代码自动高亮相同代码 minimap-highlight-selected 实现minimap高亮选择内容 terminal-plus 代替系统控制台 project-manager 快速打开储存的项目,管理多个项目 autocomplete-paths 自动补全文件路径 file-icons 左侧树状图下，根据文件类型左侧显示不同图标 expose 在多个文件中切换 主题 主题 说明 atom-material-ui A dynamic UI theme for Atom that (kinda) follows Google’s Material Design Guidelines. Best with Atom Material Syntax. 以上是常用的python插件, 足够满足你的日常使用了]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Tips]]></title>
    <url>%2F2018%2F02%2F12%2FVisual%20Studio%20Tips%2F</url>
    <content type="text"><![CDATA[在 Visual Studio 中开发代码而无需创建项目或解决方案在 Visual Studio 2017 中，你可以在 Visual Studio 中打开几乎任何类型的基于目录的项目的代码，而无需创建解决方案或者项目文件。 这意味着（例如，在 Git 上找到一个代码项目时）可以克隆该项目，然后在 Visual Studio 中直接打开并开始开发，而无需创建解决方案或项目。 在任意位置打开代码可以通过以下方式，在 Visual Studio 中打开代码： 在 Visual Studio 菜单栏上，依次选择“文件”、“打开”、“文件夹”，然后浏览到代码位置。 在包含该代码的文件夹的上下文（右键单击）菜单上，选择“在 Visual Studio 中打开”命令。 选择 Visual Studio“开始”页上的“打开文件夹”链接。 打开从 GitHub 存储库中克隆的代码。 调试代码不通过项目或解决方案即可在 Visual Studio 中调试代码。 对某些语言进行调试时，可能需要在代码项目中指定一个有效的启动文件，例如脚本、可执行文件或项目。 调试代码时，Visual Studio 会首先运行此指定代码。 工具栏上“开始”按钮旁的下拉列表框中列出了 Visual Studio 检测到的所有启动项，以及你在文件夹中专门选择的项。 Visual Studio 会自动识别项目，但是需要你将脚本（例如 Python 和 JavaScript）显式选择为启动项之后，项目才会出现在列表中。 此外，某些启动项（例如 MSBuild 和 CMake）可能有多个生成配置，这些生成配置会显示在运行按钮的下拉列表中。 调试可执行文件 在 Visual Studio 菜单上，选择“调试”。 在下拉菜单上，选择该项目，或者选择想要在解决方案资源管理器中显示为启动项的项目或文件。 选择 F5 键开始调试。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cannot start process the working directory '' does not exist]]></title>
    <url>%2F2017%2F12%2F26%2Fcannot%20start%20process%20the%20working%20directory%20''%20does%20not%20exist%2F</url>
    <content type="text"><![CDATA[问题描述突然有一天启动pycharm时，Python Console报出错误：cannot start process the working directory ‘’ does not exist 如图所示： 解决方法：删除项目文件夹下面的两个文件: 再次重启pycharm打开项目文件，则恢复正常。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Statistical Inference and Estimation]]></title>
    <url>%2F2017%2F12%2F04%2FStatistical%20Inference%20and%20Estimation%2F</url>
    <content type="text"><![CDATA[Statistical InferenceRecall, a statistical inference aims at learning characteristics of the population from a sample; the population characteristics are parameters and sample characteristics are statistics. A statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. Estimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Confidence Intervals = gives a range of values for the parameter Interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. Hypothesis tests = tests for a specific value(s) of the parameter. In order to perform these inferential tasks, i.e., make inference about the unknown population parameter from the sample statistic, we need to know the likely values of the sample statistic. What would happen if we do sampling many times? CLT,Central Limit TheoremSampling distribution of the sample mean: For categorical data, the CLT holds for the sampling distribution of the sample proportion. So, what is the 95% confidence interval? Based on the CLT, the 95% CI is Point EstimationEstimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Model &amp; EstimationA statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. In models, the focus is on estimating the model parameters. The basic inference tools (e.g., point estimation, hypothesis testing, and confidence intervals) will be applied to the these parameters. When discussing models, we will keep in mind the following parts: ObjectiveState what the objective is for this model. For instance, “Estimate the probability that a characteristic is present given the value of the explanatory values are … “ Model Structure Confidence IntervalsGeneral form of a confidence interval (CI)A confidence interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. The general form: estimate ± critical value × std.dev of the estimate estimate ± margin of errorFor example: sample mean ± critical value × estimated standard error The CIs differ based on: The parameter of interest, e.g., population mean, population proportion, difference in population’s means, etc… Design of the sample: SRS, stratified, experiments Confidence level or a confidence coefficient, (1 - α)100%, e.g., 95%, 99%, 90%, 80%, corresponding, respectively, to α values of 0.05, 0.01, 0.1, 0.2, etc… Interpretation of a Confidence IntervalIn most general terms, for a 95% CI, we say “we are 95% confident that the true population parameter is between the lower and upper calculated values”. A 95% CI for a population parameter DOES NOT mean that the interval has a probability of 0.95 that the true value of the parameter falls in the interval. The CI either contains the parameter or it does not contain it. The probability is associated with the process that generated the interval. And if we repeat this process many times, 95% of all intervals should in fact contain the true value of the parameter. What does a 99% CI say? Would you choose a 99% or 95% CI, and why? TradeoffsWe want confidence coefficient to be closer to 1. We want the sample size to be as small as possible (but not too small). This is a practical issue. We want the CI to be as narrow as possible As we increase the sample estimate, the CI …? As we decrease st. dev, the CI …? As we decrease the confidence level, (1-α), the CI …? As we increase sample size….? z-Tests &amp; Intervals More About Confidence IntervalsSimplified Expression for a 95% Confidence Interval Generalizing the 95% Confidence Interval Height Example Assume that the s is known and is equal to 3. We want to estimate the unknown true height of our population. Point sample estimate, can be the sample mean, 66.463. What is the distribution of the sample mean? What is the 95% confidence interval? What does it mean? Confidence Intervals for Proportions in NewspapersAs found in CNN in June, 2006: The stated Margin of error: +/- 3% Therefore, this would be the Confidence interval: 62%+/- 3%. We can be really confident that between 59% and 65% of all U.S. adults disapprove of how President Bush is handling the situation in Iraq. Hypothesis TestingBasic approach to hypothesis testing State a model describing the relationship between the explanatory variables and the outcome variable(s) in the population and the nature of the variability. State all of your assumptions. Specify the null and alternative hypotheses in terms of the parameters of the model. Invent a test statistic that will tend to be different under the null and alternative hypotheses. Using the assumptions of step 1, find the theoretical sampling distribution of the statistic under the null hypothesis of step 2. Ideally the form of the sampling distribution should be one of the “standard distributions”(e.g. normal, t, binomial..) Calculate a p-value, as the area under the sampling distribution more extreme than your statistic. Depends on the form of the alternative hypothesis. Choose your acceptable type 1 error rate (alpha) and apply the decision rule: reject the null hypothesis if the p-value is less than alpha, otherwise do not reject. Making the DecisionIt is either likely or unlikely that we would collect the evidence we did given the initial assumption. (Note: “likely” or “unlikely” is measured by calculating a probability!) If it is likely, then we “do not reject” our initial assumption. There is not enough evidence to do otherwise. If it is unlikely, then: either our initial assumption is correct and we experienced an unusual event or,our initial assumption is incorrectIn statistics, if it is unlikely, we decide to “reject” our initial assumption. Example: Criminal Trial AnalogyFirst, state 2 hypotheses, the null hypothesis (“H0”) and the alternative hypothesis (“HA”) H0: Defendant is not guilty. HA: Defendant is guilty. Usually the H0 is a statement of “no effect”, or “no change”, or “chance only” about a population parameter. While the HA , depending on the situation, is that there is a difference, trend, effect, or a relationship with respect to a population parameter. It can one-sided and two-sided. In two-sided we only care there is a difference, but not the direction of it. In one-sided we care about a particular direction of the relationship. We want to know if the value is strictly larger or smaller. Then, collect evidence, such as finger prints, blood spots, hair samples, carpet fibers, shoe prints, ransom notes, handwriting samples, etc. (In statistics, the data are the evidence.) Next, you make your initial assumption. Defendant is innocent until proven guilty. In statistics, we always assume the null hypothesis is true. Then, make a decision based on the available evidence. If there is sufficient evidence (“beyond a reasonable doubt”), reject the null hypothesis. (Behave as if defendant is guilty.) If there is not enough evidence, do not reject the null hypothesis. (Behave as if defendant is not guilty.) If the observed outcome, e.g., a sample statistic, is surprising under the assumption that the null hypothesis is true, but more probable if the alternative is true, then this outcome is evidence against H0 and in favor of HA. An observed effect so large that it would rarely occur by chance is called statistically significant (i.e., not likely to happen by chance). Using the p-value to make the decisionThe p-value represents how likely we would be to observe such an extreme sample if the null hypothesis were true. The p-value is a probability computed assuming the null hypothesis is true, that the test statistic would take a value as extreme or more extreme than that actually observed. Since it’s a probability, it is a number between 0 and 1. The closer the number is to 0 means the event is “unlikely.” So if p-value is “small,” (typically, less than 0.05), we can then reject the null hypothesis. Significance level and p-valueSignificance level, α, is a decisive value for p-value. In this context, significant does not mean “important”, but it means “not likely to happened just by chance”. α is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If α = 1 we always reject the null, if α = 0 we never reject the null hypothesis. In articles, journals, etc… you may read: “The results were significant (p&lt;0.05).” So if p=0.03, it’s significant at the level of α = 0.05 but not at the level of α = 0.01. If we reject the H0 at the level of α = 0.05 (which corresponds to 95% CI), we are saying that if H0 is true, the observed phenomenon would happen no more than 5% of the time (that is 1 in 20). If we choose to compare the p-value to α = 0.01, we are insisting on a stronger evidence! Errors in Hypothesis Testing PowerThe power of a statistical test is its probability of rejecting the null hypothesis if the null hypothesis is false. That is, power is the ability to correctly reject H0 and detect a significant effect. In other words, power is one minus the type II error risk. Which error is worse? Type I = you are innocent, yet accused of cheating on the test. Type II = you cheated on the test, but you are found innocent. This depends on the context of the problem too. But in most cases scientists are trying to be “conservative”; it’s worse to make a spurious discovery than to fail to make a good one. Our goal it to increase the power of the test that is to minimize the length of the CI. We need to keep in mind: the effect of the sample size, the correctness of the underlying assumptions about the population, statistical vs. practical significance, etc… sometimes the p-value is too low because of the large sample size, and we may have statistical significance but not really practical significance! That’s why most statisticians are much more comfortable with using CI than tests. There is a need for a further generalization. What if we can’t assume that σ is known? In this case we would use s (the sample standard deviation) to estimate σ. If the sample is very large, we can treat σ as known by assuming that σ = s. According to the law of large numbers, this is not too bad a thing to do. But if the sample is small, the fact that we have to estimate both the standard deviation and the mean adds extra uncertainty to our inference. In practice this means that we need a larger multiplier for the standard error. We need one-sample t-test. One sample t-test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Sampling distribution</tag>
        <tag>Central Limit Theorem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One-Way Tables and Goodness-of-Fit Test]]></title>
    <url>%2F2017%2F12%2F04%2FOne-Way%20Tables%20and%20Goodness-of-Fit%20Test%2F</url>
    <content type="text"><![CDATA[Statistical InferenceRecall, a statistical inference aims at learning characteristics of the population from a sample; the population characteristics are parameters and sample characteristics are statistics. A statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. Estimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Confidence Intervals = gives a range of values for the parameter Interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. Hypothesis tests = tests for a specific value(s) of the parameter. In order to perform these inferential tasks, i.e., make inference about the unknown population parameter from the sample statistic, we need to know the likely values of the sample statistic. What would happen if we do sampling many times? CLT,Central Limit TheoremSampling distribution of the sample mean: For categorical data, the CLT holds for the sampling distribution of the sample proportion. So, what is the 95% confidence interval? Based on the CLT, the 95% CI is Point EstimationEstimation represents ways or a process of learning and determining the population parameter based on the model fitted to the data. Point estimation and interval estimation, and hypothesis testing are three main ways of learning about the population parameter from the sample statistic. An estimator is particular example of a statistic, which becomes an estimate when the formula is replaced with actual observed sample values. Point estimation = a single value that estimates the parameter. Point estimates are single values calculated from the sample Model &amp; EstimationA statistical model is a representation of a complex phenomena that generated the data. It has mathematical formulations that describe relationships between random variables and parameters. It makes assumptions about the random variables, and sometimes parameters. A general form: data = model + residuals Model should explain most of the variation in the data Residuals are a representation of a lack-of-fit, that is of the portion of the data unexplained by the model. In models, the focus is on estimating the model parameters. The basic inference tools (e.g., point estimation, hypothesis testing, and confidence intervals) will be applied to the these parameters. When discussing models, we will keep in mind the following parts: ObjectiveState what the objective is for this model. For instance, “Estimate the probability that a characteristic is present given the value of the explanatory values are … “ Model Structure Confidence IntervalsGeneral form of a confidence interval (CI)A confidence interval estimates are intervals within which the parameter is expected to fall, with a certain degree of confidence. The general form: estimate ± critical value × std.dev of the estimate estimate ± margin of errorFor example: sample mean ± critical value × estimated standard error The CIs differ based on: The parameter of interest, e.g., population mean, population proportion, difference in population’s means, etc… Design of the sample: SRS, stratified, experiments Confidence level or a confidence coefficient, (1 - α)100%, e.g., 95%, 99%, 90%, 80%, corresponding, respectively, to α values of 0.05, 0.01, 0.1, 0.2, etc… Interpretation of a Confidence IntervalIn most general terms, for a 95% CI, we say “we are 95% confident that the true population parameter is between the lower and upper calculated values”. A 95% CI for a population parameter DOES NOT mean that the interval has a probability of 0.95 that the true value of the parameter falls in the interval. The CI either contains the parameter or it does not contain it. The probability is associated with the process that generated the interval. And if we repeat this process many times, 95% of all intervals should in fact contain the true value of the parameter. What does a 99% CI say? Would you choose a 99% or 95% CI, and why? TradeoffsWe want confidence coefficient to be closer to 1. We want the sample size to be as small as possible (but not too small). This is a practical issue. We want the CI to be as narrow as possible As we increase the sample estimate, the CI …? As we decrease st. dev, the CI …? As we decrease the confidence level, (1-α), the CI …? As we increase sample size….? z-Tests &amp; Intervals More About Confidence IntervalsSimplified Expression for a 95% Confidence Interval Generalizing the 95% Confidence Interval Height Example Assume that the s is known and is equal to 3. We want to estimate the unknown true height of our population. Point sample estimate, can be the sample mean, 66.463. What is the distribution of the sample mean? What is the 95% confidence interval? What does it mean? Confidence Intervals for Proportions in NewspapersAs found in CNN in June, 2006: The stated Margin of error: +/- 3% Therefore, this would be the Confidence interval: 62%+/- 3%. We can be really confident that between 59% and 65% of all U.S. adults disapprove of how President Bush is handling the situation in Iraq. Hypothesis TestingBasic approach to hypothesis testing State a model describing the relationship between the explanatory variables and the outcome variable(s) in the population and the nature of the variability. State all of your assumptions. Specify the null and alternative hypotheses in terms of the parameters of the model. Invent a test statistic that will tend to be different under the null and alternative hypotheses. Using the assumptions of step 1, find the theoretical sampling distribution of the statistic under the null hypothesis of step 2. Ideally the form of the sampling distribution should be one of the “standard distributions”(e.g. normal, t, binomial..) Calculate a p-value, as the area under the sampling distribution more extreme than your statistic. Depends on the form of the alternative hypothesis. Choose your acceptable type 1 error rate (alpha) and apply the decision rule: reject the null hypothesis if the p-value is less than alpha, otherwise do not reject. Making the DecisionIt is either likely or unlikely that we would collect the evidence we did given the initial assumption. (Note: “likely” or “unlikely” is measured by calculating a probability!) If it is likely, then we “do not reject” our initial assumption. There is not enough evidence to do otherwise. If it is unlikely, then: either our initial assumption is correct and we experienced an unusual event or,our initial assumption is incorrectIn statistics, if it is unlikely, we decide to “reject” our initial assumption. Example: Criminal Trial AnalogyFirst, state 2 hypotheses, the null hypothesis (“H0”) and the alternative hypothesis (“HA”) H0: Defendant is not guilty. HA: Defendant is guilty. Usually the H0 is a statement of “no effect”, or “no change”, or “chance only” about a population parameter. While the HA , depending on the situation, is that there is a difference, trend, effect, or a relationship with respect to a population parameter. It can one-sided and two-sided. In two-sided we only care there is a difference, but not the direction of it. In one-sided we care about a particular direction of the relationship. We want to know if the value is strictly larger or smaller. Then, collect evidence, such as finger prints, blood spots, hair samples, carpet fibers, shoe prints, ransom notes, handwriting samples, etc. (In statistics, the data are the evidence.) Next, you make your initial assumption. Defendant is innocent until proven guilty. In statistics, we always assume the null hypothesis is true. Then, make a decision based on the available evidence. If there is sufficient evidence (“beyond a reasonable doubt”), reject the null hypothesis. (Behave as if defendant is guilty.) If there is not enough evidence, do not reject the null hypothesis. (Behave as if defendant is not guilty.) If the observed outcome, e.g., a sample statistic, is surprising under the assumption that the null hypothesis is true, but more probable if the alternative is true, then this outcome is evidence against H0 and in favor of HA. An observed effect so large that it would rarely occur by chance is called statistically significant (i.e., not likely to happen by chance). Using the p-value to make the decisionThe p-value represents how likely we would be to observe such an extreme sample if the null hypothesis were true. The p-value is a probability computed assuming the null hypothesis is true, that the test statistic would take a value as extreme or more extreme than that actually observed. Since it’s a probability, it is a number between 0 and 1. The closer the number is to 0 means the event is “unlikely.” So if p-value is “small,” (typically, less than 0.05), we can then reject the null hypothesis. Significance level and p-valueSignificance level, α, is a decisive value for p-value. In this context, significant does not mean “important”, but it means “not likely to happened just by chance”. α is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If α = 1 we always reject the null, if α = 0 we never reject the null hypothesis. In articles, journals, etc… you may read: “The results were significant (p&lt;0.05).” So if p=0.03, it’s significant at the level of α = 0.05 but not at the level of α = 0.01. If we reject the H0 at the level of α = 0.05 (which corresponds to 95% CI), we are saying that if H0 is true, the observed phenomenon would happen no more than 5% of the time (that is 1 in 20). If we choose to compare the p-value to α = 0.01, we are insisting on a stronger evidence! Errors in Hypothesis Testing PowerThe power of a statistical test is its probability of rejecting the null hypothesis if the null hypothesis is false. That is, power is the ability to correctly reject H0 and detect a significant effect. In other words, power is one minus the type II error risk. Which error is worse? Type I = you are innocent, yet accused of cheating on the test. Type II = you cheated on the test, but you are found innocent. This depends on the context of the problem too. But in most cases scientists are trying to be “conservative”; it’s worse to make a spurious discovery than to fail to make a good one. Our goal it to increase the power of the test that is to minimize the length of the CI. We need to keep in mind: the effect of the sample size, the correctness of the underlying assumptions about the population, statistical vs. practical significance, etc… sometimes the p-value is too low because of the large sample size, and we may have statistical significance but not really practical significance! That’s why most statisticians are much more comfortable with using CI than tests. There is a need for a further generalization. What if we can’t assume that σ is known? In this case we would use s (the sample standard deviation) to estimate σ. If the sample is very large, we can treat σ as known by assuming that σ = s. According to the law of large numbers, this is not too bad a thing to do. But if the sample is small, the fact that we have to estimate both the standard deviation and the mean adds extra uncertainty to our inference. In practice this means that we need a larger multiplier for the standard error. We need one-sample t-test. One sample t-test]]></content>
      <categories>
        <category>statistics</category>
      </categories>
      <tags>
        <tag>Sampling distribution</tag>
        <tag>Central Limit Theorem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸PCA]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8PCA%2F</url>
    <content type="text"><![CDATA[降维技术PCA(Principal Component Analysis,主成分分析)在PCA中，数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。然后会发现，大部分方差都包含在最前面的几个新坐标轴中。因此，我们可以忽略余下的坐标轴，即对数据进行了降维处理。 FA(Factor Analysis,因子分析)在因子分析中，假设在观察数据的生成中有一些观察不到的隐变量(latent variable)。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。 ICA(Independent Component Analysis,独立成分分析)ICA假设数据是从N个数据源生成的，这额鹅鹅鹅一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。 PCA用一组正交基对坐标轴进行旋转，坐标轴的旋转并没有减少数据的维度。特征值分析是线性代数中的一个领域，它能够通过数据的一般格式来揭示数据的“真实”结构，即我们常说的特征向量和特征值。NumPy中有寻找特征向量和特征值得模块linalg,它有eig()方法，该方法用于求解特征向量和特征值。 在NumPy中实现PCA的伪代码1234567去除平均值计算协方差矩阵计算协方差矩阵的特征值和特征向量将特征值从大到小排序保留最上面的N个特征向量将数据转换到上述N个特征向量构建的新空间中 PCA的代码实现12345678910111213141516171819202122232425262728from numpy import *def loadDataSet(fileName, delim='\t'): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [map(float,line) for line in stringArr] return mat(datArr)def pca(dataMat, topNfeat=9999999): meanVals = mean(dataMat, axis=0) meanRemoved = dataMat - meanVals #remove mean covMat = cov(meanRemoved, rowvar=0) eigVals,eigVects = linalg.eig(mat(covMat)) eigValInd = argsort(eigVals) #sort, sort goes smallest to largest eigValInd = eigValInd[:-(topNfeat+1):-1] #cut off unwanted dimensions redEigVects = eigVects[:,eigValInd] #reorganize eig vects largest to smallest lowDDataMat = meanRemoved * redEigVects#transform data into new dimensions reconMat = (lowDDataMat * redEigVects.T) + meanVals return lowDDataMat, reconMatdef replaceNanWithMean(): datMat = loadDataSet('secom.data', ' ') numFeat = shape(datMat)[1] for i in range(numFeat): meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[0],i]) #values that are not NaN (a number) datMat[nonzero(isnan(datMat[:,i].A))[0],i] = meanVal #set NaN values to mean return datMat 在上面的代码中，首先计算并减去原始数据集的平均值。 然后，计算协方差矩阵及其特征值，接着利用argsort()函数对特征值进行从小到大的排序。 根据特征值排序结果的逆序就可以得到topNfeat个最大的特征向量。这些特征向量构成后面对数据进行转换的矩阵，该矩阵则利用N个特征将原始数据转换到新空间中。 最后，原始数据乘上特征向量矩阵，数据被重构返回。 PCA小结降维技术使得数据变得更易使用，并且它们往往能够去除数据中的噪声，使得其他机器学习任务更加精确。降维往往作为预处理步骤，在数据应用到其他算法之前清洗数据。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>因子分析</tag>
        <tag>独立成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸CART]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8CART%2F</url>
    <content type="text"><![CDATA[CART(Classification And Regression Trees)既可以用于分类还可以用于回归。 CART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一棵子树或者单个值。字典还包含特征和特征值这两个键，它们给出切分算法所有的特征和特征值。 建立树节点：1234567classtreeNode()： def __init__(self,feat,val,right,left)： featureToSplitOn = feat valueOfSplit = val rightBranch = right leftBranch = left 函数createTree()的伪代码大致如下：123456找到最佳的待切分特征： 如果该节点不能再分，将该节点存为叶节点 执行二元切分 在右子树调用createTree()方法 在左子树调用createTree()方法 CART算法的实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\t') fltLine = map(float,curLine) #map all elements to float() dataMat.append(fltLine) return dataMatdef binSplitDataSet(dataSet, feature, value): mat0 = dataSet[nonzero(dataSet[:,feature] &gt; value)[0],:][0] mat1 = dataSet[nonzero(dataSet[:,feature] &lt;= value)[0],:][0] return mat0,mat1def regLeaf(dataSet):#returns the value used for each leaf return mean(dataSet[:,-1])def regErr(dataSet): return var(dataSet[:,-1]) * shape(dataSet)[0]def linearSolve(dataSet): #helper function used in two places m,n = shape(dataSet) X = mat(ones((m,n))); Y = mat(ones((m,1)))#create a copy of data with 1 in 0th postion X[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]#and strip out Y xTx = X.T*X if linalg.det(xTx) == 0.0: raise NameError('This matrix is singular, cannot do inverse,\n\ try increasing the second value of ops') ws = xTx.I * (X.T * Y) return ws,X,Ydef modelLeaf(dataSet):#create linear model and return coeficients ws,X,Y = linearSolve(dataSet) return wsdef modelErr(dataSet): ws,X,Y = linearSolve(dataSet) yHat = X * ws return sum(power(Y - yHat,2))def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)): tolS = ops[0]; tolN = ops[1] #if all the target variables are the same value: quit and return value if len(set(dataSet[:,-1].T.tolist()[0])) == 1: #exit cond 1 return None, leafType(dataSet) m,n = shape(dataSet) #the choice of the best feature is driven by Reduction in RSS error from mean S = errType(dataSet) bestS = inf; bestIndex = 0; bestValue = 0 for featIndex in range(n-1): for splitVal in set(dataSet[:,featIndex]): mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): continue newS = errType(mat0) + errType(mat1) if newS &lt; bestS: bestIndex = featIndex bestValue = splitVal bestS = newS #if the decrease (S-bestS) is less than a threshold don't do the split if (S - bestS) &lt; tolS: return None, leafType(dataSet) #exit cond 2 mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): #exit cond 3 return None, leafType(dataSet) return bestIndex,bestValue#returns the best feature to split on #and the value used for that splitdef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering feat, val = chooseBestSplit(dataSet, leafType, errType, ops)#choose the best split if feat == None: return val #if the splitting hit a stop condition return val retTree = &#123;&#125; retTree['spInd'] = feat retTree['spVal'] = val lSet, rSet = binSplitDataSet(dataSet, feat, val) retTree['left'] = createTree(lSet, leafType, errType, ops) retTree['right'] = createTree(rSet, leafType, errType, ops) return retTree 树剪枝通过降低决策树的复杂度来避免过拟合的过程称为剪枝。在chooseBestSplit()中的提前终止条件，实际上是在进行一种所谓的预剪枝操作。另一种形式的剪枝需要使用测试集和训练集，称为后剪枝。 停止条件对误差的数量级十分敏感，通过不断修改停止条件来得到合理结果并不是很好的办法。后剪枝，利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。 后剪枝函数prune()的伪代码如下： 123456基于已有的树切分测试数据： 如果存在任一子集是一棵树，则在该子集递归剪枝过程 计算将当前两个叶节点合并后的误差 计算不合并的误差 如果合并会降低误差的话，就将叶节点合并 回归树剪枝函数Python代码： 123456789101112131415161718192021222324252627def isTree(obj): return (type(obj).__name__=='dict')def getMean(tree): if isTree(tree['right']): tree['right'] = getMean(tree['right']) if isTree(tree['left']): tree['left'] = getMean(tree['left']) return (tree['left']+tree['right'])/2.0 def prune(tree, testData): if shape(testData)[0] == 0: return getMean(tree) #if we have no test data collapse the tree if (isTree(tree['right']) or isTree(tree['left'])):#if the branches are not trees try to prune them lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) if isTree(tree['left']): tree['left'] = prune(tree['left'], lSet) if isTree(tree['right']): tree['right'] = prune(tree['right'], rSet) #if they are now both leafs, see if we can merge them if not isTree(tree['left']) and not isTree(tree['right']): lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +\ sum(power(rSet[:,-1] - tree['right'],2)) treeMean = (tree['left']+tree['right'])/2.0 errorMerge = sum(power(testData[:,-1] - treeMean,2)) if errorMerge &lt; errorNoMerge: print "merging" return treeMean else: return tree else: return tree]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>CART</tag>
        <tag>regression tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸SVD]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8SVD%2F</url>
    <content type="text"><![CDATA[利用SVD，我们能够用小得多的数据集来表示原始数据集。这样做，实际上是去除了噪声和冗余信息。 在PCA中得到的是矩阵的特征值，它们告诉我们数据集中的重要特征。这里的奇异值也是如此。奇异值和特征值是有关系的。这里的奇异值就是矩阵Data*DataT特征值的平方根。 相似度计算：123456789101112131415from numpy import *from numpy import linalg as ladef ecludSim(inA,inB): return 1.0/(1.0 + la.norm(inA - inB))def pearsSim(inA,inB): if len(inA) &lt; 3 : return 1.0 return 0.5+0.5*corrcoef(inA, inB, rowvar = 0)[0][1]def cosSim(inA,inB): num = float(inA.T*inB) denom = la.norm(inA)*la.norm(inB) return 0.5+0.5*(num/denom) 基于物品相似度的推荐引擎：1234567891011121314151617181920212223242526def standEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 for j in range(n): userRating = dataMat[user,j] if userRating == 0: continue overLap = nonzero(logical_and(dataMat[:,item].A&gt;0, \ dataMat[:,j].A&gt;0))[0] if len(overLap) == 0: similarity = 0 else: similarity = simMeas(dataMat[overLap,item], \ dataMat[overLap,j]) print 'the %d and %d similarity is: %f' % (item, j, similarity) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotaldef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst): unratedItems = nonzero(dataMat[user,:].A==0)[1]#find unrated items if len(unratedItems) == 0: return 'you rated everything' itemScores = [] for item in unratedItems: estimatedScore = estMethod(dataMat, user, simMeas, item) itemScores.append((item, estimatedScore)) return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N] 利用SVD提高推荐的效果基于SVD的评分估计 1234567891011121314151617def svdEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 U,Sigma,VT = la.svd(dataMat) Sig4 = mat(eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix xformedItems = dataMat.T * U[:,:4] * Sig4.I #create transformed items for j in range(n): userRating = dataMat[user,j] if userRating == 0 or j==item: continue similarity = simMeas(xformedItems[item,:].T,\ xformedItems[j,:].T) print 'the %d and %d similarity is: %f' % (item, j, similarity) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotal 小结SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留均值80%~90%的能量，就可以得到重要的特征并去掉噪声。协同过滤的核心是相似度计算方法；有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐引擎的效果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸ModelTree]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8ModelTree%2F</url>
    <content type="text"><![CDATA[模型树用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把也节点设定为分段线性函数，这里所谓的分段线性(piecewis linear)是指模型由多个线性片段组成。即分段线性模型。 相比CART，叶节点误差的计算需要稍加变化，对于给定的数据集，应该先用线性的模型来对它进行拟合，然后计算真实值与预测值的差值。最后将这些差值的平方求和就得到了所需的误差。 叶节点误差的计算：1234567891011121314151617181920def linearSolve(dataSet): #helper function used in two places m,n = shape(dataSet) X = mat(ones((m,n))); Y = mat(ones((m,1)))#create a copy of data with 1 in 0th postion X[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]#and strip out Y xTx = X.T*X if linalg.det(xTx) == 0.0: raise NameError('This matrix is singular, cannot do inverse,\n\ try increasing the second value of ops') ws = xTx.I * (X.T * Y) return ws,X,Ydef modelLeaf(dataSet):#create linear model and return coeficients ws,X,Y = linearSolve(dataSet) return wsdef modelErr(dataSet): ws,X,Y = linearSolve(dataSet) yHat = X * ws return sum(power(Y - yHat,2)) 用树回归进行预测的代码：12345678910111213141516171819202122232425def regTreeEval(model, inDat): return float(model)def modelTreeEval(model, inDat): n = shape(inDat)[1] X = mat(ones((1,n+1))) X[:,1:n+1]=inDat return float(X*model)def treeForeCast(tree, inData, modelEval=regTreeEval): if not isTree(tree): return modelEval(tree, inData) if inData[tree['spInd']] &gt; tree['spVal']: if isTree(tree['left']): return treeForeCast(tree['left'], inData, modelEval) else: return modelEval(tree['left'], inData) else: if isTree(tree['right']): return treeForeCast(tree['right'], inData, modelEval) else: return modelEval(tree['right'], inData) def createForeCast(tree, testData, modelEval=regTreeEval): m=len(testData) yHat = mat(zeros((m,1))) for i in range(m): yHat[i,0] = treeForeCast(tree, mat(testData[i]), modelEval) return yHat 调用函数treeForeCast()时需要指定树的类型，以便在叶节点上能够调用合适的模型。参数modelEval是对叶节点数据进行预测的函数的引用。treeForeCast()自顶向下遍历整棵树，直到命中叶节点为止。一旦到达叶节点，它就会在输入数据上调用modelEval()函数，而该函数的默认值是regTreeEval()。 若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则成为模型树。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>model tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸LinearRegression]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8LinearRegression%2F</url>
    <content type="text"><![CDATA[线性回归解析式求解： 线性回归解析式解Python代码实现123456789101112131415161718192021222324from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats numFeat = len(open(fileName).readline().split('\t')) - 1 #get number of fields dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr =[] curLine = line.strip().split('\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMatdef standRegres(xArr,yArr): xMat = mat(xArr); yMat = mat(yArr).T xTx = xMat.T*xMat if linalg.det(xTx) == 0.0: print "This matrix is singular, cannot do inverse" return ws = xTx.I * (xMat.T*yMat) return ws 局部加权线性回归局部加权线性回归(Locally Weighted Linear Regression,LWLR)。在该算法中，我们给待预测点附近的每个点赋予一定的权重；然后在这个子集上基于最小均方差来进行普通的回归。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。 该算法解出回归系数w的形式如下： 其中w是一个矩阵，用来给每个数据点赋予权重。 LWLR使用“核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下： 这样就构建了一个只含对角元素的权重矩阵w，并且点x与x(i)越近，w(i,i)将会越大。参数k决定了对附近的点赋予多大的权重，则也是使用LWLR时唯一需要考虑的参数。 下面是参数k与权重的关系： 局部加权线性回归解析式解Python代码实现123456789101112131415161718192021def lwlr(testPoint,xArr,yArr,k=1.0): xMat = mat(xArr); yMat = mat(yArr).T m = shape(xMat)[0] weights = mat(eye((m))) for j in range(m): #next 2 lines create weights matrix diffMat = testPoint - xMat[j,:] # weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2)) xTx = xMat.T * (weights * xMat) if linalg.det(xTx) == 0.0: print "This matrix is singular, cannot do inverse" return ws = xTx.I * (xMat.T * (weights * yMat)) return testPoint * wsdef lwlrTest(testArr,xArr,yArr,k=1.0): #loops over all the data points and applies lwlr to each one m = shape(testArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHat 局部加权线性回归的问题在于，每次必须在整个数据集上运行，也就是说为了做出预测，必须保存所有的训练数据。 岭回归和逐步线性回归岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入lamda来限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫做缩减（shrinkage）。 岭回归中的岭是什么？岭回归使用了单位矩阵乘以常量lamda，最先是用来处理特征数多于样本数的情况，我们观察其中的单位矩阵I，可以看到值1贯穿整个对角线，其余元素全是0。形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中的“岭”的由来。 岭回归解析式解Python代码实现123456789101112131415161718192021222324def ridgeRegres(xMat,yMat,lam=0.2): xTx = xMat.T*xMat denom = xTx + eye(shape(xMat)[1])*lam if linalg.det(denom) == 0.0: print "This matrix is singular, cannot do inverse" return ws = denom.I * (xMat.T*yMat) return ws def ridgeTest(xArr,yArr): xMat = mat(xArr); yMat=mat(yArr).T yMean = mean(yMat,0) yMat = yMat - yMean #to eliminate X0 take mean off of Y #regularize X's xMeans = mean(xMat,0) #calc mean then subtract it off xVar = var(xMat,0) #calc variance of Xi then divide by it xMat = (xMat - xMeans)/xVar numTestPts = 30 wMat = zeros((numTestPts,shape(xMat)[1])) for i in range(numTestPts): ws = ridgeRegres(xMat,yMat,exp(i-10)) wMat[i,:]=ws.T return wMat 岭回归的回归系数变化图这里的lambda应以指数级变化，这样可以看出lambda在取非常小的值时和取非常大的值时分别对结果造成的影响。 还有一些其他缩减方法，如lasso、LAR、PCA回归以及子集选择等。与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。 在增加如下约束时，普通的最小二乘法回归会得到与岭回归的一样的公式： 上式限定了所有回归系数的平方和不能大于lambda。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得出一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。 与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下： 唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭：在lambda足够小的时候，一些系数会因此被迫缩减到0，这个特性可以帮助我们更好地理解数据。这两个约束条件在公式上看起来相差无几，但细微的变化却极大地增加了计算复杂度（为了在这个新的约束条件下解出回归系数，需要使用二次规划算法）。 前向逐步回归前向逐步回归属于一种贪心算法，即每一步都尽可能减少误差。一开始所有的权重都设为0，然后每一步所做的决策是对某个权重增加或减少一个很小的值。 该算法的伪代码如下所示：12345678910数据标准化，使其分布满足0均值和单位方差在每轮迭代过程中： 设置当前最小误差lowestError为正无穷 对每个特征： 增大或缩小： 改变一个系数得到一个新的W 计算新W下的误差 如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W 将W设置为新的Wbest 前向逐步回归python代码实现123456789101112131415161718192021222324252627def rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays return ((yArr-yHatArr)**2).sum()def stageWise(xArr,yArr,eps=0.01,numIt=100): xMat = mat(xArr); yMat=mat(yArr).T yMean = mean(yMat,0) yMat = yMat - yMean #can also regularize ys but will get smaller coef xMat = regularize(xMat) m,n=shape(xMat) returnMat = zeros((numIt,n)) #testing code remove ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy() for i in range(numIt):#could change this to while loop #print ws.T lowestError = inf; for j in range(n): for sign in [-1,1]: wsTest = ws.copy() wsTest[j] += eps*sign yTest = xMat*wsTest rssE = rssError(yMat.A,yTest.A) if rssE &lt; lowestError: lowestError = rssE wsMax = wsTest ws = wsMax.copy() returnMat[i,:]=ws.T return returnMat 逐步线性回归，主要的优点在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。 当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias),与此同时却减少了模型的方差。 权衡偏差与方差 误差由三个部分组成：偏差、误差和噪声。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>岭回归</tag>
        <tag>局部加权线性回归</tag>
        <tag>逐步回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸k-Means]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%89%8B%E6%92%B8k-Means%2F</url>
    <content type="text"><![CDATA[K-Means聚类算法K-Means聚类算法伪代码12345678创建k个点作为起始质心当任意一个点的簇分配结果发生改变时 对数据集中的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到距其最近的簇 对每一个簇，计算簇中所有点的均值并将均值作为质心 K-Means聚类辅助函数1234567891011121314151617181920212223from numpy import *def loadDataSet(fileName): #general function to parse tab -delimited floats dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\t') fltLine = map(float,curLine) #map all elements to float() dataMat.append(fltLine) return dataMatdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)def randCent(dataSet, k): n = shape(dataSet)[1] centroids = mat(zeros((k,n)))#create centroid mat for j in range(n):#create random cluster centers, within bounds of each dimension minJ = min(dataSet[:,j]) rangeJ = float(max(dataSet[:,j]) - minJ) centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) return centroids K-Means聚类主函数12345678910111213141516171819202122def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2)))#create mat to assign data points #to a centroid, also holds SE of each point centroids = createCent(dataSet, k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m):#for each data point assign it to the closest centroid minDist = inf; minIndex = -1 for j in range(k): distJI = distMeas(centroids[j,:],dataSet[i,:]) if distJI &lt; minDist: minDist = distJI; minIndex = j if clusterAssment[i,0] != minIndex: clusterChanged = True clusterAssment[i,:] = minIndex,minDist**2 print centroids for cent in range(k):#recalculate centroids ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean return centroids, clusterAssment K-Means算法只能收敛到局部最小值，而非全局最小值。 对聚类得到的簇进行后处理上面在包含簇分配结果额矩阵中保存着每个点的误差，即该点到簇质心的距离平方值。下面会利用该误差来评价聚类质量的方法。 一种用于度量聚类效果的指标是SSE(Sum of Squared Error,误差平方和)。SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值得方法是增加簇的个数，但是这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。 那么如何进行改进？ 一种方法是将具有最大SSE值得簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-Means算法，其中的k设为2. 为了保持簇总数不变，可以将某两个簇进行合并。如何合并？ 有两种可以量化的办法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。第一种思路通过计算所有质心之间的距离，然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。 二分K-Means聚类算法为克服K-Means算法收敛于局部最小值得问题，有人提出了另一个称为二分K-Means（bisecting K-Means）的算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。 二分K-Means算法的伪代码：12345678将所有点看成一个簇当簇数目小于k时对于每一个簇 计算总误差 在给定的簇上面进行K-Means聚类（k=2） 计算将该簇一分为二之后的总误差选择使得误差最小的那个簇进行划分操作 二分K-Means的代码实现：12345678910111213141516171819202122232425262728293031def biKmeans(dataSet, k, distMeas=distEclud): m = shape(dataSet)[0] clusterAssment = mat(zeros((m, 2))) centroid0 = mean(dataSet, axis=0).tolist()[0] centList = [centroid0] # create a list with one centroid for j in range(m): # calc initial Error clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :]) ** 2 while (len(centList) &lt; k): lowestSSE = inf for i in range(len(centList)): ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :] # get the data points currently in cluster i centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) sseSplit = sum(splitClustAss[:, 1]) # compare the SSE to the currrent minimum sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1]) print "sseSplit, and notSplit: ", sseSplit, sseNotSplit if (sseSplit + sseNotSplit) &lt; lowestSSE: bestCentToSplit = i bestNewCents = centroidMat bestClustAss = splitClustAss.copy() lowestSSE = sseSplit + sseNotSplit bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # change 1 to 3,4, or whatever bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit print 'the bestCentToSplit is: ', bestCentToSplit print 'the len of bestClustAss is: ', len(bestClustAss) centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0] # replace a centroid with two best centroids centList.append(bestNewCents[1, :].tolist()[0]) clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # reassign new clusters, and SSE return mat(centList), clusterAssment]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>KMeans</tag>
        <tag>二分K-Means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sequential Minimal Optimization]]></title>
    <url>%2F2017%2F11%2F29%2FSMO%2F</url>
    <content type="text"><![CDATA[INTRODUCTIONTraining a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size,which allows SMO to handle very large training sets.SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets.Instead of previous SVM learning algorithms that use numerical quadratic programming (QP) as an inner loop, SMO uses an analytic QP step. Overview of Support Vector Machines Lagrange dual formSVM在求解过程中首先用了拉格朗日对偶，作用是将w的计算提前并消除w，使得优化函数变为拉格朗日乘子的单一参数优化问题。对偶函数最后的优化问题： There is a one-to-one relationship between each Lagrange multiplier and each training example.Once the Lagrange multipliers are determined, the normal vector wand the threshold b can be derived from the Lagrange multipliers: linearly unseparableOf course, not all data sets are linearly separable. There may be no hyperplane that splits the positive examples from the negative examples. In the formulation above, the non-separable case would correspond to an infinite solution. However, in 1995, Cortes &amp; Vapnik suggested a modification to the original optimization statement (3) which allows, but penalizes, the failure of an example to reach the correct margin. That modification is: generalized to non-linear classifiersSVMs can be even further generalized to non-linear classifiers. The output of a non-linear SVM is explicitly computed from the Lagrange multipliers: Karush-Kuhn-Tucker (KKT) conditionsThe QP problem in equation (11), above, is the QP problem that the SMO algorithm will solve. In order to make the QP problem above be positive definite, the kernel function K must obey Mercer’s conditions.The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient conditions for an optimal point of a positive definite QP problem. The KKT conditions for the QP problem (11) are particularly simple. The QP problem is solved when, for all i: where ui is the output of the SVM for the ith training example. Notice that the KKT conditionscan be evaluated on one example at a time, which will be useful in the construction of the SMOalgorithm.### SEQUENTIAL MINIMAL OPTIMIZATIONSequential Minimal Optimization (SMO) is a simple algorithm that can quickly solve the SVM QP problem without any extra matrix storage and without using numerical QP optimization steps at all. SMO decomposes the overall QP problem into QP sub-problems, using Osuna’s theorem to ensure convergence.Unlike the previous methods, SMO chooses to solve the smallest possible optimization problem at every step. For the standard SVM QP problem, the smallest possible optimization problem involves two Lagrange multipliers, because the Lagrange multipliers must obey a linear equality constraint. At every step, SMO chooses two Lagrange multipliers to jointly optimize, finds the optimal values for these multipliers, and updates the SVM to reflect the new optimal values.The advantage of SMO lies in the fact that solving for two Lagrange multipliers can be done analytically. Thus, numerical QP optimization is avoided entirely. The inner loop of the algorithm can be expressed in a short amount of C code, rather than invoking an entire QP library routine. Even though more optimization sub-problems are solved in the course of the algorithm,each sub-problem is so fast that the overall QP problem is solved quickly.In addition, SMO requires no extra matrix storage at all. Thus, very large SVM training problems can fit inside of the memory of an ordinary personal computer or workstation. Because no matrix algorithms are used in SMO, it is less susceptible to numerical precision problems.There are two components to SMO: an analytic method for solving for the two Lagrange multipliers, and a heuristic for choosing which multipliers to optimize.假设我们选取了初始值{a_1,a_2,a_3,…,a_n}满足了问题中的约束条件。接下来，{a_3,a_4,…,a_n}，这样W就是a_1和a_2的函数。并且a_1和a_2满足条件： 由于{a_3,a_4,…,a_n}都是已知固定值，因此右边是一个具体的常数。当y_1和y_2异号时，也就是一个为1，一个为-1时，他们可以表示成一条直线，斜率为1；同理当y_1和y_2同号时，斜率为-1. Solving for Two Lagrange MultipliersIn order to solve for the two Lagrange multipliers, SMO first computes the constraints on these multipliers and then solves for the constrained minimum. For convenience, all quantities that refer to the first multiplier will have a subscript 1, while all quantities that refer to the second multiplier will have a subscript 2. Because there are only two multipliers, the constraints can be easily be displayed in two dimensions (see figure 3). The bound constraints (9) cause the Lagrange multipliers to lie within a box, while the linear equality constraint (6) causes the Lagrange multipliers to lie on a diagonal line. Thus, the constrained minimum of the objective function must lie on a diagonal line segment (as shown in figure 3). This constraint explains why two is the minimum number of Lagrange multipliers that can be optimized: if SMO optimized only one multiplier, it could not fulfill the linear equality constraint at every step. The ends of the diagonal line segment can be expressed quite simply. Without loss of generality,the algorithm first computes the second Lagrange multiplier a_2 and computes the ends of thediagonal line segment in terms of a_2. If the target y_1 does not equal the target y_2, then thefollowing bounds apply to a_2: L = max(0,a_2 - a_1), H = min(C,C + a_2- a_1). If the target y_1 equals the target y_2, then the following bounds apply to a_2: L = max(0,a_2 + a_1 - C), H = min(C, a_2 + a_1). 然后将a_1用a_2表示： 然后反代入W中，得： 展开后W可以表示成a_2的二次函数。这样，直接求导可以得到a_2，然而要保证满足L&lt;=a_2&lt;=H，我们使用new,unclipped表示求导求出来的a_2，然后最后的a_2,要根据下面情况得到： 这样得到新的a_2，就可以得到新的a_1。 论文中的表达方式： Heuristics for Choosing Which Multipliers To OptimizeAs long as SMO always optimizes and alters two Lagrange multipliers at every step and at least one of the Lagrange multipliers violated the KKT conditions before the step, then each step will decrease the objective function according to Osuna’s theorem [16]. Convergence is thus guaranteed. In order to speed convergence, SMO uses heuristics to choose which two Lagrange multipliers to jointly optimize. There are two separate choice heuristics: one for the first Lagrange multiplier and one for the second. The choice of the first heuristic provides the outer loop of the SMO algorithm. The outer loop first iterates over the entire training set, determining whether each example violates the KKT conditions (12). If an example violates the KKT conditions, it is then eligible for optimization. After one pass through the entire training set, the outer loop iterates over all examples whose Lagrange multipliers are neither 0 nor C (the non-bound examples). Again, each example is checked against the KKT conditions and violating examples are eligible for optimization. The outer loop makes repeated passes over the non-bound examples until all of the non-bound examples obey the KKT conditions within e. The outer loop then goes back and iterates over the entire training set. The outer loop keeps alternating between single passes over the entire training set and multiple passes over the non-bound subset until the entire training set obeys the KKT conditions within e, whereupon the algorithm terminates. The first choice heuristic concentrates the CPU time on the examples that are most likely to violate the KKT conditions: the non-bound subset. As the SMO algorithm progresses, examples that are at the bounds are likely to stay at the bounds, while examples that are not at the bounds will move as other examples are optimized. The SMO algorithm will thus iterate over the nonbound subset until that subset is self-consistent, then SMO will scan the entire data set to search for any bound examples that have become KKT violated due to optimizing the non-bound subset. Notice that the KKT conditions are checked to be within e of fulfillment. Typically, e is set to be 10-3. Recognition systems typically do not need to have the KKT conditions fulfilled to high accuracy: it is acceptable for examples on the positive margin to have outputs between 0.999 and 1.001. The SMO algorithm (and other SVM algorithms) will not converge as quickly if it is required to produce very high accuracy output. Once a first Lagrange multiplier is chosen, SMO chooses the second Lagrange multiplier tomaximize the size of the step taken during joint optimization. Now, evaluating the kernelfunction K is time consuming, so SMO approximates the step size by the absolute value of the numerator in equation (16): |E_1 - E_2|. SMO keeps a cached error value E for every non-bound example in the training set and then chooses an error to approximately maximize the step size. If E_1 is positive, SMO chooses an example with minimum error E2. If E1 is negative, SMO choosesan example with maximum error E_2. Under unusual circumstances, SMO cannot make positive progress using the second choiceheuristic described above. For example, positive progress cannot be made if the first and second training examples share identical input vectors x, which causes the objective function to become semi-definite. In this case, SMO uses a hierarchy of second choice heuristics until it finds a pair of Lagrange multipliers that can be make positive progress. Positive progress can be determined by making a non-zero step size upon joint optimization of the two Lagrange multipliers . The hierarchy of second choice heuristics consists of the following. If the above heuristic does not make positive progress, then SMO starts iterating through the non-bound examples, searching for an second example that can make positive progress. If none of the non-bound examples make positive progress, then SMO starts iterating through the entire training set until an example is found that makes positive progress. Both the iteration through the non-bound examples and the iteration through the entire training set are started at random locations, in order not to bias SMO towards the examples at the beginning of the training set. In extremely degenerate circumstances, none of the examples will make an adequate second example. When this happens, the first example is skipped and SMO continues with another chosen first example. Computing the ThresholdThe threshold b is re-computed after each step, so that the KKT conditions are fulfilled for both optimized examples. The following threshold b_1 is valid when the new a_1 is not at the bounds,because it forces the output of the SVM to be y_1 when the input is x_1: The following threshold b_2 is valid when the new a_2 is not at bounds, because it forces the output of the SVM to be y_2 when the input is x_2: When both b_1 and b_2 are valid, they are equal. When both new Lagrange multipliers are at bound and if L is not equal to H, then the interval between b_1 and b_2 are all thresholds that are consistent with the KKT conditions. SMO chooses the threshold to be halfway in between b_1 and b_2.### An Optimization for Linear SVMsTo compute a linear SVM, only a single weight vector w needs to be stored, rather than all of the training examples that correspond to non-zero Lagrange multipliers. If the joint optimization succeeds, the stored weight vector needs to be updated to reflect the new Lagrange multiplier values. The weight vector update is easy, due to the linearity of the SVM: Pseudo Code DetailsThe pseudo-code below describes the entire SMO algorithm:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293target = desired output vectorpoint = training point matrixprocedure takeStep(i1,i2) if (i1 == i2) return 0 alph1 = Lagrange multiplier for i1 y1 = target[i1] E1 = SVM output on point[i1] – y1 (check in error cache) s = y1*y2 Compute L, H via equations (13) and (14) if (L == H) return 0 k11 = kernel(point[i1],point[i1]) k12 = kernel(point[i1],point[i2]) k22 = kernel(point[i2],point[i2]) eta = k11+k22-2*k12 if (eta &gt; 0) &#123; a2 = alph2 + y2*(E1-E2)/eta if (a2 &lt; L) a2 = L else if (a2 &gt; H) a2 = H &#125; else &#123; Lobj = objective function at a2=L Hobj = objective function at a2=H if (Lobj &lt; Hobj-eps) a2 = L else if (Lobj &gt; Hobj+eps) a2 = H else a2 = alph2 &#125; if (|a2-alph2| &lt; eps*(a2+alph2+eps)) return 0 a1 = alph1+s*(alph2-a2) Update threshold to reflect change in Lagrange multipliers Update weight vector to reflect change in a1 &amp; a2, if SVM is linear Update error cache using new Lagrange multipliers Store a1 in the alpha array Store a2 in the alpha array return 1endprocedureprocedure examineExample(i2) y2 = target[i2] alph2 = Lagrange multiplier for i2 E2 = SVM output on point[i2] – y2 (check in error cache) r2 = E2*y2 if ((r2 &lt; -tol &amp;&amp; alph2 &lt; C) || (r2 &gt; tol &amp;&amp; alph2 &gt; 0)) &#123; if (number of non-zero &amp; non-C alpha &gt; 1) &#123; i1 = result of second choice heuristic (section 2.2) if takeStep(i1,i2) return 1 &#125; loop over all non-zero and non-C alpha, starting at a random point &#123; i1 = identity of current alpha if takeStep(i1,i2) return 1 &#125; loop over all possible i1, starting at a random point &#123; i1 = loop variable if (takeStep(i1,i2) return 1 &#125; &#125; return 0endproceduremain routine: numChanged = 0; examineAll = 1; while (numChanged &gt; 0 | examineAll) &#123; numChanged = 0; if (examineAll) loop I over all training examples numChanged += examineExample(I) else loop I over examples where alpha is not 0 &amp; not C numChanged += examineExample(I) if (examineAll == 1) examineAll = 0 else if (numChanged == 0) examineAll = 1 &#125; via Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines;© 1998 John Platt 支持向量机（五）SMO算法 【机器学习详解】SMO算法剖析]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BFGS算法]]></title>
    <url>%2F2017%2F11%2F19%2FBFGS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>BFGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸AdaBoost]]></title>
    <url>%2F2017%2F11%2F17%2F%E6%89%8B%E6%92%B8AdaBoost%2F</url>
    <content type="text"><![CDATA[AdaBoost算法概述： 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。 缺点：对离群点敏感。 适用数据类型：数值型和标称型数据。 将不同的分类器组合起来，成为集成方法（ensemble method）或者元算法（meta-algorithm)。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。 bagging(bootstrap aggregating)，是在从原始数据集选择s次后得到s个新的数据集的一种技术，新数据集和原始数据集大小相等。有放回的抽样，所以允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。在s个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了s个分类器。当我们要对新数据进行分类时，就可以应用这s个分类器进行分类。选择分类器投票结果中最多的类别作为最后的分类结果。 boosting和bagging类似，所使用的多个分类器类型都是一致的，但是boosting是通过串行训练而获得，每个新分类器都是根据已训练出的分类器的性能来进行训练。 AdaBoost代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# 生成数据集from numpy import *def loadSimpData(): datMat = matrix([[ 1. , 2.1], [ 2. , 1.1], [ 1.3, 1. ], [ 1. , 1. ], [ 2. , 1. ]]) classLabels = [1.0, 1.0, -1.0, -1.0, 1.0] return datMat,classLabels# 单层决策树生成函数# 单层决策树预测函数def stumpClassify(dataMatrix, dimen, threshVal, threshIneq): # just classify the data """ 通过阈值比较对数据进行分类。只用于连续输入变量。 :param dataMatrix: :param dimen:特征 :param threshVal:阈值 :param threshIneq:小于还是大于等于 :return: """ retArray = ones((shape(dataMatrix)[0], 1)) if threshIneq == 'lt': retArray[dataMatrix[:, dimen] &lt;= threshVal] = -1.0 else: retArray[dataMatrix[:, dimen] &gt; threshVal] = -1.0 return retArray# 单层决策树的创建def buildStump(dataArr, classLabels, D): """ 遍历stumpClassify()函数所有的可能输入值，并找到数据集上最佳的单层决策树。只用于连续输入变量。 :param dataArr: :param classLabels: :param D:权重向量 :return: """ dataMatrix = mat(dataArr) labelMat = mat(classLabels).T m, n = shape(dataMatrix) numSteps = 10.0 # 循环次数，用于计算步长 bestStump = &#123;&#125; # 用于存储给定权重向量D时所得到的最佳单层决策树的相关信息 bestClasEst = mat(zeros((m, 1))) # 标签的估计值 minError = inf # init error sum, to +infinity for i in range(n): # loop over all dimensions rangeMin = dataMatrix[:, i].min() rangeMax = dataMatrix[:, i].max() stepSize = (rangeMax - rangeMin) / numSteps for j in range(-1, int(numSteps) + 1): # loop over all range in current dimension for inequal in ['lt', 'gt']: # go over less than and greater than threshVal = (rangeMin + float(j) * stepSize) predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal) # call stump classify with i, j, lessThan errArr = mat(ones((m, 1))) errArr[predictedVals == labelMat] = 0 weightedError = D.T * errArr # calc total error multiplied by D # print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError) if weightedError &lt; minError: minError = weightedError bestClasEst = predictedVals.copy() bestStump['dim'] = i bestStump['thresh'] = threshVal bestStump['ineq'] = inequal return bestStump, minError, bestClasEst# 基于单层决策树的AdaBoost训练过程：def adaBoostTrainDS(dataArr,classLabels,numIt=40): """ 基于单层决策树的AdaBoost训练过程 :param dataArr:数据集 :param classLabels:类别标签 :param numIt:迭代次数 :return: """ weakClassArr = [] m = shape(dataArr)[0] D = mat(ones((m,1))/m) #init D to all equal aggClassEst = mat(zeros((m,1))) for i in range(numIt): bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump #print "D:",D.T alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0 bestStump['alpha'] = alpha weakClassArr.append(bestStump) #store Stump Params in Array #print "classEst: ",classEst.T expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy D = multiply(D,exp(expon)) #Calc New D for next iteration D = D/D.sum() #calc training error of all classifiers, if this is 0 quit for loop early (use break) aggClassEst += alpha*classEst #print "aggClassEst: ",aggClassEst.T aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1))) errorRate = aggErrors.sum()/m print "total error: ",errorRate if errorRate == 0.0: break return weakClassArr,aggClassEstimport matplotlib.pyplot as pltdef plotROC(predStrengths, classLabels): """ 画ROC曲线 :param predStrengths:预测概率 :param classLabels:真实标签 :return: """ cur = (1.0,1.0) #cursor ySum = 0.0 #variable to calculate AUC numPosClas = sum(array(classLabels)==1.0) yStep = 1/float(numPosClas) xStep = 1/float(len(classLabels)-numPosClas) sortedIndicies = predStrengths.argsort()#get sorted index, it's reverse fig = plt.figure() fig.clf() ax = plt.subplot(111) #loop through all the values, drawing a line segment at each point for index in sortedIndicies.tolist()[0]: if classLabels[index] == 1.0: delX = 0; delY = yStep; else: delX = xStep; delY = 0; ySum += cur[1] #draw line from cur to (cur[0]-delX,cur[1]-delY) ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY], c='b') cur = (cur[0]-delX,cur[1]-delY) ax.plot([0,1],[0,1],'b--') plt.xlabel('False positive rate'); plt.ylabel('True positive rate') plt.title('ROC curve for AdaBoost horse colic detection system') ax.axis([0,1,0,1]) plt.show() print "the Area Under the Curve is: ",ySum*xStep via 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸SVM]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8SVM%2F</url>
    <content type="text"><![CDATA[基于最大间隔分隔数据 优点：泛化错误率低，计算开销不大，结果易解释。 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。 适用数据类型：数值型和标称型数据。 SMO伪代码123456789创建一个alpha向量并将其初始化为0向量当迭代次数小于最大迭代次数时（外循环） 对数据集中的每个数据向量（内循环）： 如果该数据向量可以被优化： 随机选择另外一个数据向量 同时优化这两个向量 如果两个向量都不能被优化，退出内循环 如果所有向量都没被优化，增加迭代数目，继续下一次循环 SMO代码实现首先定义一些SMO算法中的辅助函数12345678910111213141516171819202122232425def loadDataSet(fileName): dataMat = [] labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = line.strip().split('\t') dataMat.append([float(lineArr[0]), float(lineArr[1])]) labelMat.append(float(lineArr[2])) return dataMat, labelMatdef selectJrand(i, m): j = i # we want to select any J not equal to i while (j == i): j = int(random.uniform(0, m)) return jdef clipAlpha(aj, H, L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj 简化版SMO算法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def smoSimple(dataMatIn, classLabels, C, toler, maxIter): dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() b = 0 m, n = shape(dataMatrix) alphas = mat(zeros((m, 1))) iter = 0 while (iter &lt; maxIter): alphaPairsChanged = 0 for i in range(m): fXi = float(multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[i, :].T)) + b Ei = fXi - float(labelMat[i]) # if checks if an example violates KKT conditions if ((labelMat[i] * Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i] * Ei &gt; toler) and (alphas[i] &gt; 0)): j = selectJrand(i, m) fXj = float(multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[j, :].T)) + b Ej = fXj - float(labelMat[j]) alphaIold = alphas[i].copy() alphaJold = alphas[j].copy() if (labelMat[i] != labelMat[j]): L = max(0, alphas[j] - alphas[i]) H = min(C, C + alphas[j] - alphas[i]) else: L = max(0, alphas[j] + alphas[i] - C) H = min(C, alphas[j] + alphas[i]) if L == H: print "L==H" continue eta = 2.0 * dataMatrix[i, :] * dataMatrix[j, :].T - dataMatrix[i, :] * dataMatrix[i, :].T - dataMatrix[j,:] * dataMatrix[j, :].T if eta &gt;= 0: print "eta&gt;=0" continue alphas[j] -= labelMat[j] * (Ei - Ej) / eta alphas[j] = clipAlpha(alphas[j], H, L) if (abs(alphas[j] - alphaJold) &lt; 0.00001): print "j not moving enough" continue alphas[i] += labelMat[j] * labelMat[i] * (alphaJold - alphas[j]) # update i by the same amount as j # the update is in the oppostie direction b1 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[i, :].T - labelMat[j] * (alphas[j] - alphaJold) * dataMatrix[i,:] * dataMatrix[j,:].T b2 = b - Ej - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[j, :].T - labelMat[j] * (alphas[j] - alphaJold) * dataMatrix[j,:] * dataMatrix[j,:].T if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): b = b2 else: b = (b1 + b2) / 2.0 alphaPairsChanged += 1 print "iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) if (alphaPairsChanged == 0): iter += 1 else: iter = 0 print "iteration number: %d" % iter return b, alphas 上面函数有5个输入参数，分别是：数据集、类别标签、常数C、容错率和取消前最大的循环次数。函数将多个列表和输入参数转换成NumPy矩阵，这样就可以简化很多数学处理操作。由于转置了类别标签，因此我们得到的就是一个列向量而不是列表。于是类别标签向量的每行元素都和数据矩阵中的行一一对应。构建一个alpha列矩阵，矩阵中元素都初始化为0，并建立一个iter变量。该变量存储的则是没有任何alpha改变的情况下遍历数据集的次数。当该变量达到输入值maxIter时，函数结束运行并退出。 每次循环当中，将alphaParirsChanged先设为0，然后再对整个集合顺序遍历。变量alphaPairsChanged用于记录alpha是否已经进行优化。 首先，fxi能够计算出来，这就是我们预测的类别。然后，基于这个实例的预测结果和真是结果的比对，就可以计算误差Ei。如果误差很大，那么可以对该数据实例所对应的alpha值进行优化。在if语句中，不管是正间隔还是负间隔都会被测试。并且在该if语句中，也要同时检查alpha值，以保证其不能等于0或C。由于后面alpha小于0或大于C时将调整为0或C，所以一旦在该if语句中它们等于这两个值得话，那么它们就已经在“边界”上了，因而不再能够减少或增大，因此也就不值得再对它们进行优化了。 接下来，利用辅助函数来随机选择第二个alpha值，即alpha[j]。同样，可以采用第一个alpha值（alpha[i]）的误差计算方法，来计算这个alpha值得误差。这个过程可以通过copy（）的方法来实现，因此稍后可以将新的alpha值与老的alpha值进行比较。之后开始计算L和H，它们用于将alpha[j]调整到0到C之间。如果L和H相等，就不做任何改变，直接执行continue语句。 Eta是alpha[j]的最优修改量，如果eta为0，那就是说需要退出for循环的当前迭代过程。该过程对真实SMO算法进行了简化处理。于是，可以计算出一个新的alpha[j]，然后利用辅助函数以及L和H值对其进行调整。 然后，就是检查alpha[j]是否轻微改变。如果是的话，就退出for循环。然后alpha[i]和alpha[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反（即如果一个增加，那么另外一个减少）。在对alpha[i]和alpha[j]进行优化之后，给这两个alpha值设置一个常数项b。 最后，在优化过程结束的同时，必须确保在合适的时机结束循环。如果程序执行到for循环的最后一行都不执行continue语句，那么就已经成功地改变了一对alpha，同时可以增加alphaPairsChanged的值。在for循环之外，需要检查alpha值是否做了更新，如果有更新则将iter设为0后继续运行程序。只有在所有数据集上遍历maxIter次，且不再发生任何alpha修规之后，程序才会停止并退出while循环。 利用完整Platt SMO算法加速优化在这两个版本中，实现alpha的更改和代数运算的优化环节一模一样。在优化过程中，唯一的不同就是选择alpha的方式。完整版的Platt SMO算法应用了一些能够提速的启发方法。 Platt SMO 算法是通过一个外循环来选择第一个alpha值的，并且其选择过程会在两种方式之间进行交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界alpha中实现单遍扫描。而所谓非边界alpha指的就是那些不等于边界0或C的alpha值。对整个数据集的扫描相当容易，而实现非边界alpha值的扫描时，首先需要建立这些alpha值的列表，然后再对这个表进行遍历。同时，该步骤会跳过那些已知的不会改变的alpha值。 在选择第一个alpha值后，算法会通过一个内循环来选择第二个alpha值 。在优化过程中，会通过最大化步长的方式来获得第二个alpha值。在简化版SMO算法中，我们会在选择j之后计算错误率E_j。但在这里，我们会建立一个全局的缓存用于保存误差值，并从中选择使得步长或者说E_i-E_j最大的alpha值。 完整Platt SMO的辅助函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # Initialize the structure with the parameters self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m, 1))) self.b = 0 self.eCache = mat(zeros((self.m, 2))) # first column is valid flag self.K = mat(zeros((self.m, self.m))) for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)def calcEk(oS, k): fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ekdef selectJ(i, oS, Ei): # this is the second choice -heurstic, and calcs Ej maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] # set valid #choose the alpha that gives the maximum delta E validEcacheList = nonzero(oS.eCache[:, 0].A)[0] if (len(validEcacheList)) &gt; 1: for k in validEcacheList: # loop through valid Ecache values and find the one that maximizes delta E if k == i: continue # don't calc for i, waste of time Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE &gt; maxDeltaE): maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: # in this case (first time around) we don't have any valid eCache values j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ejdef updateEk(oS, k): # after any alpha has changed update the new value in the cache Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek] 首要的事情就是建立一个数据结构来保存所有的重要值，而这个过程可以通过一个对象来完成。这里使用对象的目的并不是为了面向对象的编程，而只是作为一个数据结构来使用对象。在将值传给函数时，我们可以通过将所有数据移到一个结构中来实现，这样就可以省掉手工输入的麻烦了。而此时，数据就可以通过一个对象来进行传递。实际上，当完成其实现时，可以很容易通过Python的字典来完成。但是在访问对象成员变量时，这样做会有更多的手工输入操作，对比一下myObject.X和myObject[‘X’]就可以知道这一点。为达到这个目的，需要构建一个仅包含init方法的optStruct类。该方法可以实现其成员变量的填充。除了增加一个m*2的矩阵成员变量eCache之外，这些做法和简化SMO一模一样。eCache的第一列给出的是eCache是否有效的标志位，而第二列给出的是实际的E值。 对于给定的alpha值，第一个辅助函数calcEk()能够计算E值并返回。以前，该过程是采用内嵌的方式来完成的，这里作为单独的函数。 下一个函数selectJ()用于选择第二个alpha或者说内循环的alpha值。目标是选择合适的第二个alpha值以保证在每次优化中采用最大步长。该函数的误差值与第一个alpha值E_i和下标i有关。首先将输入值E_i在缓存中设置成为有效的。这里的有效意味着它已经计算好了。在eCaohe中，代码nonzero(oS.eCache[:,0].A)[0]构建出了一个非零表。NumPy函数nonzero()返回了一个列表，而这个列表中包含以输入列表为目录的列表值。nonzero()语句返回的是非零E值所对应的alpha值，而不是E值本身。程序会在所有的值上进行循环并选择其中使得改变最大的那个值。如果这是第一次循环的话，那么就随机选择一个alpha值。 最后一个辅助函数updateEk(),它会计算误差值并存入缓存当中。在对alpha值进行优化之后会用到这个值。 完整Platt SMO算法中的优化部分123456789101112131415161718192021222324252627282930313233343536373839404142def innerL(i, oS): Ei = calcEk(oS, i) if ((oS.labelMat[i] * Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ( (oS.labelMat[i] * Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): j, Ej = selectJ(i, oS, Ei) # this has been changed from selectJrand alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print "L==H" return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # changed for kernel if eta &gt;= 0: print "eta&gt;=0" return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) updateEk(oS, j) # added this for the Ecache if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print "j not moving enough" return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # update i by the same amount as j updateEk(oS, i) # added this for the Ecache #the update is in the oppostie direction b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1 elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 使用SelectJ()而不是selectJrand()来选择第二个alpha值。最后，在alpha值改变时更新Ecache. 完整版Platt SMO的外循环代码12345678910111213141516171819202122232425def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # full Platt SMO oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) iter = 0 entireSet = True alphaPairsChanged = 0 while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: # go over all for i in range(oS.m): alphaPairsChanged += innerL(i, oS) print "fullSet, iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) iter += 1 else: # go over non-bound (railed) alphas nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0] for i in nonBoundIs: alphaPairsChanged += innerL(i, oS) print "non-bound, iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged) iter += 1 if entireSet: entireSet = False # toggle entire set loop elif (alphaPairsChanged == 0): entireSet = True print "iteration number: %d" % iter return oS.b, oS.alphas 完整版的Platt SMO算法，其输入和函数smoSimple()完全一样。函数一开始构建一个数据结构来容纳所有的数据，然后需要对控制函数退出的一些变量进行初始化。整个代码的主体是while循环，这与smosimple（）有些类似，但是这里的循环退出条件更多一些。当迭代次数超过指定的最大值，或者遍历整个集合都未对任意alpha对进行修改时，就退出循环。这里的maxIter变量和函数smoSimple()中的作用有一点不同，后者当没有任何alpha发生改变时会将整个集合的一次遍历过程记成一次迭代，而这里的一次迭代定义为一次循环过程，而不管该循环具体做了什么事。此时，如果在优化过程中存在波动就会停止，因此这里的做法优于SMOSimple()函数中的计数方法。 while循环的内部与smoSimple()中有所不同，一开始的for循环在数据集上遍历任意可能的alpha。我们通过调用innerL()来选择第二个alpha，并在可能时对其进行优化处理。如果有任意一对alpha值发生改变，那么会返回1.第二个for循环遍历所有的非边界alpha值，也就是不在边界0或C上的值。 核函数SVM优化中一个特别好的地方就是，所有的运算都可以写成内积的形式。可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为核技巧(kernel trick)。 径向基核函数 核转换函数123456789101112131415161718192021222324252627282930def kernelTrans(X, A, kTup): # calc the kernel or transform data to a higher dimensional space m, n = shape(X) K = mat(zeros((m, 1))) if kTup[0] == 'lin': K = X * A.T # linear kernel elif kTup[0] == 'rbf': for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # divide in NumPy is element-wise not matrix like Matlab else: raise NameError('Houston We Have a Problem -- \ That Kernel is not recognized') return Kclass optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # Initialize the structure with the parameters self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m, 1))) self.b = 0 self.eCache = mat(zeros((self.m, 2))) # first column is valid flag self.K = mat(zeros((self.m, self.m))) for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup) 其中kTup是一个包含核函数信息的元组。元组的第一个参数是描述所用核函数类型的一个字符串，其他2个参数则都是核函数可能需要的可选参数。在初始化方法结束时，矩阵k先被构建，然后再通过调用函数kernelTrans()进行填充。全局的k值只需计算一次。然后，当想要使用核函数时，就可以对它进行调用。 via 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸LogisticRegression]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8LogisticRegression%2F</url>
    <content type="text"><![CDATA[LogisticRegression算法概述：利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。 优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 适用数据类型：数值型和标称型数据。 梯度下降法求最佳参数伪代码： 每个回归系数初始化为1 重复R次： a.计算整个数据集的梯度 b.使用alpha*gradient更新回归系数的向量 c.返回回归系数 梯度下降法求最佳参数代码实现：123456789101112131415161718192021222324# 定义sigmoid函数：def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 梯度下降法def gradAscent(dataMatIn, classLabels): """ 利用梯度下降法求解逻辑回归系数 :param dataMatIn:2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本 :param classLabels: :return: 返回训练好的回归系数 """ dataMatrix = mat(dataMatIn) # convert to NumPy matrix labelMat = mat(classLabels).transpose() # convert to NumPy matrix m, n = shape(dataMatrix) alpha = 0.001 # 步长 maxCycles = 500 # 迭代次数 weights = ones((n, 1)) for k in range(maxCycles): # heavy on matrix operations h = sigmoid(dataMatrix * weights) # matrix mult error = (labelMat - h) # vector subtraction weights = weights + alpha * dataMatrix.transpose() * error # matrix mult return weights 随机梯度下降法求最佳参数伪代码： 随机梯度下降法求最佳参数代码实现：12345678910def stocGradAscent(dataMatrix, classLabels): m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) # initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i] * weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights mini-batch梯度下降法求最佳参数代码实现：1234567891011121314def batchGradAscent(dataMatrix, classLabels, numIter=150): m, n = shape(dataMatrix) weights = ones(n) # initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4 / (1.0 + j + i) + 0.0001 # apha decreases with iteration, does not randIndex = int(random.uniform(0, len(dataIndex))) # go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex] * weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del (dataIndex[randIndex]) return weights LogisticRegression小结Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度下降算法。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸DecisionTree]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8DecisionTree%2F</url>
    <content type="text"><![CDATA[DecisionTree算法概述： 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配问题。 适用数据范围：数值型和标称型 DecisionTree伪代码： DecisionTree代码实现：创建数据集12345678910111213from math import logimport operatordef createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] # change to discrete values return dataSet, labels 计算给定数据集的香农熵123456789101112131415def calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = &#123;&#125; # 为所有可能分类创建字典 # 循环遍历数据集的每一行 for featVec in dataSet: # the the number of unique elements and their occurance currentLabel = featVec[-1] # 假设最后一列是label if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob, 2) # log base 2 return shannonEnt 按照给定特征划分数据集123456789def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: # 只能筛选离散取值的变量 reducedFeatVec = featVec[:axis] # chop out axis used for splitting reducedFeatVec.extend(featVec[axis + 1:]) retDataSet.append(reducedFeatVec) return retDataSet 选择最好的数据集划分方式123456789101112131415161718192021222324252627def chooseBestFeatureToSplit(dataSet): """ 选择最好的数据集划分方式 :param dataSet: :return: 最佳分割变量所在的下标 """ numFeatures = len(dataSet[0]) - 1 # the last column is used for the labels baseEntropy = calcShannonEnt(dataSet) #计算分割之前的熵 bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): # iterate over all the features # 笨拙的筛选数据集的某一列 # create a list of all the examples of this feature featList = [example[i] for example in dataSet] # get a set of unique values uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: # 遍历所有取值作为可能的分割点,每个离散值作为一个单独的分组 subDataSet = splitDataSet(dataSet, i, value) # 只能筛选离散取值的变量 prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # calculate the info gain; ie reduction in entropy if (infoGain &gt; bestInfoGain): # compare this to the best gain so far bestInfoGain = infoGain # if better than current best, set to best bestFeature = i return bestFeature # returns an integer 计算出现次数最多的分类：12345678910111213def majorityCnt(classList): """ 采用多数表决得方式确定该叶子节点的分类 :param classList: 叶子节点的所有样本标签 :return: 投票的最终类别 """ classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 创建树的函数代码12345678910111213141516171819def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 标签列 if classList.count(classList[0]) == len(classList): return classList[0] # stop splitting when all of the classes are equal if len(dataSet[0]) == 1: # stop splitting when there are no more features in dataSet return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 最佳分割变量的下标 bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel: &#123;&#125;&#125; del (labels[bestFeat]) # 从候选集中删除当前分割变量 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # copy all of labels, so trees don't mess up existing labels # 在当前节点的所有可能取值上递归继续分割 # 字典的key:特征名称、特征取值；字典的value: 字典本身 或者 叶节点的类别名称 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTree 使用决策树进行分类：12345678910111213141516171819202122def classify(inputTree, featLabels, testVec): """ 递归决策树字典，输出所在叶节点的类别 :param inputTree: 树结构以字典的形式存储 :param featLabels: 特征列表 :param testVec: 待预测的数据集 :return: 节点类别 """ firstStr = inputTree.keys()[0] secondDict = inputTree[firstStr] # 将标签字符串转换为索引 featIndex = featLabels.index(firstStr) key = testVec[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): # 如果当前节点的value仍是一个dict则继续递归 classLabel = classify(valueOfFeat, featLabels, testVec) else: # 叶节点 classLabel = valueOfFeat return classLabel 示例123dataSet, labels = createDataSet()clf_tree = createTree(dataSet, labels) 输出1234clf_treeOut[5]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; DecisionTree小结构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用Python语言内嵌的数据结构字典存储树节点信息。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撸kNN]]></title>
    <url>%2F2017%2F11%2F16%2F%E6%89%8B%E6%92%B8kNN%2F</url>
    <content type="text"><![CDATA[k近邻算法概述：k-近邻算法采用测量不同特征值之间的距离方法进行分类。 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型 kNN伪代码：对未知类别属性的数据集中的每个点依次执行以下操作： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点出现频率最高的类别作为当前点的预测分类。 kNN代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from numpy import *import operatorfrom os import listdirdef createDataSet(): """ 创建数据集 :return:特征数据 array，标注数据 list """ group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labelsdef autoNorm(dataSet): # 参数0使得函数可以从列中选取最小值，而不是选取当前行的最小值 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m, 1)) # element wise divide 矩阵数值相除 normDataSet = normDataSet / tile(ranges, (m, 1)) return normDataSet, ranges, minValsdef knn_classifier(inX, dataSet, labels, k): """ k-近邻算法 :param inX:待分类的输入向量 :param dataSet:训练样本集 :param labels:标签向量 :param k:用于选择最近邻居的数目 :return:k个近邻投票的最终类别 """ dataSetSize = dataSet.shape[0] # 计算两个向量点的距离 # tile: 复制输出，将变量内容复制成输入矩阵同样大小的矩阵 diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() # 选择距离最小的k个点 classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # 排序 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] kNN小结: k-近邻算法是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。 k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。 此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。 k-近邻算法的另一缺陷是他无法给出任何数据的基础结构信息，因此我们无法知晓平均实例样本和典型实例样本具有什么特征。 via: 《机器学习实战》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NumPy</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clustering with sklearn]]></title>
    <url>%2F2017%2F11%2F15%2FClustering%20with%20sklearn%2F</url>
    <content type="text"><![CDATA[生成聚类数据集的方法生成数据集方法：sklearn.datasets.make_blobs(n_samples,n_featurs,centers)可以生成数据集,n_samples表示个数，n_features表示特征个数，centers表示y的种类数: make_blobs函数是为聚类产生数据集产生一个数据集和相应的标签 n_samples:表示数据样本点个数,默认值100 n_features:表示数据的维度，默认值是2 centers:产生数据的中心点，默认值3 cluster_std：数据集的标准差，浮点数或者浮点数序列，默认值1.0 center_box：中心确定之后的数据边界，默认值(-10.0, 10.0) shuffle ：洗乱，默认值是True random_state:官网解释是随机生成器的种子 y3 = np.array([0]100 + [1]50 + [2]20 + [3]5)可以这样建立array数组 k-means对于方差不相等和数据与坐标轴不平行时效果不理想；对于数据大小量纲敏感。 The Influence of Data Distribution on KMeans Clustering123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import KMeansdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 400centers = 4data, y = ds.make_blobs(N, n_features=2, centers=centers, random_state=2)data2, y2 = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=(1,2.5,0.5,2), random_state=2)data3 = np.vstack((data[y == 0][:], data[y == 1][:50], data[y == 2][:20], data[y == 3][:5]))y3 = np.array([0] * 100 + [1] * 50 + [2] * 20 + [3] * 5)cls = KMeans(n_clusters=4, init='k-means++')y_hat = cls.fit_predict(data)y2_hat = cls.fit_predict(data2)y3_hat = cls.fit_predict(data3)m = np.array(((1, 1), (1, 3)))data_r = data.dot(m)y_r_hat = cls.fit_predict(data_r)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falsecm = matplotlib.colors.ListedColormap(list('rgbm'))plt.figure(figsize=(9, 10), facecolor='w')plt.subplot(421)plt.title(u'Raw data')plt.scatter(data[:, 0], data[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data, axis=0)x1_max, x2_max = np.max(data, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(422)plt.title(u'KMeans++ clustering')plt.scatter(data[:, 0], data[:, 1], c=y_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(423)plt.title(u'Data after rotation')plt.scatter(data_r[:, 0], data_r[:, 1], c=y, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data_r, axis=0)x1_max, x2_max = np.max(data_r, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(424)plt.title(u'Data rotated KMeans++ clustering')plt.scatter(data_r[:, 0], data_r[:, 1], c=y_r_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(425)plt.title(u'Unequal variance data')plt.scatter(data2[:, 0], data2[:, 1], c=y2, s=30, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data2, axis=0)x1_max, x2_max = np.max(data2, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(426)plt.title(u'Data with unequal variance KMeans++ clustering')plt.scatter(data2[:, 0], data2[:, 1], c=y2_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(427)plt.title(u'Data with unequal numbers')plt.scatter(data3[:, 0], data3[:, 1], s=30, c=y3, cmap=cm, edgecolors='none')x1_min, x2_min = np.min(data3, axis=0)x1_max, x2_max = np.max(data3, axis=0)x1_min, x1_max = expand(x1_min, x1_max)x2_min, x2_max = expand(x2_min, x2_max)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.subplot(428)plt.title(u'Data with unequal numbers KMeans++ clustering')plt.scatter(data3[:, 0], data3[:, 1], c=y3_hat, s=30, cmap=cm, edgecolors='none')plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(True)plt.tight_layout(2)plt.suptitle(u'The Influence of Data Distribution on KMeans Clustering', fontsize=18)# https://github.com/matplotlib/matplotlib/issues/829plt.subplots_adjust(top=0.92)plt.show() 输出 聚类性能的评价指标 均一性sklearn.metrics.homogeneity_score 完整性sklearn.metrics.completeness_score 均一性完整性二者的加权平均v_measure_score ARI（Adjusted Rand index(调整兰德指数)：sklearn.metrics.adjusted_rand_score AMI: sklearn.metrics.adjusted_mutual_info_score 关于指标的定义请看这里 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn import metricsy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cv2 = 2 * c * h / (c + h)v = metrics.v_measure_score(y, y_hat)print u'V-Measure：', v2, vy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 2, 3, 3]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', v# 允许不同值y = [0, 0, 0, 1, 1, 1]y_hat = [1, 1, 1, 0, 0, 0]h = metrics.homogeneity_score(y, y_hat)c = metrics.completeness_score(y, y_hat)v = metrics.v_measure_score(y, y_hat)print u'同一性(Homogeneity)：', hprint u'完整性(Completeness)：', cprint u'V-Measure：', vy = [0, 0, 1, 1]y_hat = [0, 1, 0, 1]ari = metrics.adjusted_rand_score(y, y_hat)print ariy = [0, 0, 0, 1, 1, 1]y_hat = [0, 0, 1, 1, 2, 2]ari = metrics.adjusted_rand_score(y, y_hat)print ari 输出123456789101112同一性(Homogeneity)： 0.666666666667完整性(Completeness)： 0.420619835714V-Measure： 0.515803742979 0.515803742979同一性(Homogeneity)： 1.0完整性(Completeness)： 0.521296028614V-Measure： 0.685331478962同一性(Homogeneity)： 1.0完整性(Completeness)： 1.0V-Measure： 1.0-0.50.242424242424 AffinityPropagation12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import AffinityPropagationfrom sklearn.metrics import euclidean_distancesN = 400centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)m = euclidean_distances(data, squared=True)preference = -np.median(m)print 'Preference：', preferencematplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 9), facecolor='w')for i, mul in enumerate(np.linspace(1, 4, 9)): print mul p = mul * preference model = AffinityPropagation(affinity='euclidean', preference=p) af = model.fit(data) center_indices = af.cluster_centers_indices_ n_clusters = len(center_indices) print ('p = %.1f' % mul), p, '聚类簇的个数为：', n_clusters y_hat = af.labels_ plt.subplot(3, 3, i+1) plt.title(u'Preference：%.2f，簇个数：%d' % (p, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') center = data[center_indices[k]] for x in data[cur]: plt.plot([x[0], center[0]], [x[1], center[1]], color=clr, zorder=1) plt.scatter(data[center_indices, 0], data[center_indices, 1], s=100, c=clrs, marker='*', edgecolors='k', zorder=2) plt.grid(True)plt.tight_layout()plt.suptitle(u'AP聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出1234567891011121314151617181920Preference： -5.299145530341.0p = 1.0 -5.29914553034 聚类簇的个数为： 161.375p = 1.4 -7.28632510422 聚类簇的个数为： 121.75p = 1.8 -9.27350467809 聚类簇的个数为： 112.125p = 2.1 -11.260684252 聚类簇的个数为： 102.5p = 2.5 -13.2478638258 聚类簇的个数为： 82.875p = 2.9 -15.2350433997 聚类簇的个数为： 83.25p = 3.2 -17.2222229736 聚类簇的个数为： 753.625p = 3.6 -19.2094025475 聚类簇的个数为： 74.0p = 4.0 -21.1965821214 聚类簇的个数为： 7 MeanShift Clustering12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import MeanShiftfrom sklearn.metrics import euclidean_distancesN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 9), facecolor='w')m = euclidean_distances(data, squared=True)bw = np.median(m)print bwfor i, mul in enumerate(np.linspace(0.1, 0.4, 4)): band_width = mul * bw model = MeanShift(bin_seeding=True, bandwidth=band_width) ms = model.fit(data) centers = ms.cluster_centers_ y_hat = ms.labels_ n_clusters = np.unique(y_hat).size print '带宽：', mul, band_width, '聚类簇的个数为：', n_clusters plt.subplot(2, 2, i+1) plt.title(u'带宽：%.2f，聚类簇的个数为：%d' % (band_width, n_clusters)) clrs = [] for c in np.linspace(16711680, 255, n_clusters): clrs.append('#%06x' % c) # clrs = plt.cm.Spectral(np.linspace(0, 1, n_clusters)) print clrs for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none') plt.scatter(centers[:, 0], centers[:, 1], s=150, c=clrs, marker='*', edgecolors='k') plt.grid(True)plt.tight_layout(2)plt.suptitle(u'MeanShift聚类', fontsize=20)plt.subplots_adjust(top=0.92)plt.show() 输出123456789105.31661129692带宽： 0.1 0.531661129692 聚类簇的个数为： 7['#ff0000', '#d4802a', '#aa0055', '#7f807f', '#5500aa', '#2a80d4', '#0000ff']带宽： 0.2 1.06332225938 聚类簇的个数为： 4['#ff0000', '#aa0055', '#5500aa', '#0000ff']带宽： 0.3 1.59498338907 聚类簇的个数为： 3['#ff0000', '#7f807f', '#0000ff']带宽： 0.4 2.12664451877 聚类簇的个数为： 1['#ff0000'] DBSCAN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import DBSCANfrom sklearn.preprocessing import StandardScalerdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dN = 1000centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]data, y = ds.make_blobs(N, n_features=2, centers=centers, cluster_std=[0.5, 0.25, 0.7, 0.5], random_state=0)data = StandardScaler().fit_transform(data)params = ((0.2, 5), (0.2, 10), (0.2, 15), (0.3, 5), (0.3, 10), (0.3, 15))matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'DBSCAN聚类', fontsize=20)for i in range(6): eps, min_samples = params[i] model = DBSCAN(eps=eps, min_samples=min_samples) model.fit(data) y_hat = model.labels_ core_indices = np.zeros_like(y_hat, dtype=bool) core_indices[model.core_sample_indices_] = True y_unique = np.unique(y_hat) n_clusters = y_unique.size - (1 if -1 in y_hat else 0) print y_unique, '聚类簇的个数为：', n_clusters # clrs = [] # for c in np.linspace(16711680, 255, y_unique.size): # clrs.append('#%06x' % c) plt.subplot(2, 3, i+1) clrs = plt.cm.Spectral(np.linspace(0, 0.8, y_unique.size)) for k, clr in zip(y_unique, clrs): cur = (y_hat == k) if k == -1: plt.scatter(data[cur, 0], data[cur, 1], s=20, c='k') continue plt.scatter(data[cur, 0], data[cur, 1], s=30, c=clr, edgecolors='k') plt.scatter(data[cur &amp; core_indices][:, 0], data[cur &amp; core_indices][:, 1], s=60, c=clr, marker='o', edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\epsilon$ = %.1f m = %d，聚类数目：%d' % (eps, min_samples, n_clusters), fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出1234567[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3] 聚类簇的个数为： 4[-1 0 1 2 3 4] 聚类簇的个数为： 5[-1 0] 聚类簇的个数为： 1[-1 0 1] 聚类簇的个数为： 2[-1 0 1 2 3] 聚类簇的个数为： 4 谱聚类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport matplotlib.pyplot as pltimport sklearn.datasets as dsimport matplotlib.colorsfrom sklearn.cluster import spectral_clusteringfrom sklearn.metrics import euclidean_distancesdef expand(a, b): d = (b - a) * 0.1 return a-d, b+dmatplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falset = np.arange(0, 2*np.pi, 0.1)data1 = np.vstack((np.cos(t), np.sin(t))).Tdata2 = np.vstack((2*np.cos(t), 2*np.sin(t))).Tdata3 = np.vstack((3*np.cos(t), 3*np.sin(t))).Tdata = np.vstack((data1, data2, data3))n_clusters = 3m = euclidean_distances(data, squared=True)sigma = np.median(m)plt.figure(figsize=(12, 8), facecolor='w')plt.suptitle(u'谱聚类', fontsize=20)clrs = plt.cm.Spectral(np.linspace(0, 0.8, n_clusters))for i, s in enumerate(np.logspace(-2, 0, 6)): print s af = np.exp(-m ** 2 / (s ** 2)) + 1e-6 y_hat = spectral_clustering(af, n_clusters=n_clusters, assign_labels='kmeans', random_state=1) plt.subplot(2, 3, i+1) for k, clr in enumerate(clrs): cur = (y_hat == k) plt.scatter(data[cur, 0], data[cur, 1], s=40, c=clr, edgecolors='k') x1_min, x2_min = np.min(data, axis=0) x1_max, x2_max = np.max(data, axis=0) x1_min, x1_max = expand(x1_min, x1_max) x2_min, x2_max = expand(x2_min, x2_max) plt.xlim((x1_min, x1_max)) plt.ylim((x2_min, x2_max)) plt.grid(True) plt.title(ur'$\sigma$ = %.2f' % s, fontsize=16)plt.tight_layout()plt.subplots_adjust(top=0.9)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>make_blobs</tag>
        <tag>euclidean_distances</tag>
        <tag>KMeans</tag>
        <tag>KMeans++</tag>
        <tag>homogeneity_score</tag>
        <tag>completeness_score</tag>
        <tag>v_measure_score</tag>
        <tag>adjusted_rand_score</tag>
        <tag>AffinityPropagation</tag>
        <tag>MeanShift</tag>
        <tag>spectral_clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.svm]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.svm%2F</url>
    <content type="text"><![CDATA[SVC:ovr123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import numpy as npfrom sklearn import svmfrom sklearn.model_selection import train_test_splitimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.datasets import load_irisiris = load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b, tip): acc = a.ravel() == b.ravel() print tip + '正确率：', np.mean(acc)# 分类器# clf = svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr')# ‘ovo’ 一对一, ‘ovr’ 多对多 or None 无, default=Noneclf = svm.SVC(C=0.8, kernel='rbf', gamma=20, decision_function_shape='ovr')clf.fit(x_train, y_train.ravel())# 准确率# score(X, y, sample_weight=None) ：Returns the mean accuracy on the given test data and labels.print clf.score(x_train, y_train) # 精度y_hat = clf.predict(x_train)show_accuracy(y_hat, y_train, '训练集')print clf.score(x_test, y_test)y_hat = clf.predict(x_test)show_accuracy(y_hat, y_test, '测试集')# decision_functionprint 'decision_function:\n', clf.decision_function(x_train)print '\npredict:\n', clf.predict(x_train)"""# decision_function(): Distance of the samples X to the separating hyperplane.# 分类归属于距离数值最小的类别，距离有正有负只是平面的两侧而已decision_function:[[-0.25631211 0.80529378 2.45101833] [ 2.35232967 0.82494155 -0.17727121] [ 2.45418203 0.77495649 -0.22913852] [ 2.45421283 0.77518383 -0.22939666] [-0.24854074 2.39614366 0.85239708] [ 2.46621083 0.76927647 -0.23548729] [ 2.45410944 0.77775367 -0.2318631 ] [-0.24677667 0.84905678 2.39771989] [-0.46050688 1.21154683 2.24896005] [-0.26893412 0.80449581 2.46443831]predict:[2 0 0 0 1 0 0 2 2 2"""# 画图x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点# numpy.ndarray.flat : A 1-D iterator over the array.# This is a numpy.flatiter instance, which acts similarly to,# but is not a subclass of, Python’s built-in iterator object.grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点# print 'grid_test = \n', grid_test# Z = clf.decision_function(grid_test) # 样本到决策面的距离# print Zgrid_hat = clf.predict(grid_test) # 预测分类值print grid_hatgrid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本plt.scatter(x_test[:, 0], x_test[:, 1], s=120, facecolors='none', zorder=10) # 圈中测试集样本plt.xlabel(iris_feature[0], fontsize=13)plt.ylabel(iris_feature[1], fontsize=13)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.title(u'鸢尾花SVM二特征分类', fontsize=15)plt.grid()plt.show() 输出： SVC:ovo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npfrom sklearn import svmfrom scipy import statsfrom sklearn.metrics import accuracy_scoreimport matplotlib as mplimport matplotlib.pyplot as pltdef extend(a, b, r): x = a - b m = (a + b) / 2 return m-r*x/2, m+r*x/2np.random.seed(0)N = 20x = np.empty((4*N, 2))means = [(-1, 1), (1, 1), (1, -1), (-1, -1)]sigmas = [np.eye(2), 2*np.eye(2), np.diag((1,2)), np.array(((2,1),(1,2)))]for i in range(4): # scipy.stats.multivariate_normal: # A multivariate normal random variable. # The mean keyword specifies the mean. The cov keyword specifies the covariance matrix. mn = stats.multivariate_normal(means[i], sigmas[i]*0.3) # rvs(mean=None, cov=1) Draw random samples from a multivariate normal distribution. x[i*N:(i+1)*N, :] = mn.rvs(N)a = np.array((0,1,2,3)).reshape((-1, 1))y = np.tile(a, N).flatten()clf = svm.SVC(C=1, kernel='rbf', gamma=1, decision_function_shape='ovo')# clf = svm.SVC(C=1, kernel='linear', decision_function_shape='ovr')clf.fit(x, y)y_hat = clf.predict(x)acc = accuracy_score(y, y_hat)np.set_printoptions(suppress=True)print u'预测正确的样本个数：%d，正确率：%.2f%%' % (round(acc*4*N), 100*acc)# decision_functionprint clf.decision_function(x)# print y_hatx1_min, x2_min = np.min(x, axis=0)x1_max, x2_max = np.max(x, axis=0)x1_min, x1_max = extend(x1_min, x1_max, 1.05)x2_min, x2_max = extend(x2_min, x2_max, 1.05)x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j]x_test = np.stack((x1.flat, x2.flat), axis=1)y_test = clf.predict(x_test)y_test = y_test.reshape(x1.shape)cm_light = mpl.colors.ListedColormap(['#FF8080', '#A0FFA0', '#6060FF', '#F080F0'])cm_dark = mpl.colors.ListedColormap(['r', 'g', 'b', 'm'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_test, cmap=cm_light)plt.scatter(x[:, 0], x[:, 1], s=40, c=y, cmap=cm_dark, alpha=0.7)plt.xlim((x1_min, x1_max))plt.ylim((x2_min, x2_max))plt.grid(b=True)plt.tight_layout(pad=2.5)plt.title(u'SVM多分类方法：One/One or One/Other', fontsize=18)plt.show() 输出： 参数选择:linear/rbf,c,gamma1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_irisimport numpy as npfrom sklearn import svmimport matplotlib as mplimport matplotlib.colorsimport matplotlib.pyplot as pltiris = load_iris()X = iris.datay = iris.targetX = X[y != 0, :2]y = y[y != 0]x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)# 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]def show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)# 分类器clf_param = (('linear', 0.1), ('linear', 0.5), ('linear', 1), ('linear', 2), ('rbf', 1, 0.1), ('rbf', 1, 1), ('rbf', 1, 10), ('rbf', 1, 100), ('rbf', 5, 0.1), ('rbf', 5, 1), ('rbf', 5, 10), ('rbf', 5, 100))x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = mpl.colors.ListedColormap(['#77E0A0', '#FFA0A0'])cm_dark = mpl.colors.ListedColormap(['g', 'r'])mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(14, 10), facecolor='w')for i, param in enumerate(clf_param): clf = svm.SVC(C=param[1], kernel=param[0]) if param[0] == 'rbf': clf.gamma = param[2] title = u'高斯核，C=%.1f，$\gamma$ =%.1f' % (param[1], param[2]) else: title = u'线性核，C=%.1f' % param[1] clf.fit(X, y) y_hat = clf.predict(X) show_accuracy(y_hat, y) # 准确率 # 画图 print title print '支撑向量的数目：', clf.n_support_ print '支撑向量的系数：', clf.dual_coef_ print '支撑向量：', clf.support_ plt.subplot(3, 4, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=40, cmap=cm_dark) # 样本的显示 plt.scatter(X[clf.support_, 0], X[clf.support_, 1], edgecolors='k', facecolors='none', s=100, marker='o') # 支撑向量 z = clf.decision_function(grid_test) # print 'z = \n', z z = z.reshape(x1.shape) plt.contour(x1, x2, z, colors=list('kmrmk'), linestyles=['--', '-.', '-', '-.', '--'], linewidths=[1, 0.5, 1.5, 0.5, 1], levels=[-1, -0.5, 0, 0.5, 1]) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(title, fontsize=14)plt.suptitle(u'SVM不同参数的分类', fontsize=20)plt.tight_layout(1.4)plt.subplots_adjust(top=0.92)plt.savefig('1.png')plt.show() 输出： sklearn.metrics模型评价指标：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import precision_score, recall_score, f1_score, fbeta_scorefrom sklearn.metrics import precision_recall_fscore_support, classification_reporty_true = np.array([1, 1, 1, 1, 0, 0])y_hat = np.array([1, 0, 1, 1, 1, 1])print 'Accuracy：\t', accuracy_score(y_true, y_hat)# The precision is the ratio 'tp / (tp + fp)' where 'tp' is the number of# true positives and 'fp' the number of false positives. The precision is# intuitively the ability of the classifier not to label as positive a sample# that is negative.# The best value is 1 and the worst value is 0.precision = precision_score(y_true, y_hat)print 'Precision:\t', precision# The recall is the ratio 'tp / (tp + fn)' where 'tp' is the number of# true positives and 'fn' the number of false negatives. The recall is# intuitively the ability of the classifier to find all the positive samples.# The best value is 1 and the worst value is 0.recall = recall_score(y_true, y_hat)print 'Recall: \t', recall# F1 score, also known as balanced F-score or F-measure# The F1 score can be interpreted as a weighted average of the precision and# recall, where an F1 score reaches its best value at 1 and worst score at 0.# The relative contribution of precision and recall to the F1 score are# equal. The formula for the F1 score is:# F1 = 2 * (precision * recall) / (precision + recall)print 'f1 score: \t', f1_score(y_true, y_hat)print 2 * (precision * recall) / (precision + recall)# The F-beta score is the weighted harmonic mean of precision and recall,# reaching its optimal value at 1 and its worst value at 0.# The 'beta' parameter determines the weight of precision in the combined# score. 'beta &lt; 1' lends more weight to precision, while 'beta &gt; 1'# favors recall ('beta -&gt; 0' considers only precision, 'beta -&gt; inf' only recall).print 'F-beta：'for beta in np.logspace(-3, 3, num=7, base=10): fbeta = fbeta_score(y_true, y_hat, beta=beta) print '\tbeta=%9.3f\tF-beta=%.5f' % (beta, fbeta) #print (1+beta**2)*precision*recall / (beta**2 * precision + recall)print precision_recall_fscore_support(y_true, y_hat, beta=1)print classification_report(y_true, y_hat) 输出：123456789101112131415161718192021Accuracy： 0.5Precision: 0.6Recall: 0.75f1 score: 0.6666666666670.666666666667F-beta： beta= 0.001 F-beta=0.60000 beta= 0.010 F-beta=0.60001 beta= 0.100 F-beta=0.60119 beta= 1.000 F-beta=0.66667 beta= 10.000 F-beta=0.74815 beta= 100.000 F-beta=0.74998 beta= 1000.000 F-beta=0.75000(array([ 0. , 0.6]), array([ 0. , 0.75]), array([ 0. , 0.66666667]), array([2, 4], dtype=int64)) precision recall f1-score support 0 0.00 0.00 0.00 2 1 0.60 0.75 0.67 4avg / total 0.40 0.50 0.44 6 样本不均衡问题：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import numpy as npfrom sklearn import svmimport matplotlib.colorsimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_scoreimport warningsdef show_accuracy(a, b): acc = a.ravel() == b.ravel() print '正确率：%.2f%%' % (100*float(acc.sum()) / a.size)def show_recall(y, y_hat): # print y_hat[y == 1] print '召回率：%.2f%%' % (100 * float(np.sum(y_hat[y == 1] == 1)) / np.extract(y == 1, y).size)warnings.filterwarnings("ignore") # UndefinedMetricWarningnp.random.seed(0) # 保持每次生成的数据相同c1 = 990c2 = 10N = c1 + c2x_c1 = 3*np.random.randn(c1, 2)x_c2 = 0.5*np.random.randn(c2, 2) + (4, 4)x = np.vstack((x_c1, x_c2))y = np.ones(N)y[:c1] = -1# 显示大小s = np.ones(N) * 30s[:c1] = 10# 分类器clfs = [svm.SVC(C=1, kernel='linear'), svm.SVC(C=1, kernel='linear', class_weight=&#123;-1: 1, 1: 50&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 2&#125;), svm.SVC(C=0.8, kernel='rbf', gamma=0.5, class_weight=&#123;-1: 1, 1: 10&#125;)]titles = 'Linear', 'Linear, Weight=50', 'RBF, Weight=2', 'RBF, Weight=10'x1_min, x1_max = x[:, 0].min(), x[:, 0].max() # 第0列的范围x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围x1, x2 = np.mgrid[x1_min:x1_max:500j, x2_min:x2_max:500j] # 生成网格采样点grid_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点cm_light = matplotlib.colors.ListedColormap(['#77E0A0', '#FF8080'])cm_dark = matplotlib.colors.ListedColormap(['g', 'r'])matplotlib.rcParams['font.sans-serif'] = [u'SimHei']matplotlib.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(10, 8), facecolor='w')for i, clf in enumerate(clfs): clf.fit(x, y) y_hat = clf.predict(x) # show_accuracy(y_hat, y) # 正确率 # show_recall(y, y_hat) # 召回率 print i+1, '次：' print '正确率：\t', accuracy_score(y, y_hat) print ' 精度 ：\t', precision_score(y, y_hat, pos_label=1) print '召回率：\t', recall_score(y, y_hat, pos_label=1) print 'F1-score：\t', f1_score(y, y_hat, pos_label=1) print # 画图 plt.subplot(2, 2, i+1) grid_hat = clf.predict(grid_test) # 预测分类值 grid_hat = grid_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light, alpha=0.8) plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k', s=s, cmap=cm_dark) # 样本的显示 plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.title(titles[i]) plt.grid()plt.suptitle(u'不平衡数据的处理', fontsize=18)plt.tight_layout(1.5)plt.subplots_adjust(top=0.92)plt.show() 输出： 1234567891011121314151617181920211 次：正确率： 0.99 精度 ： 0.0召回率： 0.0F1-score： 0.02 次：正确率： 0.94 精度 ： 0.142857142857召回率： 1.0F1-score： 0.253 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.7692307692314 次：正确率： 0.994 精度 ： 0.625召回率： 1.0F1-score： 0.769230769231 SVR：123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npfrom sklearn import svmimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', yprint 'SVR - RBF'svr_rbf = svm.SVR(kernel='rbf', gamma=0.2, C=100)svr_rbf.fit(x, y)print 'SVR - Linear'svr_linear = svm.SVR(kernel='linear', C=100)svr_linear.fit(x, y)print 'SVR - Polynomial'svr_poly = svm.SVR(kernel='poly', degree=3, C=100)svr_poly.fit(x, y)print 'Fit OK.'x_test = np.linspace(x.min(), 1.1*x.max(), 100).reshape(-1, 1)y_rbf = svr_rbf.predict(x_test)y_linear = svr_linear.predict(x_test)y_poly = svr_poly.predict(x_test)plt.figure(figsize=(9, 8), facecolor='w')plt.plot(x_test, y_rbf, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x_test, y_linear, 'g-', linewidth=2, label='Linear Kernel')plt.plot(x_test, y_poly, 'b-', linewidth=2, label='Polynomial Kernel')plt.plot(x, y, 'mo', markersize=6)plt.scatter(x[svr_rbf.support_], y[svr_rbf.support_], s=130, c='r', marker='*', label='RBF Support Vectors')plt.legend(loc='lower left')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.tight_layout(2)plt.show() 输出： SVR GridSearchCV example:1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom sklearn import svmfrom sklearn.model_selection import GridSearchCV # 0.17 grid_searchimport matplotlib.pyplot as pltN = 50np.random.seed(0)x = np.sort(np.random.uniform(0, 6, N), axis=0)y = 2*np.sin(x) + 0.1*np.random.randn(N)x = x.reshape(-1, 1)print 'x =\n', xprint 'y =\n', ymodel = svm.SVR(kernel='rbf')c_can = np.logspace(-2, 2, 10)gamma_can = np.logspace(-2, 2, 10)svr = GridSearchCV(model, param_grid=&#123;'C': c_can, 'gamma': gamma_can&#125;, cv=5)svr.fit(x, y)print '验证参数：\n', svr.best_params_x_test = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)y_hat = svr.predict(x_test)sp = svr.best_estimator_.support_plt.figure(facecolor='w')plt.scatter(x[sp], y[sp], s=120, c='r', marker='*', label='Support Vectors', zorder=3)plt.plot(x_test, y_hat, 'r-', linewidth=2, label='RBF Kernel')plt.plot(x, y, 'go', markersize=5)plt.legend(loc='upper right')plt.title('SVR', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.grid(True)plt.show() 输出： via SVM的两个参数 C 和 gamma]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>train_test_split</tag>
        <tag>GridSearchCV</tag>
        <tag>svm</tag>
        <tag>SVC</tag>
        <tag>SVR</tag>
        <tag>precision_score</tag>
        <tag>recall_score</tag>
        <tag>f1_score</tag>
        <tag>fbeta_score</tag>
        <tag>precision_recall_fscore_support</tag>
        <tag>classification_report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging]]></title>
    <url>%2F2017%2F11%2F14%2FBagging%2F</url>
    <content type="text"><![CDATA[引子假设一个分类器分类正确的概率为0.6，如果有n个独立的分类器，选择其中的k个采用投票的方式，作为预测结果，则预测正确的概率为： 123456789101112131415161718import operator# operator.__mul__(a, b)# Return a * b, for a and b numbers.def c(n, k): # 求组合数 return reduce(operator.mul, range(n-k+1, n+1)) / reduce(operator.mul, range(1, k+1))def bagging(n, p): s = 0 for i in range(n / 2 + 1, n + 1): s += c(n, i) * p ** i * (1 - p) ** (n - i) return sfor t in range(9, 100, 10): # 假设事件发生的概率为0.6 print t, '次采样正确率：', bagging(t, 0.6) 输出12345678910119 次采样正确率： 0.7334323219 次采样正确率： 0.81390797858529 次采样正确率： 0.86378705133639 次采样正确率： 0.89794136871149 次采样正确率： 0.92242443765259 次采样正确率： 0.94044799573269 次采样正确率： 0.95394975650579 次采样正确率： 0.96418969283989 次采样正确率： 0.97202751600799 次采样正确率： 0.97806955787 BaggingRegressor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn.linear_model import RidgeCVfrom sklearn.ensemble import BaggingRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesdef f(x): return 0.5*np.exp(-(x+3) **2) + np.exp(-x**2) + + 0.5*np.exp(-(x-3) ** 2)np.random.seed(0)N = 200x = np.random.rand(N) * 10 - 5 # [-5,5)x = np.sort(x)y = f(x) + 0.05*np.random.randn(N)x.shape = -1, 1ridge = RidgeCV(alphas=np.logspace(-3, 2, 10), fit_intercept=False)ridged = Pipeline([('poly', PolynomialFeatures(degree=10)), ('Ridge', ridge)])bagging_ridged = BaggingRegressor(ridged, n_estimators=100, max_samples=0.3)dtr = DecisionTreeRegressor(max_depth=5)regs = [ ('DecisionTree Regressor', dtr), ('Ridge Regressor(6 Degree)', ridged), ('Bagging Ridge(6 Degree)', bagging_ridged), ('Bagging DecisionTree Regressor', BaggingRegressor(dtr, n_estimators=100, max_samples=0.3))]x_test = np.linspace(1.1*x.min(), 1.1*x.max(), 1000)mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(12, 8), facecolor='w')plt.plot(x, y, 'ro', label=u'训练数据')plt.plot(x_test, f(x_test), color='k', lw=3.5, label=u'真实值')clrs = 'bmyg'for i, (name, reg) in enumerate(regs): reg.fit(x, y) y_test = reg.predict(x_test.reshape(-1, 1)) plt.plot(x_test, y_test.ravel(), color=clrs[i], lw=i+1, label=name, zorder=6-i)plt.legend(loc='upper left')plt.xlabel('X', fontsize=15)plt.ylabel('Y', fontsize=15)plt.title(u'回归曲线拟合', fontsize=21)plt.ylim((-0.2, 1.2))plt.tight_layout(2)plt.grid(True)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>Bagging</tag>
        <tag>BaggingRegressor</tag>
        <tag>reduce</tag>
        <tag>DecisionTreeRegressor</tag>
        <tag>RidgeCV</tag>
        <tag>PolynomialFeatures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeRegressor]]></title>
    <url>%2F2017%2F11%2F14%2Fsklearn.tree.DecisionTreeRegressor%2F</url>
    <content type="text"><![CDATA[导入包：1234import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressor 生成数据集：12345678N = 100x = np.random.rand(N) * 6 - 3 # [-3,3)x.sort()y = np.sin(x) + np.random.randn(N) * 0.05print yx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的print x DecisionTreeRegressor：12345678910reg = DecisionTreeRegressor(criterion='mse', max_depth=9)dt = reg.fit(x, y)x_test = np.linspace(-3, 3, 50).reshape(-1, 1)y_hat = dt.predict(x_test)plt.plot(x, y, 'ro', ms=6, label='Actual')plt.plot(x_test, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='upper left')plt.grid()plt.show() 比较决策树的深度影响：123456789101112131415161718depth = [2, 4, 6, 8, 10]clr = 'rgbmy'reg = [DecisionTreeRegressor(criterion='mse', max_depth=depth[0]), DecisionTreeRegressor(criterion='mse', max_depth=depth[1]), DecisionTreeRegressor(criterion='mse', max_depth=depth[2]), DecisionTreeRegressor(criterion='mse', max_depth=depth[3]), DecisionTreeRegressor(criterion='mse', max_depth=depth[4])]plt.plot(x, y, 'k^', linewidth=2, label='Actual')x_test = np.linspace(-3, 3, 50).reshape(-1, 1)for i, r in enumerate(reg): dt = r.fit(x, y) y_hat = dt.predict(x_test) plt.plot(x_test, y_hat, '-', color=clr[i], linewidth=2, label='Depth=%d' % depth[i])plt.legend(loc='upper left')plt.grid()plt.show() 多变量决策树回归：12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeRegressorN = 400x = np.random.rand(N) * 8 - 4 # [-4,4)x.sort()y1 = np.sin(x) + 3 + np.random.randn(N) * 0.1y2 = np.cos(0.3*x) + np.random.randn(N) * 0.01y = np.vstack((y1, y2)).Tx = x.reshape(-1, 1) # 转置后，得到N个样本，每个样本都是1维的deep = 5 # 树的深度reg = DecisionTreeRegressor(criterion='mse', max_depth=deep)dt = reg.fit(x, y)x_test = np.linspace(-4, 4, num=1000).reshape(-1, 1)print x_testy_hat = dt.predict(x_test)print y_hatplt.scatter(y[:, 0], y[:, 1], c='r', s=40, label='Actual')plt.scatter(y_hat[:, 0], y_hat[:, 1], c='g', marker='s', s=100, label='Depth=%d' % deep, alpha=1)plt.legend(loc='upper left')plt.xlabel('y1')plt.ylabel('y2')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>DecisionTreeRegressor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.tree.DecisionTreeClassifier]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.tree.DecisionTreeClassifier%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn import treefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pydotplus 导入数据集：1234567891011121314def iris_type(s): it = &#123;'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2&#125; return it[s]from sklearn.datasets import load_irisiris=load_iris()X = iris.data[:, :2]y = iris.targetx_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica' 决策树参数估计：123456# min_samples_split = 10：如果该结点包含的样本数目大于10，则(有可能)对其分支# min_samples_leaf = 10：若将某结点分支后，得到的每个子结点样本数目都大于10，则完成分支；否则，不进行分支model = DecisionTreeClassifier(criterion='entropy', max_depth=6)model = model.fit(x_train, y_train)y_test_hat = model.predict(x_test) # 测试数据 决策树保存1234567891011121314# 1、输出with open('iris.dot', 'w') as f: tree.export_graphviz(model, out_file=f)# 2、给定文件名# tree.export_graphviz(model, out_file='iris.dot')# 3、输出为pdf格式dot_data = tree.export_graphviz(model, out_file=None, feature_names=iris_feature_E, class_names=iris_class, filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf('iris.pdf')f = open('iris.png', 'wb')f.write(graph.create_png())f.close() 决策树画图1234567891011121314151617181920212223242526272829303132mpl.rcParams['font.sans-serif'] = [u'SimHei']mpl.rcParams['axes.unicode_minus'] = FalseN, M = 50, 50 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_show = np.stack((x1.flat, x2.flat), axis=1) # 测试点print x_show.shapecm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_show_hat = model.predict(x_show) # 预测值print y_show_hat.shapeprint y_show_haty_show_hat = y_show_hat.reshape(x1.shape) # 使之与输入的形状相同print y_show_hatplt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light) # 预测值的显示print y_testprint y_test.ravel()plt.scatter(x_test[:, 0], x_test[:, 1], c=y_test.ravel(), edgecolors='k', s=120, cmap=cm_dark, marker='*') # 测试数据plt.scatter(x[:, 0], x[:, 1], c=y.ravel(), edgecolors='k', s=40, cmap=cm_dark) # 全部数据plt.xlabel(iris_feature[0], fontsize=15)plt.ylabel(iris_feature[1], fontsize=15)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid(True)plt.title(u'鸢尾花数据的决策树分类', fontsize=17)plt.show() 决策树树状图 决策分割面 预测1234567y_test = y_test.reshape(-1)print y_test_hatprint y_testresult = (y_test_hat == y_test) # True则预测正确，False则预测错误acc = np.mean(result)print '准确度: %.2f%%' % (100 * acc) 过拟合：错误率12345678910111213141516171819depth = np.arange(1, 15)err_list = []for d in depth: clf = DecisionTreeClassifier(criterion='entropy', max_depth=d) clf = clf.fit(x_train, y_train) y_test_hat = clf.predict(x_test) # 测试数据 result = (y_test_hat == y_test) # True则预测正确，False则预测错误 err = 1 - np.mean(result) err_list.append(err) # print d, ' 准确度: %.2f%%' % (100 * err) print d, ' 错误率: %.2f%%' % (100 * err)plt.figure(facecolor='w')plt.plot(depth, err_list, 'ro-', lw=2)plt.xlabel(u'决策树深度', fontsize=15)plt.ylabel(u'错误率', fontsize=15)plt.title(u'决策树深度与过拟合', fontsize=17)plt.grid(True)plt.show() 输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>tree</tag>
        <tag>train_test_split</tag>
        <tag>DecisionTreeClassifier</tag>
        <tag>pydotplus</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticNetCV]]></title>
    <url>%2F2017%2F11%2F13%2FElasticNetCV%2F</url>
    <content type="text"><![CDATA[导入包：12345678910import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import ElasticNetCVimport sklearn.datasetsfrom sklearn.preprocessing import PolynomialFeatures, StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import mean_squared_error 导入数据集：1234567891011def not_empty(s): return s != ''data = sklearn.datasets.load_boston()x = np.array(data.data)y = np.array(data.target)print x.shapeprint y.shapex_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=0) 初始化模型Pipeline：1234567model = Pipeline([ ('ss', StandardScaler()), ('poly', PolynomialFeatures(degree=3, include_bias=True)), ('linear', ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.99, 1], alphas=np.logspace(-3, 2, 5), fit_intercept=False, max_iter=1e3, cv=3))]) 模型训练：123456model.fit(x_train, y_train.ravel())linear = model.get_params('linear')['linear']print u'超参数：', linear.alpha_print u'L1 ratio：', linear.l1_ratio_print u'系数：', linear.coef_.ravel() 模型预测：123456y_pred = model.predict(x_test)r2 = model.score(x_test, y_test)mse = mean_squared_error(y_test, y_pred)print 'R2:', r2print u'均方误差：', mse 模型效果图形化展示：12345678910111213t = np.arange(len(y_pred))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.figure(facecolor='w')plt.plot(t, y_test.ravel(), 'r-', lw=2, label=u'真实值')plt.plot(t, y_pred, 'g-', lw=2, label=u'估计值')plt.legend(loc='best')plt.title(u'波士顿房价预测', fontsize=18)plt.xlabel(u'样本编号', fontsize=15)plt.ylabel(u'房屋价格', fontsize=15)plt.grid()plt.show() 输出1234567(506L, 13L)(506L,)超参数： 0.316227766017L1 ratio： 0.99R2: 0.772203419261均方误差： 18.9675965682]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>PolynomialFeatures</tag>
        <tag>linear_model</tag>
        <tag>ElasticNetCV</tag>
        <tag>ElasticNet</tag>
        <tag>StandardScaler</tag>
        <tag>train_test_split</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插值]]></title>
    <url>%2F2017%2F11%2F13%2F%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[插值的定义插值法又称“内插法”，是利用函数f(x)在某区间中已知的若干点的函数值，作出适当的特定函数，在区间的其他点上用这特定函数的值作为函数f(x)的近似值，这种方法称为插值法。如果这特定函数是多项式，就称它为插值多项式。 插入法的拉丁文原意是“内部插入”，即在已知的函数表中，插入一些表中没有列出的、所需要的中间值。 若函数f(x)在自变数x一些离散值所对应的函数值为已知，则可以作一个适当的特定函数p(x)，使得p(x)在这些离散值所取的函数值，就是f(x)的已知值。从而可以用p(x)来估计f(x)在这些离散值之间的自变数所对应的函数值，这种方法称为插值法。 scipy的插值模块 Lagrange插值Lagrange插值是n次多项式插值，其成功地用构造插值基函数的 方法解决了求n次多项式插值函数问题。 ★基本思想 将待求的n次多项式插值函数pn(x）改写成另一种表示方式，再利用插值条件⑴确定其中的待定函数，从而求出插值多项式。 Newton插值Newton插值也是n次多项式插值，它提出另一种构造插值多项式的方法，与Lagrange插值相比，具有承袭性和易于变动节点的特点。 ★基本思想 将待求的n次插值多项式Pn（x）改写为具有承袭性的形式，然后利用插值条件⑴确定Pn（x）的待定系数，以求出所要的插值函数。 Hermite插值Hermite插值是利用未知函数f(x)在插值节点上的函数值及导数值来构造插值多项式的，其提法为：给定n+1个互异的节点x0,x1，……,xn上的函数值和导数值求一个2n+1次多项式H2n+1(x)满足插值条件 H2n+1(xk)=yk H’2n+1(xk)=y’k k=0,1,2，……，n ⒀ 如上求出的H2n+1(x）称为2n+1次Hermite插值函数，它与被插函数一般有更好的密合度. ★基本思想 利用Lagrange插值函数的构造方法，先设定函数形式，再利用插值条件⒀求出插值函数. 插值举例1234567891011121314151617181920212223242526272829303132#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poissonfrom scipy.interpolate import BarycentricInterpolatorfrom scipy.interpolate import CubicSpline x = np.random.poisson(lam=5, size=10000) print x pillar = 15 a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) plt.grid() # plt.show() print a print a[0].sum() rv = poisson(5) x1 = a[1] y1 = rv.pmf(x1) itp = BarycentricInterpolator(x1, y1) # 重心插值 x2 = np.linspace(x.min(), x.max(), 50) y2 = itp(x2) cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 plt.legend(loc='upper right') plt.grid() plt.show() 拟合与插值的区别插值曲线要过数据点，拟合曲线整体效果更好。 插值是指已知某函数的在若干离散点上的函数值或者导数信息，通过求解该函数中待定形式的插值函数以及待定系数，使得该函数在给定离散点上满足约束。 拟合是指已知某函数的若干离散函数值，通过调整该函数中若干待定系数，使得该函数与已知点集的差别（最小二乘意义）最小。如果待定函数是线性，就叫线性拟合或线性回归，否则叫非线性拟合或非线性回归。表达式也可以是分段函数，这种情况下叫样条拟合。 从几何意义上讲，拟合是给定了空间中的一些点，找到一个已知形式未知参数的连续曲面来最大限度地逼近这些点；而插值是找到一个（或几个分片光滑的）连续曲面来穿过这些点。 随着插值节点的增多，多项式次数也在增高，插值曲线在一些区域出现跳跃，并且越来越偏离原始曲线。 为了解决这个问题，人们发明了分段插值法。分段插值一般不会使用四次以上的多项式，而二次多项式会出现尖点，也是有问题的。所以就剩下线性和三次插值，最后使用最多的还是线性分段插值，这个好处是显而易见的。 via 百度百科-插值法 scipy.interpolate 知乎-拟合与插值的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>插值</tag>
        <tag>重心插值</tag>
        <tag>三次样条插值</tag>
        <tag>Hermite插值</tag>
        <tag>临近点插值</tag>
        <tag>线性插值</tag>
        <tag>Newton插值</tag>
        <tag>Lagrange插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model%2F</url>
    <content type="text"><![CDATA[导入数据集12345678910from sklearn import datasetsfrom sklearn.model_selection import train_test_splitboston = datasets.load_boston()X = boston.datay = boston.target"""划分数据集"""x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1) GridSearchCV寻参123456789lr = linear_model.LinearRegression()model = Lasso()# model = Ridge()alpha_can = np.logspace(-3, 2, 10)lasso_model = GridSearchCV(model, param_grid=&#123;'alpha': alpha_can&#125;, cv=5)lasso_model.fit(x_train, y_train)print '超参数：\n', lasso_model.best_params_ 预测12y_hat = lasso_model.predict(np.array(x_test)) 模型评价1234mse = np.average((y_hat - np.array(y_test)) ** 2) # Mean Squared Errorrmse = np.sqrt(mse) # Root Mean Squared Errorprint mse, rmse 或者用scikit-learn计算MSE/RMSE1234from sklearn import metricsprint "MSE:",metrics.mean_squared_error(y_test, y_hat)print "RMSE:",np.sqrt(metrics.mean_squared_error(y_test, y_hat)) cross_val_predict123456789boston = datasets.load_boston()X = boston.datay = boston.targetfrom sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(model, X, y, cv=10)print "MSE:",metrics.mean_squared_error(y, predicted)print "RMSE:",np.sqrt(metrics.mean_squared_error(y, predicted)) 输出图形展示这里画图真实值和预测值的变化关系，离中间的直线y=x直线越近的点代表预测损失越低。 12345678910t = np.arange(len(x_test))mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falseplt.plot(t, y_test, 'r-', linewidth=2, label=u'真实数据')plt.plot(t, y_hat, 'g-', linewidth=2, label=u'预测数据')plt.title(u'线性回归预测销量', fontsize=18)plt.legend(loc='upper right')plt.grid()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>scipy</tag>
        <tag>GridSearchCV</tag>
        <tag>MSE</tag>
        <tag>RMSE</tag>
        <tag>cross_val_predict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model中的Ridge、Lasso、ElasticNet回归]]></title>
    <url>%2F2017%2F11%2F13%2FRidge_Lasso_ElasticNet%2F</url>
    <content type="text"><![CDATA[导入包：12345678import numpy as npfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCVfrom sklearn.preprocessing import PolynomialFeaturesimport matplotlib.pyplot as pltfrom sklearn.pipeline import Pipelineimport matplotlib as mplimport warnings 定义模型评价函数：123456789def xss(y, y_hat): y = y.ravel() y_hat = y_hat.ravel() tss = np.var(y) rss = np.average((y_hat - y) ** 2) r2 = 1 - rss / tss corr_coef = np.corrcoef(y, y_hat)[0, 1] return r2, corr_coef 生成训练数据12345678910warnings.filterwarnings("ignore") # ConvergenceWarningnp.random.seed(0)np.set_printoptions(linewidth=1000)N = 9x = np.linspace(0, 6, N) + np.random.randn(N)x = np.sort(x)y = x**2 - 4*x - 3 + np.random.randn(N)x.shape = -1, 1y.shape = -1, 1 模型设置1234567891011121314models = [Pipeline([('poly', PolynomialFeatures()), ('linear', LinearRegression(fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', RidgeCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', LassoCV(alphas=np.logspace(-3, 2, 50), fit_intercept=False))]), Pipeline([('poly', PolynomialFeatures()), ('linear', ElasticNetCV(alphas=np.logspace(-3, 2, 50), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], fit_intercept=False))])]mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsenp.set_printoptions(suppress=True) 模型训练与图形展示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465plt.figure(figsize=(16, 10), facecolor='w')d_pool = np.arange(1, N, 1) # 阶m = d_pool.sizeclrs = [] # 颜色for c in np.linspace(16711680, 255, m): clrs.append('#%06x' % c)line_width = np.linspace(5, 2, m)titles = u'LinearRegression', u'Ridge', u'LASSO', u'ElasticNet'tss_list = []rss_list = []ess_list = []ess_rss_list = []for t in range(4): model = models[t] plt.subplot(2, 2, t+1) plt.plot(x, y, 'ro', ms=10, zorder=N) for i, d in enumerate(d_pool): model.set_params(poly__degree=d) model.fit(x, y.ravel()) lin = model.get_params('linear')['linear'] output = u'%s：%d阶，系数为：' % (titles[t], d) if hasattr(lin, 'alpha_'): idx = output.find(u'系数') output = output[:idx] + (u'alpha=%.6f，' % lin.alpha_) + output[idx:] if hasattr(lin, 'l1_ratio_'): # 根据交叉验证结果，从输入l1_ratio(list)中选择的最优l1_ratio_(float) idx = output.find(u'系数') output = output[:idx] + (u'l1_ratio=%.6f，' % lin.l1_ratio_) + output[idx:] print output, lin.coef_.ravel() x_hat = np.linspace(x.min(), x.max(), num=100) x_hat.shape = -1, 1 y_hat = model.predict(x_hat) s = model.score(x, y) r2, corr_coef = xss(y, model.predict(x)) # print 'R2和相关系数：', r2, corr_coef # print 'R2：', s, '\n' z = N - 1 if (d == 2) else 0 label = u'%d阶，$R^2$=%.3f' % (d, s) if hasattr(lin, 'l1_ratio_'): label += u'，L1 ratio=%.2f' % lin.l1_ratio_ plt.plot(x_hat, y_hat, color=clrs[i], lw=line_width[i], alpha=0.75, label=label, zorder=z) plt.legend(loc='upper left') plt.grid(True) plt.title(titles[t], fontsize=18) plt.xlabel('X', fontsize=16) plt.ylabel('Y', fontsize=16)plt.tight_layout(1, rect=(0, 0, 1, 0.95))plt.suptitle(u'多项式曲线拟合比较', fontsize=22)plt.show()y_max = max(max(tss_list), max(ess_rss_list)) * 1.05plt.figure(figsize=(9, 7), facecolor='w')t = np.arange(len(tss_list))plt.plot(t, tss_list, 'ro-', lw=2, label=u'TSS(Total Sum of Squares)')plt.plot(t, ess_list, 'mo-', lw=1, label=u'ESS(Explained Sum of Squares)')plt.plot(t, rss_list, 'bo-', lw=1, label=u'RSS(Residual Sum of Squares)')plt.plot(t, ess_rss_list, 'go-', lw=2, label=u'ESS+RSS')plt.ylim((0, y_max))plt.legend(loc='center right')plt.xlabel(u'LinearRegression/Ridge/LASSO/Elastic Net', fontsize=15)plt.ylabel(u'XSS值', fontsize=15)plt.title(u'总平方和TSS=？', fontsize=18)plt.grid(True)plt.show() 输出如下： 123456789101112131415161718192021222324252627282930313233线性回归：1阶，系数为： [-12.12113792 3.05477422]线性回归：2阶，系数为： [-3.23812184 -3.36390661 0.90493645]线性回归：3阶，系数为： [-3.90207326 -2.61163034 0.66422328 0.02290431]线性回归：4阶，系数为： [-8.20599769 4.20778207 -2.85304163 0.73902338 -0.05008557]线性回归：5阶，系数为： [ 21.59733285 -54.12232017 38.43116219 -12.68651476 1.98134176 -0.11572371]线性回归：6阶，系数为： [ 14.73304785 -37.87317494 23.67462342 -6.07037979 0.42536833 0.06803132 -0.00859246]线性回归：7阶，系数为： [ 314.30344622 -827.89446924 857.33293186 -465.46543638 144.21883851 -25.67294678 2.44658612 -0.09675941]线性回归：8阶，系数为： [-1189.50149198 3643.69109456 -4647.92941149 3217.22814712 -1325.87384337 334.32869072 -50.57119119 4.21251817 -0.148521 ]Ridge回归：1阶，alpha=0.109854，系数为： [-11.21592213 2.85121516]Ridge回归：2阶，alpha=0.138950，系数为： [-2.90423989 -3.49931368 0.91803171]Ridge回归：3阶，alpha=0.068665，系数为： [-3.47165245 -2.85078293 0.69245987 0.02314415]Ridge回归：4阶，alpha=0.222300，系数为： [-2.84560266 -1.99887417 -0.40628792 0.33863868 -0.02674442]Ridge回归：5阶，alpha=1.151395，系数为： [-1.68160373 -1.52726943 -0.8382036 0.2329258 0.03934251 -0.00663323]Ridge回归：6阶，alpha=0.001000，系数为： [ 0.53724068 -6.00552086 -3.75961826 5.64559118 -2.21569695 0.36872911 -0.02221343]Ridge回归：7阶，alpha=0.033932，系数为： [-2.38021238 -2.26383055 -1.47715232 0.00763115 1.12242917 -0.52769633 0.09199201 -0.00560199]Ridge回归：8阶，alpha=0.138950，系数为： [-2.19299093 -1.91896884 -1.21608489 -0.19314178 0.49300277 0.05452898 -0.09690455 0.02114435 -0.00140196]LASSO：1阶，alpha=0.222300，系数为： [-10.41556797 2.66199326]LASSO：2阶，alpha=0.001000，系数为： [-3.29932625 -3.31989869 0.89878903]LASSO：3阶，alpha=0.013257，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]LASSO：4阶，alpha=0.002560，系数为： [-5.08513199 -1.41147772 0.3380565 0.0440427 0.00099807]LASSO：5阶，alpha=0.042919，系数为： [-4.11853758 -1.8643949 0.2618319 0.07954732 0.00257481 -0.00069093]LASSO：6阶，alpha=0.001000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]LASSO：7阶，alpha=0.001000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]LASSO：8阶，alpha=0.001000，系数为： [-4.62623251 -1.37717809 0.17183854 0.04307765 0.00629505 0.00069171 0.0000355 -0.00000875 -0.00000386]ElasticNet：1阶，alpha=0.021210，l1_ratio=0.100000，系数为： [-10.74762959 2.74580662]ElasticNet：2阶，alpha=0.013257，l1_ratio=0.100000，系数为： [-2.95099269 -3.48472703 0.91705013]ElasticNet：3阶，alpha=0.013257，l1_ratio=1.000000，系数为： [-4.83524033 -1.48721929 0.29726322 0.05804667]ElasticNet：4阶，alpha=0.010481，l1_ratio=0.950000，系数为： [-4.8799192 -1.5317438 0.3452403 0.04825571 0.00049763]ElasticNet：5阶，alpha=0.004095，l1_ratio=0.100000，系数为： [-4.07916291 -2.18606287 0.44650232 0.05102669 0.00239164 -0.00048279]ElasticNet：6阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.53546398 -1.70335188 0.29896515 0.05237738 0.00489432 0.00007551 -0.00010944]ElasticNet：7阶，alpha=0.001000，l1_ratio=1.000000，系数为： [-4.51456835 -1.58477275 0.23483228 0.04900369 0.00593868 0.00044879 -0.00002625 -0.00002132]ElasticNet：8阶，alpha=0.001000，l1_ratio=0.500000，系数为： [-4.53761647 -1.45230301 0.18829714 0.0427561 0.00619739 0.00068209 0.00003506 -0.00000869 -0.00000384]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>ElasticNet</tag>
        <tag>Pipeline</tag>
        <tag>Ridge</tag>
        <tag>Lasso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model.LogisticRegression]]></title>
    <url>%2F2017%2F11%2F13%2Fsklearn.linear_model.LogisticRegression%2F</url>
    <content type="text"><![CDATA[导入数据集12345from sklearn import datasetsiris = datasets.load_iris()X = iris.data[:, :2]y = iris.target 数据处理与模型拟合Pipeline1234567891011121314import numpy as npfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegressionlr = Pipeline([('sc', StandardScaler()), ('clf', LogisticRegression()) ])lr.fit(X, y.ravel())y_hat = lr.predict(X)y_hat_prob = lr.predict_proba(X)np.set_printoptions(suppress=True)print 'y_hat = \n', y_hatprint 'y_hat_prob = \n', y_hat_probprint u'准确度：%.2f%%' % (100*np.mean(y_hat == y.ravel())) 绘图：分割面123456789101112131415161718192021222324252627import matplotlib.pyplot as pltimport matplotlib as mplN, M = 500, 500 # 横纵各采样多少个值x1_min, x1_max = X[:, 0].min(), X[:, 0].max() # 第0列的范围x2_min, x2_max = X[:, 1].min(), X[:, 1].max() # 第1列的范围t1 = np.linspace(x1_min, x1_max, N)t2 = np.linspace(x2_min, x2_max, M)x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点mpl.rcParams['font.sans-serif'] = [u'simHei']mpl.rcParams['axes.unicode_minus'] = Falsecm_light = mpl.colors.ListedColormap(['#77E0A0', '#FF8080', '#A0A0FF'])cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])y_hat = lr.predict(x_test) # 预测值y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同plt.figure(facecolor='w')plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值的显示plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark) # 样本的显示plt.xlabel(u'花萼长度', fontsize=14)plt.ylabel(u'花萼宽度', fontsize=14)plt.xlim(x1_min, x1_max)plt.ylim(x2_min, x2_max)plt.grid()plt.title(u'鸢尾花Logistic回归分类效果 - 标准化', fontsize=17)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>Pipeline</tag>
        <tag>LogisticRegression</tag>
        <tag>分割面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scipy.optimize使用]]></title>
    <url>%2F2017%2F11%2F13%2Fscipy.optimize%2F</url>
    <content type="text"><![CDATA[定义损失函数123456789101112131415161718192021222324252627#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npfrom scipy.optimize import leastsqimport matplotlib.pyplot as pltdef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return y 利用scipy.optimize求解线性回归1123456789101112131415x = np.linspace(-2, 2, 50)A, B, C = 2, 3, -1y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75t = leastsq(residual, [0, 0, 0], args=(x, y))theta = t[0]print '真实值：', A, B, Cprint '预测值：', thetay_hat = theta[0] * x ** 2 + theta[1] * x + theta[2]plt.plot(x, y, 'r-', linewidth=2, label=u'Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict')plt.legend(loc='upper left')plt.grid()plt.show() 利用scipy.optimize求解线性回归1输出图例 利用scipy.optimize求解非线性回归12345678910111213141516x = np.linspace(0, 5, 100)A = 5w = 1.5y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5t = leastsq(residual2, [3, 1], args=(x, y))theta = t[0]print '真实值：', A, wprint '预测值：', thetay_hat = theta[0] * np.sin(theta[1] * x)plt.plot(x, y, 'r-', linewidth=2, label='Actual')plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict')plt.legend(loc='lower left')plt.grid()plt.show() 利用scipy.optimize求解非线性回归输出图例 使用scipy计算函数极值1234567a = opt.fmin(f, 1)b = opt.fmin_cg(f, 1)c = opt.fmin_bfgs(f, 1)print a, 1/a, math.eprint bprint c 使用scipy计算函数极值输出123456789101112131415161718Optimization terminated successfully. Current function value: 0.692201 Iterations: 16 Function evaluations: 32Optimization terminated successfully. Current function value: 0.692201 Iterations: 4 Function evaluations: 30 Gradient evaluations: 10Optimization terminated successfully. Current function value: 0.692201 Iterations: 5 Function evaluations: 24 Gradient evaluations: 8[ 0.36787109] [ 2.71834351] 2.71828182846[ 0.36787948][ 0.36787942]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimize</tag>
        <tag>scipy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读取文件数据的几种方式]]></title>
    <url>%2F2017%2F11%2F13%2Fpython%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[方式一：f = file(path)1234567891011121314151617f = file(path)x = []y = []for i, d in enumerate(f): if i == 0: continue d = d.strip() if not d: continue d = map(float, d.split(',')) x.append(d[1:-1]) y.append(d[-1])pprint(x)pprint(y)x = np.array(x)y = np.array(y) 方式二：Python自带库csv12345678import csv f = file(path, 'rb') print f d = csv.reader(f) for line in d: print line f.close() 方式三：Python自带库numpy123import numpy as npp = np.loadtxt(path, delimiter=',', skiprows=1) 方式四：Python自带库pandas123import pandas as pddata = pd.read_csv(path)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM]]></title>
    <url>%2F2017%2F11%2F12%2FHMM%2F</url>
    <content type="text"><![CDATA[HMM定义 隐马尔科夫模型的贝叶斯网络 HMM的参数HMM的确定 HMM的参数 HMM的参数总结 HMM的两个基本性质 HMM举例 示例的各个参数 示例的思考 HMM的3个基本问题 概率计算问题 直接计算法 直接计算法分析 借鉴算法的优化思想 前向算法定义：前向概率-后向概率 前向算法定义 前向算法过程 后向算法后向算法定义 后向算法过程 后向算法的说明 前后向关系 单个状态的概率 r的意义 两个状态的联合概率 期望 学习算法 大数定理 监督学习方法 Baum-Welch算法 EM过程 极大化 初始状态概率 转移概率和观测概率 预测算法近似算法 算法：走棋盘/格子取数 Viterbi算法 Viterbi算法举例 总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>Baum-Welch</tag>
        <tag>Viterbi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型]]></title>
    <url>%2F2017%2F11%2F12%2F%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[引子 Beta分布 Beta分布的期望 Beta分布图形 先验分布-共轭分布共轭先验分布的定义 二项分布的最大似然估计 二项分布与先验举例 上述过程的理论解释 先验概率和后验概率的关系 伪计数 共轭先验的直接推广 Beta分布-Dirichlet分布Dirichlet分布 Dirichlet分布的期望 Dirichlet分布分析 对称Dirichlet分布 对称Dirichlet分布的参数分析 参数a对Dirichlet分布的影响 参数选择对对称Dirichlet分布的影响 多项分布的共轭分布是Dirichlet分布 三层贝叶斯网络模型LDALDA的解释 参数的学习 似然概率 Gibbs采样和更新规则Gibbs Sampling 联合分布 计算因子 Gibbs updating rule 词分布和主题分布 LDA总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Beta分布</tag>
        <tag>先验分布</tag>
        <tag>共轭先验分布</tag>
        <tag>二项分布</tag>
        <tag>Dirichlet分布</tag>
        <tag>多项分布</tag>
        <tag>Gibbs采样</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯网络]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯 贝叶斯网络的表达贝叶斯网络定义 一个简单的贝叶斯网络 全连接贝叶斯网络 一个“正常”的贝叶斯网络 对一个实际贝叶斯网络的分析 贝叶斯网络的形式化定义 马尔科夫模型一个特殊的贝叶斯网络。 D-separation条件独立的三种类型通过贝叶斯网络判定条件独立1 通过贝叶斯网络判定条件独立2 通过贝叶斯网络判定条件独立3 举例说明这三种情况 将上述结点推广到结点集 有向分离的举例 Markov Blanket再次分析链式网络 HMM 贝叶斯网络的用途分类预测 转移概率矩阵 贝叶斯网络的构建 混合（离散+连续）网络 孩子结点是连续的 孩子结点是离散的，父节点是连续的 孩子结点是离散的，父节点是连续的 贝叶斯网络的推导 无向环 原贝叶斯网络的近似树结构 将两图的相对熵转换成变量的互信息 Chou-Liu算法 最大权生成树MSWT的建立过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>贝叶斯网络</tag>
        <tag>朴素贝叶斯</tag>
        <tag>马尔科夫</tag>
        <tag>D-separation</tag>
        <tag>Markov Blanket</tag>
        <tag>Chou-Liu</tag>
        <tag>MSWT</tag>
        <tag>转移概率矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PLSA]]></title>
    <url>%2F2017%2F11%2F12%2FPLSA%2F</url>
    <content type="text"><![CDATA[PLSA模型PLSA模型概述 PLSA模型过程 最大似然估计 目标函数分析 求隐含变量主题zk的后验概率 分析似然函数期望 完成目标函数的建立 目标函数的求解 分析第一等式 同理分析第二等式 PLSA总结 PLSA进一步思考]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
        <tag>PLSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM]]></title>
    <url>%2F2017%2F11%2F12%2FEM%2F</url>
    <content type="text"><![CDATA[引子k-Means算法 对K-Means的思考 Jensen不等式：若f是凸函数 最大似然估计 二项分布的最大似然估计 进一步考察 按照MLE的过程分析 化简对数似然函数 参数估计的结论 符合直观想象 问题：随机变量无法直接（完全）观察到 从直观理解猜测GMM的参数估计 建立目标函数 第一步：估算数据来自哪个组份 第二步：估计每个组份的参数 EM算法EM算法的提出 通过最大似然估计建立目标函数 问题的提出 Jensen不等式 寻找尽量紧的下界 进一步分析 EM算法整体框架 坐标上升 从理论公式推导GMM E-step M-step 对均值求偏导 高斯分布的均值 高斯分布的方差：求偏导，等于0 多项分布的参数 拉格朗日乘子法 求偏导，等于0 结论]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>主题模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一些概念聚类的定义聚类就是对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。 相似度/距离计算方法总结 Hellinger distance 余弦相似度与Pearson相似系数 聚类的基本思想 聚类的衡量指标 ARI AMI 轮廓系数(Silhouette) k-Meansk-Means算法 对k-Means的思考 k-Means的公式化解释 如果使用其他相似度/距离度量 Mini-batch k-Means算法描述 k-Means聚类总结 CanopyCanopy算法 层次聚类方法 AGNES和DIANA算法 密度聚类方法 DBSCAN算法 DBSCAN算法的若干概念 DBSCAN算法流程 密度最大值聚类 高局部密度点距离 簇中心的识别 密度最大值聚类过程 边界和噪声的重新认识 Affinity Propagation算法概述基本概念 Exemplar范例：即聚类族中心点； s(i,j)：数据点i与数据点j的相似度值，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力； 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵； Preference参考度或称为偏好参数：是相似度矩阵中横轴纵轴索引相同的点，如s(i,i)，若按欧氏距离计算其值应为0，但在AP聚类中其表示数据点i作为聚类中心的程度，因此不能为0。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； Responsibility，r(i,k)：吸引度信息，表示数据点k适合作为数据点i的聚类中心的程度；公式如下： 其中a(i,k’)表示除k外其他点对i点的归属度值，初始为0；s(i,k’)表示除k外其他点对i的吸引度，即i外其他点都在争夺i点的 所有权；r(i,k)表示数据点k成为数据点i的聚类中心的累积证明，r(i,k)值大于0，则表示数据点k成为聚类中心的能力强。说明：此时只考虑哪个点k成为点i的聚类中心的可能性最大，但是没考虑这个吸引度最大的k是否也经常成为其他点的聚类中心（即归属度），若点k只是点i的聚类中心，不是其他任何点的聚类中心，则会造成最终聚类中心个数大于实际的中心个数。 Availability，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的合适程度，公式如下： 其中r(i’,k)表示点k作为除i外其他点的聚类中心的相似度值，取所有大于等于0的吸引度值，加上k作为聚类中心的可能程。即点k在这些吸引度值大于0的数据点的支持下，数据点i选择k作为其聚类中心的累积证明。 Damping factor阻尼系数：为防止数据震荡，引入地衰减系数，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得1-λ倍，其中λ在0-1之间，默认为0.5。 算法流程 更新相似度矩阵中每个点的吸引度信息，计算归属度信息； 更新归属度信息，计算吸引度信息； 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 s(i,k)就相当于i对选k这个人的一个固有的偏好程度 r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） a(i,k)：从公式里可以看到，所有r(i’,k)&gt;0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)&gt;0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。 r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。 优点 不需要制定最终聚类族的个数 已有的数据点作为最终的聚类中心，而不是新生成一个族中心。 模型对数据的初始值不敏感。 对初始相似度矩阵数据的对称性没有要求。 相比与k-centers聚类方法，其结果的平方差误差较小。缺点 虽然AP算法不用提前设置聚类中心的个数，但是需要事先设置参考度，而参考度的大小与聚类中心的个数正相关； 由于AP算法每次迭代都需要更新每个数据点的吸引度值和归属度值，算法复杂度较高，在大数据量下运行时间较长。 谱和谱聚类 谱分析的整体过程 一些概念 相似度图G的建立方法 权值比较 拉普拉斯矩阵的定义 拉普拉斯矩阵及其性质 谱聚类算法：未正则拉普拉斯矩阵 谱聚类算法：随机游走拉普拉斯矩阵 谱聚类算法：对称拉普拉斯矩阵 进一步思考 随机游走和拉普拉斯矩阵的关系 标签传递算法 via Affinity Propagation: AP聚类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>k-Means</tag>
        <tag>Canopy</tag>
        <tag>AGNES</tag>
        <tag>DIANA</tag>
        <tag>DBSCAN</tag>
        <tag>AP</tag>
        <tag>谱聚类</tag>
        <tag>层次聚类</tag>
        <tag>密度最大值聚类</tag>
        <tag>拉普拉斯</tag>
        <tag>边界</tag>
        <tag>噪声</tag>
        <tag>标签传递</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2017%2F11%2F12%2FSVM%2F</url>
    <content type="text"><![CDATA[支持向量机SVM的原理和目标线性可分支持向量机硬间隔最大化hard margin maximization;硬间隔支持向量机 分割超平面 分割超平面的思考 线性支持向量机软间隔最大化soft margin maximization;软间隔支持向量机 线性分类问题 输入数据 线性可分支持向量机 非线性支持向量机核函数kernel function 支持向量机的计算过程和算法步骤推导目标函数 最大间隔分离超平面 函数间隔和几何间隔 建立目标函数 线性可分支持向量机学习算法 线性SVM的目标函数 带松弛因子的SVM拉格朗日函数 代入目标函数 最终的目标函数 线性支持向量机 线性支持向量机学习算法 损失函数分析 核函数 高斯核 SVM中系数的求解：SMO SMO:序列最小最优化 二变量优化问题 SMO的迭代公式 退出条件 SVM总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2017%2F11%2F12%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[Boosting的思想 AdaBoost算法 AdaBoost误差上限 前向分步算法 前向分步算法的含义 前向分步算法的算法框架 前向分步算法与AdaBoost 证明 基分类器 权值的计算 分类错误率 权值的更新 权值和错误率的关键解释 AdaBoost总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>AdaBoost</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2017%2F11%2F12%2FXGBoost%2F</url>
    <content type="text"><![CDATA[考虑使用二阶导信息 决策树的描述 正则项的定义 目标函数计算 构造决策树的结构 XGBoost小结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升GBDT]]></title>
    <url>%2F2017%2F11%2F11%2F%E6%8F%90%E5%8D%87GBDT%2F</url>
    <content type="text"><![CDATA[由决策树和随机森林的关系的思考 随机森林的决策树分别采样建立，相对独立。 思考： 假定当前已经得到了m-1棵决策树，是否可以通过现有样本和决策树的信息，对第m棵决策树的建立产生有益的影响呢？ 各个决策树组成随机森林后，最后的投票过程可否在建立决策树时即确定呢？ 提升的概念 提升是一个机器学习技术，可以用于回归和分类问题，它每一步产生一个弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升（Gradient Boosting） 梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合（基函数）；提升算法通过迭代的选择一个负梯度方向上的基函数来逐渐逼近局部极小值。这种在函数域的梯度提升观点对机器学习的很多领域有深刻影响。 提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。 提升算法 提升算法推导 提升算法 梯度提升决策树GBDT 参数设置和正则化 衰减因子、降采样 GBDT总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>提升</tag>
        <tag>决策树</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2017%2F11%2F11%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[BootstrapingBootstraping的名称来自成语“pull up by your own bootstraps”,意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法。本意是通过拉靴子让自己上升，不可能发生的事情。后来意思发生了转变，隐喻不需要外界帮助，仅依靠自身力量让自己变得更好。 Bagging的策略 bootstrap aggregation 从样本集中重采样（有重复的）选出n个样本 在所有属性上，对这n个样本建立分类器（ID3，C4.5，CART,SVM,Logistic回归等） 重复以上两部m次，即获得了m个分类器 将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类 Another description of Bagging 随机森林随机森林在Bagging基础上做了修改： 从样本集中用Bootstrap采样选出n个样本； 从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树； 重复以上两步m次，即建立了m棵CART决策树 这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类 随机森林/Bagging和决策树的关系 当然可以使用决策树作为基本分类器 但也可以使用SVM、Logistic回归等其他分类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 投票机制简单投票机制 一票否决（一致表决） 少数服从多数 有效多数（加权） 阈值表决贝叶斯投票机制一种可能的方案 样本不均衡的常用处理方法 使用RF建立计算样本间相似度 使用RF计算特征重要度 使用RF生成特征 总结 决策树、随机森林的代码清洗，逻辑简单，在胜任分类问题的同时，往往也可以作为对数据分布探索的首要尝试算法。 随机森林的集成思想也可以用在其他分类器的设计中。 如果通过随机森林做样本的异常值检测 统计样本间位于相同决策树的叶节点的个数，形成样本相似度矩阵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树的概念决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。决策树学习是以实例为基础的归纳学习。决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。 决策树学习的生成算法建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法： ID3(Iterative Dichotomiser) C4.5 CART(Classification And Regression Tree) 信息增益概念当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。 信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。 定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差，即：g(D,A)=H(D)-H(D/A)，显然，这即为训练数据集D和特征A的互信息。 基本记号： 信息增益的计算方法 经验条件熵H(D/A) 其他目标 关于Gini系数的讨论 Gini系数的第二定义决策树中的Gini系数和社会学上的Gini系数并不相等。 三种决策树学习算法的比较 ID3:使用信息增益/互信息g(D,A)进行特征选择 取值多的属性，更容易使数据更纯，其信息增益更大 训练得到的是一棵庞大且深度浅的树：不合理。 C4.5：信息增益率gr(D,A)=g(D,A)/H(A) CART：基尼指数一个属性的信息增益（率）/Gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强。 决策树的评价 决策树的过拟合决策树对训练属于有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。 剪枝 随机森林 剪枝三种决策树的剪枝过程算法相同，区别仅是对于当前树的评价标准不同。 信息增益、信息增益率、基尼系数剪枝总体思路： 由完全树T0开始，剪枝部分结点得到T1，再次剪枝部分结点得到T2…直到仅剩树根的树Tk 在验证数据集上对这k个树分别评价，选择损失函数最小的树Ti 剪枝系数的确定 剪枝算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2017%2F11%2F11%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic回归分类问题的首选算法; 线性回归：最大似然估计+高斯分布； Logistic回归：最大似然估计+伯努利分布； Logistic回归是事件发生几率取对数下的线性回归。高斯分布和伯努利分布都是属于指数族分布，所以说逻辑回归是广义线性模型GLM回归。 Logistic/Sigmoid函数 Logistic回归参数估计 对数似然函数 参数的迭代 Logistic回归的损失函数A： ### Logistic回归的损失函数B： 广义线性模型GLM 多分类：Softmax回归Softmax名字由来 ## Softmax回归 信息熵定义信息量 熵熵是随机变量不确定性的度量，不确定性越大，熵值越大。若随机变量退化成定值，熵最小，为0；若随机分布为均匀分布，熵最大。 熵的定义 熵的公式推导 均匀分布的信息熵 联合熵、条件熵、相对熵联合熵、条件熵 推导条件熵的定义公式 相对熵、KL散度 互信息互信息的定义 互信息与条件熵的关系 公式]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>Logistic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归与特征选择]]></title>
    <url>%2F2017%2F11%2F11%2F%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[线性回归可以对样本是非线性的，只要对参数线性。 高斯分布在做特征选择的时候，可以看回归的残差图，真正理想状态下，经过模型回归后的残差应该是服从正态分布的，而不是均匀的随机分布；此时可以画个单变量与残差的关系图，直观的看一下是不是有什么明显的一阶、高阶趋势在里面，则需要进一步特征加工。 大数定理指的是事件发生的概率在N取无穷大的时候趋近于事件发生的概率。频率的极限是概率。 使用极大似然估计解释最小二乘 最大似然估计MLE最大似然函数的假设似然函数假设每个样本出现的概率是独立的，因此整体出现的概率就是各个样本出现的概率的累计。 似然函数 高斯的对数似然与最小二乘误差服从高斯分布的情况下的对数似然函数： 解析式的求解过程 最小二乘法的本质误差服从高斯分布的情况下的对数似然函数最大化的过程等价于最小二乘。 最小二乘意义下的参数最优解 加入lamda扰动后 线性回归的复杂度惩罚因子惩罚因子的直观理解，阶数越高，模型越复杂，系数振荡的越厉害，系数的取值也越大，同时呢，系数又会有正有负，所以直接用系数的平方作为模型复杂度的惩罚因子。 Ridge岭回归 是加了L2正则项的最小二乘。LASSO 是L1正则。 Ridge岭回归与LASSO：两个都差不多，单看模型系数LASSO要稍稍比Ridge稳定一些，但如果给定模型的评价指标的化，Ridge稍微比LASSO好。LASSO可以具有特征选择的功能。 正则项与防止过拟合 广义逆矩阵（伪逆） 梯度下降算法 梯度方向 批量梯度下降算法 随机梯度下降算法 mini-batch如果不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。 模型评价TSS &gt;= ESS + RSSR方的取值范围：(-inf,1] 局部加权回归 局部加权线性回归 权值的设置]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础]]></title>
    <url>%2F2017%2F11%2F08%2FPython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[intro 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544#!/usr/bin/python# -*- coding:utf-8 -*-# 导入NumPy函数库，一般都是用这样的形式(包括别名np，几乎是约定俗成的)import numpy as npimport matplotlib as mplfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmimport timefrom scipy.optimize import leastsqfrom scipy import statsimport scipy.optimize as optimport scipyimport matplotlib.pyplot as pltfrom scipy.stats import norm, poisson# from scipy.interpolate import BarycentricInterpolator# from scipy.interpolate import CubicSplineimport math# import seaborndef residual(t, x, y): return y - (t[0] * x ** 2 + t[1] * x + t[2])def residual2(t, x, y): print t[0], t[1] return y - t[0]*np.sin(t[1]*x)# x ** x x &gt; 0# (-x) ** (-x) x &lt; 0def f(x): y = np.ones_like(x) i = x &gt; 0 y[i] = np.power(x[i], x[i]) i = x &lt; 0 y[i] = np.power(-x[i], -x[i]) return yif __name__ == "__main__": # # 开场白： # numpy是非常好用的数据包，如：可以这样得到这个二维数组 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # 正式开始 -:) # 标准Python的列表(list)中，元素本质是对象。 # 如：L = [1, 2, 3]，需要3个指针和三个整数对象，对于数值运算比较浪费内存和CPU。 # 因此，Numpy提供了ndarray(N-dimensional array object)对象：存储单一数据类型的多维数组。 # # 1.使用array创建 # 通过array函数传递list对象 # L = [1, 2, 3, 4, 5, 6] # print "L = ", L # a = np.array(L) # print "a = ", a # print type(a) # # # 若传递的是多层嵌套的list，将创建多维数组 # b = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) # print b # # # # # # # 数组大小可以通过其shape属性获得 # print a.shape # print b.shape # # # # # # 也可以强制修改shape # b.shape = 4, 3 # print b # # # # 注：从(3,4)改为(4,3)并不是对数组进行转置，而只是改变每个轴的大小，数组元素在内存中的位置并没有改变 # # # # # # 当某个轴为-1时，将根据数组元素的个数自动计算此轴的长度 # b.shape = 2, -1 # print b # print b.shape # # # # b.shape = 3, 4 # # # 使用reshape方法，可以创建改变了尺寸的新数组，原数组的shape保持不变 # c = b.reshape((4, -1)) # print "b = \n", b # print 'c = \n', c # # # # # 数组b和c共享内存，修改任意一个将影响另外一个 # b[0][1] = 20 # print "b = \n", b # print "c = \n", c # # # # # # 数组的元素类型可以通过dtype属性获得 # print a.dtype # print b.dtype # # # # # # # 可以通过dtype参数在创建时指定元素类型 # d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.float) # f = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.complex) # print d # print f # # # # # # 如果更改元素类型，可以使用astype安全的转换 # f = d.astype(np.int) # print f # # # # # 但不要强制仅修改元素类型，如下面这句，将会以int来解释单精度float类型 # d.dtype = np.int # print d # # 2.使用函数创建 # 如果生成一定规则的数据，可以使用NumPy提供的专门函数 # arange函数类似于python的range函数：指定起始值、终止值和步长来创建数组 # 和Python的range类似，arange同样不包括终值；但arange可以生成浮点类型，而range只能是整数类型 # a = np.arange(1, 10, 0.5) # print a # # # # # # linspace函数通过指定起始值、终止值和元素个数来创建数组，缺省包括终止值 # b = np.linspace(1, 10, 10) # print 'b = ', b # # # # # 可以通过endpoint关键字指定是否包括终值 # c = np.linspace(1, 10, 10, endpoint=False) # print 'c = ', c # # # # # 和linspace类似，logspace可以创建等比数列 # # 下面函数创建起始值为10^1，终止值为10^2，有20个数的等比数列 # d = np.logspace(1, 2, 10, endpoint=True) # print d # # # # # # 下面创建起始值为2^0，终止值为2^10(包括)，有10个数的等比数列 # f = np.logspace(0, 10, 11, endpoint=True, base=2) # print f # # # # # # 使用 frombuffer, fromstring, fromfile等函数可以从字节序列创建数组 # s = 'abcd' # g = np.fromstring(s, dtype=np.int8) # print g # # # 3.存取 # 3.1常规办法：数组元素的存取方法和Python的标准方法相同 # a = np.arange(10) # print a # # # 获取某个元素 # print a[3] # # # # 切片[3,6)，左闭右开 # print a[3:6] # # # # 省略开始下标，表示从0开始 # print a[:5] # # # # 下标为负表示从后向前数 # print a[3:] # # # # 步长为2 # print a[1:9:2] # # # # 步长为-1，即翻转 # print a[::-1] # # # # 切片数据是原数组的一个视图，与原数组共享内容空间，可以直接修改元素值 # a[1:4] = 10, 20, 30 # print a # # # # 因此，在实践中，切实注意原始数据是否被破坏，如： # b = a[2:5] # b[0] = 200 # print a # # # 3.2 整数/布尔数组存取 # # 3.2.1 # 根据整数数组存取：当使用整数序列对数组元素进行存取时， # 将使用整数序列中的每个元素作为下标，整数序列可以是列表(list)或者数组(ndarray)。 # 使用整数序列作为下标获得的数组不和原始数组共享数据空间。 # a = np.logspace(0, 9, 10, base=2) # print a # i = np.arange(0, 10, 2) # print i # # # # 利用i取a中的元素 # b = a[i] # print b # # # b的元素更改，a中元素不受影响 # b[2] = 1.6 # print b # print a # # 3.2.2 # 使用布尔数组i作为下标存取数组a中的元素：返回数组a中所有在数组b中对应下标为True的元素 # # 生成10个满足[0,1)中均匀分布的随机数 # a = np.random.rand(10) # print a # # # 大于0.5的元素索引 # print a &gt; 0.5 # # # # 大于0.5的元素 # b = a[a &gt; 0.5] # print b # # # # 将原数组中大于0.5的元素截取成0.5 # a[a &gt; 0.5] = 0.5 # print a # # # # b不受影响 # print b # 3.3 二维数组的切片 # [[ 0 1 2 3 4 5] # [10 11 12 13 14 15] # [20 21 22 23 24 25] # [30 31 32 33 34 35] # [40 41 42 43 44 45] # [50 51 52 53 54 55]] # a = np.arange(0, 60, 10) # 行向量 # print 'a = ', a # b = a.reshape((-1, 1)) # 转换成列向量 # print b # c = np.arange(6) # print c # f = b + c # 行 + 列 # print f # # 合并上述代码： # a = np.arange(0, 60, 10).reshape((-1, 1)) + np.arange(6) # print a # # 二维数组的切片 # print a[[0, 1, 2], [2 ,3, 4]] # print a[4, [2, 3, 4]] # print a[4:, [2, 3, 4]] # i = np.array([True, False, True, False, False, True]) # print a[i] # print a[i, 3] # # 4.1 numpy与Python数学库的时间比较 # for j in np.logspace(0, 7, 10): # j = int(j) # x = np.linspace(0, 10, j) # start = time.clock() # y = np.sin(x) # t1 = time.clock() - start # # x = x.tolist() # start = time.clock() # for i, t in enumerate(x): # x[i] = math.sin(t) # t2 = time.clock() - start # print j, ": ", t1, t2, t2/t1 # 4.2 元素去重 # 4.2.1直接使用库函数 # a = np.array((1, 2, 3, 4, 5, 5, 7, 3, 2, 2, 8, 8)) # print '原始数组：', a # # 使用库函数unique # b = np.unique(a) # print '去重后：', b # # 4.2.2 二维数组的去重，结果会是预期的么？ # c = np.array(((1, 2), (3, 4), (5, 6), (1, 3), (3, 4), (7, 6))) # print '二维数组', c # print '去重后：', np.unique(c) # # 4.2.3 方案1：转换为虚数 # # r, i = np.split(c, (1, ), axis=1) # # x = r + i * 1j # x = c[:, 0] + c[:, 1] * 1j # print '转换成虚数：', x # print '虚数去重后：', np.unique(x) # print np.unique(x, return_index=True) # 思考return_index的意义 # idx = np.unique(x, return_index=True)[1] # print '二维数组去重：\n', c[idx] # # 4.2.3 方案2：利用set # print '去重方案2：\n', np.array(list(set([tuple(t) for t in c]))) # # 4.3 stack and axis # a = np.arange(1, 10).reshape((3, 3)) # b = np.arange(11, 20).reshape((3, 3)) # c = np.arange(101, 110).reshape((3, 3)) # print 'a = \n', a # print 'b = \n', b # print 'c = \n', c # print 'axis = 0 \n', np.stack((a, b, c), axis=0) # print 'axis = 1 \n', np.stack((a, b, c), axis=1) # print 'axis = 2 \n', np.stack((a, b, c), axis=2) # 5.绘图 # 5.1 绘制正态分布概率密度函数 # mu = 0 # sigma = 1 # x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 50) # y = np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (math.sqrt(2 * math.pi) * sigma) # print x.shape # print 'x = \n', x # print y.shape # print 'y = \n', y # # plt.plot(x, y, 'ro-', linewidth=2) # plt.plot(x, y, 'r-', x, y, 'go', linewidth=2, markersize=8) # plt.grid(True) # plt.show() mpl.rcParams['font.sans-serif'] = [u'SimHei'] #FangSong/黑体 FangSong/KaiTi mpl.rcParams['axes.unicode_minus'] = False # # 5.2 损失函数：Logistic损失(-1,1)/SVM Hinge损失/ 0/1损失 # x = np.array(np.linspace(start=-2, stop=3, num=1001, dtype=np.float)) # y_logit = np.log(1 + np.exp(-x)) / math.log(2) # y_boost = np.exp(-x) # y_01 = x &lt; 0 # y_hinge = 1.0 - x # y_hinge[y_hinge &lt; 0] = 0 # plt.plot(x, y_logit, 'r-', label='Logistic Loss', linewidth=2) # plt.plot(x, y_01, 'g-', label='0/1 Loss', linewidth=2) # plt.plot(x, y_hinge, 'b-', label='Hinge Loss', linewidth=2) # plt.plot(x, y_boost, 'm--', label='Adaboost Loss', linewidth=2) # plt.grid() # plt.legend(loc='upper right') # # plt.savefig('1.png') # plt.show() # # 5.3 x^x # x = np.linspace(-1.3, 1.3, 101) # y = f(x) # plt.plot(x, y, 'g-', label='x^x', linewidth=2) # plt.grid() # plt.legend(loc='upper left') # plt.show() # # 5.4 胸型线 # x = np.arange(1, 0, -0.001) # y = (-3 * x * np.log(x) + np.exp(-(40 * (x - 1 / np.e)) ** 4) / 25) / 2 # plt.figure(figsize=(5,7)) # plt.plot(y, x, 'r-', linewidth=2) # plt.grid(True) # plt.show() # 5.5 心形线 # t = np.linspace(0, 7, 100) # x = 16 * np.sin(t) ** 3 # y = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid(True) # plt.show() # # 5.6 渐开线 # t = np.linspace(0, 50, num=1000) # x = t*np.sin(t) + np.cos(t) # y = np.sin(t) - t*np.cos(t) # plt.plot(x, y, 'r-', linewidth=2) # plt.grid() # plt.show() # # Bar # mpl.rcParams['font.sans-serif'] = [u'SimHei'] #黑体 FangSong/KaiTi # mpl.rcParams['axes.unicode_minus'] = False # x = np.arange(0, 10, 0.1) # y = np.sin(x) # plt.bar(x, y, width=0.04, linewidth=0.2) # plt.plot(x, y, 'r--', linewidth=2) # plt.title(u'Sin曲线') # plt.xticks(rotation=-60) # plt.xlabel('X') # plt.ylabel('Y') # plt.grid() # plt.show() # # 6. 概率分布 # # 6.1 均匀分布 # x = np.random.rand(10000) # t = np.arange(len(x)) # plt.hist(x, 30, color='m', alpha=0.5, label=u'均匀分布') # # plt.plot(t, x, 'r-', label=u'均匀分布') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 6.2 验证中心极限定理 # t = 1000 # a = np.zeros(10000) # for i in range(t): # a += np.random.uniform(-5, 5, 10000) # a /= t # plt.hist(a, bins=30, color='g', alpha=0.5, normed=True, label=u'均匀分布叠加') # plt.legend(loc='upper left') # plt.grid() # plt.show() # 6.21 其他分布的中心极限定理 # lamda = 10 # p = stats.poisson(lamda) # y = p.rvs(size=1000) # mx = 30 # r = (0, mx) # bins = r[1] - r[0] # plt.figure(figsize=(10, 8), facecolor='w') # plt.subplot(121) # plt.hist(y, bins=bins, range=r, color='g', alpha=0.8, normed=True) # t = np.arange(0, mx+1) # plt.plot(t, p.pmf(t), 'ro-', lw=2) # plt.grid(True) # # N = 1000 # M = 10000 # plt.subplot(122) # a = np.zeros(M, dtype=np.float) # p = stats.poisson(lamda) # for i in np.arange(N): # y = p.rvs(size=M) # a += y # a /= N # plt.hist(a, bins=20, color='g', alpha=0.8, normed=True) # plt.grid(b=True) # plt.show() # # 6.3 Poisson分布 # x = np.random.poisson(lam=5, size=10000) # print x # pillar = 15 # a = plt.hist(x, bins=pillar, normed=True, range=[0, pillar], color='g', alpha=0.5) # plt.grid() # # plt.show() # print a # print a[0].sum() # # 6.4 直方图的使用 # mu = 2 # sigma = 3 # data = mu + sigma * np.random.randn(1000) # h = plt.hist(data, 30, normed=1, color='#a0a0ff') # x = h[1] # y = norm.pdf(x, loc=mu, scale=sigma) # plt.plot(x, y, 'r--', x, y, 'ro', linewidth=2, markersize=4) # plt.grid() # plt.show() # # 6.5 插值 # rv = poisson(5) # x1 = a[1] # y1 = rv.pmf(x1) # itp = BarycentricInterpolator(x1, y1) # 重心插值 # x2 = np.linspace(x.min(), x.max(), 50) # y2 = itp(x2) # cs = scipy.interpolate.CubicSpline(x1, y1) # 三次样条插值 # plt.plot(x2, cs(x2), 'm--', linewidth=5, label='CubicSpine') # 三次样条插值 # plt.plot(x2, y2, 'g-', linewidth=3, label='BarycentricInterpolator') # 重心插值 # plt.plot(x1, y1, 'r-', linewidth=1, label='Actural Value') # 原始值 # plt.legend(loc='upper right') # plt.grid() # plt.show() # 7. 绘制三维图像 # x, y = np.ogrid[-3:3:100j, -3:3:100j] # # u = np.linspace(-3, 3, 101) # # x, y = np.meshgrid(u, u) # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # # z = x*y*np.exp(-(x**2 + y**2)/2) / math.sqrt(2*math.pi) # fig = plt.figure() # ax = fig.add_subplot(111, projection='3d') # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.coolwarm, linewidth=0.1) # # ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.Accent, linewidth=0.5) # plt.show() # # cmaps = [('Perceptually Uniform Sequential', # # ['viridis', 'inferno', 'plasma', 'magma']), # # ('Sequential', ['Blues', 'BuGn', 'BuPu', # # 'GnBu', 'Greens', 'Greys', 'Oranges', 'OrRd', # # 'PuBu', 'PuBuGn', 'PuRd', 'Purples', 'RdPu', # # 'Reds', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd']), # # ('Sequential (2)', ['afmhot', 'autumn', 'bone', 'cool', # # 'copper', 'gist_heat', 'gray', 'hot', # # 'pink', 'spring', 'summer', 'winter']), # # ('Diverging', ['BrBG', 'bwr', 'coolwarm', 'PiYG', 'PRGn', 'PuOr', # # 'RdBu', 'RdGy', 'RdYlBu', 'RdYlGn', 'Spectral', # # 'seismic']), # # ('Qualitative', ['Accent', 'Dark2', 'Paired', 'Pastel1', # # 'Pastel2', 'Set1', 'Set2', 'Set3']), # # ('Miscellaneous', ['gist_earth', 'terrain', 'ocean', 'gist_stern', # # 'brg', 'CMRmap', 'cubehelix', # # 'gnuplot', 'gnuplot2', 'gist_ncar', # # 'nipy_spectral', 'jet', 'rainbow', # # 'gist_rainbow', 'hsv', 'flag', 'prism'])] # 8.1 scipy # 线性回归例1 # x = np.linspace(-2, 2, 50) # A, B, C = 2, 3, -1 # y = (A * x ** 2 + B * x + C) + np.random.rand(len(x))*0.75 # # t = leastsq(residual, [0, 0, 0], args=(x, y)) # theta = t[0] # print '真实值：', A, B, C # print '预测值：', theta # y_hat = theta[0] * x ** 2 + theta[1] * x + theta[2] # plt.plot(x, y, 'r-', linewidth=2, label=u'Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label=u'Predict') # plt.legend(loc='upper left') # plt.grid() # plt.show() # # 线性回归例2 # x = np.linspace(0, 5, 100) # A = 5 # w = 1.5 # y = A * np.sin(w*x) + np.random.rand(len(x)) - 0.5 # # t = leastsq(residual2, [3, 1], args=(x, y)) # theta = t[0] # print '真实值：', A, w # print '预测值：', theta # y_hat = theta[0] * np.sin(theta[1] * x) # plt.plot(x, y, 'r-', linewidth=2, label='Actual') # plt.plot(x, y_hat, 'g-', linewidth=2, label='Predict') # plt.legend(loc='lower left') # plt.grid() # plt.show() # # 8.2 使用scipy计算函数极值 # a = opt.fmin(f, 1) # b = opt.fmin_cg(f, 1) # c = opt.fmin_bfgs(f, 1) # print a, 1/a, math.e # print b # print c # marker description # ”.” point # ”,” pixel # “o” circle # “v” triangle_down # “^” triangle_up # “&lt;” triangle_left # “&gt;” triangle_right # “1” tri_down # “2” tri_up # “3” tri_left # “4” tri_right # “8” octagon # “s” square # “p” pentagon # “*” star # “h” hexagon1 # “H” hexagon2 # “+” plus # “x” x # “D” diamond # “d” thin_diamond # “|” vline # “_” hline # TICKLEFT tickleft # TICKRIGHT tickright # TICKUP tickup # TICKDOWN tickdown # CARETLEFT caretleft # CARETRIGHT caretright # CARETUP caretup # CARETDOWN caretdown calc_e123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_e_small(x): n = 10 f = np.arange(1, n+1).cumprod() b = np.array([x]*n).cumprod() return np.sum(b / f) + 1def calc_e(x): reverse = False if x &lt; 0: # 处理负数 x = -x reverse = True ln2 = 0.69314718055994530941723212145818 c = x / ln2 a = int(c+0.5) b = x - a*ln2 y = (2 ** a) * calc_e_small(b) if reverse: return 1/y return yif __name__ == "__main__": t1 = np.linspace(-2, 0, 10, endpoint=False) t2 = np.linspace(0, 3, 20) t = np.concatenate((t1, t2)) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_e(x) print 'e^', x, ' = ', y[i], '(近似值)\t', math.exp(x), '(真实值)' # print '误差：', y[i] - math.exp(x) plt.figure(facecolor='w') mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 指数函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('exp(X)', fontsize=15) plt.grid(True) plt.show() calc_sin123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport mathimport matplotlib as mplimport matplotlib.pyplot as pltdef calc_sin_small(x): x2 = -x ** 2 t = x f = 1 sum = 0 for i in range(10): sum += t / f t *= x2 f *= ((2*i+2)*(2*i+3)) return sumdef calc_sin(x): a = x / (2*np.pi) k = np.floor(a) a = x - k*2*np.pi return calc_sin_small(a)if __name__ == "__main__": t = np.linspace(-2*np.pi, 2*np.pi, 100, endpoint=False) print t # 横轴数据 y = np.empty_like(t) for i, x in enumerate(t): y[i] = calc_sin(x) print 'sin(', x, ') = ', y[i], '(近似值)\t', math.sin(x), '(真实值)' # print '误差：', y[i] - math.exp(x) mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False plt.figure(facecolor='w') plt.plot(t, y, 'r-', t, y, 'go', linewidth=2) plt.title(u'Taylor展式的应用 - 正弦函数', fontsize=18) plt.xlabel('X', fontsize=15) plt.ylabel('sin(X)', fontsize=15) plt.xlim((-7, 7)) plt.ylim((-1.1, 1.1)) plt.grid(True) plt.show() class_intro123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding:utf-8 -*-class People: def __init__(self, n, a, s): self.name = n self.age = a self.__score = s self.print_people() # self.__print_people() # 私有函数的作用 def print_people(self): str = u'%s的年龄：%d，成绩为：%.2f' % (self.name, self.age, self.__score) print str __print_people = print_peopleclass Student(People): def __init__(self, n, a, w): People.__init__(self, n, a, w) self.name = 'Student ' + self.name def print_people(self): str = u'%s的年龄：%d' % (self.name, self.age) print strdef func(p): p.age = 11if __name__ == '__main__': p = People('Tom', 10, 3.14159) func(p) # p传入的是引用类型 p.print_people() print # 注意分析下面语句的打印结果，是否觉得有些“怪异”？ j = Student('Jerry', 12, 2.71828) print # 成员函数 p.print_people() j.print_people() print People.print_people(p) People.print_people(j) stat12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport mathimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmdef calc_statistics(x): n = x.shape[0] # 样本个数 # 手动计算 m = 0 m2 = 0 m3 = 0 m4 = 0 for t in x: m += t m2 += t*t m3 += t**3 m4 += t**4 m /= n m2 /= n m3 /= n m4 /= n mu = m sigma = np.sqrt(m2 - mu*mu) skew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3 kurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3 print '手动计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 使用系统函数验证 mu = np.mean(x, axis=0) sigma = np.std(x, axis=0) skew = stats.skew(x) kurtosis = stats.kurtosis(x) return mu, sigma, skew, kurtosisif __name__ == '__main__': d = np.random.randn(100000) print d mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 一维直方图 mpl.rcParams[u'font.sans-serif'] = 'SimHei' mpl.rcParams[u'axes.unicode_minus'] = False y1, x1, dummy = plt.hist(d, bins=50, normed=True, color='g', alpha=0.75) t = np.arange(x1.min(), x1.max(), 0.05) y = np.exp(-t**2 / 2) / math.sqrt(2*math.pi) plt.plot(t, y, 'r-', lw=2) plt.title(u'高斯分布，样本个数：%d' % d.shape[0]) plt.grid(True) plt.show() d = np.random.randn(100000, 2) mu, sigma, skew, kurtosis = calc_statistics(d) print '函数库计算均值、标准差、偏度、峰度：', mu, sigma, skew, kurtosis # 二维图像 N = 30 density, edges = np.histogramdd(d, bins=[N, N]) print '样本总数：', np.sum(density) density /= density.max() x = y = np.arange(N) t = np.meshgrid(x, y) fig = plt.figure(facecolor='w') ax = fig.add_subplot(111, projection='3d') ax.scatter(t[0], t[1], density, c='r', s=15*density, marker='o', depthshade=True) ax.plot_surface(t[0], t[1], density, cmap=cm.Accent, rstride=2, cstride=2, alpha=0.9, lw=0.75) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.title(u'二元高斯分布，样本个数：%d' % d.shape[0], fontsize=20) plt.tight_layout(0.1) plt.show() MultiGuass12345678910111213141516171819202122232425262728293031#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': x1, x2 = np.mgrid[-5:5:51j, -5:5:51j] x = np.stack((x1, x2), axis=2) plt.figure(figsize=(9, 8), facecolor='w') sigma = (np.identity(2), np.diag((3,3)), np.diag((2,5)), np.array(((2,1), (2,5)))) for i in np.arange(4): ax = plt.subplot(2, 2, i+1, projection='3d') norm = stats.multivariate_normal((0, 0), sigma[i]) y = norm.pdf(x) ax.plot_surface(x1, x2, y, cmap=cm.Accent, rstride=1, cstride=1, alpha=0.9, lw=0.3) ax.set_xlabel(u'X') ax.set_ylabel(u'Y') ax.set_zlabel(u'Z') plt.suptitle(u'二元高斯分布方差比较', fontsize=18) plt.tight_layout(1.5) plt.show() gamma12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom scipy.special import gammafrom scipy.special import factorialmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'if __name__ == '__main__': N = 5 x = np.linspace(0, N, 50) y = gamma(x+1) plt.figure(facecolor='w') plt.plot(x, y, 'r-', x, y, 'm*', lw=2) z = np.arange(0, N+1) f = factorial(z, exact=True) # 阶乘 print f plt.plot(z, f, 'go', markersize=8) plt.grid(b=True) plt.xlim(-0.1,N+0.1) plt.ylim(0.5, np.max(y)*1.05) plt.xlabel(u'X', fontsize=15) plt.ylabel(u'Gamma(X) - 阶乘', fontsize=15) plt.title(u'阶乘和Gamma函数', fontsize=16) plt.show() Benford123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# -*- coding:utf-8 -*-# /usr/bin/pythonimport numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom time import timefrom scipy.special import factorialimport mathmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def top1(number, a): number /= a while number &gt;= 10: number /= 10 a *= 10 return number, adef top2(number, N2): while number &gt;= N2: number /= 10 n = number while number &gt;= 10: number /= 10 return n, numberdef top3(number): number -= int(number) return int(10 ** number)def top4(number): number -= int(number) frequency[int(10 ** number) - 1] += 1if __name__ == '__main__': N = 100000 x = range(1, N+1) frequency = np.zeros(9, dtype=np.int) f = 1 print '开始计算...' t0 = time() # top1 # a = 1 # for t in x: # f *= t # i, a = top1(f, a) # # print t, i, f, a # frequency[i-1] += 1 # top2 # N2 = N ** 3 # for t in x: # f *= t # f, i = top2(f, N2) # frequency[i-1] += 1 # Top 3：实现1 # f = 0 # for t in x: # f += math.log10(t) # frequency[top3(f) - 1] += 1 # Top 3：实现2 # y = np.cumsum(np.log10(x)) # for t in y: # frequency[top3(t) - 1] += 1 # Top 4：本质与Top3相同 y = np.cumsum(np.log10(x)) map(top4, y) t1 = time() print '耗时：', t1 - t0 print frequency t = np.arange(1, 10) plt.plot(t, frequency, 'r-', t, frequency, 'go', lw=2, markersize=8) for x,y in enumerate(frequency): plt.text(x+1.1, y, frequency[x], verticalalignment='top', fontsize=15) plt.title(u'%d!首位数字出现频率' % N, fontsize=18) plt.xlim(0.5, 9.5) plt.ylim(0, max(frequency)*1.03) plt.grid(b=True) plt.show() # 使用numpy # N = 170 # x = np.arange(1, N+1) # f = np.zeros(9, dtype=np.int) # t1 = time() # y = factorial(x, exact=False) # z = map(top, y) # t2 = time() # print '耗时 = \t', t2 - t1 # for t in z: # f[t-1] += 1 # print f Pearson123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltimport warningsmpl.rcParams['axes.unicode_minus'] = Falsempl.rcParams['font.sans-serif'] = 'SimHei'def calc_pearson(x, y): std1 = np.std(x) # np.sqrt(np.mean(x**2) - np.mean(x)**2) std2 = np.std(y) cov = np.cov(x, y, bias=True)[0,1] return cov / (std1 * std2)def intro(): N = 10 x = np.random.rand(N) y = 2 * x + np.random.randn(N) * 0.1 print x print y print '系统计算：', stats.pearsonr(x, y)[0] print '手动计算：', calc_pearson(x, y)def rotate(x, y, theta=45): data = np.vstack((x, y)) # print data mu = np.mean(data, axis=1) mu = mu.reshape((-1, 1)) # print mu data -= mu # print data theta *= (np.pi / 180) c = np.cos(theta) s = np.sin(theta) m = np.array(((c, -s), (s, c))) return m.dot(data) + mudef pearson(x, y, tip): clrs = list('rgbmycrgbmycrgbmycrgbmyc') plt.figure(figsize=(10, 8), facecolor='w') for i, theta in enumerate(np.linspace(0, 90, 6)): xr, yr = rotate(x, y, theta) p = stats.pearsonr(xr, yr)[0] # print calc_pearson(xr, yr) print '旋转角度：', theta, 'Pearson相关系数：', p str = u'相关系数：%.3f' % p plt.scatter(xr, yr, s=40, alpha=0.9, linewidths=0.5, c=clrs[i], marker='o', label=str) plt.legend(loc='upper left', shadow=True) plt.xlabel(u'X') plt.ylabel(u'Y') plt.title(u'Pearson相关系数与数据分布：%s' % tip, fontsize=18) plt.grid(b=True) plt.show()if __name__ == '__main__': # warnings.filterwarnings(action='ignore', category=RuntimeWarning) np.random.seed(0) # intro() N = 1000 # tip = u'一次函数关系' # x = np.random.rand(N) # y = np.zeros(N) + np.random.randn(N)*0.001 # tip = u'二次函数关系' # x = np.random.rand(N) # y = x ** 2 #+ np.random.randn(N)*0.002 # tip = u'正切关系' # x = np.random.rand(N) * 1.4 # y = np.tan(x) # tip = u'二次函数关系' # x = np.linspace(-1, 1, 101) # y = x ** 2 tip = u'椭圆' x, y = np.random.rand(2, N) * 60 - 30 y /= 5 idx = (x**2 / 900 + y**2 / 36 &lt; 1) x = x[idx] y = y[idx] pearson(x, y, tip) candle123456789101112131415161718192021222324252627282930#!/usr/bin/python# -*- coding:utf-8 -*-import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom matplotlib.finance import candlestick_ohlcif __name__ == "__main__": mpl.rcParams['font.sans-serif'] = [u'SimHei'] mpl.rcParams['axes.unicode_minus'] = False np.set_printoptions(suppress=True, linewidth=100, edgeitems=5) data = np.loadtxt('7.SH600000.txt', dtype=np.float, delimiter='\t', skiprows=2, usecols=(1, 2, 3, 4)) data = data[:30] N = len(data) t = np.arange(1, N+1).reshape((-1, 1)) data = np.hstack((t, data)) fig, ax = plt.subplots(facecolor='w') fig.subplots_adjust(bottom=0.2) candlestick_ohlc(ax, data, width=0.6, colorup='r', colordown='g', alpha=0.9) plt.xlim((0, N+1)) plt.grid(b=True) plt.title(u'股票K线图', fontsize=18) plt.tight_layout(2) plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化]]></title>
    <url>%2F2017%2F11%2F08%2F%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[凸集基本概念凸集保凸运算凸集和凸函数凸函数图像的上方区域，一定是凸集；一个函数图像的上方区域为凸集，则该函数是凸函数。 仿射集（Affine Set）定义：通过集合C中任意两个不同点的直线仍然在集合C内，则称集合C为仿射集。 仿射集的例子：直线、平面、超平面 凸集集合C内任意两点间的线段均在集合C内，则称集合C为凸集。 两个点和k个点的表达版本内涵是一样的。 仿射集和凸集的关系因为仿射集的条件比凸集的条件强，所以，仿射集必然是凸集。 凸包集合C的所有点的凸组合形成的集合，叫做集合c的凸包。集合C的凸包是能够包含C的最小的凸集。 超平面和半空间 保持凸性的运算 集合交运算 仿射变换 函数f=ax+b的形式，称函数是仿射的，即线性函数加常数的形式。 透视变换 透视函数对向量进行伸缩（规范化），使得最后一维的分量为1并舍弃之。 凸集的透视变换仍然是凸集。 投射变换（线性分式变换） 分割超平面分割超平面的定义 分割超平面的构造两个集合的距离，定义为两个集合间元素的最短距离。 做集合A和集合B最短线段的垂直平分线。 支撑超平面 凸函数基本概念一般化定义 一阶可微 二阶可微 凸函数举例 Jensen不等式：若f是凸函数 凸函数保凸运算保持函数凸性的算子 共轭函数共轭函数的定义 共轭函数的理解 共轭函数的求解举例 凸优化一般提法对偶函数 强对偶KKT条件强对偶条件 Karush-Kuhn-Tucker(KKT)条件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode 调试适配器进程已意外终止]]></title>
    <url>%2F2017%2F10%2F24%2Fvscode%20%E8%B0%83%E8%AF%95%E9%80%82%E9%85%8D%E5%99%A8%E8%BF%9B%E7%A8%8B%E5%B7%B2%E6%84%8F%E5%A4%96%E7%BB%88%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[隔了N天以后，重新打开，一进来就重新下载C依赖，然后提示reload。然后莫名的就可以用了。我晕。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李德荃关于偏度与峰度的讲解]]></title>
    <url>%2F2017%2F10%2F24%2F%E6%9D%8E%E5%BE%B7%E8%8D%83%E5%85%B3%E4%BA%8E%E5%81%8F%E5%BA%A6%E4%B8%8E%E5%B3%B0%E5%BA%A6%E7%9A%84%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[偏度这一指标，又称偏斜系数、偏态系数，是用来帮助判断数据序列的分布规律性的指标。 在数据序列呈对称分布（正态分布）的状态下，其均值、中位数和众数重合。且在这三个数的两侧，其它所有的数据完全以对称的方式左右分布。 如果数据序列的分布不对称，则均值、中位数和众数必定分处不同的位置。这时，若以均值为参照点，则要么位于均值左侧的数据较多，称之为右偏；要么位于均值右侧的数据较多，称之为左偏；除此无它。考虑到所有数据与均值之间的离差之和应为零这一约束，则当均值左侧数据较多的时候，均值的右侧必定存在数值较大的“离群”数据；同理，当均值右侧数据较多的时候，均值的左侧必定存在数值较小的“离群”数据。 一般将偏度定义为三阶中心矩与标准差的三次幂之比。 在上述定义下，偏度系数的取值无非三种情景： 1.当数据序列呈正态分布的时候，由于均值两侧的数据完全对称分布，其三阶中心矩必定为零，于是满足正态分布的数据序列的偏度系数必定等于零。 2.当数据序列非对称分布的时候，如果均值的左侧数据较多，则其右侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取正值。因此，当数据的分布呈右偏的时候，其偏度系数将大于零。 3.当数据序列非对称分布的时候，如果均值的右侧数据较多，则其左侧的“离群”数据对三阶中心矩的计算结果影响至巨，乃至于三阶中心矩取负值。因此，当数据的分布呈左偏的时候，偏度系数将小于零。在右偏的分布中，由于大部分数据都在均值的左侧，且均值的右侧存在“离群”数据，这就使得分布曲线的右侧出现一个长长的拖尾；而在左偏的分布中，由于大部分数据都在均值的右侧，且均值的左侧存在“离群”数据，从而造成分布曲线的左侧出现一个长长的拖尾。 可见，在偏度系数的绝对值较大的时候，最有可能的含义是“离群”数据离群的程度很高（很大或很小），亦即分布曲线某侧的拖尾很长。 但“拖尾很长”与“分布曲线很偏斜”不完全等价。例如，也不能排除在数据较少的那一侧，只是多数数据的离差相对于另一侧较大，但不存在明显“离群”数据的情景。所以，为准确判断分布函数的偏斜程度，最好的办法是直接观察分布曲线的几何图形。 与偏度（系数）一样，峰度（系数）也是一个用于评价数据系列分布特征的指标。根据这两个指标，我们可以判断数据系列的分布是否满足正态性，进而评价平均数指标的使用价值。一般地，对于一个偏态分布、肥尾分布特征很明显的数据序列来说，平均数这个指标极易令人误解数据序列分布的集中位置及其集中程度，故此使用起来要极其谨慎。 峰度（系数）等于数据序列的四阶中心矩与标准差的四次幂之比。设若先将数据标准化，则峰度（系数）相当于标准化数据序列的四阶中心矩。 显然，一个数据距离均值越远，其对四阶中心矩计算结果的影响越大。是故，峰度（系数）是一个用于衡量离群数据离群度的指标。峰度（系数）越大，说明该数据系列中的极端值越多。这在数据序列的分布曲线图中来看，体现为存在明显的“肥尾”。当然，峰度（系数）较大也可能说明离群数据取值的极端性很严重，或者各数据距离均值的距离普遍较远。可见，峰度（系数）的大小到底能说明什么问题，最好还是看图确定。 根据Jensen不等式，可以确定出峰度（系数）的取值范围：它的下限不会低于1，上限不会高于数据的个数。 有一些典型分布的峰度（系数）值得特别关注。例如，正态分布的峰度（系数）为常数3，均匀分布的峰度（系数）为常数1.6。在统计实践中，我们经常把这两个典型的分布曲线作为评价样本数据序列分布性态的参照。 在金融学中，峰度这个指标具有一定的意义。一项金融资产，设若其预期收益率的峰度较高，则说明该项资产的预期收益率有相对较高的概率取极端值。换句话说，该项资产未来行市发生剧烈波动的概率相对较高。 这个讲解从实用角度解释了偏度与峰度，即这两个指标侧重于对图形的描述，在计算出具体的偏度与峰度后，更主要是要参考图形来分类分析，而不是单纯依照数值简单判断。 其二，根据这两个指标的分类，使用者可以按照自己的需求在原公式的基础上编程时再细化，即把图形数据化（因为如果数据量很大的话每个都要附上图形，计算机能受得了，但分析者还不得累死），这样得出的结果更有助于分析归纳，否则单单依靠原公式，分析归纳的难度有些大。 第三，这两个指标都是基于均值、标准差而来的，所以分析时可以根据均值与标准差来判别长尾属于哪种类型，从而确定其影响。 via http://bbs.pinggu.org/thread-3417092-1-1.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>偏度</tag>
        <tag>峰度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次python数据处理性能调优]]></title>
    <url>%2F2017%2F10%2F21%2F%E8%AE%B0%E4%B8%80%E6%AC%A1python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Python垃圾回收机制根据官方的描叙，Python中，有2中方式将会触发垃圾回收：1、用户显示调用gc.collect()2、每次Python为新对象分配内存时，检查threshold阀值，当对象数量超过threshold设置的阀值就开始进行垃圾回收。 因为数据量太大，处理过程中留下太多暂时不能清除的变量，而python的垃圾回收却在一遍一遍地扫这些不断在增长的列表，导致程序受到的影响越来越大。赶紧证实一下，import gc，然后在数据载入模块前gc.disable（）,结束后再gc.enable()。结果原来要跑将近两个小时的程序，这下不用5分钟就跑完了。cool~！用gc.get_count()也证明了之前的猜想，在第一次运行之后临时变量数目就从几百上升到百万，并一直在涨。 如果你的python程序在处理大数据量的问题，并且出现某个子程序在做同样量的工作，却越跑越慢的情况。 程序代码： 12345678910111213141516171819202122def series_trans(dataset): # 一行转多行 dataset_trans = pd.DataFrame(&#123;'user_id':dataset['user_id'], 'shop_id':dataset['shop_id'], 'time_stamp':dataset['time_stamp'], 'longitude':dataset['longitude'], 'latitude':dataset['latitude'], 'wifi_infos':dataset['wifi_splits']&#125;) return dataset_transprint time.asctime( time.localtime(time.time()) )dataset_trans = series_trans(dataset.loc[0,:])for i in xrange(1,dataset.shape[0]): if i % 10000 == 0: print i print time.asctime( time.localtime(time.time()) ) dataset_trans = dataset_trans.append(series_trans(dataset.loc[i,:])) 程序的性能表现如下： 1234567891011121314151617181920212223242526272829303132333435363738Sat Oct 21 11:01:30 201710000Sat Oct 21 11:02:07 201720000Sat Oct 21 11:03:41 201730000Sat Oct 21 11:06:12 201740000Sat Oct 21 11:09:37 201750000Sat Oct 21 11:13:55 201760000Sat Oct 21 11:19:10 201770000Sat Oct 21 11:25:26 201780000Sat Oct 21 11:33:03 201790000Sat Oct 21 11:41:21 2017100000Sat Oct 21 11:50:33 2017110000Sat Oct 21 12:00:41 2017120000Sat Oct 21 12:11:45 2017130000Sat Oct 21 12:23:43 2017140000Sat Oct 21 12:36:37 2017150000Sat Oct 21 12:50:25 2017160000Sat Oct 21 13:05:10 2017170000Sat Oct 21 13:20:46 2017180000Sat Oct 21 13:37:18 2017 采用：dataset_trans.loc[10:19,] = dataset_trans会提示错误：info_idx = indexer[info_axis]IndexError: tuple index out of range DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.所以一般说来dataframe就是a set of columns, each column is an array of values. In pandas, the array is one way or another a (maybe variant of) numpy ndarray. 而ndarray本身不存在一种in place append的操作。。。因为它实际上是一段连续内存。。。任何需要改变ndarray长度的操作都涉及分配一段长度合适的新的内存，然后copy。。。这是这类操作慢的原因。。。如果pandas dataframe没有用其他设计减少copy的话，我相信Bren说的”That’s probably as efficient as any”是很对的。。。所以in general, 正如Bren说的。。。Pandas/numpy structures are fundamentally not suited for efficiently growing.Matti 和 arynaq说的是两种常见的对付这个问题的方法。。。我想Matti实际的意思是把要加的rows收集成起来然后concatenate, 这样只copy一次。arynaq的方法就是预先分配内存比较好理解。。。如果你真的需要incrementally build a dataframe的话，估计你需要实际测试一下两种方法。。。我的建议是，如有可能，尽力避免incrementally build a dataframe, 比如用其他data structure 收集齐所有data然后转变成dataframe做分析。。。 via Python垃圾回收(gc)拖累了程序执行性能？ python pandas 怎样高效地添加一行数据？]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio Code工具上手体验]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20Code%E5%B7%A5%E5%85%B7%E4%B8%8A%E6%89%8B%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[下载安装Visual Studio Code,地址：https://code.visualstudio.com/Download 在VSCode内安装c++插件：打开vscode，Ctrl+P之后输入ext install c++ 弹出： 12扩展：商店 错误 不用管它，重新再来一次，Ctrl+P之后输入ext install c++ 安装第一个官方的。 重启vscode。然后发现重新启动，vscode开始自动下载插件依赖。依赖自动下载安装完成。 抬头，提示reload。 查看本机gcc环境： 123456789E:\Code\solar-correlation-map&gt;gcc -vUsing built-in specs.COLLECT_GCC=gccCOLLECT_LTO_WRAPPER=D:/TDM-GCC-64/bin/../libexec/gcc/x86_64-w64-mingw32/5.1.0/lto-wrapper.exeTarget: x86_64-w64-mingw32Configured with: ../../../src/gcc-5.1.0/configure --build=x86_64-w64-mingw32 --enable-targets=all --enable-languages=ada,c,c++,fortran,lto,objc,obj-c++ --enable-libgomp --enable-lto --enable-graphite --enable-cxx-flags=-DWINPTHREAD_STATIC --disable-build-with-cxx --disable-build-poststage1-with-cxx --enable-libstdcxx-debug --enable-threads=posix --enable-version-specific-runtime-libs --enable-fully-dynamic-string --enable-libstdcxx-threads --enable-libstdcxx-time --with-gnu-ld --disable-werror --disable-nls --disable-win32-registry --prefix=/mingw64tdm --with-local-prefix=/mingw64tdm --with-pkgversion=tdm64-1 --with-bugurl=http://tdm-gcc.tdragon.net/bugsThread model: posixgcc version 5.1.0 (tdm64-1) 新建文件夹，在文件夹内新建文件hello.cpp：123456789\#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 0; cout&lt;&lt;a; return 0;&#125; 按下F5，启动调试，提示需要配置。将launch.json的文件内容替换成如下： 123456789101112131415161718192021&#123; "version": "0.2.0", "configurations": [ &#123; "name": "C++ Launch (GDB)", // 配置名称，将会在启动配置的下拉菜单中显示 "type": "cppdbg", // 配置类型，这里只能为cppdbg "request": "launch", // 请求配置类型，可以为launch（启动）或attach（附加） "launchOptionType": "Local", // 调试器启动类型，这里只能为Local "targetArchitecture": "x86", // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 "program": "$&#123;file&#125;.exe", // 将要进行调试的程序的路径 // "miDebuggerPath":"c:\\MinGW\\bin\\gdb.exe", // miDebugger的路径，注意这里要与MinGw的路径对应 "miDebuggerPath":"D:\\TDM-GCC-64\\bin\\gdb.exe", "args": ["blackkitty", "1221", "# #"], // 程序调试时传递给程序的命令行参数，一般设为空即可 "stopAtEntry": false, // 设为true时程序将暂停在程序入口处，一般设置为false "cwd": "$&#123;workspaceRoot&#125;", // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 "externalConsole": true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 "preLaunchTask": "g++" // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; 注意miDebuggerPath要与MinGw的路径对应 替换后保存，然后切换至hello.cpp，按F5进行调试，此时会弹出一个信息框要求你配置任务运行程序，点击它~ 选择任务运行程序，点击Others，跳出tasks.json的配置文件。替换成如下代码: 123456789101112131415161718&#123; "version": "0.1.0", "command": "g++", "args": ["-g","$&#123;file&#125;","-o","$&#123;file&#125;.exe"], // 编译命令参数 "problemMatcher": &#123; "owner": "cpp", "fileLocation": ["relative", "$&#123;workspaceRoot&#125;"], "pattern": &#123; "regexp": "^(.*):(\\d+):(\\d+):\\s+(warning|error):\\s+(.*)$", "file": 1, "line": 2, "column": 3, "severity": 4, "message": 5 &#125; &#125;&#125; 保存一下，然后切换至hello.cpp，再次按F5启动调试: 控制台输出： 12345678910111213141516171819202122=thread-group-added,id="i1"GNU gdb (GDB) 7.9.1Copyright (C) 2015 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-w64-mingw32".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".=cmd-param-changed,param="pagination",value="off"[New Thread 16288.0x3d10]Breakpoint 1, main () at e:\Code\C_playground\hello.cpp:55 int a = 0;[Inferior 1 (process 16288) exited normally]The program 'e:\Code\C_playground\hello.cpp.exe' has exited with code 0 (0x00000000). 文档目录如下：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pydotplus绘决策树]]></title>
    <url>%2F2017%2F10%2F17%2F%E4%BD%BF%E7%94%A8pydotplus%E7%BB%98%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[首先安装pydotplus pip install pydotplus 然后打印训练好的clf: 123456789import pydotplusdot_data = tree.export_graphviz(clf, out_file=None, feature_names=X_dummy.columns, class_names='target', filled=True, rounded=True, special_characters=True)graph = pydotplus.graph_from_dot_data(dot_data)graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") 发现报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-27-2c78351c7fea&gt;", line 7, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; prog=self.prog: self.write(path, format=f, prog=prog) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write fobj.write(self.create(prog, format)) File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create 'GraphViz\'s executables not found')InvocationException: GraphViz's executables not found 提示没有找到&#39;GraphViz\&#39;s executables&#39;: 下载安装graphviz-2.38.msiurl：http://www.graphviz.org/pub/graphviz/stable/windows/graphviz-2.38.msi 源地址下载比较慢或者打不开，可以从这里下载：http://download.csdn.net/download/boredbird32/10025853 添加安装路径到系统Path环境变量：D:\Program Files (x86)\Graphviz2.38\;D:\Program Files (x86)\Graphviz2.38\bin\ 重新启动Python环境发现还是报同样的错误。 点开文件&quot;D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py&quot; 找到 Method 3 (Windows only)那一段，修改里面的path为当时安装graphviz的地址。 12345678910111213141516# Method 3 (Windows only)if os.sys.platform == 'win32': # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" 重启，又报错： 12345678910111213Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-7-3735ccaa5368&gt;", line 5, in &lt;module&gt; graph.write_pdf("E:\\ScoreCard\\fpd30.pdf") File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1810, in &lt;lambda&gt; lambda path, File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1918, in write File "D:\ProgramData\Anaconda2\lib\site-packages\pydotplus\graphviz.py", line 1960, in create if self.progs is None:InvocationException: GraphViz's executables not found 继续点进去，修改源文件：加上调试语句： 123456789101112131415161718192021222324252627282930313233343536373839# Method 3 (Windows only)if os.sys.platform == 'win32': print('come inside method 3') # Try and work out the equivalent of "C:\Program Files" on this # machine (might be on drive D:, or in a different language) if 'PROGRAMFILES' in os.environ: # Note, we could also use the win32api to get this # information, but win32api may not be installed. path = os.path.join( os.environ['PROGRAMFILES'], 'ATT', 'GraphViz', 'bin' ) else: # Just in case, try the default... # path = r"C:\Program Files\att\Graphviz\bin" path = r"D:\Program Files (x86)\Graphviz2.38\bin" print(path) progs = __find_executables(path) print(progs) if progs is not None: print("Used default install location") return progsfor path in ( '/usr/bin', '/usr/local/bin', '/opt/local/bin', '/opt/bin', '/sw/bin', '/usr/share', '/Applications/Graphviz.app/Contents/MacOS/'): progs = __find_executables(path) if progs is not None: # print("Used path") return progs# Failed to find GraphVizreturn None 再次运行，发现： 12345InvocationException: GraphViz's executables not foundcome inside method 3C:\Program Files\ATT\GraphViz\binNone 问题就出在这个path上。那干脆我们就直接在判断条件外面指定path： 12345path = r"D:\Program Files (x86)\Graphviz2.38\bin"# print(path)progs = __find_executables(path)# print(progs) 再次运行，问题解决了： 1234567come inside method 3D:\Program Files (x86)\Graphviz2.38\bin&#123;'twopi': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\twopi.exe', 'fdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\fdp.exe', 'circo': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\circo.exe', 'neato': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\neato.exe', 'dot': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe', 'sfdp': 'D:\\Program Files (x86)\\Graphviz2.38\\bin\\sfdp.exe'&#125;Used default install locationOut[3]: True 最后注释掉测试语句。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio 无法编译新打开的C文件]]></title>
    <url>%2F2017%2F10%2F17%2FVisual%20Studio%20%E6%97%A0%E6%B3%95%E7%BC%96%E8%AF%91%E6%96%B0%E6%89%93%E5%BC%80%E7%9A%84C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[用 Visual Studio打开有c文件的文件夹，发现里面的c文件都无法编译，导航栏里面连生成按钮都没有，调试按钮也是灰色的。 需在项目中单个添加c文件。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>Visual Studio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次诡异的Python中文乱码]]></title>
    <url>%2F2017%2F10%2F17%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E8%AF%A1%E5%BC%82%E7%9A%84Python%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[有一天突然出现了这样的中文乱码： 我的第一反应就是检查：# -*- coding:utf-8 -*- 然后检查： 12345import syssys.getdefaultencoding()Out[5]: 'utf-8' 这，，是中文编码的啊，再检查： 12345678from chardet import detectcfg.dataset_train['white_list_type'][0]Out[8]: '\xb7\xc7\xb0\xd7\xc3\xfb\xb5\xa5'detect(cfg.dataset_train['white_list_type'][0])Out[9]: &#123;'confidence': 0.0, 'encoding': None&#125; 终于发现问题了，&#39;encoding&#39;: None 然后，那就在read_csv的时候指定编码： 123456789101112131415161718192021222324dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8')Traceback (most recent call last): File "D:\ProgramData\Anaconda2\lib\site-packages\IPython\core\interactiveshell.py", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-13-a9c949155761&gt;", line 1, in &lt;module&gt; dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv',encoding='utf-8') File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 646, in parser_f return _read(filepath_or_buffer, kwds) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 401, in _read data = parser.read() File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 939, in read ret = self._engine.read(nrows) File "D:\ProgramData\Anaconda2\lib\site-packages\pandas\io\parsers.py", line 1508, in read data = self._reader.read(nrows) File "pandas\parser.pyx", line 848, in pandas.parser.TextReader.read (pandas\parser.c:10415) File "pandas\parser.pyx", line 870, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:10691) File "pandas\parser.pyx", line 947, in pandas.parser.TextReader._read_rows (pandas\parser.c:11728) File "pandas\parser.pyx", line 1049, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:13162) File "pandas\parser.pyx", line 1108, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:14116) File "pandas\parser.pyx", line 1206, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:16172) File "pandas\parser.pyx", line 1222, in pandas.parser.TextReader._string_convert (pandas\parser.c:16400) File "pandas\parser.pyx", line 1458, in pandas.parser._string_box_utf8 (pandas\parser.c:22072)UnicodeDecodeError: 'utf8' codec can't decode byte 0xb7 in position 0: invalid start byte 什么情况，一阵百度，无解。 然后用notepad++打开看下我的csv文件编码： 居然没有一种格式是选中的。。。怎么会出现这种情况，我从数据库导出的时候明明已经指定了编码格式为utf-8，而且后面又打开指定了编码格式为utf-8，怎么现在会是未指定的状态。。 好吧，重新指定csv文件编码为utf-8，再重新试下： 12345678910111213dataset = pd.read_csv('E:\\ScoreCard\\fpd30_analy_tmp02.csv')detect(dataset['white_list_type'][0])Out[17]: &#123;'confidence': 0.938125, 'encoding': 'utf-8'&#125;dataset['white_list_type'][0]Out[18]: '\xe9\x9d\x9e\xe7\x99\xbd\xe5\x90\x8d\xe5\x8d\x95'dataset['white_list_type']Out[19]: 0 非白名单1 普通白名单2 普通白名单 一切又正常了。诡异。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Django REST Framework创建API服务]]></title>
    <url>%2F2017%2F10%2F15%2F%E5%9F%BA%E4%BA%8EDjango%20REST%20Framework%E5%88%9B%E5%BB%BAAPI%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Django REST Framework 安装 123pip install djangorestframeworkpip install markdown # Markdown support for the browsable API.pip install django-filter # Filtering support 看下我的环境：1234567(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;pip freezeDjango==1.11.6django-bootstrap3==9.0.0django-filter==1.0.4djangorestframework==3.7.0Markdown==2.6.9pytz==2017.2 创建一个新的项目django_rest，在项目下创建名为api的应用。 cmd.exe 12345678910E:\Code\virtualenvs\myenvs&gt;.\Scripts\activate.bat(myenvs) E:\Code\virtualenvs\myenvs&gt;python .\Scripts\django-admin.py startproject django_rest(myenvs) E:\Code\virtualenvs\myenvs&gt;cd django_rest\(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py startapp api(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt; 打开settings.py文件，添加应用。 settings.py 12345678910111213141516171819202122# Application definitionINSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'api',]......# 在文件末尾添加REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES':( 'rest_framework.permissions.IsAuthenticated', )&#125; “rest_framework”为Django REST Framework应用，”api”为我们自己创建的应用。默认的权限策略可以设置在全局范围内，通过DEFAULT_PERMISSION_CLASSES设置。 通过migrate命令执行数据库迁移。 cmd.exe 1234(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py makemigrations(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python .\manage.py migrate 通过createsuperuser命令创建超级管理员账户admin/admin123456。 cmd.exe 123456(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py createsuperuserUsername (leave blank to use 'maomaochong'): adminEmail address: admin@email.comPassword:Password (again):Superuser created successfully. 创建数据序列化，在api应用下创建serializers.py文件。 serializers.py 1234567891011121314from django.contrib.auth.models import User,Groupfrom rest_framework import serializersclass UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups')class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') Serializers用于定义API的表现形式，如返回哪些字段、返回怎样的格式等。这里序列化Django自带的User和Group。 编写视图文件，打开api应用下的views.py文件，编写如下代码。 views.py 123456789101112131415161718192021222324# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.contrib.auth.models import User,Groupfrom rest_framework import viewsetsfrom api.serializers import UserSerializer,GroupSerializer# ViewSets 定义视图的展现形式class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializerclass GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all().order_by('-date_joined') serializer_class = GroupSerializer 在Django REST framework 中，ViewSets用于定义视图的展现形式，例如返回哪些内容，需要做哪些权限处理。 在URL中会定义相应的规则到ViewSet。ViewSet则通过serializer_class 找到对应的Serializers。这里讲User和Group的所有对象赋予queryset，并返回这些值。在UserSerializer和GroupSerializer中定义要返回的字段。 打开…/django_rest/urls.py文件，添加api的路由配置。 urls.py 12345678910111213141516171819from django.conf.urls import url, includefrom django.contrib import adminfrom rest_framework import routersfrom api import views# Routers provide an easy way of automatically determining the URL conf.router = routers.DefaultRouter()router.register(r'users',views.UserViewSet)router.register(r'groups',views.GroupViewSet)# Wire up out API using automatic URL routing.# Additionally,we include login URLs for the browsable API.urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework'))] 因为使用的是ViewSets,所以可以使用routers类自动生成URL conf。 通过runserver命令启动服务。 cmd.exe 12345678(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py runserverPerforming system checks...System check identified no issues (0 silenced).October 15, 2017 - 12:32:14Django version 1.11.6, using settings 'django_rest.settings'Starting development server at http://127.0.0.1:8000/Quit the server with CTRL-BREAK. 用超级管理员admin/admin123456登录。 接下来在django_rest项目的基础上，创建模型，打开api应用下的models.py文件。 models.py 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.db import models# Create your models here.# 发布会class Event(models.Model): name = models.CharField(max_length=100) # 发布会标题 limit = models.IntegerField() # 限制人数 status = models.BooleanField() # 状态 address = models.CharField(max_length=200) # 地址 start_time = models.DateTimeField('events time') # 发布会时间 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） def __str__(self): return self.name# 嘉宾class Guest(models.Model): event = models.ForeignKey(Event) # 关联发布会id realname = models.CharField(max_length=64) # 姓名 phone = models.CharField(max_length=16) # 手机号 email = models.EmailField() # 邮箱 sign = models.BooleanField() # 签到状态 create_time = models.DateTimeField(auto_now=True) # 创建时间（自动获取当前时间） class Meta: unique_together = ('phone', 'event') def __str__(self): return self.realname 然后执行数据库迁移。 cmd.exe 123456789(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py makemigrations apiMigrations for 'api': api\migrations\0001_initial.py - Create model Event - Create model Guest - Alter unique_together for guest (1 constraint(s))(myenvs) E:\Code\virtualenvs\myenvs\django_rest&gt;python manage.py migrate 添加发布会数据序列化，打开api应用下的serializers.py文件（上面创建的）。 serializers.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 from django.contrib.auth.models import User,Group from rest_framework import serializers from api.models import Event,Guest class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url','username','email','groups') class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = ('url','name') class EventSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Event fields = ('url','name','address','start_time','limit','status') class GuestSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Guest fields = ('url','realname','phone','email','sign','event')``` 打开api应用下的views.py文件，定义发布会和嘉宾视图类。&gt; views.py``` python # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.shortcuts import render # Create your views here. from django.contrib.auth.models import User,Group from rest_framework import viewsets from api.serializers import UserSerializer,GroupSerializer, EventSerializer, GuestSerializer from api.models import Event, Guest # ViewSets 定义视图的展现形式 class UserViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializer class GroupViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Group.objects.all() serializer_class = GroupSerializer class EventViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Event.objects.all() serializer_class = EventSerializer class GuestViewSet(viewsets.ModelViewSet): """ API endpoint that allows users to be viewed or edited. """ queryset = Guest.objects.all() serializer_class = GuestSerializer``` 打开.../django_rest/urls.py文件，添加URL配置。&gt; urls.py``` python from django.conf.urls import url, include from django.contrib import admin from rest_framework import routers from api import views # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r'users',views.UserViewSet) router.register(r'groups',views.GroupViewSet) router.register(r'events',views.EventViewSet) router.register(r'guests',views.GuestViewSet) # Wire up out API using automatic URL routing. # Additionally,we include login URLs for the browsable API. urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^',include(router.urls)), url(r'^api-auth/',include('rest_framework.urls', namespace='rest_framework')) ]]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python类属性和方法的封装]]></title>
    <url>%2F2017%2F10%2F14%2Fpython%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95%E7%9A%84%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[封装是一种限制直接访问目标属性和方法的机制，但同时它又有利于对数据（对象的方法）进行操作。 封装是一种将抽象性函数接口的实现细节部分包装、隐藏起来的方法。同时，它也是一种防止外界调用端，去访问对象内部实现细节的手段，这个手段是由编程语言本身来提供的。 对象所有的内部表征对于外部来说都是隐藏的，只有对象能直接与内部数据交互。首先，我们需要理解公开（public）和私有（non-public）实例变量和方法。 公开实例变量对于 Python 的类，我们可以使用 constructor 方法初始化公开实例变量： 123class Person: def __init__(self,first_name): self.first_name = first_name 下面我们应用 first_name 的值作为公开实例变量的变元。 12a = Person('zhang')print a.first_name # -&gt; zhang 在类别内： 12class Person: first_name = 'zhang' 现在我们不需要再对 first_name 赋值，所有赋值到 a 的目标都将有类的属性： 12a = Person()print a.first_name # -&gt; zhang 现在我们已经学会如何使用公开实例变量和类属性。除此之外，我们还能管理公开部分的变量值，即对象可以管理其变量的值：Get 和 Set 变量值。保留 Person 类，我们希望能给 first_name 变量赋另外一个值： 123a = Person('zhang')a.first_name = 'Z'print a.first_name # -&gt; Z 如上我们将另外一个值赋予了 first_name 实例变量，因为它又是一个公开变量，所以会更新变量值。 私有实例变量和公开实例变量一样，我们可以使用 constructor 方法或在类的内部声明而定义一个私有实例变量。语法上的不同在于私有实例变量在变量名前面加一个下划线： 1234class Person: def __init__(self,first_name,email): self.first_name = first_name self._email = email 上述定义的 email 变量就是私有变量。12a = Person('zhang','zhang@mail.com')print a._email # -&gt; zhang@mail.com 我们可以访问并且更新它，私有变量仅是一个约定，即他们需要被视为 API 非公开的部分。所以我们可以使用方法在类的定义中完成操作，例如使用两种方法展示私有实例的值与更新实例的值： 12345678910class Person: def __init__(self,first_name,email): self.first_name = first_name self.__email = email def update_email(self,new_email): self.__email = new_email def email(self): return self.__email 现在我们可以使用方法更新或访问私有变量。 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma.__email = 'new_email@mail.com'print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; zhang@mail.coma.update_email('update_email@mail.com')print a.__email # -&gt; new_email@mail.comprint a.email() # -&gt; update_email@mail.com 从上面可见，以双下划线打头的名称会导致出现名称重整的行为。具体来说就是上面的这个类中的私有属性会被分别重命名为_C__private 和 _C__private_name。但是为啥两个值不一样？？ 豁然开朗！ 12345678910a = Person('zhang','zhang@mail.com')print a.email() # -&gt; zhang@mail.coma._email = 'new_email@mail.com'print a._email # -&gt; new_email@mail.comprint a.email() # -&gt; new_email@mail.coma.update_email('update_email@mail.com')print a._email # -&gt; update_email@mail.comprint a.email() # -&gt; update_email@mail.com]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django自带的server局域网访问]]></title>
    <url>%2F2017%2F10%2F11%2Fdjango%E8%87%AA%E5%B8%A6%E7%9A%84server%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[问题描述用(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver启动服务，对应的访问地址为： 12Starting development server at http://127.0.0.1:8000/ 然后，我把127.0.0.1改为本机的地址发现居然不能访问。 解决方法使用python .\manage.py runserver 0.0.0.0:8000启动服务： 又出现新的错误： 12345678910111213141516171819202122232425DisallowedHost at /post/act/Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Request Method: GETRequest URL: http://10.83.2.132:8000/post/act/?eid=1&amp;status=%27F%27Django Version: 1.11.6Exception Type: DisallowedHostException Value: Invalid HTTP_HOST header: '10.83.2.132:8000'. You may need to add u'10.83.2.132' to ALLOWED_HOSTS.Exception Location: E:\Code\virtualenvs\myenvs\lib\site-packages\django\http\request.py in get_host, line 113Python Executable: E:\Code\virtualenvs\myenvs\Scripts\python.exePython Version: 2.7.13Python Path: ['E:\\Code\\virtualenvs\\myenvs\\src', 'E:\\Code\\virtualenvs\\myenvs\\Scripts\\python27.zip', 'E:\\Code\\virtualenvs\\myenvs\\DLLs', 'E:\\Code\\virtualenvs\\myenvs\\lib', 'E:\\Code\\virtualenvs\\myenvs\\lib\\plat-win', 'E:\\Code\\virtualenvs\\myenvs\\lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs\\Scripts', 'D:\\ProgramData\\Anaconda2\\Lib', 'D:\\ProgramData\\Anaconda2\\DLLs', 'D:\\ProgramData\\Anaconda2\\Lib\\lib-tk', 'E:\\Code\\virtualenvs\\myenvs', 'E:\\Code\\virtualenvs\\myenvs\\lib\\site-packages']Server time: 星期三, 11 十月 2017 15:59:19 +0800 解决方法去django-admin.py startproject project-name创建的项目中去修改 setting.py 文件： 12 ALLOWED_HOSTS = [‘*’] ＃在这里请求的host添加了＊ 又又出现新的错误：报错SyntaxError: invalid syntax: 这尼玛，这是什么鬼，我的引号呢？？？终端上为什么没有引号，于是我有尝试了&quot;*&quot;双引号，依然不行。然后无穷的百度（动词），居然没一个类似我的问题。老天干嘛要为难我这个小白啊。 解决方法一不做，二不休： 123# ALLOWED_HOSTS = ['*']ALLOWED_HOSTS = list('*') 居然works！ 加逗号也是可以的： 12ALLOWED_HOSTS = ['*',] 效果 参考： django自带的server 让外网主机访问]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Forbidden (CSRF cookie not set.)]]></title>
    <url>%2F2017%2F10%2F11%2FForbidden%20(CSRF%20cookie%20not%20set.)%2F</url>
    <content type="text"><![CDATA[问题描述用postman提交post请求时，终端打印错误Forbidden (CSRF cookie not set.)： 1234567[11/Oct/2017 14:55:00] "GET /post/post/ HTTP/1.1" 200 47Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 14:55:16] "POST /post/post/ HTTP/1.1" 403 2829Forbidden (CSRF cookie not set.): /post/post/[11/Oct/2017 15:00:06] "POST /post/post/ HTTP/1.1" 403 2829Performing system checks... 情况如下图所示： 解决方法修改settings.py文件，注释掉 django.middleware.csrf.CsrfViewMiddleware&#39;, 效果post请求处理正常。 参考： (django1.10)访问url报错Forbidden (CSRF cookie not set.): xxx]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>CSRF</tag>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore规则不生效的解决办法]]></title>
    <url>%2F2017%2F10%2F11%2Fgitignore%E8%A7%84%E5%88%99%E4%B8%8D%E7%94%9F%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题描述发现添加.gitignore文件后，本地做的修改仍被push到GitHub。.gitignore规则并没有生效。 解决方法原因是如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交： git rm -r --cached . git add . git commit -m &#39;update .gitignore&#39; 如何避免 忽略规则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 ##最后需要强调的一点是，如果你不慎在创建.gitignore文件之前就push了项目，那么即使你在.gitignore文件中写入新的过滤规则，这些规则也不会起作用，Git仍然会对所有文件进行版本管理。简单来说，出现这种问题的原因就是Git已经开始管理这些文件了，所以你无法再通过过滤规则过滤它们。因此一定要养成在项目开始就创建.gitignore文件的习惯，否则一旦push，处理起来会非常麻烦。 参考： Git忽略规则.gitignore梳理 Git忽略文件.gitignore的使用 Git忽略规则和.gitignore规则不生效的解决办法 github官方给的Python项目.gitignore文件模板]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python环境设置默认编码为utf8的方法]]></title>
    <url>%2F2017%2F10%2F11%2FPython%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE%E9%BB%98%E8%AE%A4%E7%BC%96%E7%A0%81%E4%B8%BAutf8%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[从网上fork了虫师的代码，发现每个文件都没有加头注释指定文件编码，我这一个一个改得什么时候。想着虫师也是老司机了，不可能连IDE自动添加头注释都不知道，应该是哪边统一设置的。果然他是直接修改的Python环境配置。 设置之前： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 52853 52854Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'ascii' 设置之后： 1234567891011121314151617D:\ProgramData\Anaconda2\python.exe "D:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev\pydevconsole.py" 55595 55596Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]Type "copyright", "credits" or "license" for more information.IPython 5.1.0 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.PyDev console: using IPython 5.1.0import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler'])Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32import syssys.getdefaultencoding()Out[3]: 'utf-8' 如何设置： 可以在Python安装目录下的Lib/site-packages目录中，新建一个sitecustomize.py文件（也可以建在其它地方，然后手工导入，建在这里，每次启动Python的时候设置将自动生效），内容如下： 123import syssys.setdefaultencoding('utf-8') #set default encoding to utf-8 参考： Python设置默认编码为utf8的方法 python错误：AttributeError: ‘module’ object has no attribute ‘setdefaultencoding’问题的解决方法]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ImportError No module named functools]]></title>
    <url>%2F2017%2F10%2F11%2FImportError%20No%20module%20named%20functools%2F</url>
    <content type="text"><![CDATA[问题描述今天早上从git pull代码后，启动本地的virtualenv,发现报错： 12345678910111213(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserverTraceback (most recent call last): File "E:\Code\virtualenvs\myenvs\lib\site.py", line 703, in &lt;module&gt; main() File "E:\Code\virtualenvs\myenvs\lib\site.py", line 692, in main aliasmbcs() File "E:\Code\virtualenvs\myenvs\lib\site.py", line 515, in aliasmbcs import locale, codecs File "E:\Code\virtualenvs\myenvs\lib\locale.py", line 17, in &lt;module&gt; import functoolsImportError: No module named functools 解决方法检查ENV_FOLDER\Lib\orig-prefix.txt文件发现： 1d:\program files\anaconda2 这个路径应该是我宿舍电脑上的Python环境路径，现在改为我这台电脑的Python路径： 1D:\ProgramData\Anaconda2 再次启动，works!问题解决了 123456789(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserverPerforming system checks...System check identified no issues (0 silenced).October 11, 2017 - 10:05:10Django version 1.11.6, using settings 'bbs.settings'Starting development server at http://127.0.0.1:8000/Quit the server with CTRL-BREAK. 如何避免在E:\Code\virtualenvs\下添加.gitignore文件，把环境配置文件给过滤掉，两边环境不一样省得冲突。 1234*.pyc/myenvs/Lib/orig-prefix.txt 参考： changing virtualenv folder on windows]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API 设计指南-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FRESTful%20API%20Design%20Guide%2F</url>
    <content type="text"><![CDATA[这篇文章写的也很好，很上一篇有很大重复，甚至有些矛盾的地方。但是读下来，依然还是有启发的。 一、协议API与用户的通信协议，总是使用HTTPs协议。 #二、域名应该尽量将API部署在专用域名之下。12https://api.example.com 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。12https://example.org/api/ 三、版本（Versioning）应该将API的版本号放入URL。12https://api.example.com/v1/ 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观 四、路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。123456https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。1234&#123; error: "Invalid API key"&#125; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。1234567&#123;"link": &#123; "rel": "collection https://www.example.com/zoos", "href": "https://api.example.com/zoos", "title": "List of zoos", "type": "application/vnd.yourformat+json"&#125;&#125; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 123456&#123; "current_user_url": "https://api.github.com/user", "authorizations_url": "https://api.github.com/authorizations", // ...&#125; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 12345&#123; "message": "Requires authentication", "documentation_url": "https://developer.github.com/v3"&#125; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他 API的身份认证应该使用OAuth 2.0框架。 服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 via RESTful API 设计指南]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解RESTful架构-来自阮一峰]]></title>
    <url>%2F2017%2F10%2F10%2FUnderstanding%20RESTful%20Framework%2F</url>
    <content type="text"><![CDATA[最近在研究Python开发接口微服务，以便用Python开发出来的模型与服务能对接上，提供复杂模型上线的方案。看到了阮一峰的《理解RESTful架构》这篇文章，写的真好，虽然我是还没入门的外外行。 什么是RESTful架构一、起源REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。 文中写到： 123"本文研究计算机科学两大前沿----软件和网络----的交叉点。长期以来，软件研究主要关注软件设计的分类、设计方法的演化，很少客观地评估不同的设计选择对系统行为的影响。而相反地，网络研究主要关注系统之间通信行为的细节、如何改进特定通信机制的表现，常常忽视了一个事实，那就是改变应用程序的互动风格比改变互动协议，对整体表现有更大的影响。我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。"(This dissertation explores a junction on the frontiers of two research disciplines in computer science: software and networking. Software research has long been concerned with the categorization of software designs and the development of design methodologies, but has rarely been able to objectively evaluate the impact of various design choices on system behavior. Networking research, in contrast, is focused on the details of generic communication behavior between systems and improving the performance of particular communication techniques, often ignoring the fact that changing the interaction style of an application can have more impact on performance than the communication protocols used for that interaction. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. ) 二、名称Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。我对这个词组的翻译是”表现层状态转化”。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 三、资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 四、表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 五、状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 六、综述综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 七、误区RESTful架构有一些典型的设计误区。最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：12POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：12345 POST /transaction HTTP/1.1 Host: 127.0.0.1 from=1&amp;to=2&amp;amount=500.00 另一个设计误区，就是在URI中加入版本号： 123456 http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分参见Versioning REST Services：123456 Accept: vnd.example-com.foo+json; version=1.0 Accept: vnd.example-com.foo+json; version=1.1 Accept: vnd.example-com.foo+json; version=2.0 via 理解RESTful架构]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django处理HTTP请求流程]]></title>
    <url>%2F2017%2F10%2F10%2Fdjango%20flow%2F</url>
    <content type="text"><![CDATA[Based on @FULLSTACKCTO’s understanding of Django, this is how a user request is responded to. 1: User requests a page 2: Request reaches Request Middlewares, which could manipulate or answer the request 3: The URLConffinds the related View using urls.py 4: View Middlewares are called, which could manipulate or answer the request 5: The view function is invoked 6: The view could optionally access data through models 7: All model-to-DB interactions are done via a manager 8: Views could use a special context if needed 9: The context is passed to the Template for rendering a: Template uses Filters and Tags to render the output b: Output is returned to the view c: HTTPResponse is sent to the Response Middlerwares d: Any of the response middlewares can enrich the response or return a completely new response e: The response is sent to the user’s browser. via here Django Flowchart]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POST和GET区别]]></title>
    <url>%2F2017%2F10%2F10%2Fget%20or%20post%2F</url>
    <content type="text"><![CDATA[HTTP协议定义了很多与服务器交互的方法，最基本的有4种，分别是GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作，其中最常见请求方式是GET和POST，并且现在浏览器一般只支持GET和POST方法。 GET一般用于获取/查询资源信息，而POST一般用于更新资源信息，他们之间主要区别如下： 1）根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的，这里安全是指该操作用于获取信息而非修改信息，幂等是指对同一URL的多个请求应该返回同样的结果（这一点在实质实现时，可能并不满足）；POST表示可能修改变服务器上的资源的请求。 2）GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64编码；POST把提交的数据则放置在是HTTP包的包体中。 3）因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系，理论上URL长度是没有限制的，即HTTP协议没有规定URL的长度，但在实质中，特定的浏览器可能对这个长度做了限制；理论上POST也是没有大小限制的，HTTP协议规范也没有进行大小限制，但在服务端通常会对这个大小做一个限制，当然这个限制比GET宽松的多，即使用POST可以提交的数据量比GET大得多。 最后，网上有人说，POST的安全性要比GET的安全性高，实质上POST跟GET都是明文传输，这可以通过类似WireShark工具看到。总之，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求。 via 浅析HTTP中POST和GET区别并用Python模拟其响应和请求 浅谈HTTP中Get与Post的区别 GET和POST的区别]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[understanding-cnn]]></title>
    <url>%2F2017%2F10%2F10%2Funderstanding-cnn%2F</url>
    <content type="text"><![CDATA[(this page is currently in draft form) Visualizing what ConvNets learnSeveral approaches for understanding and visualizing Convolutional Networks have been developed in the literature, partly as a response the common criticism that the learned features in a Neural Network are not interpretable. In this section we briefly survey some of these approaches and related work. Visualizing the activations and first-layer weightsLayer Activations. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates. Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local. Conv/FC Filters. The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasn’t been trained for long enough, or possibly a very low regularization strength that may have led to overfitting. Typical-looking filters on the first CONV layer (left), and the 2nd CONV layer (right) of a trained AlexNet. Notice that the first-layer weights are very nice and smooth, indicating nicely converged network. The color/grayscale features are clustered because the AlexNet contains two separate streams of processing, and an apparent consequence of this architecture is that one stream develops high-frequency grayscale features and the other low-frequency color features. The 2nd CONV layer weights are not as interpretable, but it is apparent that they are still smooth, well-formed, and absent of noisy patterns. Retrieving images that maximally activate a neuronAnother visualization technique is to take a large dataset of images, feed them through the network and keep track of which images maximally activate some neuron. We can then visualize the images to get an understanding of what the neuron is looking for in its receptive field. One such visualization (among others) is shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.: Maximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the receptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a function of a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies, text, or specular highlights. One problem with this approach is that ReLU neurons do not necessarily have any semantic meaning by themselves. Rather, it is more appropriate to think of multiple ReLU neurons as the basis vectors of some space that represents in image patches. In other words, the visualization is showing the patches at the edge of the cloud of representations, along the (arbitrary) axes that correspond to the filter weights. This can also be seen by the fact that neurons in a ConvNet operate linearly over the input space, so any arbitrary rotation of that space is a no-op. This point was further argued in Intriguing properties of neural networks by Szegedy et al., where they perform a similar visualization along arbitrary directions in the representation space. Embedding the codes with t-SNEConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier. We can get a rough idea about the topology of this space by embedding images into two dimensions so that their low-dimensional representation has approximately equal distances than their high-dimensional representation. There are many embedding methods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of the best-known methods that consistently produces visually-pleasing results. To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid: t-SNE embedding of a set of images based on their CNN codes. Images that are nearby each other are also close in the CNN representation space, which implies that the CNN “sees” them as being very similar. Notice that the similarities are more often class-based and semantic rather than pixel and color-based. For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to t-SNE visualization of CNN codes. Occluding parts of the imageSuppose that a ConvNet classifies an image as a dog. How can we be certain that it’s actually picking up on the dog in the image as opposed to some contextual cues from the background or some other miscellaneous object? One way of investigating which part of the image some classification prediction is coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of the position of an occluder object. That is, we iterate over regions of the image, set a patch of the image to be all zero, and look at the probability of the class. We can visualize the probability as a 2-dimensional heat map. This approach has been used in Matthew Zeiler’s Visualizing and Understanding Convolutional Networks: Three input images (top). Notice that the occluder region is shown in grey. As we slide the occluder over the image we record the probability of the correct class and then visualize it as a heatmap (shown below each image). For instance, in the left-most image we see that the probability of Pomeranian plummets when the occluder covers the face of the dog, giving us some level of confidence that the dog’s face is primarily responsible for the high classification score. Conversely, zeroing out other parts of the image is seen to have relatively negligible impact. Visualizing the data gradient and friendsData Gradient. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps DeconvNet. Visualizing and Understanding Convolutional Networks Guided Backpropagation. Striving for Simplicity: The All Convolutional Net Reconstructing original images based on CNN CodesUnderstanding Deep Image Representations by Inverting Them How much spatial information is preserved?Do ConvNets Learn Correspondence? (tldr: yes) Plotting performance as a function of image attributesImageNet Large Scale Visual Recognition Challenge Fooling ConvNetsExplaining and Harnessing Adversarial Examples Comparing ConvNets to Human labelersWhat I learned from competing against a ConvNet on ImageNet]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ConvNet</tag>
        <tag>ReLU</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-1]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-1%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Visualizing the loss function Optimization Strategy #1: Random Search Strategy #2: Random Local Search Strategy #3: Following the gradient Computing the gradient Numerically with finite differences Analytically with calculus Gradient descent Summary IntroductionIn the previous section we introduced two key components in context of the image classification task: A (parameterized) score function mapping the raw image pixels to class scores (e.g. a linear function) A loss function that measured the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM). Concretely, recall that the linear function had the form \( f(x_i, W) = W x_i \) and the SVM we developed was formulated as: $$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)$$ We saw that a setting of the parameters \(W\) that produced predictions for examples \(x_i\) consistent with their ground truth labels \(y_i\) would also have a very low loss \(L\). We are now going to introduce the third and last key component: optimization. Optimization is the process of finding the set of parameters \(W\) that minimize the loss function. Foreshadowing: Once we understand how these three core components interact, we will revisit the first component (the parameterized function mapping) and extend it to functions much more complicated than a linear mapping: First entire Neural Networks, and then Convolutional Neural Networks. The loss functions and the optimization process will remain relatively unchanged. Visualizing the loss functionThe loss functions we’ll look at in this class are usually defined over very high-dimensional spaces (e.g. in CIFAR-10 a linear classifier weight matrix is of size [10 x 3073] for a total of 30,730 parameters), making them difficult to visualize. However, we can still gain some intuitions about one by slicing through the high-dimensional space along rays (1 dimension), or along planes (2 dimensions). For example, we can generate a random weight matrix \(W\) (which corresponds to a single point in the space), then march along a ray and record the loss function value along the way. That is, we can generate a random direction \(W_1\) and compute the loss along this direction by evaluating \(L(W + a W_1)\) for different values of \(a\). This process generates a simple plot with the value of \(a\) as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss \( L(W + a W_1 + b W_2) \) as we vary \(a, b\). In a plot, \(a, b\) could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color: Loss function landscape for the Multiclass SVM (without regularization) for one single example (left,middle) and for a hundred examples (right) in CIFAR-10. Left: one-dimensional loss by only varying a. Middle, Right: two-dimensional loss slice, Blue = low loss, Red = high loss. Notice the piecewise-linear structure of the loss function. The losses for multiple examples are combined with average, so the bowl shape on the right is the average of many piece-wise linear bowls (such as the one in the middle). We can explain the piecewise-linear structure of the loss function by examining the math. For a single example we have: $$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]$$ It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the \(\max(0,-)\) function) linear functions of \(W\). Moreover, each row of \(W\) (i.e. \(w_j\)) sometimes has a positive sign in front of it (when it corresponds to a wrong class for an example), and sometimes a negative sign (when it corresponds to the correct class for that example). To make this more explicit, consider a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regularization) becomes: $$\begin{align}L_0 = &amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\L_1 = &amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\L_2 = &amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\L = &amp; (L_0 + L_1 + L_2)/3\end{align}$$ Since these examples are 1-dimensional, the data \(x_i\) and weights \(w_j\) are numbers. Looking at, for instance, \(w_0\), some terms above are linear functions of \(w_0\) and each is clamped at zero. We can visualize this as follows: 1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 30,730-dimensional version of this shape. As an aside, you may have guessed from its bowl-shaped appearance that the SVM cost function is an example of a convex function There is a large amount of literature devoted to efficiently minimizing these types of functions, and you can also take a Stanford class on the topic ( convex optimization ). Once we extend our score functions \(f\) to Neural Networks our objective functions will become non-convex, and the visualizations above will not feature bowls but complex, bumpy terrains. Non-differentiable loss functions. As a technical note, you can also see that the kinks in the loss function (due to the max operation) technically make the loss function non-differentiable because at these kinks the gradient is not defined. However, the subgradient still exists and is commonly used instead. In this class will use the terms subgradient and gradient interchangeably. OptimizationTo reiterate, the loss function lets us quantify the quality of any particular set of weights W. The goal of optimization is to find W that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. For those of you coming to this class with previous experience, this section might seem odd since the working example we’ll use (the SVM loss) is a convex problem, but keep in mind that our goal is to eventually optimize Neural Networks where we can’t easily use any of the tools developed in the Convex Optimization literature. Strategy #1: A first very bad idea solution: Random searchSince it is so simple to check how good a given set of parameters W is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows: 12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines) In the code above, we see that we tried out several random weight vectors W, and some of them work better than others. We can take the best weights W found by this search and try it out on the test set: 1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555 With the best W this gives an accuracy of about 15.5%. Given that guessing classes completely at random achieves only 10%, that’s not a very bad outcome for a such a brain-dead random search solution! Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time. Our strategy will be to start with random weights and iteratively refine them over time to get lower loss Blindfolded hiker analogy. One analogy that you may find helpful going forward is to think of yourself as hiking on a hilly terrain with a blindfold on, and trying to reach the bottom. In the example of CIFAR-10, the hills are 30,730-dimensional, since the dimensions of W are 3073 x 10. At every point on the hill we achieve a particular loss (the height of the terrain). Strategy #2: Random Local SearchThe first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random \(W\), generate random perturbations \( \delta W \) to it and if the loss at the perturbed \(W + \delta W\) is lower, we will perform an update. The code for this procedure is as follows: 12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss) Using the same number of loss function evaluations as before (1000), this approach achieves test set classification accuracy of 21.4%. This is better, but still wasteful and computationally expensive. Strategy #3: Following the GradientIn the previous section we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). This direction will be related to the gradient of the loss function. In our hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest. In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is a generalization of slope for functions that don’t take a single number but a vector of numbers. Additionally, the gradient is just a vector of slopes (more commonly referred to as derivatives) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension. Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both. Computing the gradient numerically with finite differencesThe formula given above allows us to compute the gradient numerically. Here is a generic function that takes a function f, a vector x to evaluate the gradient on, and returns the gradient of f at x: 123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return grad Following the gradient formula we gave above, the code above iterates over all dimensions one by one, makes a small change h along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable grad holds the full gradient in the end. Practical considerations. Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: \( [f(x+h) - f(x-h)] / 2 h \) . See wiki for details. We can use the function given above to compute the gradient at any point and for any function. Lets compute the gradient for the CIFAR-10 loss function at some random point in the weight space: 12345678# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient The gradient tells us the slope of the loss function along every dimension, which we can use to make an update: 12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036 Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase. Effect of step size. The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. As we will see later in the course, choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network. In our blindfolded hill-descent analogy, we feel the hill below our feet sloping in some direction, but the step length we should take is uncertain. If we shuffle our feet carefully we can expect to make consistent but very small progress (this corresponds to having a small step size). Conversely, we can choose to make a large, confident step in an attempt to descend faster, but this may not pay off. As you can see in the code example above, at some point taking a bigger step gives a higher loss as we “overstep”. Visualizing the effect of step size. We start at some particular spot W and evaluate the gradient (or rather its negative - the white arrow) which tells us the direction of the steepest decrease in the loss function. Small steps are likely to lead to consistent but slow progress. Large steps can lead to better progress but are more risky. Note that eventually, for a large step size we will overshoot and make the loss worse. The step size (or as we will later call it - the learning rate) will become one of the most important hyperparameters that we will have to carefully tune. A problem of efficiency. You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. In our example we had 30730 parameters in total and therefore had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better. Computing the gradient analytically with CalculusThe numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of h, while the true gradient is defined as the limit as h goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check. Lets use the example of the SVM loss function for a single datapoint: $$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]$$ We can differentiate the function with respect to the weights. For example, taking the gradient with respect to \(w_{y_i}\) we obtain: $$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$ where \(\mathbb{1}\) is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector \(x_i\) scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of \(W\) that corresponds to the correct class. For the other rows where \(j \neq y_i \) the gradient is: $$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$ Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update. Gradient DescentNow that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows: 12345# Vanilla Gradient Descentwhile True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # perform parameter update This simple loop is at the core of all Neural Network libraries. There are other ways of performing the optimization (e.g. LBFGS), but Gradient Descent is currently by far the most common and established way of optimizing Neural Network loss functions. Throughout the class we will put some bells and whistles on the details of this loop (e.g. the exact details of the update equation), but the core idea of following the gradient until we’re happy with the results will remain the same. Mini-batch gradient descent. In large-scale applications (such as the ILSVRC challenge), the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update: 123456# Vanilla Minibatch Gradient Descentwhile True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter update The reason this works well is that the examples in the training data are correlated. To see this, consider the extreme case where all 1.2 million images in ILSVRC are in fact made up of exact duplicates of only 1000 unique images (one for each class, or in other words 1200 identical copies of each image). Then it is clear that the gradients we would compute for all 1200 identical copies would all be the same, and when we average the data loss over all 1.2 million images we would get the exact same loss as if we only evaluated on a small subset of 1000. In practice of course, the dataset would not contain duplicate images, the gradient from a mini-batch is a good approximation of the gradient of the full objective. Therefore, much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates. The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent). This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times. Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent (i.e. mentions of MGD for “Minibatch Gradient Descent”, or BGD for “Batch gradient descent” are rare to see), where it is usually assumed that mini-batches are used. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2. Summary Summary of the information flow. The dataset of pairs of (x,y) is given and fixed. The weights start out as random numbers and can change. During the forward pass the score function computes class scores, stored in vector f. The loss function contains two components: The data loss computes the compatibility between the scores f and the labels y. The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent. In this section, We developed the intuition of the loss function as a high-dimensional optimization landscape in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped. We motivated the idea of optimizing the loss function withiterative refinement, where we start with a random set of weights and refine them step by step until the loss is minimized. We saw that the gradient of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of h used in computing the numerical gradient). We saw that the parameter update requires a tricky setting of the step size (or the learning rate) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections. We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient. We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop. Coming up: The core takeaway from this section is that the ability to compute the gradient of a loss function with respect to its weights (and have some intuitive understanding of it) is the most important skill needed to design, train and understand neural networks. In the next section we will develop proficiency in computing the gradient analytically using the chain rule, otherwise also referred to as backpropagation. This will allow us to efficiently optimize relatively arbitrary loss functions that express all kinds of Neural Networks, including Convolutional Neural Networks.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimization-2]]></title>
    <url>%2F2017%2F10%2F10%2Foptimization-2%2F</url>
    <content type="text"><![CDATA[Table of Contents: Introduction Simple expressions, interpreting the gradient Compound expressions, chain rule, backpropagation Intuitive understanding of backpropagation Modularity: Sigmoid example Backprop in practice: Staged computation Patterns in backward flow Gradients for vectorized operations Summary IntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks. Problem statement. The core problem studied in this section is as follows: We are given some function \(f(x)\) where \(x\) is a vector of inputs and we are interested in computing the gradient of \(f\) at \(x\) (i.e. \(\nabla f(x)\) ). Motivation. Recall that the primary reason we are interested in this problem is that in the specific case of Neural Networks, \(f\) will correspond to the loss function ( \(L\) ) and the inputs \(x\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \((x_i,y_i), i=1 \ldots N\) and the weights and biases \(W,b\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples \(x_i\), in practice we usually only compute the gradient for the parameters (e.g. \(W,b\)) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on \(x_i\) can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing. If you are coming to this class and you’re comfortable with deriving gradients with chain rule, we would still like to encourage you to at least skim this section, since it presents a rarely developed view of backpropagation as backward flow in real-valued circuits and any insights you’ll gain may help you throughout the class. Simple expressions and interpretation of the gradientLets start simple so that we can develop the notation and conventions for more complex expressions. Consider a simple multiplication function of two numbers \(f(x,y) = x y\). It is a matter of simple calculus to derive the partial derivative for either input: $$f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x$$ Interpretation. Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point: $$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$ A technical note is that the division sign on the left-hand sign is, unlike the division sign on the right-hand sign, not a division. Instead, this notation indicates that the operator \( \frac{d}{dx} \) is being applied to the function \(f\), and returns a different function (the derivative). A nice way to think about the expression above is that when \(h\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, if \(x = 4, y = -3\) then \(f(x,y) = -12\) and the derivative on \(x\) \(\frac{\partial f}{\partial x} = -3\). This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount. This can be seen by rearranging the above equation ( \( f(x + h) = f(x) + h \frac{df(x)}{dx} \) ). Analogously, since \(\frac{\partial f}{\partial y} = 4\), we expect that increasing the value of \(y\) by some very small amount \(h\) would also increase the output of the function (due to the positive sign), and by \(4h\). The derivative on each variable tells you the sensitivity of the whole expression on its value. As mentioned, the gradient \(\nabla f\) is the vector of partial derivatives, so we have that \(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]\). Even though the gradient is technically a vector, we will often use terms such as “the gradient on x” instead of the technically correct phrase “the partial derivative on x” for simplicity. We can also derive the derivatives for the addition operation: $$f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$$ that is, the derivative on both \(x,y\) is one regardless of what the values of \(x,y\) are. This makes sense, since increasing either \(x,y\) would increase the output of \(f\), and the rate of that increase would be independent of what the actual values of \(x,y\) are (unlike the case of multiplication above). The last function we’ll use quite a bit in the class is the max operation: $$f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x &gt;= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y &gt;= x)$$ That is, the (sub)gradient is 1 on the input that was larger and 0 on the other input. Intuitively, if the inputs are \(x = 4,y = 2\), then the max is 4, and the function is not sensitive to the setting of \(y\). That is, if we were to increase it by a tiny amount \(h\), the function would keep outputting 4, and therefore the gradient is zero: there is no effect. Of course, if we were to change \(y\) by a large amount (e.g. larger than 2), then the value of \(f\) would change, but the derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the \(\lim_{h \rightarrow 0}\) in its definition. Compound expressions with chain ruleLets now start to consider more complicated expressions that involve multiple composed functions, such as \(f(x,y,z) = (x + y) z\). This expression is still simple enough to differentiate directly, but we’ll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: \(q = x + y\) and \(f = q z\). Moreover, we know how to compute the derivatives of both expressions separately, as seen in the previous section. \(f\) is just multiplication of \(q\) and \(z\), so \(\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q\), and \(q\) is addition of \(x\) and \(y\) so \( \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1 \). However, we don’t necessarily care about the gradient on the intermediate value \(q\) - the value of \(\frac{\partial f}{\partial q}\) is not useful. Instead, we are ultimately interested in the gradient of \(f\) with respect to its inputs \(x,y,z\). The chain rule tells us that the correct way to “chain” these gradient expressions together is through multiplication. For example, \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x} \). In practice this is simply a multiplication of the two numbers that hold the two gradients. Lets see this with an example: 1234567891011121314# set some inputsx = -2; y = 5; z = -4# perform the forward passq = x + y # q becomes 3f = q * z # f becomes -12# perform the backward pass (backpropagation) in reverse order:# first backprop through f = q * zdfdz = q # df/dz = q, so gradient on z becomes 3dfdq = z # df/dq = z, so gradient on q becomes -4# now backprop through q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!dfdy = 1.0 * dfdq # dq/dy = 1 At the end we are left with the gradient in the variables [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of backpropagation. Going forward, we will want to use a more concise notation so that we don’t have to keep writing the df part. That is, for example instead of dfdq we would simply write dq, and always assume that the gradient is with respect to the final output. This computation can also be nicely visualized with a circuit diagram: -2-4x5-4y-43z3-4q+-121f* The real-valued “circuit” on left shows the visual representation of the computation. The forward pass computes values from inputs to output (shown in green). The backward pass then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit. Intuitive understanding of backpropagationNotice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs. This extra multiplication (for each input) due to the chain rule can turn a single and relatively useless gate into a cog in a complex circuit such as an entire neural network. Lets get an intuition for how this works by referring again to the example. The add gate received inputs [-2, 5] and computed output 3. Since the gate is computing the addition operation, its local gradient for both of its inputs is +1. The rest of the circuit computed the final value, which is -12. During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was -4. If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as “wanting” the output of the add gate to be lower (due to negative sign), and with a force of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 * -4 = -4). Notice that this has the desired effect: If x,y were to decrease (responding to their negative gradient) then the add gate’s output would decrease, which in turn makes the multiply gate’s output increase. Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher. Modularity: Sigmoid exampleThe gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point: $$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$ as we will see later in the class, this expression describes a 2-dimensional neuron (with inputs x and weights w) that uses the sigmoid activation function. But for now lets think of this very simply as just a function from inputs w,x to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more: $$f(x) = \frac{1}{x}\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = -1/x^2\\f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = 1\\f(x) = e^x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = e^x\\f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = a$$ Where the functions \(f_c, f_a\) translate the input by a constant of \(c\) and scale the input by a constant of \(a\), respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do need the gradients for the constants. \(c,a\). The full circuit then looks as follows: 2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.206.000.204.000.20+1.000.20+-1.00-0.20*-10.37-0.53exp1.37-0.53+10.731.001/x Example circuit for a 2D neuron with a sigmoid activation function. The inputs are [x0,x1] and the (learnable) weights of the neuron are [w0,w1,w2]. As we will see later, the neuron computes a dot product with the input and then its activation is softly squashed by the sigmoid function to be in range from 0 to 1. In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function \(\sigma(x)\). It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator): $$\sigma(x) = \frac{1}{1+e^{-x}} \\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)= \left( 1 - \sigma(x) \right) \sigma(x)$$ As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code: 123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuit Implementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables. The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort. Backprop in practice: Staged computationLets see this with another example. Suppose that we have a function of the form: $$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$ To be clear, this function is completely useless and it’s not clear why you would ever want to compute its gradient, except for the fact that it is a good example of backpropagation in practice. It is very important to stress that if you were to launch into performing the differentiation with respect to either \(x\) or \(y\), you would end up with very large and complex expressions. However, it turns out that doing so is completely unnecessary because we don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the forward pass of such expression: 123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8) Phew, by the end of the expression we have computed the forward pass. Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass (sigy, num, sigx, xpy, xpysqr, den, invden) we will have the same variable, but one that begins with a d, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to: 123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phew Notice a few things: Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them. Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add. Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit: 3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.002.002.00max-10.002.00+-20.001.002 An example circuit demonstrating the intuition behind the operations that backpropagation performs during the backward pass in order to compute the gradients on the inputs. Sum operation distributes gradients equally to all its inputs. Max operation routes the gradient to the higher input. Multiply gate takes the input activations, swaps them and multiplies by its gradient. Looking at the diagram above as an example, we can see that: The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged. The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero. The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00. Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted \(w^Tx_i\) (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples \(x_i\) by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases. Gradients for vectorized operationsThe above sections were concerned with single variables, but all concepts extend in a straight-forward manner to matrix and vector operations. However, one must pay closer attention to dimensions and transpose operations. Matrix-Matrix multiply gradient. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations: 123456789# forward passW = np.random.randn(5, 10)X = np.random.randn(10, 3)D = W.dot(X)# now suppose we had the gradient on D from above in the circuitdD = np.random.randn(*D.shape) # same shape as DdW = dD.dot(X.T) #.T gives the transpose of the matrixdX = W.T.dot(dD) Tip: use dimension analysis! Note that you do not need to remember the expressions for dW and dX because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights dW must be of the same size as W after it is computed, and that it must depend on matrix multiplication of X and dD (as is the case when both X,W are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, X is of size [10 x 3] and dD of size [5 x 3], so if we want dW and W has shape [5 x 10], then the only way of achieving this is with dD.dot(X.T), as shown above. Work with small, explicit examples. Some people may find it difficult at first to derive the gradient updates for some vectorized expressions. Our recommendation is to explicitly write out a minimal vectorized example, derive the gradient on paper and then generalize the pattern to its efficient, vectorized form. Erik Learned-Miller has also written up a longer related document on taking matrix/vector derivatives which you might find helpful. Find it here. Summary We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher. We discussed the importance of staged computation for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time. In the next section we will start to define Neural Networks, and backpropagation will allow us to efficiently compute the gradients on the connections of the neural network, with respect to a loss function. In other words, we’re now ready to train Neural Nets, and the most conceptually difficult part of this class is behind us! ConvNets will then be a small step away. References Automatic differentiation in machine learning: a survey]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
        <tag>Random Search</tag>
        <tag>gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的-m参数]]></title>
    <url>%2F2017%2F10%2F09%2Fpython%E7%9A%84-m%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[单个文件创建测试文件：E:\Code\ScoreCard&gt;路径下创建testm.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module ') 在终端上运行： 查看帮助文档： 1-m mod : run library module as a script (terminates option list) 创建模块测试-m作用文档目录结构： E:\Code\ScoreCard\package1&gt;路径下创建testm1.py文件，内容如下：123456import sysprint(sys.path)if __name__ == "__main__": print ('This is main of module 1') E:\Code\ScoreCard\package2&gt;路径下创建testm2.py文件，内容如下：1234567import sysfrom package1 import testm1print(sys.path)if __name__ == "__main__": print ('This is main of module 2') 在终端上测试,效果如下: 结论123456-m 是把模块当作脚本来启动；直接启动是把run.py文件，所在的目录放到了sys.path属性中，见sys.path输出列表的第一个；模块启动是把你输入命令的目录（也就是当前路径），放到了sys.path属性中，见sys.path输出列表的第一个；当需要启动的py文件引用了一个模块。你需要注意：在启动的时候需要考虑sys.path中有没有你import的模块的路径！这个时候，到底是使用直接启动，还是以模块的启动？目的就是把import的那个模块的路径放到sys.path中。 参考here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linear-classify]]></title>
    <url>%2F2017%2F10%2F08%2FLinear%20Classification%2F</url>
    <content type="text"><![CDATA[Table of Contents: Intro to Linear classification Linear score function Interpreting a linear classifier Loss function Multiclass SVM Softmax classifier SVM vs Softmax Interactive Web Demo of Linear Classification Summary Linear ClassificationIn the last section we introduced the problem of Image Classification, which is the task of assigning a single label to an image from a fixed set of categories. Morever, we described the k-Nearest Neighbor (kNN) classifier which labels images by comparing them to (annotated) images from the training set. As we saw, kNN has a number of disadvantages: The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size. Classifying a test image is expensive since it requires a comparison to all training images. Overview. We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function. Parameterized mapping from images to label scoresThe first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. As before, let’s assume a training dataset of images \( x_i \in R^D \), each associated with a label \( y_i \). Here \( i = 1 \dots N \) and \( y_i \in { 1 \dots K } \). That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \(f: R^D \mapsto R^K\) that maps the raw image pixels to class scores. Linear classifier. In this module we will start out with arguably the simplest possible function, a linear mapping: $$f(x_i, W, b) = W x_i + b$$ In the above equation, we are assuming that the image \(x_i\) has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, \(x_i\) contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data \(x_i\). However, you will often hear people use the terms weights and parameters interchangeably. There are a few things to note: First, note that the single matrix multiplication \(W x_i\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of W. Notice also that we think of the input data \( (x_i, y_i) \) as given and fixed, but we have control over the setting of the parameters W,b. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes. An advantage of this approach is that the training data is used to learn the parameters W,b, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores. Lastly, note that to classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images. Foreshadowing: Convolutional Neural Networks will map image pixels to scores exactly as shown above, but the mapping ( f ) will be more complex and will contain more parameters. Interpreting a linear classifierNotice that a linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. Depending on precisely what values we set for these weights, the function has the capacity to like or dislike (depending on the sign of each weight) certain colors at certain positions in the image. For instance, you can imagine that the “ship” class might be more likely if there is a lot of blue on the sides of an image (which could likely correspond to water). You might expect that the “ship” classifier would then have a lot of positive weights across its blue channel weights (presence of blue increases score of ship), and negative weights in the red/green channels (presence of red/green decreases the score of ship). An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it’s looking at a dog. Analogy of images as high-dimensional points. Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points. Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing: Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores. As we saw above, every row of \(W\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \(W\), the corresponding line in the pixel space will rotate in different directions. The biases \(b\), on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in \( x_i = 0 \) would always give score of zero regardless of the weights, so all lines would be forced to cross the origin. Interpretation of linear classifiers as template matching.Another interpretation for the weights \(W\) is that each row of \(W\) corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance. Skipping ahead a bit: Example learned weights at the end of learning for CIFAR-10. Note that, for example, the ship template contains a lot of blue pixels as expected. This template will therefore give a high score once it is matched against images of ships on the ocean with an inner product. Additionally, note that the horse template seems to contain a two-headed horse, which is due to both left and right facing horses in the dataset. The linear classifier merges these two modes of horses in the data into a single template. Similarly, the car classifier seems to have merged several modes into a single template which has to identify cars from all sides, and of all colors. In particular, this template ended up being red, which hints that there are more red cars in the CIFAR-10 dataset than of any other color. The linear classifier is too weak to properly account for different-colored cars, but as we will see later neural networks will allow us to perform this task. Looking ahead a bit, a neural network will be able to develop intermediate neurons in its hidden layers that could detect specific car types (e.g. green car facing left, blue car facing front, etc.), and neurons on the next layer could combine these into a more accurate car score through a weighted sum of the individual car detectors. Bias trick. Before moving on we want to mention a common simplifying trick to representing the two parameters \(W,b\) as one. Recall that we defined the score function as: $$f(x_i, W, b) = W x_i + b$$ As we proceed through the material it is a little cumbersome to keep track of two sets of parameters (the biases \(b\) and weights \(W\)) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \(x_i\) with one additional dimension that always holds the constant \(1\) - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f(x_i, W) = W x_i$$ With our CIFAR-10 example, \(x_i\) is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and \(W\) is now [10 x 3073] instead of [10 x 3072]. The extra column that \(W\) now corresponds to the bias \(b\). An illustration might help clarify: Illustration of the bias trick. Doing a matrix multiplication and then adding a bias vector (left) is equivalent to adding a bias dimension with a constant of 1 to all input vectors and extending the weight matrix by 1 column - a bias column (right). Thus, if we preprocess our data by appending ones to all vectors we only have to learn a single matrix of weights instead of two matrices that hold the weights and the biases. Image data preprocessing. As a quick note, in the examples above we used the raw pixel values (which range from [0…255]). In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to center your data by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 … 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1]. Of these, zero mean centering is arguably more important but we will have to wait for its justification until we understand the dynamics of gradient descent. Loss functionIn the previous section we defined a function from the pixel values to class scores, which was parameterized by a set of weights \(W\). Moreover, we saw that we don’t have control over the data \( (x_i,y_i) \) (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data. For example, going back to the example image of a cat and its scores for the classes “cat”, “dog” and “ship”, we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well. Multiclass Support Vector Machine lossThere are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin \(\Delta\). Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good). Let’s now get more precise. Recall that for the i-th example we are given the pixels of image \( x_i \) and the label \( y_i \) that specifies the index of the correct class. The score function takes the pixels and computes the vector \( f(x_i, W) \) of class scores, which we will abbreviate to \(s\) (short for scores). For example, the score for the j-th class is the j-th element: \( s_j = f(x_i, W)_j \). The Multiclass SVM loss for the i-th example is then formalized as follows: $$L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)$$ Example. Lets unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \( s = [13, -7, 11]\), and that the first class is the true class (i.e. \(y_i = 0\)). Also assume that \(\Delta\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (\(j \neq y_i\)), so we get two terms: $$L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)$$ You can see that the first term gives zero since [-7 - 13 + 10] gives a negative number, which is then thresholded to zero with the \(max(0,-)\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; Any additional difference above the margin is clamped at zero with the max operation. The second term computes [11 - 13 + 10] which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \(y_i\) to be larger than the incorrect class scores by at least by \(\Delta\) (delta). If this is not the case, we will accumulate loss. Note that in this particular module we are working with linear score functions ( \( f(x_i; W) = W x_i \) ), so we can also rewrite the loss function in this equivalent form: $$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$$ where \(w_j\) is the j-th row of \(W\) reshaped as a column. However, this will not necessarily be the case once we start to consider more complex forms of the score function \(f\). A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero \(max(0,-)\) function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form \(max(0,-)^2\) that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation. The loss function quantifies our unhappiness with predictions on the training set The Multiclass Support Vector Machine “wants” the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible. Regularization. There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and \(L_i = 0\) for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters \( \lambda W \) where \( \lambda &gt; 1 \) will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30. In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty \(R(W)\). The most common regularization penalty is the L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters: $$R(W) = \sum_k\sum_l W_{k,l}^2$$ In the expression above, we are summing up all the squared elements of \(W\). Notice that the regularization function is not a function of the data, it is only based on the weights. Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss \(L_i\) over all examples) and the regularization loss. That is, the full Multiclass SVM loss becomes: $$L = \underbrace{ \frac{1}{N} \sum_i L_i }\text{data loss} + \underbrace{ \lambda R(W) }\text{regularization loss} \\$$ Or expanding this out in its full form: $$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$$ Where \(N\) is the number of training examples. As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter \(\lambda\). There is no simple way of setting this hyperparameter and it is usually determined by cross-validation. In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested). The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector \(x = [1,1,1,1] \) and two weight vectors \(w_1 = [1,0,0,0]\), \(w_2 = [0.25,0.25,0.25,0.25] \). Then \(w_1^Tx = w_2^Tx = 1\) so both weight vectors lead to the same dot product, but the L2 penalty of \(w_1\) is 1.0 while the L2 penalty of \(w_2\) is only 0.25. Therefore, according to the L2 penalty the weight vector \(w_2\) would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in \(w_2\) are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting. Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights \(W\) but not the biases \(b\). However, in practice this often turns out to have a negligible effect. Lastly, note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of \(W = 0\). Code. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment The takeaway from this section is that the SVM loss takes one particular approach to measuring how consistent the predictions on training data are with the ground truth labels. Additionally, making good predictions on the training set is equivalent to minimizing the loss. All we have to do now is to come up with a way to find the weights that minimize the loss. Practical ConsiderationsSetting Delta. Note that we brushed over the hyperparameter \(\Delta\) and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to \(\Delta = 1.0\) in all cases. The hyperparameters \(\Delta\) and \(\lambda\) seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights \(W\) has direct effect on the scores (and hence also their differences): As we shrink all values inside \(W\) the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. \(\Delta = 1\), or \(\Delta = 100\)) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength \(\lambda\)). Relation to Binary Support Vector Machine. You may be coming to this class with previous experience with Binary Support Vector Machines, where the loss for the i-th example can be written as: $$L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)$$ where \(C\) is a hyperparameter, and \(y_i \in \{ -1,1 \} \). You can convince yourself that the formulation we presented in this section contains the binary SVM as a special case when there are only two classes. That is, if we only had two classes then the loss reduces to the binary SVM shown above. Also, \(C\) in this formulation and \(\lambda\) in our formulation control the same tradeoff and are related through reciprocal relation \(C \propto \frac{1}{\lambda}\). Aside: Optimization in primal. If you’re coming to this class with previous knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm, etc. In this class (as is the case with Neural Networks in general) we will always work with the optimization objectives in their unconstrained primal form. Many of these objectives are technically not differentiable (e.g. the max(x,y) function isn’t because it has a kink when x=y), but in practice this is not a problem and it is common to use a subgradient. Aside: Other Multiclass SVM formulations. It is worth noting that the Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes. Another commonly used form is the One-Vs-All (OVA) SVM which trains an independent binary SVM for each class vs. all other classes. Related, but less common to see in practice is also the All-vs-All (AVA) strategy. Our formulation follows the Weston and Watkins 1999 (pdf) version, which is a more powerful version than OVA (in the sense that you can construct multiclass datasets where this version can achieve zero data loss, but OVA cannot. See details in the paper if interested). The last formulation you may see is a Structured SVM, which maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class. Understanding the differences between these formulations is outside of the scope of the class. The version presented in these notes is a safe bet to use in practice, but the arguably simplest OVA strategy is likely to work just as well (as also argued by Rikin et al. 2004 in In Defense of One-Vs-All Classification (pdf)). Softmax classifierIt turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the Softmax classifier, which has a different loss function. If you’ve heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes. Unlike the SVM which treats the outputs \(f(x_i,W)\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \(f(x_i; W) = W x_i\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: $$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$$ where we are using the notation \(f_j\) to mean the j-th element of the vector of class scores \(f\). As before, the full loss for the dataset is the mean of \(L_i\) over all training examples together with a regularization term \(R(W)\). The function \(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the softmax function: It takes a vector of arbitrary real-valued scores (in \(z\)) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate. Information theory view. The cross-entropy between a “true” distribution \(p\) and an estimated distribution \(q\) is defined as: $$H(p,q) = - \sum_x p(x) \log q(x)$$ The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( \(q = e^{f_{y_i}} / \sum_j e^{f_j} \) as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \(p = [0, \ldots 1, \ldots, 0]\) contains a single 1 at the \(y_i\) -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as \(H(p,q) = H(p) + D_{KL}(p||q)\), and the entropy of the delta function \(p\) is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer. Probabilistic interpretation. Looking at the expression, we see that $$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }$$ can be interpreted as the (normalized) probability assigned to the correct label \(y_i\) given the image \(x_i\) and parameterized by \(W\). To see this, remember that the Softmax classifier interprets the scores inside the output vector \(f\) as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term \(R(W)\) in the full loss function as coming from a Gaussian prior over the weight matrix \(W\), where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class. Practical issues: Numeric stability. When you’re writing code for computing the Softmax function in practice, the intermediate terms \(e^{f_{y_i}}\) and \(\sum_j e^{f_j}\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \(C\) and push it into the sum, we get the following (mathematically equivalent) expression: $$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}$$ We are free to choose the value of \(C\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \(C\) is to set \(\log C = -\max_j f_j \). This simply states that we should shift the values inside the vector \(f\) so that the highest value is zero. In code: 123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer Possibly confusing naming conventions. To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses the cross-entropy loss. The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. In particular, note that technically it doesn’t make sense to talk about the “softmax loss”, since softmax is just the squashing function, but it is a relatively commonly used shorthand. SVM vs. SoftmaxA picture might help clarify the distinction between the Softmax and SVM classifiers: Example of the difference between the SVM and Softmax classifiers for one datapoint. In both cases we compute the same score vector f (e.g. by matrix multiplication in this section). The difference is in the interpretation of the scores in f: The SVM interprets these as class scores and its loss function encourages the correct class (class 2, in blue) to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low). The final loss for this example is 1.58 for the SVM and 1.04 (note this is 1.04 using the natural logarithm, not base 2 or base 10) for the Softmax classifier, but note that these numbers are not comparable; They are only meaningful in relation to loss computed within the same classifier and with the same data. Softmax classifier provides “probabilities” for each class. Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute “probabilities” for all labels. For example, given an image the SVM classifier might give you scores [12.5, 0.6, -23.0] for the classes “cat”, “dog” and “ship”. The softmax classifier can instead compute the probabilities of the three labels as [0.9, 0.09, 0.01], which allows you to interpret its confidence in each class. The reason we put the word “probabilities” in quotes, however, is that how peaky or diffuse these probabilities are depends directly on the regularization strength \(\lambda\) - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute: $$[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]$$ Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength \(\lambda\) was higher, the weights \(W\) would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute: $$[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]$$ where the probabilites are now more diffuse. Moreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength \(\lambda\), the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. In practice, SVM and Softmax are usually comparable. The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better. Compared to the Softmax classifier, the SVM is a more local objective, which could be thought of either as a bug or a feature. Consider an example that achieves the scores [10, -2, 3] and where the first class is correct. An SVM (e.g. with desired margin of \(\Delta = 1\)) will see that the correct class already has a score higher than the margin compared to the other classes and it will compute loss of zero. The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero. However, these scenarios are not equivalent to a Softmax classifier, which would accumulate a much higher loss for the scores [10, 9, 9] than for [10, -100, -100]. In other words, the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud. Interactive web demo We have written an interactive web demo to help your intuitions with linear classifiers. The demo visualizes the loss functions discussed in this section using a toy 3-way classification on 2D data. The demo also jumps ahead a bit and performs the optimization, which we will discuss in full detail in the next section. SummaryIn summary, We defined a score function from image pixels to class scores (in this section, a linear function that depends on weights W and biases b). Unlike kNN classifier, the advantage of this parametric approach is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with W, not an exhaustive comparison to every single training example. We introduced the bias trick, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix. We defined a loss function (we introduced two commonly used losses for linear classifiers: the SVM and the Softmax) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss. We now saw one way to take a dataset of images and map each one to class scores based on a set of parameters, and we saw two examples of loss functions that we can use to measure the quality of the predictions. But how do we efficiently determine the parameters that give the best (lowest) loss? This process is optimization, and it is the topic of the next section. Further ReadingThese readings are optional and contain pointers of interest. Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>linear</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image Classification With KNN]]></title>
    <url>%2F2017%2F10%2F08%2FImage%20Classification%20With%20KNN%2F</url>
    <content type="text"><![CDATA[This is an introductory lecture designed to introduce people from outside of Computer Vision to the Image Classification problem, and the data-driven approach. The Table of Contents: Intro to Image Classification, data-driven approach, pipeline Nearest Neighbor Classifier k-Nearest Neighbor Validation sets, Cross-validation, hyperparameter tuning Pros/Cons of Nearest Neighbor Summary Summary: Applying kNN in practice Further Reading Image ClassificationMotivation. In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification. Example. For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as “cat”. Challenges. Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values: Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible. Illumination conditions. The effects of illumination are drastic on the pixel level. Background clutter. The objects of interest may blend into their environment, making them hard to identify. Intra-class variation. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance. A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations. Data-driven approach. How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like: The image classification pipeline. We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows: Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set. Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model. Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth). Nearest Neighbor ClassifierAs our first approach, we will develop what we call a Nearest Neighbor Classifier. This classifier has nothing to do with Convolutional Neural Networks and it is very rarely used in practice, but it will allow us to get an idea about the basic approach to an image classification problem. Example image classification dataset: CIFAR-10. One popular toy image classification dataset is the CIFAR-10 dataset. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example “airplane, automobile, bird, etc”). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below you can see 10 random example images from each one of the 10 classes: Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right you can see an example result of such a procedure for 10 example test images. Notice that in only about 3 out of 10 examples an image of the same class is retrieved, while in the other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car. You may have noticed that we left unspecified the details of exactly how we compare two images, which in this case are just two blocks of 32 x 32 x 3. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors \( I_1, I_2 \) , a reasonable choice for comparing them might be the L1 distance: $$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$ Where the sum is taken over all pixels. Here is the procedure visualized: Let’s also look at how we might implement the classifier in code. First, let’s load the CIFAR-10 data into memory as 4 arrays: the training data/labels and the test data/labels. In the code below, Xtr (of size 50,000 x 32 x 32 x 3) holds all the images in the training set, and a corresponding 1-dimensional array Ytr (of length 50,000) holds the training labels (from 0 to 9): 1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 Now that we have all images stretched out as rows, here is how we could train and evaluate a classifier: 123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) Notice that as an evaluation criterion, it is common to use the accuracy, which measures the fraction of predictions that were correct. Notice that all classifiers we will build satisfy this one common API: they have a train(X,y) function that takes the data and the labels to learn from. Internally, the class should build some kind of model of the labels and how they can be predicted from the data. And then there is a predict(X) function, which takes new data and predicts the labels. Of course, we’ve left out the meat of things - the actual classifier itself. Here is an implementation of a simple Nearest Neighbor classifier with the L1 distance that satisfies this template: 123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred If you ran this code, you would see that this classifier only achieves 38.6% on CIFAR-10. That’s more impressive than guessing at random (which would give 10% accuracy since there are 10 classes), but nowhere near human performance (which is estimated at about 94%) or near state-of-the-art Convolutional Neural Networks that achieve about 95%, matching human accuracy (see the leaderboard of a recent Kaggle competition on CIFAR-10). The choice of distance.There are many other ways of computing distances between vectors. Another common choice could be to instead use the L2 distance, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form: $$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$ In other words we would be computing the pixelwise difference as before, but this time we square all of them, add them up and finally take the square root. In numpy, using the code from above we would need to only replace a single line of code. The line that computes the distances: 1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) Note that I included the np.sqrt call above, but in a practical nearest neighbor application we could leave out the square root operation because square root is a monotonic function. That is, it scales the absolute sizes of the distances but it preserves the ordering, so the nearest neighbors with or without it are identical. If you ran the Nearest Neighbor classifier on CIFAR-10 with this distance, you would obtain 35.4% accuracy (slightly lower than our L1 distance result). L1 vs. L2. It is interesting to consider differences between the two metrics. In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. That is, the L2 distance prefers many medium disagreements to one big one. L1 and L2 distances (or equivalently the L1/L2 norms of the differences between a pair of images) are the most commonly used special cases of a p-norm. k - Nearest Neighbor ClassifierYou may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what’s called a k-Nearest Neighbor Classifier. The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers: In practice, you will almost always want to use k-Nearest Neighbor. But what value of k should you use? We turn to this problem next. Validation sets for Hyperparameter tuningThe k-nearest neighbor classifier requires a setting for k. But what number works best? Additionally, we saw that there are many different distance functions we could have used: L1 norm, L2 norm, there are many other choices we didn’t even consider (e.g. dot products). These choices are called hyperparameters and they come up very often in the design of many Machine Learning algorithms that learn from data. It’s often not obvious what values/settings one should choose. You might be tempted to suggest that we should try out many different values and see what works best. That is a fine idea and that’s indeed what we will do, but this must be done very carefully. In particular, we cannot use the test set for the purpose of tweaking hyperparameters. Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end. Otherwise, the very real danger is that you may tune your hyperparameters to work well on the test set, but if you were to deploy your model you could see a significantly reduced performance. In practice, we would say that you overfit to the test set. Another way of looking at it is that if you tune your hyperparameters on the test set, you are effectively using the test set as the training set, and therefore the performance you achieve on it will be too optimistic with respect to what you might actually observe when you deploy your model. But if you only use the test set once at end, it remains a good proxy for measuring the generalization of your classifier (we will see much more discussion surrounding generalization later in the class). Evaluate on the test set only a single time, at the very end. Luckily, there is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a validation set. Using CIFAR-10 as an example, we could for example use 49,000 of the training images for training, and leave 1,000 aside for validation. This validation set is essentially used as a fake test set to tune the hyper-parameters. Here is what this might look like in the case of CIFAR-10: 123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) By the end of this procedure, we could plot a graph that shows which values of k work best. We would then stick with this value and evaluate once on the actual test set. Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance. Cross-validation.In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called cross-validation. Working with our previous example, the idea is that instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of k works by iterating over different validation sets and averaging the performance across these. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds. In practice. In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive. The splits people tend to use is between 50%-90% of the training data for training and rest for validation. However, this depends on multiple factors: For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation. Pros and Cons of Nearest Neighbor classifier. It is worth considering some advantages and drawbacks of the Nearest Neighbor classifier. Clearly, one advantage is that it is very simple to implement and understand. Additionally, the classifier takes no time to train, since all that is required is to store and possibly index the training data. However, we pay that computational cost at test time, since classifying a test example requires a comparison to every single training example. This is backwards, since in practice we often care about the test time efficiency much more than the efficiency at training time. In fact, the deep neural networks we will develop later in this class shift this tradeoff to the other extreme: They are very expensive to train, but once the training is finished it is very cheap to classify a new test example. This mode of operation is much more desirable in practice. As an aside, the computational complexity of the Nearest Neighbor classifier is an active area of research, and several Approximate Nearest Neighbor (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. FLANN). These algorithms allow one to trade off the correctness of the nearest neighbor retrieval with its space/time complexity during retrieval, and usually rely on a pre-processing/indexing stage that involves building a kdtree, or running the k-means algorithm. The Nearest Neighbor Classifier may sometimes be a good choice in some settings (especially if the data is low-dimensional), but it is rarely appropriate for use in practical image classification settings. One problem is that images are high-dimensional objects (i.e. they often contain many pixels), and distances over high-dimensional spaces can be very counter-intuitive. The image below illustrates the point that the pixel-based L2 similarities we developed above are very different from perceptual similarities: Here is one more visualization to convince you that using pixel differences to compare images is inadequate. We can use a visualization technique called t-SNE to take the CIFAR-10 images and embed them in two dimensions so that their (local) pairwise distances are best preserved. In this visualization, images that are shown nearby are considered to be very near according to the L2 pixelwise distance we developed above: In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity. For example, a dog can be seen very near a frog since both happen to be on white background. Ideally we would like images of all of the 10 classes to form their own clusters, so that images of the same class are nearby to each other regardless of irrelevant characteristics and variations (such as the background). However, to get this property we will have to go beyond raw pixels. SummaryIn summary: We introduced the problem of Image Classification, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. We introduced a simple classifier called the Nearest Neighbor classifier. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them. We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call validation set. We try different hyperparameter values and keep the values that lead to the best performance on the validation set. If the lack of training data is a concern, we discussed a procedure called cross-validation, which can help reduce noise in estimating which hyperparameters work best. Once the best hyperparameters are found, we fix them and perform a single evaluation on the actual test set. We saw that Nearest Neighbor can get us about 40% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image. Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content. In next lectures we will embark on addressing these challenges and eventually arrive at solutions that give 90% accuracies, allow us to completely discard the training set once learning is complete, and they will allow us to evaluate a test image in less than a millisecond. Summary: Applying kNN in practiceIf you wish to apply kNN in practice (hopefully not on images, or perhaps as only a baseline) proceed as follows: Preprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization. If your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA (wiki ref, CS229ref, blog ref) or even Random Projections. Split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive). Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of k (e.g. the more the better) and across different distance types (L1 and L2 are good candidates) If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. FLANN) to accelerate the retrieval (at cost of some accuracy). Take note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be burned on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data. Further ReadingHere are some (optional) links you may find interesting for further reading: A Few Useful Things to Know about Machine Learning, where especially section 6 is related but the whole paper is a warmly recommended reading. Recognizing and Learning Object Categories, a short course of object categorization at ICCV 2005.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Image Classification</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fpython-numpy-tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial was contributed by Justin Johnson. We will use the Python programming language for all assignments in this course.Python is a great general-purpose programming language on its own, but with thehelp of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerfulenvironment for scientific computing. We expect that many of you will have some experience with Python and numpy;for the rest of you, this section will serve as a quick crash course both onthe Python programming language and on the use of Python for scientificcomputing. Some of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page. You can also find an IPython notebook version of this tutorial here created by Volodymyr Kuleshov and Isaac Caswell for CS 228. Table of contents: Python Basic data types Containers Lists Dictionaries Sets Tuples Functions Classes Numpy Arrays Array indexing Datatypes Array math Broadcasting SciPy Image operations MATLAB files Distance between points Matplotlib Plotting Subplots Images PythonPython is a high-level, dynamically typed multiparadigm programming language.Python code is often said to be almost like pseudocode, since it allows youto express very powerful ideas in very few lines of code while being veryreadable. As an example, here is an implementation of the classic quicksortalgorithm in Python: 1234567891011def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) / 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right) print quicksort([3,6,8,10,1,2,1])# Prints "[1, 1, 2, 3, 6, 8, 10]" Python versionsThere are currently two different supported versions of Python, 2.7 and 3.4.Somewhat confusingly, Python 3.0 introduced many backwards-incompatible changesto the language, so code written for 2.7 may not work under 3.4 and vice versa.For this class all code will use Python 2.7. You can check your Python version at the command line by runningpython --version. Basic data typesLike most languages, Python has a number of basic types including integers,floats, booleans, and strings. These data types behave in ways that arefamiliar from other programming languages. Numbers: Integers and floats work as you would expect from other languages: 1234567891011121314x = 3print type(x) # Prints "&lt;type 'int'&gt;"print x # Prints "3"print x + 1 # Addition; prints "4"print x - 1 # Subtraction; prints "2"print x * 2 # Multiplication; prints "6"print x ** 2 # Exponentiation; prints "9"x += 1print x # Prints "4"x *= 2print x # Prints "8"y = 2.5print type(y) # Prints "&lt;type 'float'&gt;"print y, y + 1, y * 2, y ** 2 # Prints "2.5 3.5 5.0 6.25" Note that unlike many languages, Python does not have unary increment (x++)or decrement (x--) operators. Python also has built-in types for long integers and complex numbers;you can find all of the detailsin the documentation. Booleans: Python implements all of the usual operators for Boolean logic,but uses English words rather than symbols (&amp;&amp;, ||, etc.): 1234567t = Truef = Falseprint type(t) # Prints "&lt;type 'bool'&gt;"print t and f # Logical AND; prints "False"print t or f # Logical OR; prints "True"print not t # Logical NOT; prints "False"print t != f # Logical XOR; prints "True" Strings: Python has great support for strings: 12345678hello = 'hello' # String literals can use single quotesworld = "world" # or double quotes; it does not matter.print hello # Prints "hello"print len(hello) # String length; prints "5"hw = hello + ' ' + world # String concatenationprint hw # prints "hello world"hw12 = '%s %s %d' % (hello, world, 12) # sprintf style string formattingprint hw12 # prints "hello world 12" String objects have a bunch of useful methods; for example: 12345678s = "hello"print s.capitalize() # Capitalize a string; prints "Hello"print s.upper() # Convert a string to uppercase; prints "HELLO"print s.rjust(7) # Right-justify a string, padding with spaces; prints " hello"print s.center(7) # Center a string, padding with spaces; prints " hello "print s.replace('l', '(ell)') # Replace all instances of one substring with another; # prints "he(ell)(ell)o"print ' world '.strip() # Strip leading and trailing whitespace; prints "world" You can find a list of all string methods in the documentation. ContainersPython includes several built-in container types: lists, dictionaries, sets, and tuples. ListsA list is the Python equivalent of an array, but is resizeableand can contain elements of different types: 123456789xs = [3, 1, 2] # Create a listprint xs, xs[2] # Prints "[3, 1, 2] 2"print xs[-1] # Negative indices count from the end of the list; prints "2"xs[2] = 'foo' # Lists can contain elements of different typesprint xs # Prints "[3, 1, 'foo']"xs.append('bar') # Add a new element to the end of the listprint xs # Prints "[3, 1, 'foo', 'bar']"x = xs.pop() # Remove and return the last element of the listprint x, xs # Prints "bar [3, 1, 'foo']" As usual, you can find all the gory details about listsin the documentation. Slicing:In addition to accessing list elements one at a time, Python providesconcise syntax to access sublists; this is known as slicing: 123456789nums = range(5) # range is a built-in function that creates a list of integersprint nums # Prints "[0, 1, 2, 3, 4]"print nums[2:4] # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"print nums[2:] # Get a slice from index 2 to the end; prints "[2, 3, 4]"print nums[:2] # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"print nums[:] # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"print nums[:-1] # Slice indices can be negative; prints ["0, 1, 2, 3]"nums[2:4] = [8, 9] # Assign a new sublist to a sliceprint nums # Prints "[0, 1, 8, 9, 4]" We will see slicing again in the context of numpy arrays. Loops: You can loop over the elements of a list like this: 1234animals = ['cat', 'dog', 'monkey']for animal in animals: print animal# Prints "cat", "dog", "monkey", each on its own line. If you want access to the index of each element within the body of a loop,use the built-in enumerate function: 1234animals = ['cat', 'dog', 'monkey']for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line List comprehensions:When programming, frequently we want to transform one type of data into another.As a simple example, consider the following code that computes square numbers: 12345nums = [0, 1, 2, 3, 4]squares = []for x in nums: squares.append(x ** 2)print squares # Prints [0, 1, 4, 9, 16] You can make this code simpler using a list comprehension: 123nums = [0, 1, 2, 3, 4]squares = [x ** 2 for x in nums]print squares # Prints [0, 1, 4, 9, 16] List comprehensions can also contain conditions: 123nums = [0, 1, 2, 3, 4]even_squares = [x ** 2 for x in nums if x % 2 == 0]print even_squares # Prints "[0, 4, 16]" DictionariesA dictionary stores (key, value) pairs, similar to a Map in Java oran object in Javascript. You can use it like this: 12345678910d = &#123;'cat': 'cute', 'dog': 'furry'&#125; # Create a new dictionary with some dataprint d['cat'] # Get an entry from a dictionary; prints "cute"print 'cat' in d # Check if a dictionary has a given key; prints "True"d['fish'] = 'wet' # Set an entry in a dictionaryprint d['fish'] # Prints "wet"# print d['monkey'] # KeyError: 'monkey' not a key of dprint d.get('monkey', 'N/A') # Get an element with a default; prints "N/A"print d.get('fish', 'N/A') # Get an element with a default; prints "wet"del d['fish'] # Remove an element from a dictionaryprint d.get('fish', 'N/A') # "fish" is no longer a key; prints "N/A" You can find all you need to know about dictionariesin the documentation. Loops: It is easy to iterate over the keys in a dictionary: 12345d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal in d: legs = d[animal] print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" If you want access to keys and their corresponding values, use the iteritems method: 1234d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal, legs in d.iteritems(): print 'A %s has %d legs' % (animal, legs)# Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs" Dictionary comprehensions:These are similar to list comprehensions, but allow you to easily constructdictionaries. For example: 123nums = [0, 1, 2, 3, 4]even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;print even_num_to_square # Prints "&#123;0: 0, 2: 4, 4: 16&#125;" SetsA set is an unordered collection of distinct elements. As a simple example, considerthe following: 12345678910animals = &#123;'cat', 'dog'&#125;print 'cat' in animals # Check if an element is in a set; prints "True"print 'fish' in animals # prints "False"animals.add('fish') # Add an element to a setprint 'fish' in animals # Prints "True"print len(animals) # Number of elements in a set; prints "3"animals.add('cat') # Adding an element that is already in the set does nothingprint len(animals) # Prints "3"animals.remove('cat') # Remove an element from a setprint len(animals) # Prints "2" As usual, everything you want to know about sets can be foundin the documentation. Loops:Iterating over a set has the same syntax as iterating over a list;however since sets are unordered, you cannot make assumptions about the orderin which you visit the elements of the set: 1234animals = &#123;'cat', 'dog', 'fish'&#125;for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: fish", "#2: dog", "#3: cat" Set comprehensions:Like lists and dictionaries, we can easily construct sets using set comprehensions: 123from math import sqrtnums = &#123;int(sqrt(x)) for x in range(30)&#125;print nums # Prints "set([0, 1, 2, 3, 4, 5])" TuplesA tuple is an (immutable) ordered list of values.A tuple is in many ways similar to a list; one of the most important differences is thattuples can be used as keys in dictionaries and as elements of sets, while lists cannot.Here is a trivial example: 12345d = &#123;(x, x + 1): x for x in range(10)&#125; # Create a dictionary with tuple keyst = (5, 6) # Create a tupleprint type(t) # Prints "&lt;type 'tuple'&gt;"print d[t] # Prints "5"print d[(1, 2)] # Prints "1" The documentation has more information about tuples. FunctionsPython functions are defined using the def keyword. For example: 1234567891011def sign(x): if x &gt; 0: return 'positive' elif x &lt; 0: return 'negative' else: return 'zero'for x in [-1, 0, 1]: print sign(x)# Prints "negative", "zero", "positive" We will often define functions to take optional keyword arguments, like this: 12345678def hello(name, loud=False): if loud: print 'HELLO, %s!' % name.upper() else: print 'Hello, %s' % namehello('Bob') # Prints "Hello, Bob"hello('Fred', loud=True) # Prints "HELLO, FRED!" There is a lot more information about Python functionsin the documentation. ClassesThe syntax for defining classes in Python is straightforward: 12345678910111213141516class Greeter(object): # Constructor def __init__(self, name): self.name = name # Create an instance variable # Instance method def greet(self, loud=False): if loud: print 'HELLO, %s!' % self.name.upper() else: print 'Hello, %s' % self.name g = Greeter('Fred') # Construct an instance of the Greeter classg.greet() # Call an instance method; prints "Hello, Fred"g.greet(loud=True) # Call an instance method; prints "HELLO, FRED!" You can read a lot more about Python classesin the documentation. NumpyNumpy is the core library for scientific computing in Python.It provides a high-performance multidimensional array object, and tools for working with thesearrays. If you are already familiar with MATLAB, you might findthis tutorial useful to get started with Numpy. ArraysA numpy array is a grid of values, all of the same type, and is indexed by a tuple ofnonnegative integers. The number of dimensions is the rank of the array; the shapeof an array is a tuple of integers giving the size of the array along each dimension. We can initialize numpy arrays from nested Python lists,and access elements using square brackets: 123456789101112import numpy as npa = np.array([1, 2, 3]) # Create a rank 1 arrayprint type(a) # Prints "&lt;type 'numpy.ndarray'&gt;"print a.shape # Prints "(3,)"print a[0], a[1], a[2] # Prints "1 2 3"a[0] = 5 # Change an element of the arrayprint a # Prints "[5, 2, 3]"b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 arrayprint b.shape # Prints "(2, 3)"print b[0, 0], b[0, 1], b[1, 0] # Prints "1 2 4" Numpy also provides many functions to create arrays: 1234567891011121314151617181920import numpy as npa = np.zeros((2,2)) # Create an array of all zerosprint a # Prints "[[ 0. 0.] # [ 0. 0.]]" b = np.ones((1,2)) # Create an array of all onesprint b # Prints "[[ 1. 1.]]"c = np.full((2,2), 7) # Create a constant arrayprint c # Prints "[[ 7. 7.] # [ 7. 7.]]"d = np.eye(2) # Create a 2x2 identity matrixprint d # Prints "[[ 1. 0.] # [ 0. 1.]]" e = np.random.random((2,2)) # Create an array filled with random valuesprint e # Might print "[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]" You can read about other methods of array creationin the documentation. Array indexingNumpy offers several ways to index into arrays. Slicing:Similar to Python lists, numpy arrays can be sliced.Since arrays may be multidimensional, you must specify a slice for each dimensionof the array: 12345678910111213141516171819import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Use slicing to pull out the subarray consisting of the first 2 rows# and columns 1 and 2; b is the following array of shape (2, 2):# [[2 3]# [6 7]]b = a[:2, 1:3]# A slice of an array is a view into the same data, so modifying it# will modify the original array.print a[0, 1] # Prints "2"b[0, 0] = 77 # b[0, 0] is the same piece of data as a[0, 1]print a[0, 1] # Prints "77" You can also mix integer indexing with slice indexing.However, doing so will yield an array of lower rank than the original array.Note that this is quite different from the way that MATLAB handles arrayslicing: 123456789101112131415161718192021222324import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Two ways of accessing the data in the middle row of the array.# Mixing integer indexing with slices yields an array of lower rank,# while using only slices yields an array of the same rank as the# original array:row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of aprint row_r1, row_r1.shape # Prints "[5 6 7 8] (4,)"print row_r2, row_r2.shape # Prints "[[5 6 7 8]] (1, 4)"# We can make the same distinction when accessing columns of an array:col_r1 = a[:, 1]col_r2 = a[:, 1:2]print col_r1, col_r1.shape # Prints "[ 2 6 10] (3,)"print col_r2, col_r2.shape # Prints "[[ 2] # [ 6] # [10]] (3, 1)" Integer array indexing:When you index into numpy arrays using slicing, the resulting array viewwill always be a subarray of the original array. In contrast, integer arrayindexing allows you to construct arbitrary arrays using the data from anotherarray. Here is an example: 1234567891011121314151617import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])# An example of integer array indexing.# The returned array will have shape (3,) and print a[[0, 1, 2], [0, 1, 0]] # Prints "[1 4 5]"# The above example of integer array indexing is equivalent to this:print np.array([a[0, 0], a[1, 1], a[2, 0]]) # Prints "[1 4 5]"# When using integer array indexing, you can reuse the same# element from the source array:print a[[0, 0], [1, 1]] # Prints "[2 2]"# Equivalent to the previous integer array indexing exampleprint np.array([a[0, 1], a[0, 1]]) # Prints "[2 2]" One useful trick with integer array indexing is selecting or mutating oneelement from each row of a matrix: 1234567891011121314151617181920212223import numpy as np# Create a new array from which we will select elementsa = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])print a # prints "array([[ 1, 2, 3], # [ 4, 5, 6], # [ 7, 8, 9], # [10, 11, 12]])"# Create an array of indicesb = np.array([0, 2, 0, 1])# Select one element from each row of a using the indices in bprint a[np.arange(4), b] # Prints "[ 1 6 7 11]"# Mutate one element from each row of a using the indices in ba[np.arange(4), b] += 10print a # prints "array([[11, 2, 3], # [ 4, 5, 16], # [17, 8, 9], # [10, 21, 12]]) Boolean array indexing:Boolean array indexing lets you pick out arbitrary elements of an array.Frequently this type of indexing is used to select the elements of an arraythat satisfy some condition. Here is an example: 1234567891011121314151617181920import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2. print bool_idx # Prints "[[False False] # [ True True] # [ True True]]"# We use boolean array indexing to construct a rank 1 array# consisting of the elements of a corresponding to the True values# of bool_idxprint a[bool_idx] # Prints "[3 4 5 6]"# We can do all of the above in a single concise statement:print a[a &gt; 2] # Prints "[3 4 5 6]" For brevity we have left out a lot of details about numpy array indexing;if you want to know more you shouldread the documentation. DatatypesEvery numpy array is a grid of elements of the same type.Numpy provides a large set of numeric datatypes that you can use to construct arrays.Numpy tries to guess a datatype when you create an array, but functions that constructarrays usually also include an optional argument to explicitly specify the datatype.Here is an example: 12345678910import numpy as npx = np.array([1, 2]) # Let numpy choose the datatypeprint x.dtype # Prints "int64"x = np.array([1.0, 2.0]) # Let numpy choose the datatypeprint x.dtype # Prints "float64"x = np.array([1, 2], dtype=np.int64) # Force a particular datatypeprint x.dtype # Prints "int64" You can read all about numpy datatypesin the documentation. Array mathBasic mathematical functions operate elementwise on arrays, and are availableboth as operator overloads and as functions in the numpy module: 123456789101112131415161718192021222324252627282930313233import numpy as npx = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the array# [[ 6.0 8.0]# [10.0 12.0]]print x + yprint np.add(x, y)# Elementwise difference; both produce the array# [[-4.0 -4.0]# [-4.0 -4.0]]print x - yprint np.subtract(x, y)# Elementwise product; both produce the array# [[ 5.0 12.0]# [21.0 32.0]]print x * yprint np.multiply(x, y)# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print x / yprint np.divide(x, y)# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print np.sqrt(x) Note that unlike MATLAB, * is elementwise multiplication, not matrixmultiplication. We instead use the dot function to compute innerproducts of vectors, to multiply a vector by a matrix, and tomultiply matrices. dot is available both as a function in the numpymodule and as an instance method of array objects: 123456789101112131415161718192021import numpy as npx = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print v.dot(w)print np.dot(v, w)# Matrix / vector product; both produce the rank 1 array [29 67]print x.dot(v)print np.dot(x, v)# Matrix / matrix product; both produce the rank 2 array# [[19 22]# [43 50]]print x.dot(y)print np.dot(x, y) Numpy provides many useful functions for performing computations onarrays; one of the most useful is sum: 1234567import numpy as npx = np.array([[1,2],[3,4]])print np.sum(x) # Compute sum of all elements; prints "10"print np.sum(x, axis=0) # Compute sum of each column; prints "[4 6]"print np.sum(x, axis=1) # Compute sum of each row; prints "[3 7]" You can find the full list of mathematical functions provided by numpyin the documentation. Apart from computing mathematical functions using arrays, we frequentlyneed to reshape or otherwise manipulate data in arrays. The simplest exampleof this type of operation is transposing a matrix; to transpose a matrix,simply use the T attribute of an array object: 123456789101112import numpy as npx = np.array([[1,2], [3,4]])print x # Prints "[[1 2] # [3 4]]"print x.T # Prints "[[1 3] # [2 4]]"# Note that taking the transpose of a rank 1 array does nothing:v = np.array([1,2,3])print v # Prints "[1 2 3]"print v.T # Prints "[1 2 3]" Numpy provides many more functions for manipulating arrays; you can see the full listin the documentation. BroadcastingBroadcasting is a powerful mechanism that allows numpy to work with arrays of differentshapes when performing arithmetic operations. Frequently we have a smaller array and alarger array, and we want to use the smaller array multiple times to perform some operationon the larger array. For example, suppose that we want to add a constant vector to eachrow of a matrix. We could do it like this: 123456789101112131415161718import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = np.empty_like(x) # Create an empty matrix with the same shape as x# Add the vector v to each row of the matrix x with an explicit loopfor i in range(4): y[i, :] = x[i, :] + v# Now y is the following# [[ 2 2 4]# [ 5 5 7]# [ 8 8 10]# [11 11 13]]print y This works; however when the matrix x is very large, computing an explicit loopin Python could be slow. Note that adding the vector v to each row of the matrixx is equivalent to forming a matrix vv by stacking multiple copies of v vertically,then performing elementwise summation of x and vv. We could implement thisapproach like this: 12345678910111213141516import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])vv = np.tile(v, (4, 1)) # Stack 4 copies of v on top of each otherprint vv # Prints "[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]"y = x + vv # Add x and vv elementwiseprint y # Prints "[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" Numpy broadcasting allows us to perform this computation without actuallycreating multiple copies of v. Consider this version, using broadcasting: 1234567891011import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint y # Prints "[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]" The line y = x + v works even though x has shape (4, 3) and v has shape(3,) due to broadcasting; this line works as if v actually had shape (4, 3),where each row was a copy of v, and the sum was performed elementwise. Broadcasting two arrays together follows these rules: If the arrays do not have the same rank, prepend the shape of the lower rank arraywith 1s until both shapes have the same length. The two arrays are said to be compatible in a dimension if they have the samesize in the dimension, or if one of the arrays has size 1 in that dimension. The arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwisemaximum of shapes of the two input arrays. In any dimension where one array had size 1 and the other array had size greater than 1,the first array behaves as if it were copied along that dimension If this explanation does not make sense, try reading the explanationfrom the documentationor this explanation. Functions that support broadcasting are known as universal functions. You can findthe list of all universal functionsin the documentation. Here are some applications of broadcasting: 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:# [[ 4 5]# [ 8 10]# [12 15]]print np.reshape(v, (3, 1)) * w# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:# [[2 4 6]# [5 7 9]]print x + v# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:# [[ 5 6 7]# [ 9 10 11]]print (x.T + w).T# Another solution is to reshape w to be a row vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print x + np.reshape(w, (2, 1))# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:# [[ 2 4 6]# [ 8 10 12]]print x * 2 Broadcasting typically makes your code more concise and faster, so youshould strive to use it where possible. Numpy DocumentationThis brief overview has touched on many of the important things that you need toknow about numpy, but is far from complete. Check out thenumpy referenceto find out much more about numpy. SciPyNumpy provides a high-performance multidimensional array and basic tools tocompute with and manipulate these arrays.SciPybuilds on this, and providesa large number of functions that operate on numpy arrays and are useful fordifferent types of scientific and engineering applications. The best way to get familiar with SciPy is tobrowse the documentation.We will highlight some parts of SciPy that you might find useful for this class. Image operationsSciPy provides some basic functions to work with images.For example, it has functions to read images from disk into numpy arrays,to write numpy arrays to disk as images, and to resize images.Here is a simple example that showcases these functions: 12345678910111213141516171819from scipy.misc import imread, imsave, imresize# Read an JPEG image into a numpy arrayimg = imread('assets/cat.jpg')print img.dtype, img.shape # Prints "uint8 (400, 248, 3)"# We can tint the image by scaling each of the color channels# by a different scalar constant. The image has shape (400, 248, 3);# we multiply it by the array [1, 0.95, 0.9] of shape (3,);# numpy broadcasting means that this leaves the red channel unchanged,# and multiplies the green and blue channels by 0.95 and 0.9# respectively.img_tinted = img * [1, 0.95, 0.9]# Resize the tinted image to be 300 by 300 pixels.img_tinted = imresize(img_tinted, (300, 300))# Write the tinted image back to diskimsave('assets/cat_tinted.jpg', img_tinted) Left: The original image. Right: The tinted and resized image. MATLAB filesThe functions scipy.io.loadmat and scipy.io.savemat allow you to read andwrite MATLAB files. You can read about themin the documentation. Distance between pointsSciPy defines some useful functions for computing distances between sets of points. The function scipy.spatial.distance.pdist computes the distance between all pairsof points in a given set: 123456789101112131415161718import numpy as npfrom scipy.spatial.distance import pdist, squareform# Create the following array where each row is a point in 2D space:# [[0 1]# [1 0]# [2 0]]x = np.array([[0, 1], [1, 0], [2, 0]])print x# Compute the Euclidean distance between all rows of x.# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],# and d is the following array:# [[ 0. 1.41421356 2.23606798]# [ 1.41421356 0. 1. ]# [ 2.23606798 1. 0. ]]d = squareform(pdist(x, 'euclidean'))print d You can read all the details about this functionin the documentation. A similar function (scipy.spatial.distance.cdist) computes the distance between all pairsacross two sets of points; you can read about itin the documentation. MatplotlibMatplotlib is a plotting library.In this section give a brief introduction to the matplotlib.pyplot module,which provides a plotting system similar to that of MATLAB. PlottingThe most important function in matplotlib is plot,which allows you to plot 2D data. Here is a simple example: 12345678910import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)plt.show() # You must call plt.show() to make graphics appear. Running this code produces the following plot: With just a little bit of extra work we can easily plot multiple linesat once, and add a title, legend, and axis labels: 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])plt.show() You can read much more about the plot functionin the documentation. SubplotsYou can plot different things in the same figure using the subplot function.Here is an example: 1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show() You can read much more about the subplot functionin the documentation. ImagesYou can use the imshow function to show images. Here is an example: 12345678910111213141516171819import numpy as npfrom scipy.misc import imread, imresizeimport matplotlib.pyplot as pltimg = imread('assets/cat.jpg')img_tinted = img * [1, 0.95, 0.9]# Show the original imageplt.subplot(1, 2, 1)plt.imshow(img)# Show the tinted imageplt.subplot(1, 2, 2)# A slight gotcha with imshow is that it might give strange results# if presented with data that is not uint8. To work around this, we# explicitly cast the image to uint8 before displaying it.plt.imshow(np.uint8(img_tinted))plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPython Tutorial]]></title>
    <url>%2F2017%2F10%2F08%2Fipython-tutorial%2F</url>
    <content type="text"><![CDATA[In this class, we will use IPython notebooks for theprogramming assignments. An IPython notebook lets you write and execute Pythoncode in your web browser. IPython notebooks make it very easy to tinker withcode and execute it in bits and pieces; for this reason IPython notebooks arewidely used in scientific computing. Installing and running IPython is easy. From the command line, the followingwill install IPython: 1pip install "ipython[notebook]" Once you have IPython installed, start it with this command: 1ipython notebook Once IPython is running, point your web browser at http://localhost:8888 tostart using IPython notebooks. If everything worked correctly, you shouldsee a screen like this, showing all available IPython notebooks in the currentdirectory: If you click through to a notebook file, you will see a screen like this: An IPython notebook is made up of a number of cells. Each cell can containPython code. You can execute a cell by clicking on it and pressing Shift-Enter.When you do so, the code in the cell will run, and the output of the cellwill be displayed beneath the cell. For example, after running the first cellthe notebook looks like this: Global variables are shared between cells. Executing the second cell thus givesthe following result: By convention, IPython notebooks are expected to be run from top to bottom.Failing to execute some cells or executing cells out of order can result inerrors: After you have modified an IPython notebook for one of the assignments bymodifying or executing some of its cells, remember to save your changes! This has only been a brief introduction to IPython notebooks, but it shouldbe enough to get you up and running on the assignments for this course.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>IPython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[15]反向解析路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B15%5D%E5%8F%8D%E5%90%91%E8%A7%A3%E6%9E%90%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[给url起一个名字1url(r'^(?P&lt;id&gt;\d+)/$', views.posts_detail, name="detail"), Template中 1&lt;a href="&#123;% url 'detail' id=obj.id %&#125;"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt; Python代码中 123456from django.shortcuts import redirect # 调转请求from django.urls import reverse # 反向解析url的...return redirect(reverse("detail", kwargs=&#123;"id": 3&#125;)) Model中 get_absolute_url() 12345def get_absolute_url(self): return reverse("detail", kwargs=&#123;"id": self.id&#125;) # return "/post/&#123;&#125;/".format(self.id) # return "/post/%s/" % (self.id) url的命令空间 namespace在一级的url设置的，用于区分不用的app，因为不同的app下面可能存在同名的 url1234567url(r'^post/', include(posts_urls, namespace="post")),&lt;a href="&#123;% url 'post:detail' id=obj.id %&#125;"&gt;def get_absolute_url(self): return reverse("post:detail", kwargs=&#123;"id": self.id&#125;) via Django1.10教程 -15 -反向解析路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[14]动态路由]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B14%5D%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[Django中的URL介绍/bbs/urls.py 1234567from posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^post/', include(posts_urls)),] /posts/urls.py 1234567from . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^detail/$', views.posts_detail), ... http://127.0.0.1:8000/post/detail/ 动态路由和参数传递页面点击是通过设置a标签来调转的 后端view中通过id来查询不同的帖子对象1url(r'^detail/(?P&lt;id&gt;\d+)/$', views.posts_detail,), 在模板中组合起来1&lt;h2 class="blog-post-title"&gt;&lt;a href="/post/detail/&#123;&#123;obj.id&#125;&#125;/"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/a&gt;&lt;/h2&gt; via Django1.10教程 -14 -动态路由]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[13]从数据库中获取某个对象]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B13%5D%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E8%8E%B7%E5%8F%96%E6%9F%90%E4%B8%AA%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[获取对象123456models.Post.objects.get(id=3)models.Post.objects.get(title="隔壁老王是谁？") # 获取 标题是隔壁老王是谁？的帖子models.Post.objects.get(title__icontains="隔壁") # 获取 标题中包含隔壁两个字 的帖子 404页面，get_object_or_4041234from django.shortcuts import get_object_or_404get_object_or_404(models.Post, id=3) 新建了一个detail.html页面用于展示的是帖子详情 12345&lt;div class="row"&gt;&lt;h1&gt;&#123;&#123;obj.title&#125;&#125;&lt;/h1&gt;&gt;&lt;p&gt;&#123;&#123;obj.content&#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;!-- /.row --&gt; 修改views.py123456789def posts_detail(request): # obj = models.Post.objects.get(id=3) obj = get_object_or_404(models.Post,id=1) data = &#123; "obj":obj &#125; return render(request,"detail.html",data) 效果 via Django1.10教程 -13 -从数据库中获取某个对象]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[12]Queryset介绍以及Template context补充]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B12%5DQueryset%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8ATemplate%20context%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[Django shell1python manage.py shell 进入django shell,可以在命令行做一些操作。 12345678910111213141516#查询出所有的post对象，for循环遍历queryset = models.Post.objects.all()for obj in queryset: print(obj.title) print(obj.content) print(obj.update) print(obj.timestamp) print(obj.id) print(obj.pk)#创建一条新的post记录models.Post.objects.create(title="abc", content="abc abc abc")#获取所有的记录条数models.Post.objects.all().count() queryset介绍queryset 是一个可以遍历取值的结果集 queryset 里面都是 对象，可以通过对象.字段名的形式取值 一个对象就对应了数据库里面的一条记录 数据库里面的一条记录 Python中的对象 如何在前端使用queryset1. 从数据库里取出数据1queryset = models.Post.objects.all() 2. 把渠道的数据塞进 data1data = &#123;"queryset": queryset&#125; 即，修改views.py 12345678910from . import modelsdef posts_home(request): queryset = models.Post.objects.all() data = &#123; "queryset": queryset, "name": "home", "age": "18", &#125; return render(request,"base.html",data) 3. 用data去填充前端的页面1render(request, "xxx.html", data) 123456789&#123;% for obj in queryset %&#125; &lt;div class="blog-post"&gt; &lt;h2 class="blog-post-title"&gt;&#123;&#123; obj.title &#125;&#125;&lt;/h2&gt; &lt;p class="blog-post-meta"&gt;&#123;&#123; obj.timestamp &#125;&#125;&lt;a href="#"&gt;Mark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&#123;&#123; obj.content &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;!-- /.blog-post --&gt;&#123;% endfor %&#125; 效果 补充 Django的模板语言语法12345&#123;% for x in xx %&#125;&#123;% endfor %&#125;&#123;&#123; val &#125;&#125; via Django1.10教程 -12 -Queryset介绍以及Template context补充]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[11]模板Template context和Bootstrap使用]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B11%5D%E6%A8%A1%E6%9D%BFTemplate%20context%E5%92%8CBootstrap%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Template context视图views.py中：1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): context = &#123; "title"： "home" &#125; return render(request,"index.html",context)def posts_create(request): context = &#123; "title"： "create" &#125; return render(request,"index.html",context)def posts_detail(request): context = &#123; "title"： "detail" &#125; return render(request,"index.html",context) index.html中用 context去填充模板 index.html，然后再返回 index.html中添加如下代码：1&lt;h1&gt;Hello &#123;&#123; title &#125;&#125;&#125;&lt;/h1&gt; context是一个字典,存放要渲染到页面的数据 Bootstrap模板使用1. 下载模板文件模板文件链接 2. 把html文件拷贝到了templates文件夹下面3. 新建一个statics文件夹，用于存放css文件和js文件4. 把css和js文件拷贝到statics文件夹下5. 在settings.py中配置statics文件夹1234STATICFILES_DIRS = [ os.path.join(BASE_DIR, "statics")] 6. 将html文件中指定位置的css和js文件的路径修改为/static/…文档结构如下： 效果如图： via Django1.10教程 -11 -模板Template context和Bootstrap使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[10]模板Template的配置]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B10%5D%E6%A8%A1%E6%9D%BFTemplate%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[BASE_DIR 和 os.path.join(xx, xxx)12345import os# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 创建一个templates目录 在settings.py中配置templates路径123456789101112131415TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR,"templates")], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 在templates文件夹下创建html文件12345678910&lt;!DOCTYPE html&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;try django&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello World! Django!&lt;/h1&gt;&gt;&lt;/body&gt;&lt;/html&gt; 在views.py中使用templates目录下的html文件12345678910# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return render(request,"index.html") 查看效果 via Django1.10教程 -10 -模板Template的配置]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[09]配置URL]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B09%5D%E9%85%8D%E7%BD%AEURL%2F</url>
    <content type="text"><![CDATA[bbs URL ConfigurationThe urlpatterns list routes URLs to views. For more information please see: https://docs.djangoproject.com/en/1.11/topics/http/urls/ Examples:Function views1. Add an import: from my_app import views 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, views.home, name=&apos;home&apos;) Class-based views1. Add an import: from other_app.views import Home 2. Add a URL to urlpatterns: url(r&apos;^$&apos;, Home.as_view(), name=&apos;home&apos;) Including another URLconf1. Import the include() function: from django.conf.urls import url, include 2. Add a URL to urlpatterns: url(r&apos;^blog/&apos;, include(&apos;blog.urls&apos;)) project(project/urls.py)中的urls.py常见的几种写法1、基于function的view详见上回 2、基于class的view1、修改posts[app]下views.py123456from django.views.generic import ListViewclass PostList(ListView): """post的视图类""" def get(self,request): return HttpResponse("&lt;h1&gt;post的视图类&lt;/h1&gt;") 2、修改bbs[project]下urls.py1234567from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), url(r'^home/', views.PostList.as_view()),] 3、效果 3、view别名12345678from app01 import views as app01_viewsfrom app02 import views as app02_viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', app01_views.posts_home), url(r'^home/', app02_views.PostList.as_view()),] 4、在app中的urls.py的使用1、posts[app]下增加urls.py：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 2、在project下的urls.py 中设置第一级的路由：12345678910from django.conf.urls import url,includefrom django.contrib import adminfrom posts import viewsfrom posts import urls as posts_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^home/', views.posts_home), # url(r'^home/', views.PostList.as_view()), url(r'^post/',include(posts_urls))] 3、在app下的urls.py中设置第二级路由，同第一步：1234567from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^home/', views.posts_home),] 多视图示例1、app下的urls.py1234567891011from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^$', views.posts_home), url(r'^create/$', views.posts_create), url(r'^update/$', views.posts_update), url(r'^detail/$', views.posts_detail), url(r'^delete/$', views.posts_delete), url(r'^', views.posts_home),] 2、app下的view.py12345678910111213141516171819202122# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!")def posts_create(request): return HttpResponse("&lt;h1&gt;posts_create&lt;/h1&gt;")def posts_update(request): return HttpResponse("&lt;h1&gt;posts_update&lt;/h1&gt;")def posts_detail(request): return HttpResponse("&lt;h1&gt;posts_detail&lt;/h1&gt;")def posts_delete(request): return HttpResponse("&lt;h1&gt;posts_delete&lt;/h1&gt;") 注： 1、如果匹配到排在上面的正则表达式那么就不会适配下面的正则表达式了 2、不要忘记加’^’和’$’ via Class-based views via Django1.10教程 -09 -配置URL]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[08]第一个view（视图）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B08%5D%E7%AC%AC%E4%B8%80%E4%B8%AAview%EF%BC%88%E8%A7%86%E5%9B%BE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[定义我们第一个视图修改posts[app]下views.py12345# Create your views here.from django.http import HttpResponsedef posts_home(request): return HttpResponse("Hello World!") url建立映射到view修改bbs[project]下urls.py123456from posts import viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', views.posts_home),] 效果 via Django1.10教程 -08 -第一个view（视图）]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[07]增删改查（CRUD）]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B07%5D%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%EF%BC%88CRUD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CRUD 缩写 动作名称 SQL HTTP 实际动作 C Create INSERT PUT/POST 增加 R Retrieve SELECT GET 查询 U Update UPDATE POST/PUT/PATCH 更新 D Delete DELETE DELETE 删除 HTTP请求方法 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 via Django1.10教程 -07 -增删改查（CRUD） via HTTP请求方法]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[06]定制admin]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B06%5D%E5%AE%9A%E5%88%B6admin%2F</url>
    <content type="text"><![CDATA[在post/admin.py文件中添加如下代码：1234567891011class PostAdmin(admin.ModelAdmin): list_display = [,] # 控制页面展示哪些字段 list_display_links = [,] # 控制哪些字段是超链接 list_filter = [,] # 支持在右侧过滤的字段 search_fields = [,] # 支持搜索的字段 list_editable = [,] # 支持直接编辑的字段，注意！不能与list_display_links重复！ class Meta: model = model.Post 补充如何修改admin中显示的app名字？ 1. 在posts/apps.py中123PostsConfig类中添加verbose_name = "帖子" 2. posts/init.py12default_app_config = "posts.apps.PostsConfig" 示例1、修改posts[app]下admin.py1234567891011121314151617181920# Register your models here.from posts import modelsclass PostAdmin(admin.ModelAdmin): """docstring for PostAdmin""" list_display = ["title","content"] list_display_links = ["title"] list_filter = ["timestamp","content"] search_fields = ["title","content"] list_editable = ["content"] # def __init__(self, arg): # super(PostAdmin, self).__init__() # self.arg = arg class Meta: model = models.Post admin.site.register(models.Post,PostAdmin) 2、修改posts[app]下apps.py1234class PostsConfig(AppConfig): name = 'posts' verbose_name = "帖子" 3、修改posts[app]下init.py12default_app_config = "posts.apps.PostsConfig" 4、查看效果 via Django1.10教程[06]_定制admin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[05]model与admin的关系]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B05%5Dmodel%E4%B8%8Eadmin%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[修改app的models中class名称尝试去掉models.py里面 把class Posts改成 class Post 一行代码让model在admin中可见1(myenvs) E:\Code\virtualenvs\myenvs\src&gt;python .\manage.py runserver via Django1.10教程[05]_model与admin的关系]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[04]App和Model]]></title>
    <url>%2F2017%2F10%2F08%2FDjango%5B04%5DApp%E5%92%8CModel%2F</url>
    <content type="text"><![CDATA[Django中project和app分别是什么？ Project：python manage.py startproject [你的project名] app: python manage.py startapp [你的app名字] project是项目，project下面分一个或多个app 我们的第一个App12python manage.py startapp [app的名字] 一定要注意：把你的app在project的settings.py里面的INSTALLED_APPS加上 Modelmodels.py里面创建类（与数据库建立联系的） 1234python manage.py makemigrations（告诉Django我设计了一些表结构，你去准备一下）python manage.py migrate（告诉Django去数据库里操作一下刚才的动作） MVC:Model View Controllers MTV:Model View Template 1、创建一个app 2、修改app目录下的models.py文件12345678910111213from django.db import modelsclass Posts(models.Model): title = models.CharField(max_length=256) # 标题，存文字的 content = models.TextField() # 内容 update = models.DateTimeField(auto_now=True,auto_now_add=False) # 更新时间 timestamp = models.DateTimeField(auto_now=False,auto_now_add=True) #创建时间 def __str__(): # python3 return self.title def __unicode__(): #python2 return self.title 3、修改project目录下的setting.py文件INSTALLED_APPS 中添加你刚创建的app名称。 4、提交修改至数据库 via Django1.10教程[04]之App和Model via Django1.10教程[04]_APPandModel补录]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[03]DjangoAdmin]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B03%5DDjangoAdmin%2F</url>
    <content type="text"><![CDATA[WSGIWSGI(Web server gateway interface)：Web服务器网关接口是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。自从WSGI被开发出来以后，许多其它语言中也出现了类似接口。 web app web server(nginx/tomcat) createsuperuser注意事项注：至少八个字符，不能是简单的数字 Django admin使用的介绍 python manage.py runserver 127.0.0.1:8000 浏览器里面输入：http://127.0.0.1:8000/admin/ 输入用户名、密码登录即可。 urls.py使用的介绍django的路由是通过正则表达式来匹配的。 via Django1.10教程[03]之DjangoAdmin via Django1.10教程[03]之DjangoAdmin]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[01]Virtualenv&Django]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B01%5DVirtualenv%26Django%2F</url>
    <content type="text"><![CDATA[配置虚拟环境安装virtualenv:12pip install virtualenv 使用virtualenv创建虚拟环境：virtualenv [环境（文件夹）名] 启用虚拟环境：.\Scripts\activate 退出虚拟环境：deactivate 安装Django12pip install django 第一个Django项目1、python manage.py startproject my_first(我们的项目名) 2、python manage.py runserver 127.0.0.1:8000 补充： 1、PowerShell里面执行activate失败？ 输入： set-executionpolicy RemoteSigned 2、pip freeze命令使用 1、pip freeze &gt; requirements.txt (保存依赖包到requirements.txt) 2、pip install -r requirements.txt (批量安装项目需要的所有依赖包) via Django1.10教程[01]之virtualenv使用]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django[02]第一个django程序]]></title>
    <url>%2F2017%2F10%2F07%2FDjango%5B02%5D%E7%AC%AC%E4%B8%80%E4%B8%AAdjango%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[创建一个bbs项目 bbs可以作为一个独立的app部署，重命名文件名为src 同步数据表：python .\manage.py migrate 修改setting.py配置文件 创建超级用户：python .\manage.py createsuperuser 启动服务：python .\manage.py runserver 浏览器中输入http://127.0.0.1:8000/admin/进入django的管理后台 via Django1.10教程[02]之第一个Django程序]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error Analysis (Post-Modeling)]]></title>
    <url>%2F2017%2F10%2F07%2FError%20Analysis%20(Post-Modeling)%2F</url>
    <content type="text"><![CDATA[Error analysis is a broad term that refers to analyzing the misclassified or high error observations from your model and deciding on your next steps for improvement. This is performed after training your first model. Possible next steps include collecting more data, splitting the problem apart, or engineering new features that address the errors. To use error analysis for feature engineering, you’ll need to understand why your model missed its mark. Here’s how: Start with larger errors: Error analysis is typically a manual process. You won’t have time to scrutinize every observation. We recommend starting with those that had higher error scores. Look for patterns that you can formalize into new features. Segment by classes: Another technique is to segment your observations and compare the average error within each segment. You can try creating indicator variables for the segments with the highest errors. Unsupervised clustering: If you have trouble spotting patterns, you can run an unsupervised clustering algorithm on the misclassified observations. We don’t recommend blindly using those clusters as a new feature, but they can make it easier to spot patterns. Remember, the goal is to understand why observations were misclassified. Ask colleagues or domain experts: This is a great complement to any of the other three techniques. Asking a domain expert is especially useful if you’ve identified a pattern of poor performance (e.g. through segmentations) but don’t yet understand why.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 Tips for Writing Better Python]]></title>
    <url>%2F2017%2F10%2F07%2F5%20Tips%20for%20Writing%20Better%20Python%2F</url>
    <content type="text"><![CDATA[1. Make your code a PIP-installable PackageWhen you come across a new Python package, it’s always easier to start using it if all you have to do is run “pip install” followed by the package name or location. There are a number of ways to do this, my “go to” being to create a setup.py file for my project. Assume we have a simple Flask program in “flask_example.py”: 12345678910111213from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello, World!'def main(): app.run()if __name__ == ‘__main__’: main() We can turn this into an installable Python package by first moving it into a separate folder (let’s call this “flask_example/”. Then, we can make a setup.py file in the root project folder that looks like this: 123456789101112131415from distutils.core import setupsetup( name='flask_example', version='1.0', description='Hello, World! in flask.', packages=['flask_example'], install_requires=[ 'Flask==0.12.2' ], entry_points = &#123; 'console_scripts': 'runserver=flask_example.flask_example:main' &#125;) This has a few advantages that comes with it. First, you can now install your app locally using “pip install -e .” This makes it easier for developers to clone and install your project because the setup.py file will take care of all the heavy lifting. Second, with the setup.py file comes dependency management. The install_requires variable allows you to define packages and specific versions to use. If you’re not sure what packages and versions you are using, run “pip freeze” to view them. Lastly, this allows you to define entry points for your package, which allows you to now execute the code on the command line by simply running “runserver”. 2. Lint Your Code in a Pre-Commit HookUsing a linter can fix so many problems in code. PyLint is a great linter for Python, and if you’re using a version control system like Git, you can make Git run your code through a linter before it lets you commit your code. To do this, install the PyLint package. 1pip install pylint Then, add the following code to .git/hooks/pre-commit. If you already have a pre-commit hook doing something, simple append the pylint command to the end of your file. 123#!/bin/shpylint &lt;your_package_name&gt; This will catch all kinds of mistakes before they even make it into your Git repository. You’ll be able to say goodbye to accidentally pushing code with syntax errors, along with the many other things a good linter catches. 3. Use Absolute Imports over Relative ImportsIn python, there are very few situations where you should be using relative module paths in your import statements (e.g. from . import module_name). If you’ve gone through the process of creating a setup.py (or similar mechanism) for your Python project, then you can simply reference submodules by their full module path. Absolute imports are recommended by PEP-8, the Python style guide. This is because they’re more informative in their names and, according to the Python Software Foundation, are “better behaved.” I’ve been in positions where using relative imports has quickly become a nightmare. It’s fine when you first start coding, but once you start moving modules around and doing significant refactoring, they can really cause quite the headache. 4. Context ManagersWhenever you’re opening a file, stream, or connection, you’re usually working with a context manager. Context managers are great because when used properly they can handle the closing of your file should an exception be thrown. In order to do this, simply use the with keyword. The following is how most beginner Python programmers would probably write to a file. 1234f = open(‘newfile.txt’, ‘w’)f.write(‘Hello, World!’)f.close() This is pretty straightforward. But imagine this: You’re writing thousands of lines to a file. At some point, an exception is thrown. After that happens, your file isn’t properly closed, and all the data you thought you had already written to the file is corrupt or non-existent. Don’t worry though, with some simple refactoring we can ensure the file closes properly, even if an exception is encountered. We can do this as shown below. 123with open(‘file’, ‘w’) as file: file.write(‘Hello, World!’) Volla! It really is that simple. Additionally, the code looks much cleaner like this, and is more concise. You can also open multiple context managers with a single “with” statement, eliminating the need to have nested “with” statements. 1234with open(‘file1’, ‘w’) as f1, open(‘file2’, ‘w’) as f2: f1.write(‘Hello’) f2.write(‘World’) 5. Use Well-Named Functions and VariablesIn Python, and untyped languages especially, it can easily become unclear what functions are returning what values. Especially when you’re just a consumer of some functions in a library. If you can save the developer the 5 minutes it takes to look up the function in your documentation, than that’s actually a really valuable improvement. But how do we do this? How can doing something as simple as changing the name of a variable save development time? There are 3 main things I like to take into consideration when naming a function or variable: What the function or variable does Any units associated with the function or variable The data type the function or variable evaluates to For example, if I want to create a function to calculate the area of a rectangle, I might name it “calc_rect_area”. But this doesn’t really let the user know much. Is it going to return the value, or is it going to store it somewhere? Is the value in feet? Meters? To enhance the name of this function, I would change it to “get_rect_area_sq_ft”. This makes it clear to the user that the function gets and returns the area. It also lets the user know that the area is in square feet. If you can save the developer 5 minutes here and there with some nicely named functions and variables, that time starts to add up, and they’ll appreciate your code all the more. ConclusionThese tips are ones that I have found to be helpful over my years as a Python programmer. Some of them I figured out on my own over time, some of them others had to teach me so I could learn them. I hope this list helps you in your effort to write better Python. via here]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可视化—matploblib 解决中文显示的问题]]></title>
    <url>%2F2017%2F10%2F07%2F%E5%8F%AF%E8%A7%86%E5%8C%96%E2%80%94matploblib%20%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[导入相关包123from matplotlib import mplimport matplotlib.pyplot as plt 指定字体123mpl.rcParams['font.sans-serif'] = ['SimHei']mpl.rcParams['axes.unicode_minus'] = False 测试：使用中文12plt.title(u'我是中文')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（五）之增加高阶特征]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8B%E5%A2%9E%E5%8A%A0%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解分割模型带来了模型效果的提升。这节我们尝试通过增加高阶特征来增强模型的表达能力。由于备选变量比较多，盲目的全部生成高阶特征会造成特征数量指数级的爆炸式增长，这个不是我想要的，所以先根据IV值进行单变量特征选择，筛选比较重要的变量，然后再次基础上才生成高阶特征。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp from sklearn.preprocessing import PolynomialFeatures get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') poly_features = [] y_train = dataset_train['target'] poly = PolynomialFeatures(2) X_train = poly.fit_transform(dataset_train[poly_features]) del dataset_train import gc gc.collect() # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df_poly.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path Poly') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance Poly') plt.axis('tight') plt.show() fig2.show() fig1.show() 注：X_train第一列是常数项，就是系数图中的那条蓝色线。 从模型的表现来看，模型效果并没有显著提升，根据奥卡姆剃刀原理，放弃采用高阶特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（六）之模型融合]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到增加高阶特征并没有带来模型效果显著提升。考虑到sigmoid函数在0.5处附近区分度比较大，在远离0.5处概率值变换不是很明显，考虑sigmoid梯度平缓的地方，拆分成&lt;0.3和&gt;0.7分别单独领出来，重新建模，期望能提升模型效果。 按照0.3和0.7两个分割点划分，需要重新再训练出三个模型，然后再把这三个模型整体的效果与未重新训练前的效果进行对比。 首先定义几个函数，省得写一些重复的代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240 def plot_ks(proba,target,axistype=0,out_path=False): a = pd.DataFrame(np.array([proba,target]).T,columns=['proba','target']) a.sort_values(by='proba',ascending=False,inplace=True) a['sum_Times']=a['target'].cumsum() total_1 = a['target'].sum() total_0 = len(a) - a['target'].sum() a['temp'] = 1 a['Times']=a['temp'].cumsum() a['cdf1'] = a['sum_Times']/total_1 a['cdf0'] = (a['Times'] - a['sum_Times'])/total_0 a['ks'] = a['cdf1'] - a['cdf0'] a['percent'] = a['Times']*1.0/len(a) idx = np.argmax(a['ks']) # print a.loc[idx] if axistype == 0: ''' KS曲线,横轴为按照输出的概率值排序后的观察样本比例 ''' plt.figure() plt.plot(a['percent'],a['cdf1'], label="CDF_positive") plt.plot(a['percent'],a['cdf0'],label="CDF_negative") plt.plot(a['percent'],a['ks'],label="K-S") sx = np.linspace(0,1,10) sy = sx plt.plot(sx,sy,linestyle='--',color='darkgrey',linewidth=1.2) plt.legend() plt.grid(True) ymin, ymax = plt.ylim() plt.xlabel('Sample percent') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') # 虚线 t = a.loc[idx]['percent'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(a.loc[idx]['percent'], a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) else: ''' 改变横轴,横轴为模型输出的概率值 ''' plt.figure() plt.grid(True) plt.plot(1-a['proba'],a['cdf1'], label="CDF_bad") plt.plot(1-a['proba'],a['cdf0'],label="CDF_good") plt.plot(1-a['proba'],a['ks'],label="ks") plt.legend() ymin, ymax = plt.ylim() plt.xlabel('1-[Predicted probability]') plt.ylabel('Cumulative probability') plt.title('Model Evaluation Index K-S') plt.axis('tight') plt.show() # 虚线 t = 1 - a.loc[idx]['proba'] yb = round(a.loc[idx]['cdf1'],4) yg = round(a.loc[idx]['cdf0'],4) plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--") plt.scatter([t,],[yb,], 20, color ='dodgerblue') plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.scatter([t,],[yg,], 20, color ='darkorange') plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10), textcoords='offset points', fontsize=8, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) # K-S曲线峰值 plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen') plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4) ,round(a.loc[idx]['proba'],4)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+15, -15), textcoords='offset points' , fontsize=8 ,arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.1")) plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4) ,round(a.loc[idx]['Times'],0)) , xy=(t, a.loc[idx]['ks']) , xycoords='data' , xytext=(+25, -25), textcoords='offset points' , fontsize=8 ) if out_path: file_name = out_path if isinstance(out_path, str) else None plt.savefig(file_name) else: plt.show() return a.loc[idx] ''' 搜索 最优超参数c ''' def grid_search_lr_c(X_train,y_train,df_coef_path=False ,pic_coefpath_title='Logistic Regression Path',pic_coefpath=False ,pic_performance_title='Logistic Regression Performance',pic_performance=False): # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01,class_weight='balanced') cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df['ks'] = ks coef_cv_df['c'] = cs # df_coef_path = 'E:\\Code\\ScoreCard\\agr_coef_cv_df_balanced_nextrain_beta.csv' if df_coef_path: file_name = df_coef_path if isinstance(df_coef_path, str) else None coef_cv_df.to_csv(file_name) coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') # pic_coefpath_title = 'Logistic Regression Path Class_weight Balanced Nextrain_beta' plt.title(pic_coefpath_title) plt.axis('tight') if pic_coefpath: file_name = pic_coefpath if isinstance(pic_coefpath, str) else None plt.savefig(file_name) else: plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') # pic_performance_title = 'Logistic Regression Performance Class_weight balanced Nextrain_beta' plt.title(pic_performance_title) plt.axis('tight') if pic_performance: file_name = pic_performance if isinstance(pic_performance, str) else None plt.savefig(file_name) else: plt.show() flag = coefs_&lt;0 idx = np.array(ks)[flag.sum(axis=1) == 0].argmax() return (cs[idx],ks[idx])``` **然后拆分模型，合并计算ks,将一个已经预测后的模型，根据输出的概率值，给定两个分割点，划分区间重新用LR训练。**``` python def lr_sub_train(X_train,y_train,idx): index = idx sub_xtrain = X_train.loc[index,:] sub_ytrain = y_train.loc[index] c,ks = grid_search_lr_c(sub_xtrain,sub_ytrain) clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced') clf_l1_LR.fit(sub_xtrain, sub_ytrain) sub_proba = clf_l1_LR.predict_proba(sub_xtrain)[:,1] return c,ks,sub_proba,sub_ytrain def train_segmentation_model(X_train,y_train,proba_train,left_point,right_point): idx1 = (proba_train &lt;= left_point) idx2 = (proba_train &gt; left_point)&amp;(proba_train &lt;= right_point) idx3 = (proba_train &gt; right_point) c1,ks1,sub_proba1,sub_ytrain1 = lr_sub_train(X_train,y_train,idx1) c2,ks2,sub_proba2,sub_ytrain2 = lr_sub_train(X_train,y_train,idx2) c3,ks3,sub_proba3,sub_ytrain3 = lr_sub_train(X_train,y_train,idx3) print 'c1:',c1,' ks1:',ks1 print 'c2:',c2,' ks2:',ks2 print 'c3:',c3,' ks3:',ks3 proba = [] proba.extend(sub_proba1) proba.extend(sub_proba2) proba.extend(sub_proba3) y = [] y.extend(sub_ytrain1) y.extend(sub_ytrain2) y.extend(sub_ytrain3) return get_ks(np.array(proba),np.array(y)) # 0.29072674001489612 输出 1234567891011121314151617'''Computing regularization path ...2017-10-03 16:04:45.2290002017-10-03 16:07:12.977000('This took ', datetime.timedelta(0, 147, 748000))Computing regularization path ...2017-10-03 16:07:28.1180002017-10-03 16:08:59.740000('This took ', datetime.timedelta(0, 91, 622000))Computing regularization path ...2017-10-03 16:09:03.4750002017-10-03 16:09:31.128000('This took ', datetime.timedelta(0, 27, 653000))c1: 0.00663966525938 ks1: 0.208779933516c2: 0.00189750388742 ks2: 0.201006811034c3: 0.000415735598049 ks3: 0.169741554117''' 结果并没有按照我想象的那样，反而分割后的效果没有单一模型的效果好。原因是woe变换没有重新训练?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（四）之分割模型]]></title>
    <url>%2F2017%2F10%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到根据业务理解剔除噪声样本带来了模型效果的提升。同时也感受到了“与此相关”变量的重要，在这个字段上不同取值的客户所对应的入模特征分布也许有很大差异，于是我尝试了根据这个字段分割建模，期望能达到提升的效果。 按条件分割数据集： agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 1 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 2 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 == 3 agr_dd1 + agr_dd2 + agr_dd3 + agr_dd4 &gt;= 4 在分割出的4个训练集上分布训练WOE转换，然后对测试集进行对应的WOE转换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339__author__ = 'boredbird'import pandas as pdimport woe.config as configimport woe.feature_process as fpimport woe.eval as evalimport numpy as npimport pickleimport matplotlib.pyplot as pltfrom datetime import datetimefrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import l1_min_cfrom scipy.stats import ks_2sampprint '*************************************A****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************B****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv')print '*************************************C****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_test_20160406_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************D****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv')print '*************************************E****************************************************************'dataset = pd.read_csv('E:\\ScoreCard\\pos_model_var_tbl_validation_20160806_agr.csv')dataset_dd1 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 1,:]dataset_dd2 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 2,:]dataset_dd3 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] == 3,:]dataset_dd4 = dataset.loc[dataset['agr_dd1']+dataset['agr_dd2']+dataset['agr_dd3']+dataset['agr_dd4'] &gt;= 4,:]print '*************************************F****************************************************************'dataset_dd1.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv')dataset_dd2.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv')dataset_dd3.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv')dataset_dd4.to_csv('E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv')# 'E:\\Code\\ScoreCard\\whitelist_ext_civ_list_alpha.pkl'# 'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr.csv'def process_train_woe(infile_path=None,outfile_path=None,rst_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = infile_pathcfg = config.config()cfg.load_file(config_path,data_path)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1# change feature dtypesfp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)rst = []# process woe transformation of continuous variablesprint 'cfg.global_bt',cfg.global_btprint 'cfg.global_gt', cfg.global_gtfor var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))# process woe transformation of discrete variablesfor var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'rst.append(fp.proc_woe_discrete(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05))eval.eval_feature_detail(rst, outfile_path)output = open(rst_path, 'wb')pickle.dump(rst,output)output.close()print '*************************************G****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl')print '*************************************H****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl')print '*************************************I****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl')print '*************************************J****************************************************************'process_train_woe(infile_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,outfile_path='E:\\Code\\ScoreCard\\whitelist_ext_feature_detail_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl')def process_woe_trans(in_data_path=None,rst_path=None,out_path=None):config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv'data_path = in_data_pathcfg = config.config()cfg.load_file(config_path, data_path)fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type)for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]:# fill nullcfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing'output = open(rst_path, 'rb')rst = pickle.load(output)output.close()# Training dataset Woe Transformationfor r in rst:cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name], r)cfg.dataset_train.to_csv(out_path)print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv')print '*************************************O****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv')print '*************************************P****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv')print '*************************************Q****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv')print '*************************************R****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv')print '*************************************K****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd1.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv')print '*************************************L****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd2.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv')print '*************************************M****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd3.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv')print '*************************************N****************************************************************'process_woe_trans(in_data_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4.csv' ,rst_path='E:\\Code\\ScoreCard\\whitelist_ext_civ_list_dd4.pkl' ,out_path='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv')"""Training Model"""get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticdef train_model(infile,outfile,fig1,fig2):dataset_train = pd.read_csv(infile)cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']# init a LogisticRegression modelclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3)print("Computing regularization path ...")start = datetime.now()print startcoefs_ = []ks = []for c in cs:clf_l1_LR.set_params(C=c)clf_l1_LR.fit(X_train, y_train)coefs_.append(clf_l1_LR.coef_.ravel().copy())proba = clf_l1_LR.predict_proba(X_train)[:,1]ks.append(get_ks(proba,y_train))end = datetime.now()print endprint("This took ", end - start)coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns)coef_cv_df['ks'] = kscoef_cv_df['cs'] = cscoef_cv_df.to_csv(outfile)coefs_ = np.array(coefs_)plt.figure()plt.plot(np.log10(cs), coefs_)ymin, ymax = plt.ylim()plt.xlabel('log(C)')plt.ylabel('Coefficients')plt.title('Logistic Regression Path')plt.axis('tight')plt.savefig(fig1)plt.close()plt.figure()plt.plot(np.log10(cs), ks)plt.xlabel('log(C)')plt.ylabel('ks score')plt.title('Logistic Regression Performance')plt.axis('tight')plt.savefig(fig2)plt.close()train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd1.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd1.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd1.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd2.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd2.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd2.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd3.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd3.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd3.png')train_model(infile='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv',outfile='E:\\ScoreCard\\coef_cv_df_agr_dd4.csv',fig1='E:\\ScoreCard\\LR_Path_cv_agr_dd4.png',fig2='E:\\ScoreCard\\LR_Performance_cv_agr_dd4.png') """Model Performance"""def predict_model(train_file,test_file,dataset_validation_path,c):print '*********************************************************'cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name']clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced')dataset_train = pd.read_csv(train_file)dataset_test = pd.read_csv(test_file)dataset_validation = pd.read_csv(dataset_validation_path)# fill nullfor var in candidate_var_list:if dataset_validation[var].isnull().sum()&gt;0:dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean()if dataset_test[var].isnull().sum()&gt;0:dataset_test.loc[dataset_test[var].isnull(), (var)] = dataset_test[var].mean()X_train = dataset_train[candidate_var_list]y_train = dataset_train['target']X_test = dataset_test[candidate_var_list]y_test = dataset_test['target']clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]print get_ks(proba,y_train)proba = clf_l1_LR.predict_proba(X_test)[:,1]print get_ks(proba,y_test)# predictionproba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1]print get_ks(proba,dataset_validation['target']) predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd1_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd1_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd1_woe_trans.csv' ,c=0.00948742173925)'''agr_dd1:0.3913372974170.4017604347210.41978303981'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd2_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd2_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd2_woe_trans.csv' ,c=0.00949207179031)'''agr_dd2:0.428815152590.4253306159710.423628758103'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd3_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd3_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd3_woe_trans.csv' ,c=0.0063130606165)'''agr_dd3:0.4448530527120.4429655932780.429972957194'''predict_model(train_file='E:\\ScoreCard\\pos_train_20160406_agr_dd4_woe_trans.csv' ,test_file='E:\\ScoreCard\\pos_test_20160406_agr_dd4_woe_trans.csv' ,dataset_validation_path='E:\\ScoreCard\\pos_validation_20160806_agr_dd4_woe_trans.csv' ,c=0.00479980706543)'''agr_dd4:0.456283776350.4451460672930.464371772806''' 结论由切分成4个数据集分别的ks表现来看，分割后的模型表现差异比较明显，并且agr_dd1，agr_dd2，agr_dd3，agr_dd4的ks逐渐提升，这也不难寻求业务理解。可见，在这个字段上分割模型是有意义。 逻辑回归是线性的且单一的分类器，即使我们把这4个数据集增加一个‘A’,’B’,’C’,’D’的字段标识，然后合并为一个数据集，也不能够达到分割的效果。分割模型相对于只是分割数据集会进一步增强模型的表现能力。 分割数据集 分割模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python本地模块名与系统安装的包名重名冲突问题]]></title>
    <url>%2F2017%2F10%2F04%2F%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9D%97%E5%90%8D%E4%B8%8E%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%E5%90%8D%E9%87%8D%E5%90%8D%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本地修改 woe包中的eval模块时，发现 系统老是import&quot;D:\ProgramData\Anaconda2\lib\site-packages\woe\eval.py&quot;路径下的eval文件。那是因为我已经安装了woe这个包，然后需要调用我修改后的woe源码的本地模块文件，系统自动优先import的是之前安装的woe版本。 解决方法如下： 1234567891011#-*-coding:utf-8-*-__author__='boredbird'# importwoe.configasconfig# importwoe.feature_processasfp# importwoe.evalasevalimport impconfig = imp.load_source('config',r'E:\Code\woe\woe\config.py')fp = imp.load_source('feature_process',r'E:\Code\woe\woe\feature_process.py')eval = imp.load_source('eval',r'E:\Code\woe\woe\eval.py') For Python 3.5+ use: 123456import importlib.utilspec = importlib.util.spec_from_file_location("module.name", "/path/to/file.py")foo = importlib.util.module_from_spec(spec)spec.loader.exec_module(foo)foo.MyClass() For Python 3.3 and 3.4 use: 12345from importlib.machinery import SourceFileLoaderfoo = SourceFileLoader("module.name", "/path/to/file.py").load_module()foo.MyClass()# (Although this has been deprecated in Python 3.4.) For Python 2 use: 1234import impfoo = imp.load_source('module.name', '/path/to/file.py')foo.MyClass() There are equivalent convenience functions for compiled Python files and DLLs.See it here. 另外一种方式，将要导入的模块路径添加到系统路径，并且放在&quot;D:\ProgramData\Anaconda2\lib\site-packages\这个路径前面。比如， 1234import sys; print('Python %s on %s' % (sys.version, sys.platform))sys.path.extend(['E:\\Code\\Python_Crawler', 'E:\\Code\\Python_Exercise_Code', 'E:\\Code\\Python_ML_Code', 'E:/Code/Python_Crawler']) 然后再讲你刚添加的路径元素调整在sys.path这个list中的位置。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（三）之剔除噪声]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E5%89%94%E9%99%A4%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[在上回的基础上，我们可以看到正则化带来了模型效果的提升。此时，在对建模宽表的加工过程检查发现，有一部分用户在特征加工时间窗口observe_date之前没有表现期，这部分的用户放在样本中其实是噪声，我们将之剔除，重新woe转换，然后再入模查看效果。 首先，搜索最优正则化参数c： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107 __author__ = 'boredbird' import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import l1_min_c from scipy.stats import ks_2samp import woe.config as config import woe.feature_process as fp import woe.eval as eval get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) # init a LogisticRegression model clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01) cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show() print ks print cs``` ![](https://i.imgur.com/812KAjt.png)![](https://i.imgur.com/dAO4rdb.png)然后，将最优正则化参数c代入，训练模型与预测：``` python c = 0.00211473526246312 clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01) ''' 预测并查看模型效果 ''' clf_l1_LR.fit(X_train, y_train) proba = clf_l1_LR.predict_proba(X_train)[:,1] get_ks(proba,y_train) # 0.42519514636341194 proba = clf_l1_LR.predict_proba(X_test)[:,1] get_ks(proba,y_test) # 0.42014866529356498 dataset_validation = pd.read_csv('E:\\ScoreCard\\pos_validation_20160806_agr_woe_trans.csv') # fill null for var in candidate_var_list: if dataset_validation[var].isnull().sum()&gt;0: dataset_validation.loc[dataset_validation[var].isnull(), (var)] = dataset_validation[var].mean() # prediction proba = clf_l1_LR.predict_proba(dataset_validation[candidate_var_list])[:,1] get_ks(proba,dataset_validation['target']) # 0.42007842928689282 可以看出，剔除一些噪声样本之后模型的效果有很大的提升，而且此时，训练集的效果与同时期验证，跨时期验证的效果都很接近，可以认为没有发生过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（二）之正则化]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[在上回的基础上，可以看出特征之间存在多重共线性，自然的联想到L1正则化，这次主要尝试用sklearn.linear_model中正则化的LogisticRegression去提升模型效果。 12345678910111213141516171819X = dataset_train[candidate_var_list]y = dataset_train['target']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)from sklearn.linear_model import LogisticRegressionclf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)clf_l1_LR.fit(X_train, y_train)proba = clf_l1_LR.predict_proba(X_train)[:,1]get_ks(proba,y_train)# 0.40141223793092612proba = clf_l1_LR.predict_proba(X_test)[:,1]get_ks(proba,y_test)# 0.39708735770404702 可以看出此时采用C=0.1正则化的模型已经有了很大的提升。 123456C : float, default: 1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. 可是，c是个超参数，如何设置才能达到最优的效果呢？ 还是以结果为导向，一个一个试。虽然不是全局最优，那也大差不差。 import matplotlib.pyplot as plt from datetime import datetime from sklearn import linear_model from sklearn.svm import l1_min_c cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3) print("Computing regularization path ...") start = datetime.now() print start clf_l1_LR = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6) coefs_ = [] ks = [] for c in cs: clf_l1_LR.set_params(C=c) clf_l1_LR.fit(X_train, y_train) coefs_.append(clf_l1_LR.coef_.ravel().copy()) proba = clf_l1_LR.predict_proba(X_train)[:,1] ks.append(get_ks(proba,y_train)) end = datetime.now() print end print("This took ", end - start) coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns) coef_cv_df.to_csv('E:\\Code\\ScoreCard\\coef_cv_df.csv') coefs_ = np.array(coefs_) fig1 = plt.figure('fig1') plt.plot(np.log10(cs), coefs_) ymin, ymax = plt.ylim() plt.xlabel('log(C)') plt.ylabel('Coefficients') plt.title('Logistic Regression Path') plt.axis('tight') plt.show() fig2 = plt.figure('fig2') plt.plot(np.log10(cs), ks) plt.xlabel('log(C)') plt.ylabel('ks score') plt.title('Logistic Regression Performance') plt.axis('tight') plt.show() fig2.show() fig1.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归评分卡调优（一）之数据集初探]]></title>
    <url>%2F2017%2F10%2F04%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[本系列记录我在工作中针对同一个数据集，运用逻辑回归（评分卡）模型不断尝试去提升模型ks值表现的过程。并不是范例，更谈不上指导意义。主要是为了记录自己的思考与验证过程，怕随便零散的写在代码注释里过后就忘了。 评分卡模型在数据处理过程中常用到woe转换，这部分的工作我采用了Python的包woe来完成。 Woe is a python package containing useful tools for WoE Transformation mostly used in ScoreCard Model for Credit Rating. woe包中树节点的分裂方法，是一种Local近似算法。对于每个特征，只考察分位点，减少计算复杂度；每次分裂前，重新提出候选切分点。 关于树节点分裂方法，这里有简单的介绍。 #数据初步处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170 # -*- coding:utf-8 -*- __author__ = 'boredbird' import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np import copy ''' 导入数据集与配置文件 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) ''' 填充空值 ''' for var in cfg.bin_var_list: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] columns = ['cus_id'] columns.extend(cfg.candidate_var_list) columns.append('target') dataset_train = cfg.dataset_train[columns] ''' 删除不用的变量，导出数据集，减少内存占用 ''' dataset_train.to_csv('E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv') ''' 重新导入数据集 并指定变量类型 ''' config_path = 'E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv' data_path = 'E:\\ScoreCard\\pos_model_var_eliminated_train_20160206.csv' cfg = config.config() cfg.load_file(config_path,data_path) for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 # change feature dtypes fp.change_feature_dtype(cfg.dataset_train, cfg.variable_type) rst = [] # process woe transformation of continuous variables for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: rst.append(fp.proc_woe_continuous(cfg.dataset_train,var,cfg.global_bt,cfg.global_gt,cfg.min_sample,alpha=0.05)) ''' 保存rst至文件 ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'wb') pickle.dump(rst,output) output.close() ''' 格式化输出feature_detail，查看每个变量的iV值 ''' feature_detail = eval.eval_feature_detail(rst,'E:\\Code\\ScoreCard\\whitelist_ext_feature_detail.csv')``` #Woe 转换 ``` python ''' Woe Transformation ''' ''' 加载用于存储分割点信息的列表rst ''' import pickle output = open('E:\\Code\\ScoreCard\\whitelist_ext_civ_list.pkl', 'rb') rst = pickle.load(output) output.close() ''' FILL NULL 因为在生成rst之前做了空值填充处理， 所以在利用rst进行woe转换之前，也要分别对连续变量和离散变量做同样的空值填充。 ''' for var in [tmp for tmp in cfg.bin_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(cfg.dataset_train.columns)]: # fill null cfg.dataset_train.loc[cfg.dataset_train[var].isnull(), (var)] = 'missing' #Training dataset Woe Transformation for r in rst: cfg.dataset_train[r.var_name] = fp.woe_trans(cfg.dataset_train[r.var_name],r) cfg.dataset_train.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206.csv') ''' Validation Dataset Transformation ''' #load dataset and eliminate variables dataset_validation = pd.read_csv('E:\\ScoreCard\\rawdata\\whitelist\\pos_model_var_tbl_validation_20160806.csv') var_exists_list = [rst[i].var_name for i in range(len(rst))] var_exists_list.extend(['cus_id','target',]) columns_to_drop = [var for var in dataset_validation.columns if var not in var_exists_list] dataset_validation = dataset_validation.drop(columns_to_drop,axis=1) #change variable dtypes before woe transformation for var in [tmp for tmp in cfg.bin_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = -1 for var in [tmp for tmp in cfg.discrete_var_list if tmp in list(dataset_validation.columns)]: # fill null dataset_validation.loc[dataset_validation[var].isnull(), (var)] = 'missing' #Validation dataset Woe Transformation for r in rst: if r.var_name in dataset_validation.columns: dataset_validation[r.var_name] = woe_trans(dataset_validation[r.var_name],r) dataset_validation.to_csv('E:\\ScoreCard\\pos_model_woe_transformated_validation_20160806.csv')``` #跑Logit Regression 基于woe替换后的变量，删除iV值比较低的（删除了iv小于0.01的变量），调用statsmodels跑逻辑回归。``` python import os import pandas as pd import woe.config as config import woe.feature_process as fp import woe.eval as eval import numpy as np dataset_train = pd.read_csv('E:\\ScoreCard\\pos_model_woe_transformated_train_20160206_new.csv') cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv') candidate_var_list = cfg[cfg['is_modelfeature'] == 1]['var_name'] import numpy as np from sklearn.model_selection import train_test_split X = dataset_train[candidate_var_list] y = dataset_train['target'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=32) import statsmodels.api as sm logit_mod = sm.Logit(y_train,X_train) logit_res = logit_mod.fit(disp=0) # print('Parameters: ', logit_res.params) print logit_res.summary() 由上表可以看出当前这个模型存在两个问题： 1、存在变量系数显著性检验p值不通过的变量； 2、存在coef系数为负的情况，本身经过了woe转换后的变量的woe值与LR的概率值是正相关关系，系数应该为正，这种情况应该是变量之间的多重共线性导致的。 #模型评价，K-S 1234567891011121314from scipy.stats import ks_2sampget_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statisticproba = logit_res.predict(X_train)get_ks(proba,y_train)# 0.35121344450518605proba = logit_res.predict(X_test)get_ks(proba,y_test)# 0.34393694879059489proba = logit_res.predict(dataset_validation[features_list])get_ks(proba,dataset_validation['target'])# 0.35956755211806057]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM与XGBoost对比]]></title>
    <url>%2F2017%2F10%2F02%2FLightGBM%E4%B8%8EXGBoost%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[XGBoost的系统设计 更高效的工具包LightGBM LightGBM的改进直方图算法 直方图加速 建树过程的两种方法：Level-wise和Leaf-wise 并行优化（Optimization in parallel learning）特征并行 数据并行 GOSS EFB 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT算法原理与系统设计简介-来自wepon的分享]]></title>
    <url>%2F2017%2F10%2F02%2FGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%AE%80%E4%BB%8B-%E6%9D%A5%E8%87%AAwepon%E7%9A%84%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[一、泰勒公式 定义：泰勒公式是一个用函数在某点的信息描述其附近取值的公式。局部有效性二、最优化方法2.1 梯度下降法（Gradient descend method）2.2 牛顿法（Newton’s method）三、从参数空间到函数空间3.1 从Gradient descend 到 Gradient Boosting3.2 从Newton’s method 到 Newton Boosting四、Gradient Boosting Tree算法原理五、Newton Boosting Tree算法原理：详解XGBoost5.1 模型函数形式5.2 目标函数5.2.1 正则项5.2.2 误差函数的二阶泰勒展开5.3 回归树的学习策略5.3.1 打分函数5.3.2 树节点分裂方法5.3.3 缺失值的处理5.4 其它特性]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池老司机天音的经验分享]]></title>
    <url>%2F2017%2F10%2F02%2F%E5%A4%A9%E6%B1%A0%E8%80%81%E5%8F%B8%E6%9C%BA%E5%A4%A9%E9%9F%B3%E7%9A%84%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[化繁为简，各个突破 一、数据处理 25%1.1 缺失值填充某些场景中缺失值也是一个重要特征，如金融信贷，资料完善度不高的用户是潜在的违约对象 数值型：均值，中位数 类别型：众数 通过关联信息补全1.2 数据清洗 剔除缺失值比率大的字段 离群点剔除1.3 数据处理1.3.1 异常数据的处理 log平滑 局部加权1.3.2 数据划分，样本构造 增：增加样本，滑窗采样 减：去掉异常样本，可以借助模型判断 采样：效率和效果的平衡 根据需求构建可靠的验证集 注：线上线下Label窗口大小要一致；滑窗是为了增加样本量，一般把多个窗口的数据集合在一起训练；各窗口构成的数据集Label Window无重叠，特征窗口可以重叠； 1.3.3 噪声样本剔除用一个树模型去训练你的数据，然后用得到的模型去预测数据，同一个叶子节点下偏离叶子节点均值最大的认为它是一个噪声，然后对它的差值进行排序取出前top多少个，把它去掉，然后重新训练一个比较干净的模型 二、特征工程 50%特征没做好，参数调到老 2.1 特征提取结合具体的业务场景思考问题 特征统计：key拆分，组合（按照提供的数据集粒度，不同的维度去交叉组合出更多的指标） 时间窗口统计：3/5/7/15/30 2.2 特征处理 归一化（加快训练速度并且更可能达到最优解，线性模型，对数值大小敏感的模型） onehot编码（增加哑变量） 排序 2.3 特征选择 计算特征与标签相关度（按照相关性大小排序筛选特征） LR+L1正则 树模型对特征打分（用的最多的是XGBoost，RF去对特征打分，然后筛选重要的变量再放到线性模型里面）2.4 特征组合 任意两两组合：维度大 特征选择再组合 GBDT+LR（GBDT每棵树从根节点到叶子节点的路径是一种最优组合） 三、模型设计 20%3.1 分类-回归问题 Xgboost：速度快，效果好 GBDT：拟合能力强，有过拟合风险 RF：不容易过拟合 LR：对稀疏特征效果较好；常用来做stacker3.2 Factorization Machine3.3 基于统计的方法（规则）3.4 时序问题 回归类模型 规则函数3.5 规则函数 四、模型融合 5%]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>FM</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python连接hive]]></title>
    <url>%2F2017%2F09%2F19%2F%E4%BD%BF%E7%94%A8python%E8%BF%9E%E6%8E%A5hive%2F</url>
    <content type="text"><![CDATA[Python连接hive的包有很多，我没有一一尝试，只是演示了工作当中用到的方法。本文主要采用PyHive来进行对hive的增删改查。PyHive is a collection of Python DB-API and SQLAlchemy interfaces for Presto and Hive. Requirementsinstall using1pip install SQLALchemy 1pip install pyhive[presto] 1pip install pyhive[hive] Presto engineCreate engine123```### Presto Select```df = pd.read_sql(sql=(r&apos;select * from &apos;+ &apos;schema.tablename&apos;) , con=engine) Presto Insertcon1234```### Presto UpdateHive does not support update.### Presto Delete engine.execute(“drop table schema.tablename”)123456## Hive engine### Create engine``` engine = create_engine('hive://url:port/hive/schema') Hive Select123```### Hive Insert```df.to_sql(tablename, con=engine, flavor=None, if_exists=&apos;append&apos;, index=False, chunksize=2000000) Hive UpdateHive does not support update. Hive Delete1engine.execute("drop table schema.tablename")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightgbm安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Flightgbm%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 Visual Studio 2017 1、下载源代码git clone –recursive https://github.com/Microsoft/LightGBM 2、用VS编译进入LightGBM目录，用VS打开windows/LightGBM.sln，选择DLL和x64，按Ctrl+Shift+B进行编译，dll文件就会在windows/x64/DLL/目录里 编译成功后对应目录E:\Code\LightGBM\windows\x64\DLL下会生成一些文件，如果存在DLL这个文件夹就说明安装成功了。 3、安装Python包123456chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM (master)$ cd python-package/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ python setup.py install 4、测试12345678910111213141516171819202122232425262728chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/python-package (master)$ cd ../examples/python-guide/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/LightGBM/examples/python-guide (master )$ python ./simple_example.pyLoad data...Start training...[1] valid_0's auc: 0.764496 valid_0's l2: 0.24288Training until validation scores don't improve for 5 rounds.[2] valid_0's auc: 0.766173 valid_0's l2: 0.239307[3] valid_0's auc: 0.785547 valid_0's l2: 0.235559[4] valid_0's auc: 0.797786 valid_0's l2: 0.230771[5] valid_0's auc: 0.805155 valid_0's l2: 0.226297[6] valid_0's auc: 0.800979 valid_0's l2: 0.223692[7] valid_0's auc: 0.806566 valid_0's l2: 0.220941[8] valid_0's auc: 0.808566 valid_0's l2: 0.217982[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351[10] valid_0's auc: 0.805953 valid_0's l2: 0.213064[11] valid_0's auc: 0.804631 valid_0's l2: 0.211053[12] valid_0's auc: 0.802922 valid_0's l2: 0.209336[13] valid_0's auc: 0.802011 valid_0's l2: 0.207492[14] valid_0's auc: 0.80193 valid_0's l2: 0.206016Early stopping, best iteration is:[9] valid_0's auc: 0.809041 valid_0's l2: 0.215351Save model...Start predicting...('The rmse of prediction is:', 0.4640593794679212) 或者直接通过pip安装。。。 via 在Windows下安装LightGBM的Python包 微软开源分布式高性能GB框架LightGBM安装使用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost安装与使用]]></title>
    <url>%2F2017%2F01%2F02%2Fxgboost%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[环境: win7 64位 Python 2.7.13 MINGW：GNU Make 3.82.90 Built for i686-pc-mingw32 首先，打开Git Shell，依次执行如下命令： 123456789101112131415161718git clone --recursive https://github.com/dmlc/xgboostcd xgboostgit checkout 9a48a40//新版本这一步可以省略git submodule initgit submodule updatecp make/mingw64.mk config.mkcp make/mingw64.mk dmlc-core/config.mkcd rabitmake lib/librabit_empty.a -j4cd ../dmlc-coremake -j4cd..make -j4 然后，安装到Python包中 123cd python-packagepython setup.py install 最后，导入xgboost包，测试demo 12345cd ..//或者直接进入xgboost目录cd democd guide-pythonpython basic_walkthrough.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172--1--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ git clone --recursive https://github.com/dmlc/xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code$ cd xgboostchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule initchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ git submodule updatechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cp make/mingw64.mk dmlc-core/config.mkchunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd rabit/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ make lib/librabit_empty.a -j4chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/rabit ((a764d45...))$ cd ../dmlc-corechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/dmlc-core ((b5bec54...))$ make -j4--2--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd python-packagechunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ python setup.py install--3--chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/python-package (master)$ cd ..chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost (master)$ cd demochunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo (master)$ cd guide-python/chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ python basic_walkthrough.py[09:58:28] 6513x127 matrix with 143286 entries loaded from ../data/agaricus.txt.train[09:58:28] 1611x127 matrix with 35442 entries loaded from ../data/agaricus.txt.test[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263[09:58:28] 1611x127 matrix with 35442 entries loaded from dtest.buffererror=0.021726start running example of build DMatrix from scipy.sparse CSR Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from scipy.sparse CSC Matrix[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263start running example of build DMatrix from numpy array[0] eval-error:0.042831 train-error:0.046522[1] eval-error:0.021726 train-error:0.022263chunhui.zhang@SZXH3FFK041 MINGW64 /e/Code/xgboost/demo/guide-python (master)$ via 64位Windows下安装xgboost详细参考指南（支持Python2.x和3.x）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
