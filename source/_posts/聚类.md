---
title: 聚类
date: 2017-11-12 16:15:12 
categories: "机器学习" 
tags: 
     - 聚类
     - k-Means
     - Canopy
     - AGNES
     - DIANA
     - DBSCAN
     - AP
     - 谱聚类
     - 层次聚类
     - 密度最大值聚类
     - 拉普拉斯
     - 边界
     - 噪声
     - 标签传递
description: 聚类
toc: true
---
# 一些概念
## 聚类的定义
聚类就是对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。

## 相似度/距离计算方法总结
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112160705.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### Hellinger distance
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165344.png" width="80%">
  <div class="figcaption">
  </div>
</div>

<!--more-->

### 余弦相似度与Pearson相似系数
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165507.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 聚类的基本思想
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165538.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 聚类的衡量指标
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112171852.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### ARI
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172003.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### AMI
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172036.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 轮廓系数(Silhouette)
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172157.png" width="80%">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172208.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# k-Means
## k-Means算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165628.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 对k-Means的思考
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165759.png" width="80%">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112165854.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### k-Means的公式化解释
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112170231.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 如果使用其他相似度/距离度量
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112170428.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### Mini-batch k-Means算法描述
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112171457.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## k-Means聚类总结
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112171549.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# Canopy
## Canopy算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112171730.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# 层次聚类方法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172418.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## AGNES和DIANA算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172514.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# 密度聚类方法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172835.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# DBSCAN算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112172923.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## DBSCAN算法的若干概念
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173029.png" width="80%">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173039.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## DBSCAN算法流程
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173106.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# 密度最大值聚类
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173433.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 高局部密度点距离
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173502.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 簇中心的识别
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173554.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 密度最大值聚类过程
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173620.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 边界和噪声的重新认识
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173717.png" width="80%">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173730.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# Affinity Propagation
## 算法概述
### 基本概念
* Exemplar范例：即聚类族中心点；
* s(i,j)：数据点i与数据点j的相似度值，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力；
* 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵；
* Preference参考度或称为偏好参数：是相似度矩阵中横轴纵轴索引相同的点，如s(i,i)，若按欧氏距离计算其值应为0，但在AP聚类中其表示数据点i作为聚类中心的程度，因此不能为0。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； 
* Responsibility，r(i,k)：吸引度信息，表示数据点k适合作为数据点i的聚类中心的程度；公式如下： ![](https://i.imgur.com/SUIggzc.png)
* * 其中a(i,k’)表示除k外其他点对i点的归属度值，初始为0；s(i,k’)表示除k外其他点对i的吸引度，即i外其他点都在争夺i点的 所有权；r(i,k)表示数据点k成为数据点i的聚类中心的累积证明，r(i,k)值大于0，则表示数据点k成为聚类中心的能力强。说明：此时只考虑哪个点k成为点i的聚类中心的可能性最大，但是没考虑这个吸引度最大的k是否也经常成为其他点的聚类中心（即归属度），若点k只是点i的聚类中心，不是其他任何点的聚类中心，则会造成最终聚类中心个数大于实际的中心个数。 
* Availability，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的合适程度，公式如下： ![](https://i.imgur.com/eXxCpLx.png)
* * 其中r(i’,k)表示点k作为除i外其他点的聚类中心的相似度值，取所有大于等于0的吸引度值，加上k作为聚类中心的可能程。即点k在这些吸引度值大于0的数据点的支持下，数据点i选择k作为其聚类中心的累积证明。 
* Damping factor阻尼系数：为防止数据震荡，引入地衰减系数，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得1-λ倍，其中λ在0-1之间，默认为0.5。

### 算法流程
* 1. 更新相似度矩阵中每个点的吸引度信息，计算归属度信息；
* 2. 更新归属度信息，计算吸引度信息；
* 3. 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。

关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： 

* 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 
* s(i,k)就相当于i对选k这个人的一个固有的偏好程度 
* r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 
* r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） 
* a(i,k)：从公式里可以看到，所有r(i’,k)>0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)>0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 
* a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） 
* 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。 
* r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。 
### 优点
1. 不需要制定最终聚类族的个数 
2. 已有的数据点作为最终的聚类中心，而不是新生成一个族中心。 
3. 模型对数据的初始值不敏感。 
4. 对初始相似度矩阵数据的对称性没有要求。 
5. 相比与k-centers聚类方法，其结果的平方差误差较小。
### 缺点
1. 虽然AP算法不用提前设置聚类中心的个数，但是需要事先设置参考度，而参考度的大小与聚类中心的个数正相关； 
2. 由于AP算法每次迭代都需要更新每个数据点的吸引度值和归属度值，算法复杂度较高，在大数据量下运行时间较长。

<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112173925.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# 谱和谱聚类
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174328.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 谱分析的整体过程
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174408.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 一些概念
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174436.png" width="60%">
  <div class="figcaption">
  </div>
</div>
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174445.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 相似度图G的建立方法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174548.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 权值比较
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174624.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 拉普拉斯矩阵的定义
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174651.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 拉普拉斯矩阵及其性质
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174719.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 谱聚类算法：未正则拉普拉斯矩阵
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174754.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 谱聚类算法：随机游走拉普拉斯矩阵
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174813.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 谱聚类算法：对称拉普拉斯矩阵
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174846.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 进一步思考
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112174932.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 随机游走和拉普拉斯矩阵的关系
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112175026.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 标签传递算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/聚类/TIM截图20171112175101.png" width="80%">
  <div class="figcaption">
  </div>
</div>

via
* [Affinity Propagation: AP聚类算法](http://blog.csdn.net/u010161379/article/details/51636926)