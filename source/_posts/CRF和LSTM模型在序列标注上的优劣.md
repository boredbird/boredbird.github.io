---
title: CRF和LSTM模型在序列标注上的优劣
date: 2018-07-16 16:28:12
categories: "深度学习"
tags:
     - CRF
     - LSTM
     - 序列标注
description: CRF和LSTM模型在序列标注上的优劣
toc: true
---

https://www.zhihu.com/question/46688107
## LSTM 优点
* 像RNN、LSTM、BILSTM这些模型，它们在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方
* 当数据规模较大时，BILSTM的效果应该会超过CRF
* LSTM理论上是能拟合任意函数的，对问题的假设明显放宽了很多。不过深度学习类模型的理论原理和可解释性一般。
* LSTM想stack起来、改双向、换激活函数等，只不过左手右手一个慢动作的事儿。
* LSTM可以当做对序列的一种『中间状态』的建模，建模结果还可以当做特征，扔给其他模型继续用。
* LSTM（RNN）的召回率高一些，因为长程依赖强一些，而且可以处理的数据量大。
* lstm主要用在搜索空间非常大的场景，比如机器翻译，语言模型，语音识别等场景，由于每一步都求一个最优解，每一步的预测类别一般就是整个词汇表了大概几十万吧，所以全局来说求的就不是最优解了。

## LSTM 缺点
* 对于t时刻来说，输出层y_t受到隐层h_t（包含上下文信息）和输入层x_t（当前的输入）的影响，但是y_t和其他时刻的y_t`是相互独立的，感觉像是 一种point wise，对当前t时刻来说，我们希望找到一个概率最大的y_t，但其他时刻的y_t`对当前y_t没有影响，如果y_t之间存在较强的依赖关系的话（例如，形容词后面一般接名词，存在一定的约束），LSTM无法对这些约束进行建模，LSTM模型的性能将受到限制。
* 如果需要识别的任务不需要太依赖长久的信息，此时RNN等模型只会增加额外的复杂度
* LSTM有各种GPU加速，多机异步SGD等标准大数据训练套路。但同样的问题，训练数据不够的话过拟合会很严重，效果堪忧。
* LSTM虽然部分解决了rnn梯度消失问题，但是信息在过远的距离传播中损失很厉害 其实仔细分析LSTM网络结构 他只不过是CRF的简化版而已

<!--more-->

## CRF 优点
* CRF：它不像LSTM等模型，能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的y_t，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种联合概率，优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。
* CRF不管是在实践还是理论上都要优于HMM，HMM模型的参数主要是“初始的状态分布”，“状态之间的概率转移矩阵”，“状态到观测的概率转移矩阵”，这些信息在CRF中都可以有，例如：在特征模版中考虑h(y1), f(y_i-1, y_i), g(y_i, x_i)等特征。
* 在数据规模较小时，CRF的试验效果要略优于BILSTM
* CRF在理论上更完美一些，一步一步都有比较坚实的理论基础。不过CRF的假设也比较明确，然而问题不总是能match其假设的。
* CRF的准确率高一些，我认为人为构造的特征，特别是字典特征，是非常有用的。
* crf在搜索空间较小场景基本无敌了，比如分词，词性标注，命名实体识别等，每一步预测的类别一般不会太多，比如词性标注在每步或者说每个字的预测类别也就几十个，crf求的是全局最优，也就是让整个序列最优，由于每步预测类别不多，所以全局的搜索空间也有限了。
* 从理论上crf明显优于lstm，但是，在搜索空间大的场景下还是得用lstm。
* crf在训练是 考虑的是全局序列最优 可以防止出现标注偏置问题。
* crf对远距离的建模能力 其实很强的 业内流行的“图模型加圈嘛” 而且组合自由随意 针对badcase 可以定位到是哪个特征造成的 这在商业交付和版本迭代中都非常重要
* crf 相对好解释点,这些权重都是统计概率好解释，也好理解

## CRF 缺点
* CRF比较难扩展，想在图模型上加边加圈，得重新推导公式和写代码。
* CRF针对大数据不好做。
* crf的问题在于 需要人工特征工程，这个过程比较需要经验
* crf 对数据集的语料要求更高点，标注越好，当然结果也越好；不过crf对有歧义的处理不是非常友好，受语料库的影响还蛮大的，lstm稍微好点

## 其他
* CNN＋BILSTM＋CRF：这是目前学术界比较流行的做法，BILSTM＋CRF是为了结合以上两个模型的优点，CNN主要是处理英文的情况，英文单词是由更细粒度的字母组成，这些字母潜藏着一些特征（例如：前缀后缀特征），通过CNN的卷积操作提取这些特征，在中文中可能并不适用（中文单字无法分解，除非是基于分词后），这里简单举一个例子，例如词性标注场景，单词football与basketball被标为名词的概率较高， 这里后缀ball就是类似这种特征。
