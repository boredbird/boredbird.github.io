---
title: 逻辑回归评分卡调优（五）之增加高阶特征
date: 2017-10-06 10:55:12 
categories: "机器学习" 
tags: 
     - 评分卡
     - 逻辑回归
description: 逻辑回归评分卡调优（五）之增加高阶特征
---
在[上回](https://boredbird.github.io/2017/10/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/)的基础上，我们可以看到根据业务理解[分割模型](https://boredbird.github.io/2017/10/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/)带来了模型效果的提升。这节我们尝试通过增加高阶特征来增强模型的表达能力。由于备选变量比较多，盲目的全部生成高阶特征会造成特征数量指数级的爆炸式增长，这个不是我想要的，所以先根据IV值进行单变量特征选择，筛选比较重要的变量，然后再次基础上才生成高阶特征。

    __author__ = 'boredbird'
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import l1_min_c
    from scipy.stats import ks_2samp
    from sklearn.preprocessing import PolynomialFeatures
    
    get_ks = lambda y_pred,y_true: ks_2samp(y_pred[y_true==1], y_pred[y_true!=1]).statistic
    
    dataset_train = pd.read_csv('E:\\ScoreCard\\pos_train_20160406_agr_woe_trans.csv')
    cfg = pd.read_csv('E:\\Code\\ScoreCard\\config\\config_whitelist_ext_pos.csv')
    
    poly_features = [
	
	]
    
    y_train = dataset_train['target']
    poly = PolynomialFeatures(2)
    X_train = poly.fit_transform(dataset_train[poly_features])
    
    del dataset_train
    import gc
    gc.collect()

    # init a LogisticRegression model
    clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01)
    cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3)
    
    print("Computing regularization path ...")
    start = datetime.now()
    print start
    coefs_ = []
    ks = []
    for c in cs:
    clf_l1_LR.set_params(C=c)
    clf_l1_LR.fit(X_train, y_train)
    coefs_.append(clf_l1_LR.coef_.ravel().copy())
    
    proba = clf_l1_LR.predict_proba(X_train)[:,1]
    ks.append(get_ks(proba,y_train))
    
    end = datetime.now()
    print end
    print("This took ", end - start)
    
    coef_cv_df = pd.DataFrame(coefs_)
    coef_cv_df.to_csv('E:\\Code\\ScoreCard\\agr_coef_cv_df_poly.csv')
    
    coefs_ = np.array(coefs_)
    
    fig1 = plt.figure('fig1')
    plt.plot(np.log10(cs), coefs_)
    ymin, ymax = plt.ylim()
    plt.xlabel('log(C)')
    plt.ylabel('Coefficients')
    plt.title('Logistic Regression Path Poly')
    plt.axis('tight')
    plt.show()
    
    fig2 = plt.figure('fig2')
    plt.plot(np.log10(cs), ks)
    plt.xlabel('log(C)')
    plt.ylabel('ks score')
    plt.title('Logistic Regression Performance Poly')
    plt.axis('tight')
    plt.show()
    
    fig2.show()
    fig1.show()

![](https://i.imgur.com/t4Z4KC8.png)

![](https://i.imgur.com/7gkS2nc.png)

注：X_train第一列是常数项，就是系数图中的那条蓝色线。

从模型的表现来看，模型效果并没有显著提升，根据奥卡姆剃刀原理，放弃采用高阶特征。