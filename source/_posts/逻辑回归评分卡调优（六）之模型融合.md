---
title: 逻辑回归评分卡调优（六）之模型融合
date: 2017-10-06 10:55:12 
categories: "机器学习" 
tags: 
     - 评分卡
     - 逻辑回归
description: 逻辑回归评分卡调优（六）之模型融合
toc: true
---
在[上回](https://boredbird.github.io/2017/10/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8B%E5%A2%9E%E5%8A%A0%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81/)的基础上，我们可以看到[增加高阶特征](https://boredbird.github.io/2017/10/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%AF%84%E5%88%86%E5%8D%A1%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8B%E5%A2%9E%E5%8A%A0%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81/)并没有带来模型效果显著提升。考虑到sigmoid函数在0.5处附近区分度比较大，在远离0.5处概率值变换不是很明显，考虑sigmoid梯度平缓的地方，拆分成<0.3和>0.7分别单独领出来，重新建模，期望能提升模型效果。

按照0.3和0.7两个分割点划分，需要重新再训练出三个模型，然后再把这三个模型整体的效果与未重新训练前的效果进行对比。

**首先定义几个函数，省得写一些重复的代码：**
``` python
    
    def plot_ks(proba,target,axistype=0,out_path=False):
    	a = pd.DataFrame(np.array([proba,target]).T,columns=['proba','target'])
    	a.sort_values(by='proba',ascending=False,inplace=True)
    	a['sum_Times']=a['target'].cumsum()
    	total_1 = a['target'].sum()
    	total_0 = len(a) - a['target'].sum()
    
    	a['temp'] = 1
    	a['Times']=a['temp'].cumsum()
    	a['cdf1'] = a['sum_Times']/total_1
    	a['cdf0'] = (a['Times'] - a['sum_Times'])/total_0
    	a['ks'] = a['cdf1'] - a['cdf0']
    	a['percent'] = a['Times']*1.0/len(a)
    
    	idx = np.argmax(a['ks'])
    	# print a.loc[idx]
    
    	if axistype == 0:
    	'''
    	KS曲线,横轴为按照输出的概率值排序后的观察样本比例
    	'''
    	plt.figure()
    	plt.plot(a['percent'],a['cdf1'], label="CDF_positive")
    	plt.plot(a['percent'],a['cdf0'],label="CDF_negative")
    	plt.plot(a['percent'],a['ks'],label="K-S")
    
    	sx = np.linspace(0,1,10)
    	sy = sx
    	plt.plot(sx,sy,linestyle='--',color='darkgrey',linewidth=1.2)
    
    	plt.legend()
    	plt.grid(True)
    	ymin, ymax = plt.ylim()
    	plt.xlabel('Sample percent')
    	plt.ylabel('Cumulative probability')
    	plt.title('Model Evaluation Index K-S')
    	plt.axis('tight')
    
    	# 虚线
    	t = a.loc[idx]['percent']
    	yb = round(a.loc[idx]['cdf1'],4)
    	yg = round(a.loc[idx]['cdf0'],4)
    
    	plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--")
    	plt.scatter([t,],[yb,], 20, color ='dodgerblue')
    	plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5),
     	textcoords='offset points', fontsize=8,
     	arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
    
    	plt.scatter([t,],[yg,], 20, color ='darkorange')
    	plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10),
     	textcoords='offset points', fontsize=8,
     	arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
    	# K-S曲线峰值
    	plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen')
    	plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4)
    	,round(a.loc[idx]['proba'],4))
     	, xy=(a.loc[idx]['percent'], a.loc[idx]['ks'])
     	, xycoords='data'
     	, xytext=(+15, -15),
     	textcoords='offset points'
     	, fontsize=8
     	,arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
    	plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4)
       	,round(a.loc[idx]['Times'],0))
     	, xy=(a.loc[idx]['percent'], a.loc[idx]['ks'])
     	, xycoords='data'
     	, xytext=(+25, -25),
     	textcoords='offset points'
     	, fontsize=8
     	)
    
    	else:
    	'''
    	改变横轴,横轴为模型输出的概率值
    	'''
	    plt.figure()
	    plt.grid(True)
	    plt.plot(1-a['proba'],a['cdf1'], label="CDF_bad")
	    plt.plot(1-a['proba'],a['cdf0'],label="CDF_good")
	    plt.plot(1-a['proba'],a['ks'],label="ks")
	    
	    plt.legend()
	    ymin, ymax = plt.ylim()
	    plt.xlabel('1-[Predicted probability]')
	    plt.ylabel('Cumulative probability')
	    plt.title('Model Evaluation Index K-S')
	    plt.axis('tight')
	    plt.show()
	    # 虚线
	    t = 1 - a.loc[idx]['proba']
	    yb = round(a.loc[idx]['cdf1'],4)
	    yg = round(a.loc[idx]['cdf0'],4)
	    
	    plt.plot([t,t],[yb,yg], color ='red', linewidth=1.4, linestyle="--")
	    plt.scatter([t,],[yb,], 20, color ='dodgerblue')
	    plt.annotate(r'$recall_p=%s$' % round(a.loc[idx]['cdf1'],4), xy=(t, yb), xycoords='data', xytext=(+10, -5),
	     textcoords='offset points', fontsize=8,
	     arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
	    
	    plt.scatter([t,],[yg,], 20, color ='darkorange')
	    plt.annotate(r'$recall_n=%s$' % round(a.loc[idx]['cdf0'],4), xy=(t, yg), xycoords='data', xytext=(+10, -10),
	     textcoords='offset points', fontsize=8,
	     arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
	    # K-S曲线峰值
	    plt.scatter([t,],[a.loc[idx]['ks'],], 20, color ='limegreen')
	    plt.annotate(r'$ks=%s,p=%s$' % (round(a.loc[idx]['ks'],4)
	    ,round(a.loc[idx]['proba'],4))
	     , xy=(t, a.loc[idx]['ks'])
	     , xycoords='data'
	     , xytext=(+15, -15),
	     textcoords='offset points'
	     , fontsize=8
	     ,arrowprops=dict(arrowstyle='->', connectionstyle="arc3,rad=.1"))
	    plt.annotate(r'$percent=%s,cnt=%s$' % (round(a.loc[idx]['percent'],4)
	       ,round(a.loc[idx]['Times'],0))
	     , xy=(t, a.loc[idx]['ks'])
	     , xycoords='data'
	     , xytext=(+25, -25),
	     textcoords='offset points'
	     , fontsize=8
	     )
	    
	    if out_path:
	    file_name = out_path if isinstance(out_path, str) else None
	    plt.savefig(file_name)
	    else:
	    plt.show()
	    
	    return a.loc[idx]
	    
    
    ''' 搜索 最优超参数c '''
    def grid_search_lr_c(X_train,y_train,df_coef_path=False
     	,pic_coefpath_title='Logistic Regression Path',pic_coefpath=False
     	,pic_performance_title='Logistic Regression Performance',pic_performance=False):
	    # init a LogisticRegression model
	    clf_l1_LR = LogisticRegression(C=0.1, penalty='l1', tol=0.01,class_weight='balanced')
	    cs = l1_min_c(X_train, y_train, loss='log') * np.logspace(0, 3)
	    
	    print("Computing regularization path ...")
	    start = datetime.now()
	    print start
	    coefs_ = []
	    ks = []
	    for c in cs:
	    clf_l1_LR.set_params(C=c)
	    clf_l1_LR.fit(X_train, y_train)
	    coefs_.append(clf_l1_LR.coef_.ravel().copy())
	    
	    proba = clf_l1_LR.predict_proba(X_train)[:,1]
	    ks.append(get_ks(proba,y_train))
	    
	    end = datetime.now()
	    print end
	    print("This took ", end - start)
	    coef_cv_df = pd.DataFrame(coefs_,columns=X_train.columns)
	    coef_cv_df['ks'] = ks
	    coef_cv_df['c'] = cs
	    # df_coef_path = 'E:\\Code\\ScoreCard\\agr_coef_cv_df_balanced_nextrain_beta.csv'
	    if df_coef_path:
	    file_name = df_coef_path if isinstance(df_coef_path, str) else None
	    coef_cv_df.to_csv(file_name)
	    
	    coefs_ = np.array(coefs_)
	    
	    fig1 = plt.figure('fig1')
	    plt.plot(np.log10(cs), coefs_)
	    ymin, ymax = plt.ylim()
	    plt.xlabel('log(C)')
	    plt.ylabel('Coefficients')
	    # pic_coefpath_title = 'Logistic Regression Path Class_weight Balanced Nextrain_beta'
	    plt.title(pic_coefpath_title)
	    plt.axis('tight')
	    if pic_coefpath:
	    file_name = pic_coefpath if isinstance(pic_coefpath, str) else None
	    plt.savefig(file_name)
	    else:
	    plt.show()
	    
	    fig2 = plt.figure('fig2')
	    plt.plot(np.log10(cs), ks)
	    plt.xlabel('log(C)')
	    plt.ylabel('ks score')
	    # pic_performance_title = 'Logistic Regression Performance Class_weight balanced Nextrain_beta'
	    plt.title(pic_performance_title)
	    plt.axis('tight')
	    if pic_performance:
	    file_name = pic_performance if isinstance(pic_performance, str) else None
	    plt.savefig(file_name)
	    else:
	    plt.show()
	    
	    
	    flag = coefs_<0
	    idx = np.array(ks)[flag.sum(axis=1) == 0].argmax()
	    
	    return (cs[idx],ks[idx])
```    
    
**然后拆分模型，合并计算ks,将一个已经预测后的模型，根据输出的概率值，给定两个分割点，划分区间重新用LR训练。**

``` python
    def lr_sub_train(X_train,y_train,idx):
	    index = idx
	    sub_xtrain = X_train.loc[index,:]
	    sub_ytrain = y_train.loc[index]
	    
	    c,ks = grid_search_lr_c(sub_xtrain,sub_ytrain)
	    clf_l1_LR = LogisticRegression(C=c, penalty='l1', tol=0.01,class_weight='balanced')
	    clf_l1_LR.fit(sub_xtrain, sub_ytrain)
	    sub_proba = clf_l1_LR.predict_proba(sub_xtrain)[:,1]
	    
	    return c,ks,sub_proba,sub_ytrain
    
    
    def train_segmentation_model(X_train,y_train,proba_train,left_point,right_point):
	    idx1 = (proba_train <= left_point)
	    idx2 = (proba_train > left_point)&(proba_train <= right_point)
	    idx3 = (proba_train > right_point)
	    c1,ks1,sub_proba1,sub_ytrain1 = lr_sub_train(X_train,y_train,idx1)
	    c2,ks2,sub_proba2,sub_ytrain2 = lr_sub_train(X_train,y_train,idx2)
	    c3,ks3,sub_proba3,sub_ytrain3 = lr_sub_train(X_train,y_train,idx3)
	    print 'c1:',c1,' ks1:',ks1
	    print 'c2:',c2,' ks2:',ks2
	    print 'c3:',c3,' ks3:',ks3
	    
	    proba = []
	    proba.extend(sub_proba1)
	    proba.extend(sub_proba2)
	    proba.extend(sub_proba3)
	    
	    y = []
	    y.extend(sub_ytrain1)
	    y.extend(sub_ytrain2)
	    y.extend(sub_ytrain3)
	    
	    return get_ks(np.array(proba),np.array(y))
	    # 0.29072674001489612
```

**输出**

``` python
    '''
    Computing regularization path ...
    2017-10-03 16:04:45.229000
    2017-10-03 16:07:12.977000
    ('This took ', datetime.timedelta(0, 147, 748000))
    Computing regularization path ...
    2017-10-03 16:07:28.118000
    2017-10-03 16:08:59.740000
    ('This took ', datetime.timedelta(0, 91, 622000))
    Computing regularization path ...
    2017-10-03 16:09:03.475000
    2017-10-03 16:09:31.128000
    ('This took ', datetime.timedelta(0, 27, 653000))
    c1: 0.00663966525938  ks1: 0.208779933516
    c2: 0.00189750388742  ks2: 0.201006811034
    c3: 0.000415735598049  ks3: 0.169741554117
    '''	    
```

结果并没有按照我想象的那样，反而分割后的效果没有单一模型的效果好。原因是woe变换没有重新训练?