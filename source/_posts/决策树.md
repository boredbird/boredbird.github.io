---
title: 决策树
date: 2017-11-11 13:25:12 
categories: "机器学习" 
tags: 
     - 决策树
description: 决策树
toc: true
---
## 决策树的概念
决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。
决策树学习是以实例为基础的归纳学习。
决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。

## 决策树学习的生成算法
建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有以下三种算法：
* ID3(Iterative Dichotomiser)
* C4.5
* CART(Classification And Regression Tree)

## 信息增益
### 概念
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。

信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。

<!--more-->

#### 定义：
特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差，即：g(D,A)=H(D)-H(D/A)，显然，这即为训练数据集D和特征A的互信息。

#### 基本记号：
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111135956.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 信息增益的计算方法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111140116.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 经验条件熵H(D/A)
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111140225.png" width="60%">
  <div class="figcaption">
  </div>
</div>

### 其他目标
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111140314.png" width="60%">
  <div class="figcaption">
  </div>
</div>

### 关于Gini系数的讨论
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111140829.png" width="80%">
  <div class="figcaption">
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111140904.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### Gini系数的第二定义
决策树中的Gini系数和社会学上的Gini系数并不相等。

<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111141620.png" width="80%">
  <div class="figcaption">
  </div>
</div>
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111141639.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 三种决策树学习算法的比较
* ID3:使用信息增益/互信息g(D,A)进行特征选择
* * 取值多的属性，更容易使数据更纯，其信息增益更大
* * 训练得到的是一棵庞大且深度浅的树：不合理。
* C4.5：信息增益率gr(D,A)=g(D,A)/H(A)
* CART：基尼指数
一个属性的信息增益（率）/Gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强。

## 决策树的评价
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111142828.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 决策树的过拟合
决策树对训练属于有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。
* 剪枝
* 随机森林

## 剪枝
三种决策树的剪枝过程算法相同，区别仅是对于当前树的评价标准不同。
* 信息增益、信息增益率、基尼系数
剪枝总体思路：
* 由完全树T0开始，剪枝部分结点得到T1，再次剪枝部分结点得到T2...直到仅剩树根的树Tk
* 在验证数据集上对这k个树分别评价，选择损失函数最小的树Ti

### 剪枝系数的确定
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111143402.png" width="80%">
  <div class="figcaption">
  </div>
</div>

### 剪枝算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/决策树和随机森林/TIM截图20171111143441.png" width="80%">
  <div class="figcaption">
  </div>
</div>
