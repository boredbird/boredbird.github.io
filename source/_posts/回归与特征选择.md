---
title: 回归与特征选择
date: 2017-11-09 20:09:12 
categories: "机器学习" 
tags: 
     - 回归
     - 特征选择
description: 回归与特征选择
toc: true
---
# 线性回归
## 高斯分布
在做特征选择的时候，可以看回归的残差图，真正理想状态下，经过模型回归后的残差应该是服从正态分布的，而不是均匀的随机分布；此时可以画个单变量与残差的关系图，直观的看一下是不是有什么明显的一阶、高阶趋势在里面，则需要进一步特征加工。

大数定理指的是事件发生的概率在N取无穷大的时候趋近于事件发生的概率。频率的极限是概率。

### 使用极大似然估计解释最小二乘
![](https://i.imgur.com/BLDhXge.png)

## 最大似然估计MLE
### 最大似然函数的假设
似然函数假设每个样本出现的概率是独立的，因此整体出现的概率就是各个样本出现的概率的累计。

<!--more-->

### 似然函数
![](https://i.imgur.com/GBtnLHR.png)

### 高斯的对数似然与最小二乘
误差服从高斯分布的情况下的对数似然函数：
![](https://i.imgur.com/nmi0cPX.png)

### 解析式的求解过程
![](https://i.imgur.com/EWSxXsA.png)

## 最小二乘法的本质
误差服从高斯分布的情况下的对数似然函数最大化的过程等价于最小二乘。
### 最小二乘意义下的参数最优解
![](https://i.imgur.com/oQUeY8A.png)

### 加入lamda扰动后
![](https://i.imgur.com/AenQijb.png)

## 线性回归的复杂度惩罚因子
惩罚因子的直观理解，阶数越高，模型越复杂，系数振荡的越厉害，系数的取值也越大，同时呢，系数又会有正有负，所以直接用系数的平方作为模型复杂度的惩罚因子。

![](https://i.imgur.com/zK6q3bk.png)

Ridge岭回归 是加了L2正则项的最小二乘。
LASSO 是L1正则。

Ridge岭回归与LASSO：
两个都差不多，单看模型系数LASSO要稍稍比Ridge稳定一些，但如果给定模型的评价指标的化，Ridge稍微比LASSO好。LASSO可以具有特征选择的功能。

## 正则项与防止过拟合
![](https://i.imgur.com/guH0G4L.png)
![](https://i.imgur.com/l08gh4J.png)

## 广义逆矩阵（伪逆）
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/回归与特征选择/TIM截图20171111104849.png" width="80%">
  <div class="figcaption">
  </div>
</div>

## 梯度下降算法
<div class="fig figcenter fighighlight">
  <img src="/assets/chinahadoop/回归与特征选择/TIM截图20171111105011.png" width="80%">
  <div class="figcaption">
  </div>
</div>

# Logistic回归
## 分类问题的首选算法

# 技术点
## 梯度下降算法
## 最大似然估计
## 特征选择
